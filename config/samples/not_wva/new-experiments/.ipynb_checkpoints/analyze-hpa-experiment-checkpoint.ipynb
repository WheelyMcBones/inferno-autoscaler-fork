{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f263c0ce",
   "metadata": {},
   "source": [
    "# HPA Scaling Experiment Analysis (Enhanced with TTFT/ITL)\n",
    "\n",
    "This notebook analyzes data from Horizontal Pod Autoscaler (HPA) scaling experiments with vLLM workloads, including performance metrics.\n",
    "\n",
    "## Metrics Collected\n",
    "\n",
    "The experiment monitors:\n",
    "- **Replica counts** - Current and desired pod replicas\n",
    "- **Waiting requests** - Number of requests in queue (vLLM metric)\n",
    "- **KV cache usage** - GPU memory cache utilization percentage\n",
    "- **TTFT (Time to First Token)** - Latency to first token in ms\n",
    "- **ITL (Inter-Token Latency)** - Average latency between tokens in ms\n",
    "- **Request Rate** - Throughput in requests/minute\n",
    "- **Job status** - Active and completed load generation jobs\n",
    "- **Scaling events** - When and why HPA scaled up/down\n",
    "\n",
    "## Setup\n",
    "\n",
    "Run this notebook after collecting experiment data with the enhanced monitor script."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73071bef",
   "metadata": {},
   "source": [
    "# HPA Scaling Experiment Analysis\n",
    "\n",
    "This notebook analyzes data from Horizontal Pod Autoscaler (HPA) scaling experiments with vLLM workloads.\n",
    "\n",
    "## Overview\n",
    "\n",
    "The experiment monitors:\n",
    "- **Replica counts** - Current and desired pod replicas\n",
    "- **Waiting requests** - Number of requests in queue (vLLM metric)\n",
    "- **KV cache usage** - GPU memory cache utilization percentage\n",
    "- **Job status** - Active and completed load generation jobs\n",
    "- **Scaling events** - When and why HPA scaled up/down\n",
    "\n",
    "## Setup\n",
    "\n",
    "Run this notebook after collecting experiment data with `monitor-hpa-experiment.sh`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed1a275b",
   "metadata": {},
   "source": [
    "## Quick Start Guide\n",
    "\n",
    "1. **Set up environment**: Make sure you have the required packages installed:\n",
    "   ```bash\n",
    "   pip install pandas matplotlib jupyter\n",
    "   ```\n",
    "\n",
    "2. **Update the experiment directory path** in Section 2 (below)\n",
    "\n",
    "3. **Run all cells**: Use \"Run All\" from the menu or execute cells sequentially\n",
    "\n",
    "4. **Review the results**: \n",
    "   - Summary statistics\n",
    "   - Visual plots\n",
    "   - Scaling event details\n",
    "   - Correlation analysis\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad913a84",
   "metadata": {},
   "source": [
    "## 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b8f4cfe0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Libraries imported successfully\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "from matplotlib.patches import Rectangle\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "%matplotlib inline\n",
    "\n",
    "print(\"✓ Libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a0bb342",
   "metadata": {},
   "source": [
    "## 2. Configure Experiment Directory\n",
    "\n",
    "Set the path to your experiment data directory. This should be the output from `monitor-hpa-experiment.sh`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a7ea4460",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error: Experiment directory not found: ./experiment-data/hpa-experiment-CHANGE-THIS\n",
      "\n",
      "Available experiments:\n",
      "  No experiment-data directory found\n"
     ]
    }
   ],
   "source": [
    "# Set this to your experiment directory\n",
    "# Example: './experiment-data/hpa-experiment-20251124-120000'\n",
    "EXPERIMENT_DIR = './experiment-data/high-load-hpa-20251124-155733'\n",
    "\n",
    "# Convert to Path object\n",
    "exp_path = Path(EXPERIMENT_DIR)\n",
    "\n",
    "if not exp_path.exists():\n",
    "    print(f\"❌ Error: Experiment directory not found: {EXPERIMENT_DIR}\")\n",
    "    print(\"\\nAvailable experiments:\")\n",
    "    data_dir = Path('./experiment-data')\n",
    "    if data_dir.exists():\n",
    "        experiments = sorted(data_dir.glob('hpa-experiment-*'), reverse=True)\n",
    "        for exp in experiments[:5]:\n",
    "            print(f\"  - {exp.name}\")\n",
    "    else:\n",
    "        print(\"  No experiment-data directory found\")\n",
    "else:\n",
    "    print(f\"✓ Using experiment directory: {EXPERIMENT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33c5b987",
   "metadata": {},
   "source": [
    "## 3. Load Experiment Data\n",
    "\n",
    "Load the metadata, metrics CSV, and scaling events from the experiment directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "08f0a70b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠ No metadata file found\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "Metrics file not found: experiment-data/hpa-experiment-CHANGE-THIS/metrics.csv",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 14\u001b[0m\n\u001b[1;32m     12\u001b[0m metrics_file \u001b[38;5;241m=\u001b[39m exp_path \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetrics.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m metrics_file\u001b[38;5;241m.\u001b[39mexists():\n\u001b[0;32m---> 14\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMetrics file not found: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetrics_file\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     16\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(metrics_file)\n\u001b[1;32m     17\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtimestamp\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mto_datetime(df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtimestamp\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: Metrics file not found: experiment-data/hpa-experiment-CHANGE-THIS/metrics.csv"
     ]
    }
   ],
   "source": [
    "# Load metadata\n",
    "metadata_file = exp_path / \"metadata.json\"\n",
    "if metadata_file.exists():\n",
    "    with open(metadata_file) as f:\n",
    "        metadata = json.load(f)\n",
    "    print(\"✓ Loaded metadata\")\n",
    "else:\n",
    "    metadata = {}\n",
    "    print(\"⚠ No metadata file found\")\n",
    "\n",
    "# Load metrics CSV\n",
    "metrics_file = exp_path / \"metrics.csv\"\n",
    "if not metrics_file.exists():\n",
    "    raise FileNotFoundError(f\"Metrics file not found: {metrics_file}\")\n",
    "\n",
    "df = pd.read_csv(metrics_file)\n",
    "df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "print(f\"✓ Loaded {len(df)} data points from metrics.csv\")\n",
    "\n",
    "# Load scaling events\n",
    "scaling_log = exp_path / \"scaling.log\"\n",
    "scaling_events = []\n",
    "if scaling_log.exists():\n",
    "    with open(scaling_log) as f:\n",
    "        content = f.read()\n",
    "        # Parse scaling events\n",
    "        for block in content.split(\"========================================\"):\n",
    "            if \"SCALING EVENT\" in block:\n",
    "                lines = block.strip().split('\\n')\n",
    "                scaling_events.append({\n",
    "                    'text': block.strip(),\n",
    "                    'lines': lines\n",
    "                })\n",
    "    print(f\"✓ Found {len(scaling_events)} scaling events\")\n",
    "else:\n",
    "    print(\"⚠ No scaling log found\")\n",
    "\n",
    "print(\"\\nData loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a71a020",
   "metadata": {},
   "source": [
    "## 4. Experiment Summary\n",
    "\n",
    "Display key information about the experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "438a73de",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"EXPERIMENT SUMMARY\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Experiment Name: {metadata.get('experiment_name', 'N/A')}\")\n",
    "print(f\"Start Time:      {metadata.get('start_time', 'N/A')}\")\n",
    "print(f\"End Time:        {metadata.get('end_time', 'N/A')}\")\n",
    "print(f\"Namespace:       {metadata.get('namespace', 'N/A')}\")\n",
    "print(f\"HPA Name:        {metadata.get('hpa_name', 'N/A')}\")\n",
    "print(f\"Deployment:      {metadata.get('deployment_name', 'N/A')}\")\n",
    "print()\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"SCALING STATISTICS\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Initial Replicas:  {df['replicas'].iloc[0]}\")\n",
    "print(f\"Final Replicas:    {df['replicas'].iloc[-1]}\")\n",
    "print(f\"Max Replicas:      {df['replicas'].max()}\")\n",
    "print(f\"Min Replicas:      {df['replicas'].min()}\")\n",
    "print(f\"Scaling Events:    {len(scaling_events)}\")\n",
    "print()\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"METRIC STATISTICS\")\n",
    "print(\"=\" * 70)\n",
    "print(\"Waiting Requests:\")\n",
    "print(f\"  Mean:   {df['num_requests_waiting_current'].mean():.2f}\")\n",
    "print(f\"  Max:    {df['num_requests_waiting_current'].max():.2f}\")\n",
    "print(f\"  Target: {df['num_requests_waiting_target'].iloc[0]}\")\n",
    "print()\n",
    "print(\"KV Cache Usage (%):\")\n",
    "print(f\"  Mean:   {df['kv_cache_usage_current'].mean():.2f}\")\n",
    "print(f\"  Max:    {df['kv_cache_usage_current'].max():.2f}\")\n",
    "print(f\"  Target: {df['kv_cache_usage_target'].iloc[0]}\")\n",
    "print()\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"JOB STATISTICS\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Max Active Jobs:    {df['active_jobs'].max()}\")\n",
    "print(f\"Total Completed:    {df['completed_jobs'].iloc[-1]}\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c54bda2",
   "metadata": {},
   "source": [
    "## 5. Data Preview\n",
    "\n",
    "View the first few rows of the metrics data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f138758",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5d487b3",
   "metadata": {},
   "source": [
    "## 6. Visualization: 4-Panel Time Series Plot\n",
    "\n",
    "Create a comprehensive visualization showing:\n",
    "1. **Replica count** - Current vs desired replicas over time\n",
    "2. **Waiting requests** - Current value vs target threshold\n",
    "3. **KV cache usage** - Current percentage vs target threshold\n",
    "4. **Job activity** - Active and completed jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9681ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(4, 1, figsize=(16, 12), sharex=True)\n",
    "fig.suptitle(f\"HPA Scaling Experiment: {metadata.get('experiment_name', 'Unknown')}\", \n",
    "             fontsize=16, fontweight='bold')\n",
    "\n",
    "timestamps = df['timestamp']\n",
    "\n",
    "# Plot 1: Replica count\n",
    "ax1 = axes[0]\n",
    "ax1.plot(timestamps, df['replicas'], marker='o', label='Current Replicas', \n",
    "         linewidth=2, color='blue', markersize=4)\n",
    "ax1.plot(timestamps, df['desired_replicas'], marker='s', label='Desired Replicas', \n",
    "         linewidth=1, linestyle='--', color='orange', alpha=0.7, markersize=3)\n",
    "ax1.set_ylabel('Replica Count', fontweight='bold', fontsize=12)\n",
    "ax1.legend(loc='upper left', fontsize=10)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.set_ylim(bottom=0)\n",
    "\n",
    "# Highlight scaling events with vertical lines\n",
    "scaling_times = df[df['replicas'] != df['replicas'].shift()]['timestamp']\n",
    "for st in scaling_times:\n",
    "    ax1.axvline(x=st, color='red', linestyle=':', alpha=0.5, linewidth=1.5)\n",
    "\n",
    "# Plot 2: Number of waiting requests\n",
    "ax2 = axes[1]\n",
    "ax2.plot(timestamps, df['num_requests_waiting_current'], marker='o', \n",
    "         label='Current Waiting Requests', linewidth=2, color='green', markersize=4)\n",
    "ax2.axhline(y=df['num_requests_waiting_target'].iloc[0], color='red', \n",
    "            linestyle='--', label='Target Threshold', linewidth=2, alpha=0.7)\n",
    "ax2.fill_between(timestamps, 0, df['num_requests_waiting_target'].iloc[0], \n",
    "                 color='green', alpha=0.1, label='Safe Zone')\n",
    "ax2.set_ylabel('Waiting Requests', fontweight='bold', fontsize=12)\n",
    "ax2.legend(loc='upper left', fontsize=10)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.set_ylim(bottom=0)\n",
    "\n",
    "# Plot 3: KV Cache Usage\n",
    "ax3 = axes[2]\n",
    "ax3.plot(timestamps, df['kv_cache_usage_current'], marker='o', \n",
    "         label='Current KV Cache Usage', linewidth=2, color='purple', markersize=4)\n",
    "ax3.axhline(y=df['kv_cache_usage_target'].iloc[0], color='red', \n",
    "            linestyle='--', label='Target Threshold', linewidth=2, alpha=0.7)\n",
    "ax3.fill_between(timestamps, 0, df['kv_cache_usage_target'].iloc[0], \n",
    "                 color='purple', alpha=0.1, label='Safe Zone')\n",
    "ax3.set_ylabel('KV Cache Usage (%)', fontweight='bold', fontsize=12)\n",
    "ax3.legend(loc='upper left', fontsize=10)\n",
    "ax3.grid(True, alpha=0.3)\n",
    "ax3.set_ylim(bottom=0, top=100)\n",
    "\n",
    "# Plot 4: Active Jobs\n",
    "ax4 = axes[3]\n",
    "width = 0.002  # Bar width for time series\n",
    "ax4.bar(timestamps, df['active_jobs'], label='Active Jobs', \n",
    "        color='teal', alpha=0.7, width=width)\n",
    "ax4.bar(timestamps, df['completed_jobs'], label='Completed Jobs', \n",
    "        color='gray', alpha=0.5, width=width, bottom=df['active_jobs'])\n",
    "ax4.set_ylabel('Job Count', fontweight='bold', fontsize=12)\n",
    "ax4.set_xlabel('Time', fontweight='bold', fontsize=12)\n",
    "ax4.legend(loc='upper left', fontsize=10)\n",
    "ax4.grid(True, alpha=0.3)\n",
    "ax4.set_ylim(bottom=0)\n",
    "\n",
    "# Format x-axis\n",
    "ax4.xaxis.set_major_formatter(mdates.DateFormatter('%H:%M:%S'))\n",
    "ax4.xaxis.set_major_locator(mdates.MinuteLocator(interval=2))\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n✓ Plot generated successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28df3ba8",
   "metadata": {},
   "source": [
    "## 7. Save Plot to File (Optional)\n",
    "\n",
    "Uncomment and run to save the plot to a PNG file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c90a702",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to save the plot\n",
    "# output_file = f\"{metadata.get('experiment_name', 'experiment')}-results.png\"\n",
    "# fig.savefig(output_file, dpi=300, bbox_inches='tight')\n",
    "# print(f\"✓ Plot saved to: {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1853f4e",
   "metadata": {},
   "source": [
    "## 8. Scaling Events Details\n",
    "\n",
    "View detailed information about each scaling event."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "250cc747",
   "metadata": {},
   "outputs": [],
   "source": [
    "if scaling_events:\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"SCALING EVENTS ({len(scaling_events)} total)\")\n",
    "    print(\"=\" * 70)\n",
    "    print()\n",
    "    \n",
    "    for i, event in enumerate(scaling_events, 1):\n",
    "        print(f\"Event {i}:\")\n",
    "        print(\"-\" * 70)\n",
    "        # Print first 600 characters of each event\n",
    "        event_text = event['text']\n",
    "        if len(event_text) > 600:\n",
    "            print(event_text[:600] + \"...\")\n",
    "        else:\n",
    "            print(event_text)\n",
    "        print()\n",
    "else:\n",
    "    print(\"No scaling events detected in this experiment.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "434d2ab1",
   "metadata": {},
   "source": [
    "## 9. Additional Analysis: Metric Correlation\n",
    "\n",
    "Analyze the relationship between metrics and scaling decisions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0397aaab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate when each metric exceeded its threshold\n",
    "df['waiting_requests_exceeded'] = df['num_requests_waiting_current'] > df['num_requests_waiting_target']\n",
    "df['kv_cache_exceeded'] = df['kv_cache_usage_current'] > df['kv_cache_usage_target']\n",
    "df['any_threshold_exceeded'] = df['waiting_requests_exceeded'] | df['kv_cache_exceeded']\n",
    "\n",
    "# Correlation between metrics\n",
    "print(\"Metric Correlation Matrix:\")\n",
    "print(\"-\" * 50)\n",
    "correlation_cols = ['replicas', 'num_requests_waiting_current', 'kv_cache_usage_current', 'active_jobs']\n",
    "print(df[correlation_cols].corr().round(3))\n",
    "print()\n",
    "\n",
    "# Count threshold violations\n",
    "waiting_violations = df['waiting_requests_exceeded'].sum()\n",
    "kv_violations = df['kv_cache_exceeded'].sum()\n",
    "total_samples = len(df)\n",
    "\n",
    "print(f\"Threshold Violations:\")\n",
    "print(f\"  Waiting Requests > {df['num_requests_waiting_target'].iloc[0]}: {waiting_violations}/{total_samples} samples ({100*waiting_violations/total_samples:.1f}%)\")\n",
    "print(f\"  KV Cache > {df['kv_cache_usage_target'].iloc[0]}%: {kv_violations}/{total_samples} samples ({100*kv_violations/total_samples:.1f}%)\")\n",
    "print()\n",
    "\n",
    "# Identify which metric triggered scaling\n",
    "if len(scaling_events) > 0:\n",
    "    print(f\"Scaling Trigger Analysis:\")\n",
    "    print(f\"  Total scaling events: {len(scaling_events)}\")\n",
    "    print(f\"  Note: Check scaling.log for detailed trigger information\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae18fc47",
   "metadata": {},
   "source": [
    "## 10. Export Data for Further Analysis\n",
    "\n",
    "Export processed data to CSV for use in other tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9f72981",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add analysis columns to the dataframe\n",
    "df_export = df.copy()\n",
    "df_export['waiting_requests_exceeded'] = df['waiting_requests_exceeded']\n",
    "df_export['kv_cache_exceeded'] = df['kv_cache_exceeded']\n",
    "df_export['any_threshold_exceeded'] = df['any_threshold_exceeded']\n",
    "\n",
    "# Uncomment to export\n",
    "# output_csv = exp_path / \"analysis_results.csv\"\n",
    "# df_export.to_csv(output_csv, index=False)\n",
    "# print(f\"✓ Data exported to: {output_csv}\")\n",
    "\n",
    "print(\"Data ready for export. Uncomment the code above to save.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de99e48b",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook provides a complete analysis of your HPA scaling experiment:\n",
    "\n",
    "✅ **Data loaded** from experiment directory  \n",
    "✅ **Summary statistics** calculated  \n",
    "✅ **4-panel visualization** showing all key metrics  \n",
    "✅ **Scaling events** extracted and displayed  \n",
    "✅ **Correlation analysis** between metrics  \n",
    "✅ **Export capability** for further analysis\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "1. **Compare experiments**: Run this notebook on multiple experiment directories to compare different HPA configurations\n",
    "2. **Tune thresholds**: Use the insights to adjust HPA target values\n",
    "3. **Share results**: Export the plot and summary for documentation\n",
    "4. **Deep dive**: Examine `scaling.log` for detailed event information\n",
    "\n",
    "### Files in Experiment Directory\n",
    "\n",
    "- `metrics.csv` - Time-series data (loaded in this notebook)\n",
    "- `scaling.log` - Detailed scaling event information\n",
    "- `events.log` - Kubernetes HPA events\n",
    "- `jobs.log` - Load job status over time\n",
    "- `metadata.json` - Experiment configuration\n",
    "- `monitor.log` - Monitor script output"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
