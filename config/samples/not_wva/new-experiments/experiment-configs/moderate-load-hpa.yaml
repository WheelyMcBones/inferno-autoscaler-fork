# Moderate Load HPA Experiment Configuration
# Tests HPA with overlapping moderate load to observe gradual scaling
# Jobs run in parallel with staggered start times to create cumulative but manageable pressure
# Pattern: Job1 (0s) → Job2 (+120s, overlap with Job1) → Job3 (+120s, overlap with Job2)
# This creates sustained 22-27 req/s load periods - enough to trigger scaling without overwhelming

name: "moderate-load-hpa"
description: "HPA test with overlapping 10-15 req/s jobs for gradual scaling"
namespace: "llm-d-inference-scheduler"
deployment: "ms-inference-scheduling-llm-d-modelservice-decode"
model_name: "unsloth/Meta-Llama-3.1-8B"

hpa:
  enabled: true
  manifest: "../../manifests/hpa-vllm-combined.yaml"

jobs:
  - name: "phase-1-moderate"
    manifest: "../../workloads/sharegpt-load-job-moderate-10.yaml"  # 10 req/s
    duration: 480  # 8 minutes (extended from 6 to observe longer-term effects)
    start_delay: 0  # Start immediately
    
  - name: "phase-2-moderate-peak"
    manifest: "../../workloads/sharegpt-load-job-moderate-15.yaml"  # 15 req/s
    duration: 480  # 8 minutes (extended from 6)
    start_delay: 60  # Start 1min after experiment begins (earlier - was 120s)
    
  - name: "phase-3-moderate-sustained"
    manifest: "../../workloads/sharegpt-load-job-moderate-12.yaml"  # 12 req/s
    duration: 480  # 8 minutes (extended from 6)
    start_delay: 120  # Start 2min after experiment begins (earlier - was 240s)

# Timeline visualization (EXTENDED):
# Time:    0s    60s   120s       240s       360s       480s       540s       600s
# Job1:    [========================================================]  (10 req/s, 8min)
# Job2:         [========================================================]  (15 req/s, 8min)
# Job3:              [========================================================]  (12 req/s, 8min)
# Load:    10    25    37         37         37         37         27         12
# 
# Expected behavior (EXTENDED):
# - 0-60s: 10 req/s (single job, warm-up period)
# - 60-120s: 25 req/s (job1+job2, should start triggering scale-up - earlier than before)
# - 120-480s: 37 req/s (EXTENDED PEAK - all three running for 6 minutes, sustained high load to test new replicas)
# - 480-540s: 27 req/s (job2+job3, sustained load after job1 finishes)
# - 540-600s: 12 req/s (job3 only, gradual decrease before scale-down)

metrics:
  interval: 5
  prometheus:
    ttft:
      query: 'sum(rate(vllm:time_to_first_token_seconds_sum{model_name="unsloth/Meta-Llama-3.1-8B",namespace="llm-d-inference-scheduler"}[1m]))/sum(rate(vllm:time_to_first_token_seconds_count{model_name="unsloth/Meta-Llama-3.1-8B",namespace="llm-d-inference-scheduler"}[1m])) * 1000'
    itl:
      query: 'sum(rate(vllm:time_per_output_token_seconds_sum{model_name="unsloth/Meta-Llama-3.1-8B",namespace="llm-d-inference-scheduler"}[1m]))/sum(rate(vllm:time_per_output_token_seconds_count{model_name="unsloth/Meta-Llama-3.1-8B",namespace="llm-d-inference-scheduler"}[1m])) * 1000'
    request_rate:
      query: 'sum(rate(vllm:request_success_total{model_name="unsloth/Meta-Llama-3.1-8B",namespace="llm-d-inference-scheduler"}[1m])) * 60'
    num_requests_waiting:
      query: 'vllm:num_requests_waiting{model_name="unsloth/Meta-Llama-3.1-8B",namespace="llm-d-inference-scheduler"}'
    kv_cache_usage:
      query: 'vllm:kv_cache_usage_perc{model_name="unsloth/Meta-Llama-3.1-8B",namespace="llm-d-inference-scheduler"}'

output:
  base_dir: "experiment-data"
  include_pod_logs: true
  include_events: true

# Observation period after jobs complete to capture scale-down behavior
cooldown:
  enabled: true
  duration: 90  # Monitor for 1.5 minutes after jobs complete to observe scale-down
