# Moderate Load HPA Experiment Configuration
# Tests HPA with overlapping moderate load to observe gradual scaling
# Jobs run in parallel with staggered start times to create cumulative but manageable pressure
# Pattern: Job1 (0s) → Job2 (+120s, overlap with Job1) → Job3 (+120s, overlap with Job2)
# This creates sustained 22-27 req/s load periods - enough to trigger scaling without overwhelming

name: "moderate-load-hpa"
description: "HPA test with overlapping 10-15 req/s jobs for gradual scaling"
namespace: "llm-d-inference-scheduler"
deployment: "ms-inference-scheduling-llm-d-modelservice-decode"
model_name: "unsloth/Meta-Llama-3.1-8B"

hpa:
  enabled: true
  manifest: "../../manifests/hpa-vllm-combined.yaml"

jobs:
  - name: "phase-1-moderate"
    manifest: "../../workloads/sharegpt-load-job-moderate-10.yaml"  # 10 req/s
    duration: 360  # 6 minutes
    start_delay: 0  # Start immediately
    
  - name: "phase-2-moderate-peak"
    manifest: "../../workloads/sharegpt-load-job-moderate-15.yaml"  # 15 req/s
    duration: 360  # 6 minutes
    start_delay: 120  # Start 2min after experiment begins (240s overlap with phase-1)
    
  - name: "phase-3-moderate-sustained"
    manifest: "../../workloads/sharegpt-load-job-moderate-12.yaml"  # 12 req/s
    duration: 360  # 6 minutes
    start_delay: 240  # Start 4min after experiment begins (240s overlap with phase-2)

# Timeline visualization:
# Time:    0s        120s       240s       360s       480s       600s
# Job1:    [========================================]  (10 req/s)
# Job2:              [========================================]  (15 req/s)
# Job3:                         [========================================]  (12 req/s)
# Load:    10        25         37         27         12         0
# 
# Expected behavior:
# - 0-120s: 10 req/s (single job, warm-up period)
# - 120-240s: 25 req/s (job1+job2, should start triggering scale-up)
# - 240-360s: 37 req/s (job1+job2+job3, PEAK - all three running!)
# - 360-480s: 27 req/s (job2+job3, sustained load after job1 finishes)
# - 480-600s: 12 req/s (job3 only, gradual decrease before scale-down)

metrics:
  interval: 5
  prometheus:
    ttft:
      query: 'sum(rate(vllm:time_to_first_token_seconds_sum{model_name="unsloth/Meta-Llama-3.1-8B",namespace="llm-d-inference-scheduler"}[1m]))/sum(rate(vllm:time_to_first_token_seconds_count{model_name="unsloth/Meta-Llama-3.1-8B",namespace="llm-d-inference-scheduler"}[1m])) * 1000'
    itl:
      query: 'sum(rate(vllm:time_per_output_token_seconds_sum{model_name="unsloth/Meta-Llama-3.1-8B",namespace="llm-d-inference-scheduler"}[1m]))/sum(rate(vllm:time_per_output_token_seconds_count{model_name="unsloth/Meta-Llama-3.1-8B",namespace="llm-d-inference-scheduler"}[1m])) * 1000'
    request_rate:
      query: 'sum(rate(vllm:request_success_total{model_name="unsloth/Meta-Llama-3.1-8B",namespace="llm-d-inference-scheduler"}[1m])) * 60'
    num_requests_waiting:
      query: 'vllm:num_requests_waiting{model_name="unsloth/Meta-Llama-3.1-8B",namespace="llm-d-inference-scheduler"}'
    kv_cache_usage:
      query: 'vllm:kv_cache_usage_perc{model_name="unsloth/Meta-Llama-3.1-8B",namespace="llm-d-inference-scheduler"}'

output:
  base_dir: "experiment-data"
  include_pod_logs: true
  include_events: true

# Observation period after jobs complete to capture scale-down behavior
cooldown:
  enabled: true
  duration: 300  # Monitor for 5 minutes after jobs complete to observe scale-down
