=== Initial State at 2025-11-24T19:41:40Z ===
Name:                              vllm-hpa-combined
Namespace:                         llm-d-inference-scheduler
Labels:                            <none>
Annotations:                       <none>
CreationTimestamp:                 Mon, 24 Nov 2025 12:21:09 -0500
Reference:                         Deployment/ms-inference-scheduling-llm-d-modelservice-decode
Metrics:                           ( current / target )
  "num_requests_waiting" on pods:  0 / 2
  "kv_cache_usage_perc" on pods:   0 / 800m
Min replicas:                      1
Max replicas:                      10
Behavior:
  Scale Up:
    Stabilization Window: 0 seconds
    Select Policy: Max
    Policies:
      - Type: Pods  Value: 4  Period: 15 seconds
  Scale Down:
    Stabilization Window: 60 seconds
    Select Policy: Min
    Policies:
      - Type: Pods  Value: 1  Period: 30 seconds
Deployment pods:    1 current / 1 desired
Conditions:
  Type            Status  Reason            Message
  ----            ------  ------            -------
  AbleToScale     True    ReadyForNewScale  recommended size matches current size
  ScalingActive   True    ValidMetricFound  the HPA was able to successfully calculate a replica count from pods metric kv_cache_usage_perc
  ScalingLimited  True    TooFewReplicas    the desired replica count is less than the minimum replica count
Events:
  Type     Reason                        Age                    From                       Message
  ----     ------                        ----                   ----                       -------
  Warning  FailedGetPodsMetric           138m (x8 over 140m)    horizontal-pod-autoscaler  unable to get metric kv_cache_usage_perc: unable to fetch metrics from custom metrics API: the server could not find the metric kv_cache_usage_perc for pods
  Warning  FailedComputeMetricsReplicas  138m (x8 over 140m)    horizontal-pod-autoscaler  invalid metrics (2 invalid out of 2), first error is: failed to get pods metric value: unable to get metric num_requests_waiting: unable to fetch metrics from custom metrics API: the server could not find the metric num_requests_waiting for pods
  Warning  FailedGetPodsMetric           105m (x141 over 140m)  horizontal-pod-autoscaler  unable to get metric num_requests_waiting: unable to fetch metrics from custom metrics API: the server could not find the metric num_requests_waiting for pods

