{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a193625",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import re\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "# Plotting style - match wva_analysis.ipynb\n",
    "plt.style.use('seaborn-v0_8')\n",
    "%matplotlib inline\n",
    "\n",
    "print(\"âœ… Libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbbfa043",
   "metadata": {},
   "source": [
    "# WVA Capacity-Based Mode Analysis\n",
    "\n",
    "This notebook analyzes WVA (Workload Variant Autoscaler) experiments running in **CAPACITY-ONLY mode**.\n",
    "\n",
    "## Key Metrics\n",
    "\n",
    "### Resource Utilization\n",
    "- **KV Cache Usage** - Per-pod KV cache utilization percentage\n",
    "- **Queue Length** - Number of waiting requests per pod\n",
    "\n",
    "### Saturation Detection\n",
    "- **Saturated Pods** - Pods exceeding KV cache threshold (typically 90%)\n",
    "- **Queue Buildup** - Pods with queue length exceeding threshold (typically 10)\n",
    "\n",
    "### Performance Metrics\n",
    "- **TTFT (Time to First Token)** - Latency to first token in milliseconds\n",
    "- **ITL (Inter-Token Latency)** - Average latency between tokens in milliseconds\n",
    "\n",
    "### Scaling Behavior\n",
    "- **Scale-Up Triggers** - When saturation detected\n",
    "- **Scale-Down Safety** - When all pods have low utilization and empty queues\n",
    "- **Current Replicas** - Actual running replicas\n",
    "\n",
    "## Visualizations\n",
    "\n",
    "This notebook generates comprehensive plots with styling consistent with HPA experiments:\n",
    "\n",
    "- **Individual Metrics**: KV Cache, Queue Length, Latencies, Replica Scaling\n",
    "- **Combined Summary**: Multi-panel overview for quick comparison\n",
    "- **Per-Pod Analysis**: Individual pod KV cache utilization\n",
    "- **Scaling Context**: Scaling decisions overlaid with KV cache metrics\n",
    "\n",
    "## Workflow\n",
    "\n",
    "1. Load experiment data from timestamped directory\n",
    "2. Parse WVA controller logs\n",
    "3. Extract KV cache and queue metrics per pod\n",
    "4. Parse performance metrics (TTFT, ITL)\n",
    "5. Analyze saturation patterns\n",
    "6. Generate visualizations with consistent styling\n",
    "7. Export processed data and summary report\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f962c31e",
   "metadata": {},
   "source": [
    "## 1. Setup and Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e98d5f0b",
   "metadata": {},
   "source": [
    "## Quick Start Guide\n",
    "\n",
    "**To analyze your experiment:**\n",
    "\n",
    "1. Run cells 1-5 to load and parse data\n",
    "2. Review the summary statistics in cell 11\n",
    "3. Run individual visualization cells (sections 6-8) to see specific metrics\n",
    "4. Check scaling events in section 9\n",
    "5. Export processed data in section 11\n",
    "\n",
    "**Individual Metric Plots (one per cell):**\n",
    "- **Section 6**: KV Cache Utilization\n",
    "- **Section 7**: Queue Length  \n",
    "- **Section 7.5**: TTFT & ITL Latencies\n",
    "- **Section 7.7**: Arrival Rate\n",
    "- **Section 8**: Replica Scaling\n",
    "- **Section 8.5**: Combined Summary (all metrics in one multi-panel view)\n",
    "- **Section 9**: Scaling Events Details\n",
    "- **Section 10**: Per-Pod KV Cache Timeline\n",
    "\n",
    "**Note:** Some plots may appear empty if:\n",
    "- The experiment is still early (no data collected yet)\n",
    "- Metrics are all zero (e.g., TTFT/ITL when no requests are being processed)\n",
    "- No scaling events have occurred yet\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4655064",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import sys\n",
    "import re\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timedelta\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "from matplotlib.patches import Rectangle\n",
    "import numpy as np\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "%matplotlib inline\n",
    "\n",
    "print(\"âœ“ Libraries imported successfully\")\n",
    "\n",
    "KV_CACHE_THRESHOLD = 0.5  # Latency threshold in milliseconds to consider KV cache hit\n",
    "QUEUE_LENGTH_THRESHOLD = 5  # Queue length threshold to consider queuing delay significant\n",
    "SHOW_DESIRED_REPLICAS = True  # Whether to show desired replicas in the plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "158eab1b",
   "metadata": {},
   "source": [
    "## 2. Select Experiment Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "416864aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Auto-detected latest experiment: capacity-based-smooth-gradual-20251207-165405\n",
      "Experiment directory: experiment-data/capacity-based-smooth-gradual-20251207-165405\n",
      "Log file: experiment-data/capacity-based-smooth-gradual-20251207-165405/wva-controller-logs.jsonl\n",
      "Metrics CSV: experiment-data/capacity-based-smooth-gradual-20251207-165405/metrics.csv\n",
      "âœ“ Files found\n"
     ]
    }
   ],
   "source": [
    "# Auto-detect latest capacity-based experiment\n",
    "data_dir = Path('./experiment-data')\n",
    "if data_dir.exists():\n",
    "    experiments = sorted(data_dir.glob('capacity-based-*'), reverse=True)\n",
    "    if experiments:\n",
    "        EXPERIMENT_DIR = str(experiments[0])\n",
    "        print(f\"âœ“ Auto-detected latest experiment: {experiments[0].name}\")\n",
    "    else:\n",
    "        EXPERIMENT_DIR = './experiment-data/capacity-based-moderate-load-20251126-120000'\n",
    "        print(f\"âš  No capacity-based experiments found, using example path\")\n",
    "else:\n",
    "    EXPERIMENT_DIR = './experiment-data/capacity-based-moderate-load-20251126-120000'\n",
    "    print(f\"âš  Data directory not found, using example path\")\n",
    "\n",
    "EXPERIMENT_DIR = Path(EXPERIMENT_DIR)\n",
    "# EXPERIMENT_DIR = Path('./experiment-data/capacity-based-high-parallel-20251128-145238')\n",
    "# EXPERIMENT_DIR = Path('./experiment-data/capacity-based-high-parallel-20251128-152808')\n",
    "# EXPERIMENT_DIR = Path('./experiment-data/capacity-based-high-parallel-20251128-155627') <-- Valid example results to show\n",
    "EXPERIMENT_DIR = Path('tom_wva/config/samples/wva-experiments/experiment-data/capacity-based-moderate-parallel-extended-20251207-173750')\n",
    "LOG_FILE = EXPERIMENT_DIR / 'wva-controller-logs.jsonl'\n",
    "METRICS_CSV = EXPERIMENT_DIR / 'metrics.csv'\n",
    "\n",
    "print(f\"Experiment directory: {EXPERIMENT_DIR}\")\n",
    "print(f\"Log file: {LOG_FILE}\")\n",
    "print(f\"Metrics CSV: {METRICS_CSV}\")\n",
    "\n",
    "# Verify files exist\n",
    "if not EXPERIMENT_DIR.exists():\n",
    "    print(f\"âŒ Experiment directory not found: {EXPERIMENT_DIR}\")\n",
    "    print(f\"Please run an experiment first with: ./run-experiment.sh experiment-configs/capacity-based-moderate.yaml\")\n",
    "elif not LOG_FILE.exists():\n",
    "    print(f\"âŒ Log file not found: {LOG_FILE}\")\n",
    "else:\n",
    "    print(f\"âœ“ Files found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa55f289",
   "metadata": {},
   "source": [
    "## 3. Parse WVA Logs\n",
    "\n",
    "Extract key events from WVA controller logs:\n",
    "- **KV Cache Metrics** - Per-pod cache utilization\n",
    "- **Queue Metrics** - Per-pod request queue length\n",
    "- **Capacity Analysis** - Saturation detection and scaling decisions\n",
    "- **Overall Metrics** - Aggregate ITL/TTFT/replicas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae72a7d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_wva_logs(log_file):\n",
    "    \"\"\"Parse WVA controller logs for capacity-based experiments.\"\"\"\n",
    "    \n",
    "    kv_cache_metrics = []\n",
    "    queue_metrics = []\n",
    "    capacity_analysis = []\n",
    "    overall_metrics = []\n",
    "    scaling_decisions = []\n",
    "    \n",
    "    print(\"Parsing WVA controller logs...\")\n",
    "    with open(log_file, 'r') as f:\n",
    "        for line_num, line in enumerate(f, 1):\n",
    "            try:\n",
    "                log = json.loads(line.strip())\n",
    "                ts = log.get('ts', '')\n",
    "                msg = log.get('msg', '')\n",
    "                level = log.get('level', '')\n",
    "                \n",
    "                # Convert timestamp to datetime\n",
    "                try:\n",
    "                    dt = datetime.fromisoformat(ts.replace('Z', '+00:00'))\n",
    "                except:\n",
    "                    continue\n",
    "                \n",
    "                # Extract KV cache metrics (only if pod name is found)\n",
    "                if 'KV cache metric' in msg:\n",
    "                    pod_match = re.search(r'pod=([a-zA-Z0-9-]+)', msg)\n",
    "                    usage_match = re.search(r'usage=([0-9.]+)', msg)\n",
    "                    pct_match = re.search(r'\\(([0-9.]+)%\\)', msg)\n",
    "                    \n",
    "                    if pod_match:  # Only append if pod name is successfully extracted\n",
    "                        kv_cache_metrics.append({\n",
    "                            'timestamp': dt,\n",
    "                            'pod': pod_match.group(1),\n",
    "                            'kv_cache_usage': float(usage_match.group(1)) if usage_match else None,\n",
    "                            'kv_cache_pct': float(pct_match.group(1)) if pct_match else None\n",
    "                        })\n",
    "                \n",
    "                # Extract queue metrics (only if pod name is found)\n",
    "                elif 'Queue metric' in msg:\n",
    "                    pod_match = re.search(r'pod=([a-zA-Z0-9-]+)', msg)\n",
    "                    queue_match = re.search(r'queueLength=([0-9]+)', msg)\n",
    "                    \n",
    "                    if pod_match:  # Only append if pod name is successfully extracted\n",
    "                        queue_metrics.append({\n",
    "                            'timestamp': dt,\n",
    "                            'pod': pod_match.group(1),\n",
    "                            'queue_length': int(queue_match.group(1)) if queue_match else None\n",
    "                        })\n",
    "                \n",
    "                # Extract capacity analysis\n",
    "                elif 'Capacity analysis completed' in msg:\n",
    "                    total_match = re.search(r'totalReplicas=([0-9]+)', msg)\n",
    "                    nonsaturated_match = re.search(r'nonSaturated=([0-9]+)', msg)\n",
    "                    scaleup_match = re.search(r'shouldScaleUp=(true|false)', msg)\n",
    "                    scaledown_match = re.search(r'scaleDownSafe=(true|false)', msg)\n",
    "                    \n",
    "                    capacity_analysis.append({\n",
    "                        'timestamp': dt,\n",
    "                        'total_replicas': int(total_match.group(1)) if total_match else None,\n",
    "                        'non_saturated': int(nonsaturated_match.group(1)) if nonsaturated_match else None,\n",
    "                        'should_scale_up': scaleup_match.group(1) == 'true' if scaleup_match else None,\n",
    "                        'scale_down_safe': scaledown_match.group(1) == 'true' if scaledown_match else None\n",
    "                    })\n",
    "                \n",
    "                # Extract overall metrics (FIXED: removed âœ“ checkmark)\n",
    "                elif 'Metrics collected for VA' in msg:\n",
    "                    replicas_match = re.search(r'replicas=([0-9]+)', msg)\n",
    "                    ttft_match = re.search(r'ttft=([0-9.]+)', msg)\n",
    "                    itl_match = re.search(r'itl=([0-9.]+)', msg)\n",
    "                    cost_match = re.search(r'cost=([0-9.]+)', msg)\n",
    "                    \n",
    "                    # Extract values and remove 'ms' suffix if present\n",
    "                    ttft_str = ttft_match.group(1) if ttft_match else None\n",
    "                    itl_str = itl_match.group(1) if itl_match else None\n",
    "                    \n",
    "                    if ttft_str:\n",
    "                        ttft_val = float(ttft_str.replace('ms', ''))\n",
    "                    else:\n",
    "                        ttft_val = None\n",
    "                        \n",
    "                    if itl_str:\n",
    "                        itl_val = float(itl_str.replace('ms', ''))\n",
    "                    else:\n",
    "                        itl_val = None\n",
    "                    \n",
    "                    overall_metrics.append({\n",
    "                        'timestamp': dt,\n",
    "                        'replicas': int(replicas_match.group(1)) if replicas_match else None,\n",
    "                        'ttft': ttft_val,\n",
    "                        'itl': itl_val,\n",
    "                        'cost': float(cost_match.group(1)) if cost_match else None\n",
    "                    })\n",
    "                \n",
    "                # Extract scaling decisions (capacity-based uses \"Applied capacity decision\")\n",
    "                elif 'Applied capacity decision' in msg:\n",
    "                    current_match = re.search(r'current=([0-9]+)', msg)\n",
    "                    target_match = re.search(r'target=([0-9]+)', msg)\n",
    "                    action_match = re.search(r'action=([a-z-]+)', msg)\n",
    "                    reason_match = re.search(r'reason=([^\"]+)', msg)\n",
    "                    \n",
    "                    scaling_decisions.append({\n",
    "                        'timestamp': dt,\n",
    "                        'current_replicas': int(current_match.group(1)) if current_match else None,\n",
    "                        'target_replicas': int(target_match.group(1)) if target_match else None,\n",
    "                        'action': action_match.group(1) if action_match else None,\n",
    "                        'reason': reason_match.group(1).strip() if reason_match else None\n",
    "                    })\n",
    "                        \n",
    "            except json.JSONDecodeError:\n",
    "                continue\n",
    "            except Exception as e:\n",
    "                print(f\"Error parsing line {line_num}: {e}\")\n",
    "                continue\n",
    "    \n",
    "    return kv_cache_metrics, queue_metrics, capacity_analysis, overall_metrics, scaling_decisions\n",
    "\n",
    "# Parse the logs\n",
    "kv_cache_metrics, queue_metrics, capacity_analysis, overall_metrics, scaling_decisions = parse_wva_logs(LOG_FILE)\n",
    "\n",
    "# Convert to DataFrames\n",
    "df_kv_cache = pd.DataFrame(kv_cache_metrics)\n",
    "df_queue = pd.DataFrame(queue_metrics)\n",
    "df_capacity = pd.DataFrame(capacity_analysis)\n",
    "df_metrics = pd.DataFrame(overall_metrics)\n",
    "df_scaling = pd.DataFrame(scaling_decisions)\n",
    "\n",
    "# Remove duplicate scaling decisions (sometimes logged multiple times)\n",
    "if len(df_scaling) > 0:\n",
    "    original_count = len(df_scaling)\n",
    "    df_scaling = df_scaling.drop_duplicates(subset=['timestamp', 'current_replicas', 'target_replicas', 'action'], keep='first')\n",
    "    df_scaling = df_scaling.sort_values('timestamp').reset_index(drop=True)\n",
    "    if original_count > len(df_scaling):\n",
    "        print(f\"âš ï¸  Removed {original_count - len(df_scaling)} duplicate scaling decision entries\")\n",
    "\n",
    "print(f\"\\nâœ“ Parsed {len(df_kv_cache)} KV cache measurements\")\n",
    "print(f\"âœ“ Parsed {len(df_queue)} queue measurements\")\n",
    "print(f\"âœ“ Parsed {len(df_capacity)} capacity analyses\")\n",
    "print(f\"âœ“ Parsed {len(df_metrics)} overall metrics\")\n",
    "print(f\"âœ“ Parsed {len(df_scaling)} scaling decisions (unique)\")\n",
    "\n",
    "# Show samples\n",
    "if len(df_kv_cache) > 0:\n",
    "    print(\"\\nSample KV cache metric:\")\n",
    "    print(df_kv_cache.head(1).to_string())\n",
    "\n",
    "if len(df_capacity) > 0:\n",
    "    print(\"\\nSample capacity analysis:\")\n",
    "    print(df_capacity.head(1).to_string())\n",
    "\n",
    "if len(df_metrics) > 0:\n",
    "    print(\"\\nSample overall metrics:\")\n",
    "    print(df_metrics.head(1).to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d231db1",
   "metadata": {},
   "source": [
    "## 4. Aggregate Per-Pod Metrics\n",
    "\n",
    "Calculate average KV cache and queue metrics across all pods at each timestamp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3296d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by timestamp for KV cache\n",
    "if len(df_kv_cache) > 0:\n",
    "    df_kv_agg = df_kv_cache.groupby('timestamp').agg({\n",
    "        'kv_cache_usage': ['mean', 'max', 'min'],\n",
    "        'kv_cache_pct': ['mean', 'max', 'min']\n",
    "    }).reset_index()\n",
    "    \n",
    "    df_kv_agg.columns = ['timestamp', 'kv_usage_mean', 'kv_usage_max', 'kv_usage_min',\n",
    "                          'kv_pct_mean', 'kv_pct_max', 'kv_pct_min']\n",
    "    \n",
    "    print(f\"âœ“ Aggregated KV cache metrics: {len(df_kv_agg)} timestamps\")\n",
    "else:\n",
    "    df_kv_agg = pd.DataFrame()\n",
    "\n",
    "# Group by timestamp for queue\n",
    "if len(df_queue) > 0:\n",
    "    df_queue_agg = df_queue.groupby('timestamp').agg({\n",
    "        'queue_length': ['mean', 'max', 'min', 'sum']\n",
    "    }).reset_index()\n",
    "    \n",
    "    df_queue_agg.columns = ['timestamp', 'queue_mean', 'queue_max', 'queue_min', 'queue_total']\n",
    "    \n",
    "    print(f\"âœ“ Aggregated queue metrics: {len(df_queue_agg)} timestamps\")\n",
    "else:\n",
    "    df_queue_agg = pd.DataFrame()\n",
    "\n",
    "# Calculate relative time for all dataframes (like HPA notebook)\n",
    "if len(df_metrics) > 0:\n",
    "    start_time = df_metrics['timestamp'].min()\n",
    "    df_metrics['time_relative'] = (df_metrics['timestamp'] - start_time).dt.total_seconds() / 60\n",
    "    print(f\"âœ“ Added relative time to metrics dataframe\")\n",
    "\n",
    "if len(df_kv_agg) > 0:\n",
    "    start_time = df_kv_agg['timestamp'].min()\n",
    "    df_kv_agg['time_relative'] = (df_kv_agg['timestamp'] - start_time).dt.total_seconds() / 60\n",
    "\n",
    "if len(df_queue_agg) > 0:\n",
    "    start_time = df_queue_agg['timestamp'].min()\n",
    "    df_queue_agg['time_relative'] = (df_queue_agg['timestamp'] - start_time).dt.total_seconds() / 60\n",
    "\n",
    "if len(df_scaling) > 0:\n",
    "    start_time = df_scaling['timestamp'].min()\n",
    "    df_scaling['time_relative'] = (df_scaling['timestamp'] - start_time).dt.total_seconds() / 60\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "925d1d11",
   "metadata": {},
   "source": [
    "## 5. Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00a916a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"WVA CAPACITY-BASED EXPERIMENT SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "if len(df_scaling) > 0:\n",
    "    print(f\"\\nInitial Replicas:  {df_scaling['current_replicas'].iloc[0]}\")\n",
    "    print(f\"Final Replicas:    {df_scaling['current_replicas'].iloc[-1]}\")\n",
    "    print(f\"Max Replicas:      {df_scaling['current_replicas'].max()}\")\n",
    "    print(f\"Min Replicas:      {df_scaling['current_replicas'].min()}\")\n",
    "\n",
    "print()\n",
    "print(\"=\"*70)\n",
    "print(\"CAPACITY METRICS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "if len(df_kv_agg) > 0:\n",
    "    print(\"\\nKV Cache Utilization:\")\n",
    "    print(f\"  Mean:   {df_kv_agg['kv_pct_mean'].mean():.2f}%\")\n",
    "    print(f\"  Max:    {df_kv_agg['kv_pct_max'].max():.2f}%\")\n",
    "    print(f\"  Min:    {df_kv_agg['kv_pct_min'].min():.2f}%\")\n",
    "    \n",
    "    # Use actual threshold from WVA config\n",
    "    saturated_count = (df_kv_agg['kv_pct_max'] > KV_CACHE_THRESHOLD*100).sum()\n",
    "    saturated_pct = (saturated_count / len(df_kv_agg)) * 100\n",
    "    print(f\"  Saturation Events (>{KV_CACHE_THRESHOLD*100:.0f}%): {saturated_count}/{len(df_kv_agg)} ({saturated_pct:.1f}%)\")\n",
    "else:\n",
    "    print(\"\\nKV Cache Utilization: No data available\")\n",
    "\n",
    "if len(df_queue_agg) > 0:\n",
    "    print(\"\\nQueue Length:\")\n",
    "    print(f\"  Mean:   {df_queue_agg['queue_mean'].mean():.2f} requests\")\n",
    "    print(f\"  Max:    {df_queue_agg['queue_max'].max():.0f} requests\")\n",
    "    print(f\"  Queue Buildup Events (>{QUEUE_LENGTH_THRESHOLD}): {(df_queue_agg['queue_max'] > QUEUE_LENGTH_THRESHOLD).sum()}\")\n",
    "else:\n",
    "    print(\"\\nQueue Length: No data available\")\n",
    "\n",
    "if len(df_metrics) > 0:\n",
    "    # Check if we have any non-zero TTFT/ITL data\n",
    "    has_ttft = (df_metrics['ttft'] > 0).any() if 'ttft' in df_metrics.columns else False\n",
    "    has_itl = (df_metrics['itl'] > 0).any() if 'itl' in df_metrics.columns else False\n",
    "    \n",
    "    if has_ttft:\n",
    "        ttft_data = df_metrics['ttft'][df_metrics['ttft'] > 0]\n",
    "        print(\"\\nTTFT (Time to First Token) ms:\")\n",
    "        print(f\"  Mean:   {ttft_data.mean():.2f}\")\n",
    "        print(f\"  Max:    {ttft_data.max():.2f}\")\n",
    "        print(f\"  Min:    {ttft_data.min():.2f}\")\n",
    "    else:\n",
    "        print(\"\\nTTFT: No data collected (all zeros - metrics may not be available)\")\n",
    "    \n",
    "    if has_itl:\n",
    "        itl_data = df_metrics['itl'][df_metrics['itl'] > 0]\n",
    "        print(\"\\nITL (Inter-Token Latency) ms:\")\n",
    "        print(f\"  Mean:   {itl_data.mean():.2f}\")\n",
    "        print(f\"  Max:    {itl_data.max():.2f}\")\n",
    "        print(f\"  Min:    {itl_data.min():.2f}\")\n",
    "    else:\n",
    "        print(\"\\nITL: No data collected (all zeros - metrics may not be available)\")\n",
    "    \n",
    "    # Check for arrival rate data\n",
    "    has_arrival_rate = (df_metrics['arrival_rate'] > 0).any() if 'arrival_rate' in df_metrics.columns else False\n",
    "    if has_arrival_rate:\n",
    "        arrival_rate_data = df_metrics['arrival_rate'][df_metrics['arrival_rate'] > 0]\n",
    "        print(\"\\nArrival Rate (requests/second):\")\n",
    "        print(f\"  Mean:   {arrival_rate_data.mean():.2f}\")\n",
    "        print(f\"  Max:    {arrival_rate_data.max():.2f}\")\n",
    "        print(f\"  Min:    {arrival_rate_data.min():.2f}\")\n",
    "    else:\n",
    "        print(\"\\nArrival Rate: No data collected (all zeros)\")\n",
    "else:\n",
    "    print(\"\\nPerformance Metrics: No data available\")\n",
    "\n",
    "print()\n",
    "print(\"=\"*70)\n",
    "print(\"SCALING BEHAVIOR\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "if len(df_capacity) > 0:\n",
    "    scaleup_count = df_capacity['should_scale_up'].sum() if 'should_scale_up' in df_capacity else 0\n",
    "    scaledown_count = df_capacity['scale_down_safe'].sum() if 'scale_down_safe' in df_capacity else 0\n",
    "    \n",
    "    print(f\"\\nCapacity Analysis Events: {len(df_capacity)}\")\n",
    "    print(f\"  Scale-Up Recommendations: {scaleup_count}\")\n",
    "    print(f\"  Scale-Down Safe: {scaledown_count}\")\n",
    "    \n",
    "    if 'non_saturated' in df_capacity.columns and 'total_replicas' in df_capacity.columns:\n",
    "        avg_saturated_pct = ((df_capacity['total_replicas'] - df_capacity['non_saturated']) / df_capacity['total_replicas']).mean() * 100\n",
    "        print(f\"  Average Saturated Pods: {avg_saturated_pct:.1f}%\")\n",
    "\n",
    "if len(df_scaling) > 0:\n",
    "    action_counts = df_scaling['action'].value_counts()\n",
    "    print(f\"\\nScaling Actions: {len(df_scaling)} total\")\n",
    "    for action, count in action_counts.items():\n",
    "        pct = (count / len(df_scaling)) * 100\n",
    "        print(f\"  {action}: {count} ({pct:.1f}%)\")\n",
    "\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae8de5dc",
   "metadata": {},
   "source": [
    "## 6. Visualization: KV Cache Utilization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85a19e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create plots directory if it doesn't exist\n",
    "plots_dir = EXPERIMENT_DIR / 'plots'\n",
    "plots_dir.mkdir(exist_ok=True)\n",
    "print(f\"âœ“ Plots directory ready: {plots_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e837c95d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse arrival rate from logs\n",
    "arrival_rate_data = []\n",
    "\n",
    "print(\"Parsing arrival rate from logs...\")\n",
    "with open(LOG_FILE, 'r') as f:\n",
    "    for line in f:\n",
    "        try:\n",
    "            log = json.loads(line.strip())\n",
    "            msg = log.get('msg', '')\n",
    "            \n",
    "            # Look for \"Metrics collected for VA\" messages with arrivalRate\n",
    "            if 'Metrics collected for VA' in msg and 'arrivalRate=' in msg:\n",
    "                ts = log.get('ts', '')\n",
    "                try:\n",
    "                    dt = datetime.fromisoformat(ts.replace('Z', '+00:00'))\n",
    "                except:\n",
    "                    continue\n",
    "                \n",
    "                # Extract arrivalRate using regex\n",
    "                arrival_match = re.search(r'arrivalRate=([0-9.]+)', msg)\n",
    "                if arrival_match:\n",
    "                    arrival_rate = float(arrival_match.group(1))\n",
    "                    arrival_rate_data.append({\n",
    "                        'timestamp': dt,\n",
    "                        'arrival_rate': arrival_rate\n",
    "                    })\n",
    "        except (json.JSONDecodeError, KeyError, ValueError):\n",
    "            continue\n",
    "\n",
    "df_arrival = pd.DataFrame(arrival_rate_data)\n",
    "\n",
    "if len(df_arrival) > 0:\n",
    "    # Convert to relative time in minutes\n",
    "    df_arrival['time_min'] = (df_arrival['timestamp'] - df_arrival['timestamp'].min()).dt.total_seconds() / 60\n",
    "    print(f\"âœ“ Parsed {len(df_arrival)} arrival rate data points\")\n",
    "    print(f\"  Arrival rate range: {df_arrival['arrival_rate'].min():.2f} - {df_arrival['arrival_rate'].max():.2f} req/s\")\n",
    "else:\n",
    "    print(\"âš  No arrival rate data found in logs\")\n",
    "\n",
    "df_arrival.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a635fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot arrival rate over time\n",
    "if len(df_arrival) > 0:\n",
    "    fig, ax = plt.subplots(figsize=(20, 6))\n",
    "    \n",
    "    # Plot arrival rate\n",
    "    ax.plot(df_arrival['time_min'], df_arrival['arrival_rate'], \n",
    "            '-o', color='#8b5cf6', linewidth=2.5, markersize=5, label='Arrival Rate (req/s)')\n",
    "    ax.fill_between(df_arrival['time_min'], 0, df_arrival['arrival_rate'], \n",
    "                     color='#8b5cf6', alpha=0.2)\n",
    "    \n",
    "    ax.set_xlabel('Time (minutes from start)', fontsize=12, fontweight='bold')\n",
    "    ax.set_ylabel('Arrival Rate (req/s)', fontsize=12, fontweight='bold')\n",
    "    ax.set_title('Request Arrival Rate Over Time', fontsize=16, fontweight='bold')\n",
    "    ax.legend(loc='best', fontsize=11, framealpha=0.9)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.set_ylim(bottom=0)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save plot\n",
    "    output_file = plots_dir / 'arrival_rate.png'\n",
    "    plt.savefig(output_file, dpi=150, bbox_inches='tight')\n",
    "    print(f\"âœ“ Saved arrival rate plot to {output_file}\")\n",
    "    \n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"âš  No arrival rate data to plot\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26226bd8",
   "metadata": {},
   "source": [
    "## 6.1 Arrival Rate (Load) Over Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93389870",
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(df_kv_agg) > 0:\n",
    "    fig, ax = plt.subplots(figsize=(20, 6))\n",
    "    \n",
    "    # Plot KV cache metrics\n",
    "    ax.plot(df_kv_agg['time_relative'], df_kv_agg['kv_pct_mean'], \n",
    "            '-o', color='#8b5cf6', label='Average KV Cache %', markersize=5, linewidth=2.5)\n",
    "    ax.fill_between(df_kv_agg['time_relative'], \n",
    "                     df_kv_agg['kv_pct_min'], \n",
    "                     df_kv_agg['kv_pct_max'], \n",
    "                     alpha=0.2, color='#8b5cf6', label='Min-Max Range')\n",
    "    \n",
    "    # Add saturation threshold (from WVA configMap)\n",
    "    ax.axhline(y=KV_CACHE_THRESHOLD*100, color='#dc2626', linestyle='--', \n",
    "               label=f'Saturation Threshold ({KV_CACHE_THRESHOLD*100:.0f}%)', linewidth=2)\n",
    "    \n",
    "    # Highlight saturation events\n",
    "    saturated = df_kv_agg[df_kv_agg['kv_pct_max'] > KV_CACHE_THRESHOLD*100]\n",
    "    if len(saturated) > 0:\n",
    "        ax.scatter(saturated['time_relative'], saturated['kv_pct_max'], \n",
    "                   color=\"#dc269c\", s=150, marker='.', linewidth=3, label='Saturation Event', zorder=5)\n",
    "    \n",
    "    ax.set_xlabel('Time (minutes)', fontsize=12, fontweight='bold')\n",
    "    ax.set_ylabel('KV Cache Utilization (%)', fontsize=12, fontweight='bold')\n",
    "    ax.set_title('WVA Capacity-Based: KV Cache Utilization', fontsize=16, fontweight='bold')\n",
    "    ax.legend(loc='best', fontsize=11, framealpha=0.9)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.set_ylim(0, 105)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(EXPERIMENT_DIR / 'plots' / 'kv_cache_utilization.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(\"âœ… KV cache plot complete\")\n",
    "else:\n",
    "    print(\"âš ï¸  No KV cache data to plot\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afb742c4",
   "metadata": {},
   "source": [
    "## 7. Visualization: Queue Length"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3143d058",
   "metadata": {},
   "source": [
    "## 7.5 Visualization: Token Generation Latencies (TTFT & ITL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d8249b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(df_metrics) > 0:\n",
    "    # Check if we have any non-zero data\n",
    "    has_ttft = (df_metrics['ttft'] > 0).any() if 'ttft' in df_metrics.columns else False\n",
    "    has_itl = (df_metrics['itl'] > 0).any() if 'itl' in df_metrics.columns else False\n",
    "    \n",
    "    if has_ttft or has_itl:\n",
    "        fig, ax = plt.subplots(figsize=(20, 6))\n",
    "        \n",
    "        # Create twin axis for different scales\n",
    "        ax2 = ax.twinx()\n",
    "        \n",
    "        if has_ttft:\n",
    "            ttft_mask = df_metrics['ttft'] > 0\n",
    "            line1 = ax.plot(df_metrics['time_relative'][ttft_mask], df_metrics['ttft'][ttft_mask], \n",
    "                    label='TTFT (ms)', color='#8b5cf6', marker='o', linewidth=2.5, markersize=5, alpha=0.8)\n",
    "            ax.set_ylabel('TTFT (ms)', fontsize=12, color='#8b5cf6')\n",
    "            ax.tick_params(axis='y', labelcolor='#8b5cf6')\n",
    "        \n",
    "        if has_itl:\n",
    "            itl_mask = df_metrics['itl'] > 0\n",
    "            line2 = ax2.plot(df_metrics['time_relative'][itl_mask], df_metrics['itl'][itl_mask], \n",
    "                    label='ITL (ms)', color='#059669', marker='s', linewidth=2.5, markersize=4, alpha=0.8)\n",
    "            ax2.set_ylabel('ITL (ms)', fontsize=12, color='#059669')\n",
    "            ax2.tick_params(axis='y', labelcolor='#059669')\n",
    "        \n",
    "        ax.set_xlabel('Time (minutes from start)', fontsize=12)\n",
    "        ax.set_title('WVA Capacity-Based: Token Generation Latencies', fontsize=16, fontweight='bold')\n",
    "        \n",
    "        # Combine legends\n",
    "        if has_ttft and has_itl:\n",
    "            lines = line1 + line2\n",
    "            labels = [l.get_label() for l in lines]\n",
    "            ax.legend(lines, labels, loc='best', fontsize=11, framealpha=0.9)\n",
    "        elif has_ttft:\n",
    "            ax.legend(loc='best', fontsize=11, framealpha=0.9)\n",
    "        else:\n",
    "            ax2.legend(loc='best', fontsize=11, framealpha=0.9)\n",
    "        \n",
    "        ax.grid(True, which='major', alpha=0.3, linestyle='-')\n",
    "        ax.grid(True, which='minor', alpha=0.15, linestyle=':')\n",
    "        ax.minorticks_on()\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(EXPERIMENT_DIR / 'plots' / 'latencies.png', dpi=150, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        print(\"âœ… TTFT/ITL plot complete\")\n",
    "    else:\n",
    "        print(\"âš ï¸  No TTFT/ITL data collected (all zeros - metrics may not be available)\")\n",
    "else:\n",
    "    print(\"âš ï¸  No metrics data to plot\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6357ec5",
   "metadata": {},
   "source": [
    "## 7.7 Visualization: Arrival Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae49ca5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(df_metrics) > 0 and 'arrival_rate' in df_metrics.columns:\n",
    "    # Check if we have any non-zero arrival rate data\n",
    "    has_arrival_rate = (df_metrics['arrival_rate'] > 0).any()\n",
    "    \n",
    "    if has_arrival_rate:\n",
    "        fig, ax = plt.subplots(figsize=(20, 6))\n",
    "        \n",
    "        # Filter for non-zero values\n",
    "        arrival_rate_mask = df_metrics['arrival_rate'] > 0\n",
    "        arrival_rate_data = df_metrics[arrival_rate_mask]\n",
    "        \n",
    "        ax.plot(arrival_rate_data['time_relative'], arrival_rate_data['arrival_rate'], \n",
    "                '-', color='#06b6d4', linewidth=2.5, label='Arrival Rate')\n",
    "        ax.fill_between(arrival_rate_data['time_relative'], 0, arrival_rate_data['arrival_rate'], \n",
    "                        color='#06b6d4', alpha=0.2)\n",
    "        \n",
    "        # Add mean line\n",
    "        mean_arrival = arrival_rate_data['arrival_rate'].mean()\n",
    "        ax.axhline(y=mean_arrival, color='#ea580c', linestyle='--', \n",
    "                   label=f'Mean: {mean_arrival:.2f} req/s', linewidth=2, zorder=5)\n",
    "        \n",
    "        ax.set_xlabel('Time (minutes from start)', fontsize=12)\n",
    "        ax.set_ylabel('Arrival Rate (requests/second)', fontsize=12)\n",
    "        ax.set_title('Request Arrival Rate Evolution', fontsize=16, fontweight='bold')\n",
    "        ax.legend(loc='best', fontsize=11, framealpha=0.9)\n",
    "        ax.grid(True, which='both', alpha=0.3)\n",
    "        ax.minorticks_on()\n",
    "        ax.set_ylim(bottom=0)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(EXPERIMENT_DIR / 'plots' / 'arrival_rate.png', dpi=150, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        print(\"âœ… Arrival rate plot complete\")\n",
    "    else:\n",
    "        print(\"âš ï¸  No arrival rate data collected (all zeros - workload may not have started)\")\n",
    "else:\n",
    "    print(\"âš ï¸  No arrival rate data available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09a6b2cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(df_queue_agg) > 0:\n",
    "    fig, ax = plt.subplots(figsize=(20, 6))\n",
    "    \n",
    "    # Plot queue metrics\n",
    "    ax.plot(df_queue_agg['time_relative'], df_queue_agg['queue_mean'], \n",
    "            '-o', color='#16a34a', label='Average Queue Length', markersize=5, linewidth=2.5)\n",
    "    ax.fill_between(df_queue_agg['time_relative'], \n",
    "                     df_queue_agg['queue_min'], \n",
    "                     df_queue_agg['queue_max'], \n",
    "                     alpha=0.2, color='#16a34a', label='Min-Max Range')\n",
    "    \n",
    "    # Add queue threshold (from WVA configMap)\n",
    "    ax.axhline(y=QUEUE_LENGTH_THRESHOLD, color='#ea580c', linestyle='--', \n",
    "               label=f'Queue Threshold ({QUEUE_LENGTH_THRESHOLD})', linewidth=2)\n",
    "    \n",
    "    # Highlight queue buildup\n",
    "    buildup = df_queue_agg[df_queue_agg['queue_max'] > QUEUE_LENGTH_THRESHOLD]\n",
    "    if len(buildup) > 0:\n",
    "        ax.scatter(buildup['time_relative'], buildup['queue_max'], \n",
    "                   color='#dc2626', s=150, marker='x', linewidth=3, label='Queue Buildup', zorder=5)\n",
    "    \n",
    "    ax.set_xlabel('Time (minutes)', fontsize=12, fontweight='bold')\n",
    "    ax.set_ylabel('Queue Length (requests)', fontsize=12, fontweight='bold')\n",
    "    ax.set_title('WVA Capacity-Based: Request Queue Length', fontsize=16, fontweight='bold')\n",
    "    ax.legend(loc='best', fontsize=11, framealpha=0.9)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.set_ylim(bottom=0)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(EXPERIMENT_DIR / 'plots' / 'queue_length.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(\"âœ… Queue length plot complete\")\n",
    "else:\n",
    "    print(\"âš ï¸  No queue data to plot\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "986fbad6",
   "metadata": {},
   "source": [
    "## 8. Visualization: Replica Scaling Over Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "031b821e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(df_metrics) > 0 or len(df_scaling) > 0:\n",
    "    fig, ax = plt.subplots(figsize=(20, 6))\n",
    "    \n",
    "    # Plot actual replicas from metrics (shows when replicas are truly ready)\n",
    "    if len(df_metrics) > 0 and 'replicas' in df_metrics.columns:\n",
    "        ax.plot(df_metrics['time_relative'], df_metrics['replicas'], \n",
    "                label='Actual Replicas', marker='o', linewidth=2.5, markersize=4, \n",
    "                color='#2563eb', zorder=3)\n",
    "    \n",
    "    # Add vertical lines at scaling decision times\n",
    "#     if len(df_scaling) > 0:\n",
    "#         scale_up = df_scaling[df_scaling['action'] == 'scale-up']\n",
    "#         scale_down = df_scaling[df_scaling['action'] == 'scale-down']\n",
    "        \n",
    "#         if len(scale_up) > 0:\n",
    "#             for _, event in scale_up.iterrows():\n",
    "#                 ax.axvline(x=event['time_relative'], color='#16a34a', linestyle='--', \n",
    "#                           alpha=0.7, linewidth=2, label='Scale Up Decision' if _ == scale_up.index[0] else '')\n",
    "#                 ax.text(event['time_relative'], ax.get_ylim()[1] * 0.95, \n",
    "#                        f\"â†‘ {event['current_replicas']}â†’{event['target_replicas']}\", \n",
    "#                        rotation=0, ha='center', va='top', fontsize=10, \n",
    "#                        color='darkgreen', fontweight='bold',\n",
    "#                        bbox=dict(boxstyle='round,pad=0.4', facecolor='lightgreen', alpha=0.8))\n",
    "        \n",
    "#         if len(scale_down) > 0:\n",
    "#             for _, event in scale_down.iterrows():\n",
    "#                 ax.axvline(x=event['time_relative'], color='#ea580c', linestyle='--', \n",
    "#                           alpha=0.7, linewidth=2, label='Scale Down Decision' if _ == scale_down.index[0] else '')\n",
    "#                 ax.text(event['time_relative'], ax.get_ylim()[1] * 0.05, \n",
    "#                        f\"â†“ {event['current_replicas']}â†’{event['target_replicas']}\", \n",
    "#                        rotation=0, ha='center', va='bottom', fontsize=10, \n",
    "#                        color='darkorange', fontweight='bold',\n",
    "#                        bbox=dict(boxstyle='round,pad=0.4', facecolor='bisque', alpha=0.8))\n",
    "    \n",
    "    ax.set_xlabel('Time (minutes)', fontsize=12, fontweight='bold')\n",
    "    ax.set_ylabel('Number of Replicas', fontsize=12, fontweight='bold')\n",
    "    ax.set_title('WVA Capacity-Based: Scaling Behavior', fontsize=16, fontweight='bold')\n",
    "    ax.legend(loc='best', fontsize=11, framealpha=0.9)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.yaxis.set_major_locator(plt.MaxNLocator(integer=True))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(EXPERIMENT_DIR / 'plots' / 'replica_scaling.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(\"âœ… Replica scaling plot complete\")\n",
    "else:\n",
    "    print(\"âš ï¸  No scaling data to plot\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd05b4e3",
   "metadata": {},
   "source": [
    "## 8.3 Visualization: Scaling Decisions with Queue Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6bb6082",
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(df_metrics) > 0 and len(df_queue_agg) > 0:\n",
    "    fig, ax = plt.subplots(figsize=(20, 6))\n",
    "    \n",
    "    # Create twin axis\n",
    "    ax2 = ax.twinx()\n",
    "    \n",
    "    # Plot actual replicas on left axis (blue)\n",
    "    ax.plot(df_metrics['time_relative'], df_metrics['replicas'], \n",
    "            '-o', label='Actual Replicas', markersize=5, linewidth=2.5, color='#2563eb')\n",
    "    \n",
    "    # Plot desired replicas (from scaling decisions) if enabled and available\n",
    "    if SHOW_DESIRED_REPLICAS and len(df_scaling) > 0 and 'target_replicas' in df_scaling.columns:\n",
    "        # Use step plot for sparse scaling decision data\n",
    "        ax.step(df_scaling['time_relative'], df_scaling['target_replicas'], \n",
    "                where='post', label='Desired Replicas (WVA Decision)', \n",
    "                linewidth=2.5, linestyle='--', color='#dc2626', alpha=0.8)\n",
    "    \n",
    "    # Plot queue length on right axis (green)\n",
    "    ax2.plot(df_queue_agg['time_relative'], df_queue_agg['queue_mean'], \n",
    "             '--s', color='#16a34a', label='Average Queue Length', markersize=5, linewidth=2.5, alpha=0.8)\n",
    "    \n",
    "    # Add queue threshold on right axis\n",
    "    ax2.axhline(y=QUEUE_LENGTH_THRESHOLD, color='#ea580c', linestyle=':', \n",
    "                label=f'Queue Threshold ({QUEUE_LENGTH_THRESHOLD})', linewidth=2, alpha=0.8)\n",
    "    \n",
    "    # Color-code axes labels to match plots\n",
    "    ax.set_ylabel('Replicas', fontsize=12, fontweight='bold', color='#2563eb')\n",
    "    ax.tick_params(axis='y', labelcolor='#2563eb')\n",
    "    \n",
    "    ax2.set_ylabel('Queue Length (requests)', fontsize=12, fontweight='bold', color='#16a34a')\n",
    "    ax2.tick_params(axis='y', labelcolor='#16a34a')\n",
    "    \n",
    "    ax.set_xlabel('Time (minutes)', fontsize=12, fontweight='bold')\n",
    "    ax.set_title('Scaling Decisions with Queue Context', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # Combine legends\n",
    "    lines1, labels1 = ax.get_legend_handles_labels()\n",
    "    lines2, labels2 = ax2.get_legend_handles_labels()\n",
    "    ax.legend(lines1 + lines2, labels1 + labels2, loc='best', fontsize=11, framealpha=0.9)\n",
    "    \n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.yaxis.set_major_locator(plt.MaxNLocator(integer=True))\n",
    "    ax2.set_ylim(bottom=0)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(EXPERIMENT_DIR / 'plots' / 'scaling_queue_context.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(\"âœ… Scaling decisions with queue context plot complete\")\n",
    "else:\n",
    "    print(\"âš ï¸  Missing data for scaling/queue context plot\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b537f874",
   "metadata": {},
   "source": [
    "## 8.5 Combined Summary View\n",
    "\n",
    "All key metrics in a single multi-panel overview for comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08d242c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. Combined Summary Plot (All Metrics Over Time)\n",
    "print(\"\\nðŸ“Š Generating combined summary plot...\")\n",
    "\n",
    "# Determine which plots to create based on available data\n",
    "plots_to_create = []\n",
    "\n",
    "# Check for replicas data\n",
    "if len(df_metrics) > 0 and 'replicas' in df_metrics.columns:\n",
    "    plots_to_create.append('replicas')\n",
    "\n",
    "# Check for KV cache data\n",
    "if len(df_kv_agg) > 0:\n",
    "    plots_to_create.append('kv_cache')\n",
    "\n",
    "# Check for queue data\n",
    "if len(df_queue_agg) > 0:\n",
    "    plots_to_create.append('queue')\n",
    "\n",
    "# Check for latency data\n",
    "has_ttft = len(df_metrics) > 0 and 'ttft' in df_metrics.columns and df_metrics['ttft'].notna().any()\n",
    "has_itl = len(df_metrics) > 0 and 'itl' in df_metrics.columns and df_metrics['itl'].notna().any()\n",
    "if has_ttft or has_itl:\n",
    "    plots_to_create.append('latencies')\n",
    "\n",
    "# Check for arrival rate\n",
    "if len(df_metrics) > 0 and 'arrival_rate' in df_metrics.columns and df_metrics['arrival_rate'].notna().any():\n",
    "    plots_to_create.append('arrival_rate')\n",
    "\n",
    "# Contextual scaling panels\n",
    "if len(df_metrics) > 0 and 'replicas' in df_metrics.columns and len(df_kv_agg) > 0:\n",
    "    plots_to_create.append('scaling_kv_context')\n",
    "\n",
    "if len(df_metrics) > 0 and 'replicas' in df_metrics.columns and len(df_queue_agg) > 0:\n",
    "    plots_to_create.append('scaling_queue_context')\n",
    "\n",
    "num_plots = len(plots_to_create)\n",
    "\n",
    "if num_plots == 0:\n",
    "    print(\"âš ï¸ No data available for combined plot\")\n",
    "else:\n",
    "    # Create subplots with modern sizing\n",
    "    fig, axes = plt.subplots(num_plots, 1, figsize=(20, 4*num_plots))\n",
    "    \n",
    "    # Handle single subplot case\n",
    "    if num_plots == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    current_plot = 0\n",
    "    \n",
    "    # Panel 1: Replicas (Actual vs Desired)\n",
    "    if 'replicas' in plots_to_create:\n",
    "        ax = axes[current_plot]\n",
    "        \n",
    "        # Plot actual replicas\n",
    "        ax.plot(df_metrics['time_relative'], df_metrics['replicas'], \n",
    "                '-o', color='#2563eb', label='Actual Replicas', markersize=5, linewidth=2.5)\n",
    "        \n",
    "        # Plot desired replicas if enabled and available\n",
    "        if SHOW_DESIRED_REPLICAS and len(df_scaling) > 0 and 'target_replicas' in df_scaling.columns:\n",
    "            # Use step plot for sparse scaling decision data\n",
    "            ax.step(df_scaling['time_relative'], df_scaling['target_replicas'], \n",
    "                    where='post', label='Desired Replicas (WVA Decision)', \n",
    "                    linewidth=2.5, linestyle='--', color='#dc2626', alpha=0.8)\n",
    "        \n",
    "        ax.set_ylabel('Replicas', fontsize=12, fontweight='bold')\n",
    "        ax.set_title('Replica Count Over Time', fontsize=16, fontweight='bold')\n",
    "        ax.legend(loc='best', fontsize=11, framealpha=0.9)\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        ax.yaxis.set_major_locator(plt.MaxNLocator(integer=True))\n",
    "        ax.set_ylim(bottom=0)\n",
    "        current_plot += 1\n",
    "    \n",
    "    # Panel 2: KV Cache Utilization\n",
    "    if 'kv_cache' in plots_to_create:\n",
    "        ax = axes[current_plot]\n",
    "        \n",
    "        # Plot mean KV cache percentage\n",
    "        ax.plot(df_kv_agg['time_relative'], df_kv_agg['kv_pct_mean'], \n",
    "                '-', color='#8b5cf6', label='Mean KV Cache %', linewidth=2.5)\n",
    "        \n",
    "        # Add min/max shaded area\n",
    "        ax.fill_between(df_kv_agg['time_relative'], \n",
    "                         df_kv_agg['kv_pct_min'], \n",
    "                         df_kv_agg['kv_pct_max'], \n",
    "                         alpha=0.2, color='#8b5cf6', label='Min-Max Range')\n",
    "        \n",
    "        # Add threshold line\n",
    "        ax.axhline(y=KV_CACHE_THRESHOLD*100, color='#dc2626', linestyle='--', \n",
    "                   label=f'Saturation ({KV_CACHE_THRESHOLD*100:.0f}%)', linewidth=2)\n",
    "        \n",
    "        ax.set_ylabel('KV Cache Usage (%)', fontsize=12, fontweight='bold')\n",
    "        ax.set_title('KV Cache Utilization', fontsize=16, fontweight='bold')\n",
    "        ax.legend(loc='best', fontsize=11, framealpha=0.9)\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        ax.set_ylim(0, 105)\n",
    "        current_plot += 1\n",
    "    \n",
    "    # Panel 3: Queue Length\n",
    "    if 'queue' in plots_to_create:\n",
    "        ax = axes[current_plot]\n",
    "        \n",
    "        ax.plot(df_queue_agg['time_relative'], df_queue_agg['queue_mean'], \n",
    "                '-o', color='#16a34a', label='Average Queue Length', markersize=5, linewidth=2.5)\n",
    "        ax.fill_between(df_queue_agg['time_relative'], \n",
    "                         df_queue_agg['queue_min'], \n",
    "                         df_queue_agg['queue_max'], \n",
    "                         alpha=0.2, color='#16a34a', label='Min-Max Range')\n",
    "        \n",
    "        ax.axhline(y=QUEUE_LENGTH_THRESHOLD, color='#ea580c', linestyle='--', \n",
    "                   label=f'Queue Threshold ({QUEUE_LENGTH_THRESHOLD})', linewidth=2)\n",
    "        \n",
    "        ax.set_ylabel('Queue Length (requests)', fontsize=12, fontweight='bold')\n",
    "        ax.set_title('Request Queue Depth', fontsize=16, fontweight='bold')\n",
    "        ax.legend(loc='best', fontsize=11, framealpha=0.9)\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        ax.set_ylim(bottom=0)\n",
    "        current_plot += 1\n",
    "    \n",
    "    # Panel 4: TTFT/ITL Latencies\n",
    "    if 'latencies' in plots_to_create:\n",
    "        ax = axes[current_plot]\n",
    "        ax2 = ax.twinx()\n",
    "        \n",
    "        if has_ttft:\n",
    "            ttft_mask = df_metrics['ttft'] > 0\n",
    "            line1 = ax.plot(df_metrics['time_relative'][ttft_mask], df_metrics['ttft'][ttft_mask], \n",
    "                    label='TTFT (ms)', color='#8b5cf6', marker='o', linewidth=2.5, markersize=4, alpha=0.8)\n",
    "            ax.set_ylabel('TTFT (ms)', fontsize=12, fontweight='bold', color='#8b5cf6')\n",
    "            ax.tick_params(axis='y', labelcolor='#8b5cf6')\n",
    "        \n",
    "        if has_itl:\n",
    "            itl_mask = df_metrics['itl'] > 0\n",
    "            line2 = ax2.plot(df_metrics['time_relative'][itl_mask], df_metrics['itl'][itl_mask], \n",
    "                    label='ITL (ms)', color='#059669', marker='s', linewidth=2.5, markersize=4, alpha=0.8)\n",
    "            ax2.set_ylabel('ITL (ms)', fontsize=12, fontweight='bold', color='#059669')\n",
    "            ax2.tick_params(axis='y', labelcolor='#059669')\n",
    "        \n",
    "        ax.set_title('Token Generation Latencies', fontsize=16, fontweight='bold')\n",
    "        \n",
    "        # Combine legends\n",
    "        if has_ttft and has_itl:\n",
    "            lines = line1 + line2\n",
    "            labels = [l.get_label() for l in lines]\n",
    "            ax.legend(lines, labels, loc='best', fontsize=11, framealpha=0.9)\n",
    "        elif has_ttft:\n",
    "            ax.legend(loc='best', fontsize=11, framealpha=0.9)\n",
    "        else:\n",
    "            ax2.legend(loc='best', fontsize=11, framealpha=0.9)\n",
    "        \n",
    "        ax.grid(True, alpha=0.3)\n",
    "        current_plot += 1\n",
    "    \n",
    "    # Panel 5: Arrival Rate\n",
    "    if 'arrival_rate' in plots_to_create:\n",
    "        ax = axes[current_plot]\n",
    "        \n",
    "        arrival_rate_mask = df_metrics['arrival_rate'] > 0\n",
    "        ax.plot(df_metrics['time_relative'][arrival_rate_mask], \n",
    "                df_metrics['arrival_rate'][arrival_rate_mask], \n",
    "                label='Arrival Rate (req/s)', color='#06b6d4', marker='o', \n",
    "                linewidth=2.5, markersize=4, alpha=0.8)\n",
    "        \n",
    "        # Add mean line\n",
    "        mean_arrival = df_metrics['arrival_rate'][arrival_rate_mask].mean()\n",
    "        ax.axhline(y=mean_arrival, color='#ea580c', linestyle='--', \n",
    "                   label=f'Mean: {mean_arrival:.2f} req/s', linewidth=2, alpha=0.8)\n",
    "        \n",
    "        ax.set_ylabel('Arrival Rate (req/s)', fontsize=12, fontweight='bold')\n",
    "        ax.set_title('Request Arrival Rate', fontsize=16, fontweight='bold')\n",
    "        ax.legend(loc='best', fontsize=11, framealpha=0.9)\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        ax.set_ylim(bottom=0)\n",
    "        current_plot += 1\n",
    "    \n",
    "    # Panel 6: Scaling with KV Context\n",
    "    if 'scaling_kv_context' in plots_to_create:\n",
    "        ax = axes[current_plot]\n",
    "        ax2 = ax.twinx()\n",
    "        \n",
    "        # Plot actual replicas on left axis (blue)\n",
    "        ax.plot(df_metrics['time_relative'], df_metrics['replicas'], \n",
    "                '-o', color='#2563eb', label='Actual Replicas', markersize=5, linewidth=2.5)\n",
    "        \n",
    "        # Plot desired replicas if enabled and available\n",
    "        if SHOW_DESIRED_REPLICAS and len(df_scaling) > 0 and 'target_replicas' in df_scaling.columns:\n",
    "            # Use step plot for sparse scaling decision data\n",
    "            ax.step(df_scaling['time_relative'], df_scaling['target_replicas'], \n",
    "                    where='post', label='Desired Replicas (WVA Decision)', \n",
    "                    linewidth=2.5, linestyle='--', color='#dc2626', alpha=0.8)\n",
    "        \n",
    "        # Plot KV cache on right axis (purple)\n",
    "        ax2.plot(df_kv_agg['time_relative'], df_kv_agg['kv_pct_mean'], \n",
    "                 color='#8b5cf6', linestyle='--', marker='s', label='Avg KV Cache %', \n",
    "                 markersize=5, linewidth=2.5, alpha=0.8)\n",
    "        \n",
    "        # Add KV threshold on right axis\n",
    "        ax2.axhline(y=KV_CACHE_THRESHOLD*100, color='#dc2626', linestyle=':', \n",
    "                    label=f'KV Threshold ({KV_CACHE_THRESHOLD*100:.0f}%)', linewidth=2, alpha=0.8)\n",
    "        \n",
    "        # Color-code axes labels to match plots\n",
    "        ax.set_ylabel('Replicas', fontsize=12, fontweight='bold', color='#2563eb')\n",
    "        ax.tick_params(axis='y', labelcolor='#2563eb')\n",
    "        \n",
    "        ax2.set_ylabel('KV Cache Usage (%)', fontsize=12, fontweight='bold', color='#8b5cf6')\n",
    "        ax2.tick_params(axis='y', labelcolor='#8b5cf6')\n",
    "        \n",
    "        ax.set_title('Scaling Decisions with KV Cache Context', fontsize=16, fontweight='bold')\n",
    "        \n",
    "        # Combine legends\n",
    "        lines1, labels1 = ax.get_legend_handles_labels()\n",
    "        lines2, labels2 = ax2.get_legend_handles_labels()\n",
    "        ax.legend(lines1 + lines2, labels1 + labels2, loc='best', fontsize=11, framealpha=0.9)\n",
    "        \n",
    "        ax.grid(True, alpha=0.3)\n",
    "        ax.yaxis.set_major_locator(plt.MaxNLocator(integer=True))\n",
    "        ax2.set_ylim(0, 105)\n",
    "        current_plot += 1\n",
    "    \n",
    "    # Panel 7: Scaling with Queue Context\n",
    "    if 'scaling_queue_context' in plots_to_create:\n",
    "        ax = axes[current_plot]\n",
    "        ax2 = ax.twinx()\n",
    "        \n",
    "        # Plot actual replicas on left axis (blue)\n",
    "        ax.plot(df_metrics['time_relative'], df_metrics['replicas'], \n",
    "                '-o', color='#2563eb', label='Actual Replicas', markersize=5, linewidth=2.5)\n",
    "        \n",
    "        # Plot desired replicas if enabled and available\n",
    "        if SHOW_DESIRED_REPLICAS and len(df_scaling) > 0 and 'target_replicas' in df_scaling.columns:\n",
    "            # Use step plot for sparse scaling decision data\n",
    "            ax.step(df_scaling['time_relative'], df_scaling['target_replicas'], \n",
    "                    where='post', label='Desired Replicas (WVA Decision)', \n",
    "                    linewidth=2.5, linestyle='--', color='#dc2626', alpha=0.8)\n",
    "        \n",
    "        # Plot queue length on right axis (green)\n",
    "        ax2.plot(df_queue_agg['time_relative'], df_queue_agg['queue_mean'], \n",
    "                 color='#16a34a', linestyle='--', marker='s', label='Average Queue Length', \n",
    "                 markersize=5, linewidth=2.5, alpha=0.8)\n",
    "        \n",
    "        # Add queue threshold on right axis\n",
    "        ax2.axhline(y=QUEUE_LENGTH_THRESHOLD, color='#ea580c', linestyle=':', \n",
    "                    label=f'Queue Threshold ({QUEUE_LENGTH_THRESHOLD})', linewidth=2, alpha=0.8)\n",
    "        \n",
    "        # Color-code axes labels to match plots\n",
    "        ax.set_ylabel('Replicas', fontsize=12, fontweight='bold', color='#2563eb')\n",
    "        ax.tick_params(axis='y', labelcolor='#2563eb')\n",
    "        \n",
    "        ax2.set_ylabel('Queue Length (requests)', fontsize=12, fontweight='bold', color='#16a34a')\n",
    "        ax2.tick_params(axis='y', labelcolor='#16a34a')\n",
    "        \n",
    "        ax.set_title('Scaling Decisions with Queue Context', fontsize=16, fontweight='bold')\n",
    "        \n",
    "        # Combine legends\n",
    "        lines1, labels1 = ax.get_legend_handles_labels()\n",
    "        lines2, labels2 = ax2.get_legend_handles_labels()\n",
    "        ax.legend(lines1 + lines2, labels1 + labels2, loc='best', fontsize=11, framealpha=0.9)\n",
    "        \n",
    "        ax.grid(True, alpha=0.3)\n",
    "        ax.yaxis.set_major_locator(plt.MaxNLocator(integer=True))\n",
    "        ax2.set_ylim(bottom=0)\n",
    "        current_plot += 1\n",
    "    \n",
    "    # Add shared x-label on bottom plot\n",
    "    axes[-1].set_xlabel('Time (minutes)', fontsize=12, fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(EXPERIMENT_DIR / 'plots' / 'combined_summary.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(\"âœ… Combined summary plot complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c308bf1c",
   "metadata": {},
   "source": [
    "## 9. Scaling Events Details\n",
    "\n",
    "View detailed information about each scaling decision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "407de9ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(df_scaling) > 0:\n",
    "    print(\"=\"*70)\n",
    "    print(f\"SCALING EVENTS ({len(df_scaling)} total)\")\n",
    "    print(\"=\"*70)\n",
    "    print()\n",
    "    \n",
    "    for i, row in df_scaling.iterrows():\n",
    "        print(f\"Event {i+1}:\")\n",
    "        print(\"-\" * 70)\n",
    "        print(f\"Timestamp:        {row['timestamp']}\")\n",
    "        print(f\"Current Replicas: {row['current_replicas']}\")\n",
    "        print(f\"Target Replicas:  {row['target_replicas']}\")\n",
    "        print(f\"Action:           {row['action']}\")\n",
    "        \n",
    "        # Show reason if available\n",
    "        if 'reason' in row and pd.notna(row['reason']):\n",
    "            print(f\"Reason:           {row['reason']}\")\n",
    "        \n",
    "        # Calculate time from start\n",
    "        if 'time_relative' in row and pd.notna(row['time_relative']):\n",
    "            print(f\"Time (minutes):   {row['time_relative']:.2f}\")\n",
    "        \n",
    "        print()\n",
    "else:\n",
    "    print(\"No scaling events detected in this experiment.\")\n",
    "    print(\"\\nNote: Scaling decisions are logged when WVA makes a scaling action.\")\n",
    "    print(\"If the workload is stable and within capacity, no scaling may occur.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0efd04aa",
   "metadata": {},
   "source": [
    "## 10. Visualization: Per-Pod KV Cache Timeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8819b2e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(df_kv_cache) > 0:\n",
    "    # Get unique pods\n",
    "    pods = df_kv_cache['pod'].unique()\n",
    "    \n",
    "    if len(pods) <= 10:  # Only plot if reasonable number of pods\n",
    "        fig, ax = plt.subplots(figsize=(20, 6))\n",
    "        \n",
    "        start_time = df_kv_cache['timestamp'].min()\n",
    "        \n",
    "        # Plot each pod with different colors\n",
    "        colors = ['#2563eb', '#8b5cf6', '#16a34a', '#ea580c', '#06b6d4', \n",
    "                  '#dc2626', '#7c3aed', '#059669', '#f59e0b', '#ec4899']\n",
    "        \n",
    "        for i, pod in enumerate(pods):\n",
    "            pod_data = df_kv_cache[df_kv_cache['pod'] == pod].copy()\n",
    "            pod_data['time_relative'] = (pod_data['timestamp'] - start_time).dt.total_seconds() / 60\n",
    "            pod_data = pod_data.sort_values('time_relative')\n",
    "            \n",
    "            ax.plot(pod_data['time_relative'], pod_data['kv_cache_pct'], \n",
    "                    '-o', label=pod, markersize=4, linewidth=2, alpha=0.7,\n",
    "                    color=colors[i % len(colors)])\n",
    "        \n",
    "        ax.axhline(y=KV_CACHE_THRESHOLD*100, color='#dc2626', linestyle='--', \n",
    "                   label=f'KV Cache Saturation Threshold ({KV_CACHE_THRESHOLD*100}%)', \n",
    "                   linewidth=2.5, zorder=10)\n",
    "        \n",
    "        ax.set_xlabel('Time (minutes)', fontsize=12, fontweight='bold')\n",
    "        ax.set_ylabel('KV Cache Utilization (%)', fontsize=12, fontweight='bold')\n",
    "        ax.set_title('Per-Pod KV Cache Utilization Timeline', fontsize=16, fontweight='bold')\n",
    "        ax.legend(loc='best', fontsize=10, ncol=2, framealpha=0.9)\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        ax.set_ylim(0, 105)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(EXPERIMENT_DIR / 'plots' / 'per_pod_kv_cache.png', dpi=150, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        print(\"âœ… Per-pod KV cache plot complete\")\n",
    "    else:\n",
    "        print(f\"âš ï¸  Too many pods ({len(pods)}) to plot individual timelines\")\n",
    "else:\n",
    "    print(\"âš ï¸  No per-pod KV cache data to plot\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaf42d5b",
   "metadata": {},
   "source": [
    "## 11. Export Processed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d3dfc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save aggregated metrics\n",
    "output_files = []\n",
    "\n",
    "if len(df_kv_agg) > 0:\n",
    "    output_file = EXPERIMENT_DIR / 'kv_cache_aggregated.csv'\n",
    "    df_kv_agg.to_csv(output_file, index=False)\n",
    "    output_files.append(output_file.name)\n",
    "    print(f\"âœ“ Saved aggregated KV cache data to: {output_file}\")\n",
    "\n",
    "if len(df_queue_agg) > 0:\n",
    "    output_file = EXPERIMENT_DIR / 'queue_aggregated.csv'\n",
    "    df_queue_agg.to_csv(output_file, index=False)\n",
    "    output_files.append(output_file.name)\n",
    "    print(f\"âœ“ Saved aggregated queue data to: {output_file}\")\n",
    "\n",
    "if len(df_capacity) > 0:\n",
    "    output_file = EXPERIMENT_DIR / 'capacity_analysis.csv'\n",
    "    df_capacity.to_csv(output_file, index=False)\n",
    "    output_files.append(output_file.name)\n",
    "    print(f\"âœ“ Saved capacity analysis to: {output_file}\")\n",
    "\n",
    "# Create summary report\n",
    "summary_file = EXPERIMENT_DIR / 'ANALYSIS_SUMMARY.md'\n",
    "\n",
    "with open(summary_file, 'w') as f:\n",
    "    f.write(f\"# WVA Capacity-Based Experiment Analysis\\n\\n\")\n",
    "    f.write(f\"**Experiment:** {EXPERIMENT_DIR.name}\\n\\n\")\n",
    "    f.write(f\"**Generated:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\\n\")\n",
    "    f.write(f\"---\\n\\n\")\n",
    "    \n",
    "    f.write(f\"## Summary Statistics\\n\\n\")\n",
    "    f.write(f\"- **KV Cache Measurements:** {len(df_kv_cache)}\\n\")\n",
    "    f.write(f\"- **Queue Measurements:** {len(df_queue)}\\n\")\n",
    "    f.write(f\"- **Capacity Analyses:** {len(df_capacity)}\\n\")\n",
    "    f.write(f\"- **Scaling Decisions:** {len(df_scaling)}\\n\\n\")\n",
    "    \n",
    "    if len(df_kv_agg) > 0:\n",
    "        SATURATION_THRESHOLD = 90.0\n",
    "        saturated_count = (df_kv_agg['kv_pct_max'] > SATURATION_THRESHOLD).sum()\n",
    "        saturated_pct = (saturated_count / len(df_kv_agg)) * 100\n",
    "        \n",
    "        f.write(f\"## KV Cache Utilization\\n\\n\")\n",
    "        f.write(f\"- Average: {df_kv_agg['kv_pct_mean'].mean():.1f}%\\n\")\n",
    "        f.write(f\"- Peak: {df_kv_agg['kv_pct_max'].max():.1f}%\\n\")\n",
    "        f.write(f\"- Min: {df_kv_agg['kv_pct_min'].min():.1f}%\\n\")\n",
    "        f.write(f\"- Saturation Events (>{SATURATION_THRESHOLD}%): {saturated_count}/{len(df_kv_agg)} ({saturated_pct:.1f}%)\\n\\n\")\n",
    "    \n",
    "    if len(df_queue_agg) > 0:\n",
    "        f.write(f\"## Queue Length\\n\\n\")\n",
    "        f.write(f\"- Average: {df_queue_agg['queue_mean'].mean():.1f} requests\\n\")\n",
    "        f.write(f\"- Peak: {df_queue_agg['queue_max'].max():.0f} requests\\n\")\n",
    "        f.write(f\"- Queue Buildup Events: {(df_queue_agg['queue_max'] > 0).sum()}\\n\\n\")\n",
    "    \n",
    "    if len(df_capacity) > 0:\n",
    "        scaleup_count = df_capacity['should_scale_up'].sum() if 'should_scale_up' in df_capacity else 0\n",
    "        scaledown_count = df_capacity['scale_down_safe'].sum() if 'scale_down_safe' in df_capacity else 0\n",
    "        \n",
    "        f.write(f\"## Capacity Analysis\\n\\n\")\n",
    "        f.write(f\"- Scale-Up Recommendations: {scaleup_count}/{len(df_capacity)}\\n\")\n",
    "        f.write(f\"- Scale-Down Safe: {scaledown_count}/{len(df_capacity)}\\n\\n\")\n",
    "    \n",
    "    if len(df_scaling) > 0:\n",
    "        f.write(f\"## Scaling Behavior\\n\\n\")\n",
    "        action_counts = df_scaling['action'].value_counts()\n",
    "        for action, count in action_counts.items():\n",
    "            pct = (count / len(df_scaling)) * 100\n",
    "            f.write(f\"- {action}: {count} ({pct:.1f}%)\\n\")\n",
    "        f.write(f\"\\n- Replica Range: {df_scaling['current_replicas'].min()} - {df_scaling['current_replicas'].max()}\\n\\n\")\n",
    "    \n",
    "    if len(df_metrics) > 0:\n",
    "        has_ttft = (df_metrics['ttft'] > 0).any() if 'ttft' in df_metrics.columns else False\n",
    "        has_itl = (df_metrics['itl'] > 0).any() if 'itl' in df_metrics.columns else False\n",
    "        \n",
    "        f.write(f\"## Performance Metrics\\n\\n\")\n",
    "        \n",
    "        if has_ttft:\n",
    "            ttft_data = df_metrics['ttft'][df_metrics['ttft'] > 0]\n",
    "            f.write(f\"- TTFT: {ttft_data.mean():.2f} ms (avg), {ttft_data.max():.2f} ms (max)\\n\")\n",
    "        else:\n",
    "            f.write(f\"- TTFT: No data collected\\n\")\n",
    "        \n",
    "        if has_itl:\n",
    "            itl_data = df_metrics['itl'][df_metrics['itl'] > 0]\n",
    "            f.write(f\"- ITL: {itl_data.mean():.2f} ms (avg), {itl_data.max():.2f} ms (max)\\n\")\n",
    "        else:\n",
    "            f.write(f\"- ITL: No data collected\\n\")\n",
    "        \n",
    "        # Add arrival rate if available\n",
    "        has_arrival_rate = (df_metrics['arrival_rate'] > 0).any() if 'arrival_rate' in df_metrics.columns else False\n",
    "        if has_arrival_rate:\n",
    "            arrival_rate_data = df_metrics['arrival_rate'][df_metrics['arrival_rate'] > 0]\n",
    "            f.write(f\"- Arrival Rate: {arrival_rate_data.mean():.2f} req/s (avg), {arrival_rate_data.max():.2f} req/s (max)\\n\")\n",
    "        else:\n",
    "            f.write(f\"- Arrival Rate: No data collected\\n\")\n",
    "        \n",
    "        f.write(\"\\n\")\n",
    "    \n",
    "    f.write(f\"## Files Generated\\n\\n\")\n",
    "    for fname in output_files:\n",
    "        f.write(f\"- `{fname}` - Processed metrics\\n\")\n",
    "    f.write(f\"\\n### Plots\\n\\n\")\n",
    "    f.write(f\"- `plots/kv_cache_utilization.png` - KV cache timeline\\n\")\n",
    "    f.write(f\"- `plots/queue_length.png` - Queue length timeline\\n\")\n",
    "    f.write(f\"- `plots/latencies.png` - TTFT and ITL latencies\\n\")\n",
    "    f.write(f\"- `plots/arrival_rate.png` - Request arrival rate over time\\n\")\n",
    "    f.write(f\"- `plots/replica_scaling.png` - Replica scaling behavior\\n\")\n",
    "    f.write(f\"- `plots/combined_summary.png` - Combined multi-panel summary\\n\")\n",
    "    f.write(f\"- `plots/per_pod_kv_cache.png` - Per-pod KV cache utilization\\n\")\n",
    "\n",
    "print(f\"âœ“ Saved summary report to: {summary_file}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ANALYSIS COMPLETE\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Results saved to: {EXPERIMENT_DIR}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
