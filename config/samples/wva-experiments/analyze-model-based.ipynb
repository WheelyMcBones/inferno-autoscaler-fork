{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "69a8d48b",
   "metadata": {},
   "source": [
    "# WVA Model-Based Mode Analysis\n",
    "\n",
    "This notebook analyzes WVA (Workload Variant Autoscaler) experiments running in **MODEL-ONLY mode**.\n",
    "\n",
    "## Key Metrics\n",
    "\n",
    "### Prediction vs Reality\n",
    "- **Predicted ITL/TTFT** - From optimizer at reconciliation N (what WVA predicts)\n",
    "- **Observed ITL/TTFT** - From Prometheus at reconciliation N+1 (what actually happened)\n",
    "- **Prediction Error** - Difference between predicted and observed\n",
    "\n",
    "### Scaling Behavior\n",
    "- **Target Replicas** - What optimizer decided\n",
    "- **Current Replicas** - Actual running replicas\n",
    "- **Scaling Actions** - Scale-up, scale-down, no-change\n",
    "\n",
    "### SLO Compliance\n",
    "- **SLO ITL** - Target inter-token latency (typically 10ms)\n",
    "- **SLO TTFT** - Target time-to-first-token (typically 1000ms)\n",
    "- **SLO Violations** - When observed metrics exceed SLOs\n",
    "\n",
    "## Workflow\n",
    "\n",
    "1. Load experiment data from timestamped directory\n",
    "2. Parse WVA controller logs\n",
    "3. Align predictions (reconciliation N) with observations (reconciliation N+1)\n",
    "4. Visualize scaling behavior and performance\n",
    "5. Analyze prediction accuracy and SLO compliance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19fe0dd2",
   "metadata": {},
   "source": [
    "## 1. Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ab484ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import sys\n",
    "import re\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timedelta\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "from matplotlib.patches import Rectangle\n",
    "import numpy as np\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "%matplotlib inline\n",
    "\n",
    "print(\"✓ Libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7404df47",
   "metadata": {},
   "source": [
    "## 2. Select Experiment Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6721693c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Auto-detect latest model-based experiment\n",
    "data_dir = Path('./experiment-data')\n",
    "if data_dir.exists():\n",
    "    experiments = sorted(data_dir.glob('model-based-*'), reverse=True)\n",
    "    if experiments:\n",
    "        EXPERIMENT_DIR = str(experiments[0])\n",
    "        print(f\"✓ Auto-detected latest experiment: {experiments[0].name}\")\n",
    "    else:\n",
    "        EXPERIMENT_DIR = './experiment-data/model-based-moderate-load-20251126-120000'\n",
    "        print(f\"⚠ No model-based experiments found, using example path\")\n",
    "else:\n",
    "    EXPERIMENT_DIR = './experiment-data/model-based-moderate-load-20251126-120000'\n",
    "    print(f\"⚠ Data directory not found, using example path\")\n",
    "\n",
    "EXPERIMENT_DIR = Path(EXPERIMENT_DIR)\n",
    "LOG_FILE = EXPERIMENT_DIR / 'wva-controller-logs.jsonl'\n",
    "METRICS_CSV = EXPERIMENT_DIR / 'metrics.csv'\n",
    "\n",
    "print(f\"Experiment directory: {EXPERIMENT_DIR}\")\n",
    "print(f\"Log file: {LOG_FILE}\")\n",
    "print(f\"Metrics CSV: {METRICS_CSV}\")\n",
    "\n",
    "# Verify files exist\n",
    "if not EXPERIMENT_DIR.exists():\n",
    "    print(f\"❌ Experiment directory not found: {EXPERIMENT_DIR}\")\n",
    "    print(f\"Please run an experiment first with: ./run-experiment.sh experiment-configs/model-based-moderate.yaml\")\n",
    "elif not LOG_FILE.exists():\n",
    "    print(f\"❌ Log file not found: {LOG_FILE}\")\n",
    "else:\n",
    "    print(f\"✓ Files found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f9c559f",
   "metadata": {},
   "source": [
    "## 3. Parse WVA Logs\n",
    "\n",
    "Extract key events from WVA controller logs:\n",
    "- **Predictions** - From optimizer solution (reconciliation N)\n",
    "- **Observations** - From Prometheus metrics (reconciliation N+1)\n",
    "- **Scaling Decisions** - Target replica changes\n",
    "- **SLO Values** - ITL and TTFT thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed4f73e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_optimization_solution(solution_str):\n",
    "    \"\"\"Extract predicted ITL and TTFT from optimization solution.\"\"\"\n",
    "    itl_match = re.search(r'itl=([0-9.]+)', solution_str)\n",
    "    ttft_match = re.search(r'ttft=([0-9.]+)', solution_str)\n",
    "    replicas_match = re.search(r'numRep=([0-9]+)', solution_str)\n",
    "    maxbatch_match = re.search(r'maxBatch=([0-9]+)', solution_str)\n",
    "    cost_match = re.search(r'cost=([0-9.]+)', solution_str)\n",
    "    \n",
    "    return {\n",
    "        'predicted_itl': float(itl_match.group(1)) if itl_match else None,\n",
    "        'predicted_ttft': float(ttft_match.group(1)) if ttft_match else None,\n",
    "        'predicted_replicas': int(replicas_match.group(1)) if replicas_match else None,\n",
    "        'max_batch': int(maxbatch_match.group(1)) if maxbatch_match else None,\n",
    "        'cost': float(cost_match.group(1)) if cost_match else None\n",
    "    }\n",
    "\n",
    "# Parse logs\n",
    "predictions = []\n",
    "observations = []\n",
    "scaling_decisions = []\n",
    "slo_values = []\n",
    "\n",
    "print(\"Parsing WVA controller logs...\")\n",
    "with open(LOG_FILE, 'r') as f:\n",
    "    for line_num, line in enumerate(f, 1):\n",
    "        try:\n",
    "            log = json.loads(line.strip())\n",
    "            ts = log.get('ts', '')\n",
    "            msg = log.get('msg', '')\n",
    "            level = log.get('level', '')\n",
    "            \n",
    "            # Convert timestamp to datetime\n",
    "            try:\n",
    "                dt = datetime.fromisoformat(ts.replace('Z', '+00:00'))\n",
    "            except:\n",
    "                continue\n",
    "            \n",
    "            # Extract predictions from optimizer\n",
    "            if 'Optimization solution' in msg:\n",
    "                prediction = parse_optimization_solution(msg)\n",
    "                predictions.append({\n",
    "                    'timestamp': dt,\n",
    "                    **prediction\n",
    "                })\n",
    "            \n",
    "            # Extract scaling decisions\n",
    "            elif 'Processing decision' in msg:\n",
    "                current_match = re.search(r'current=([0-9]+)', msg)\n",
    "                target_match = re.search(r'target=([0-9]+)', msg)\n",
    "                action_match = re.search(r'action=([a-z-]+)', msg)\n",
    "                \n",
    "                scaling_decisions.append({\n",
    "                    'timestamp': dt,\n",
    "                    'current_replicas': int(current_match.group(1)) if current_match else None,\n",
    "                    'target_replicas': int(target_match.group(1)) if target_match else None,\n",
    "                    'action': action_match.group(1) if action_match else None\n",
    "                })\n",
    "            \n",
    "            # Extract SLO values\n",
    "            elif 'Found SLO for model' in msg:\n",
    "                slo_itl_match = re.search(r'slo-tpot=([0-9]+)', msg)\n",
    "                slo_ttft_match = re.search(r'slo-ttft=([0-9]+)', msg)\n",
    "                \n",
    "                slo_values.append({\n",
    "                    'timestamp': dt,\n",
    "                    'slo_itl': int(slo_itl_match.group(1)) if slo_itl_match else None,\n",
    "                    'slo_ttft': int(slo_ttft_match.group(1)) if slo_ttft_match else None\n",
    "                })\n",
    "            \n",
    "            # Extract observed metrics (from Prometheus)\n",
    "            # Note: These come from the prepareVariantAutoscalings function\n",
    "            elif '✓ Metrics collected for VA' in msg:\n",
    "                replicas_match = re.search(r'replicas=([0-9]+)', msg)\n",
    "                ttft_match = re.search(r'ttft=([0-9.]+)', msg)\n",
    "                itl_match = re.search(r'itl=([0-9.]+)', msg)\n",
    "                \n",
    "                # Extract values and remove 'ms' suffix if present\n",
    "                ttft_str = ttft_match.group(1) if ttft_match else None\n",
    "                itl_str = itl_match.group(1) if itl_match else None\n",
    "                \n",
    "                if ttft_str:\n",
    "                    ttft_val = float(ttft_str.replace('ms', ''))\n",
    "                else:\n",
    "                    ttft_val = None\n",
    "                    \n",
    "                if itl_str:\n",
    "                    itl_val = float(itl_str.replace('ms', ''))\n",
    "                else:\n",
    "                    itl_val = None\n",
    "                \n",
    "                observations.append({\n",
    "                    'timestamp': dt,\n",
    "                    'observed_replicas': int(replicas_match.group(1)) if replicas_match else None,\n",
    "                    'observed_ttft': ttft_val,\n",
    "                    'observed_itl': itl_val\n",
    "                })\n",
    "                    \n",
    "        except json.JSONDecodeError:\n",
    "            continue\n",
    "        except Exception as e:\n",
    "            print(f\"Error parsing line {line_num}: {e}\")\n",
    "            continue\n",
    "\n",
    "# Convert to DataFrames\n",
    "df_predictions = pd.DataFrame(predictions)\n",
    "df_observations = pd.DataFrame(observations)\n",
    "df_scaling = pd.DataFrame(scaling_decisions)\n",
    "df_slo = pd.DataFrame(slo_values)\n",
    "\n",
    "print(f\"\\n✓ Parsed {len(df_predictions)} predictions\")\n",
    "print(f\"✓ Parsed {len(df_observations)} observations\")\n",
    "print(f\"✓ Parsed {len(df_scaling)} scaling decisions\")\n",
    "print(f\"✓ Parsed {len(df_slo)} SLO entries\")\n",
    "\n",
    "# Show samples\n",
    "if len(df_predictions) > 0:\n",
    "    print(\"\\nSample prediction:\")\n",
    "    print(df_predictions.head(1).to_string())\n",
    "\n",
    "if len(df_observations) > 0:\n",
    "    print(\"\\nSample observation:\")\n",
    "    print(df_observations.head(1).to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "096003db",
   "metadata": {},
   "source": [
    "## 4. Align Predictions with Observations\n",
    "\n",
    "**Key Concept**: Prediction at reconciliation N applies to reconciliation N+1.\n",
    "\n",
    "- Optimizer runs at time T → predicts ITL/TTFT\n",
    "- Those predictions are for the NEXT reconciliation\n",
    "- Observed metrics at time T+interval reflect what the previous prediction estimated\n",
    "\n",
    "We align predictions with the NEXT observation to measure accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8b4a00c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort by timestamp\n",
    "df_predictions = df_predictions.sort_values('timestamp').reset_index(drop=True)\n",
    "df_observations = df_observations.sort_values('timestamp').reset_index(drop=True)\n",
    "df_scaling = df_scaling.sort_values('timestamp').reset_index(drop=True)\n",
    "\n",
    "# Align predictions with next observation\n",
    "aligned_data = []\n",
    "\n",
    "for i in range(len(df_predictions)):\n",
    "    pred_row = df_predictions.iloc[i]\n",
    "    pred_time = pred_row['timestamp']\n",
    "    \n",
    "    # Find the next observation after this prediction\n",
    "    future_obs = df_observations[df_observations['timestamp'] > pred_time]\n",
    "    \n",
    "    if len(future_obs) > 0:\n",
    "        obs_row = future_obs.iloc[0]\n",
    "        \n",
    "        # Compute prediction error\n",
    "        pred_error_itl = None\n",
    "        pred_error_ttft = None\n",
    "        \n",
    "        if pred_row['predicted_itl'] is not None and obs_row['observed_itl'] is not None:\n",
    "            pred_error_itl = obs_row['observed_itl'] - pred_row['predicted_itl']\n",
    "        \n",
    "        if pred_row['predicted_ttft'] is not None and obs_row['observed_ttft'] is not None:\n",
    "            pred_error_ttft = obs_row['observed_ttft'] - pred_row['predicted_ttft']\n",
    "        \n",
    "        aligned_data.append({\n",
    "            'prediction_time': pred_time,\n",
    "            'observation_time': obs_row['timestamp'],\n",
    "            'time_delta_seconds': (obs_row['timestamp'] - pred_time).total_seconds(),\n",
    "            'predicted_itl': pred_row['predicted_itl'],\n",
    "            'observed_itl': obs_row['observed_itl'],\n",
    "            'pred_error_itl': pred_error_itl,\n",
    "            'predicted_ttft': pred_row['predicted_ttft'],\n",
    "            'observed_ttft': obs_row['observed_ttft'],\n",
    "            'pred_error_ttft': pred_error_ttft,\n",
    "            'predicted_replicas': pred_row['predicted_replicas'],\n",
    "            'observed_replicas': obs_row['observed_replicas'],\n",
    "        })\n",
    "\n",
    "df_aligned = pd.DataFrame(aligned_data)\n",
    "\n",
    "print(f\"✓ Created {len(df_aligned)} aligned prediction-observation pairs\")\n",
    "print(f\"\\nSample aligned data:\")\n",
    "if len(df_aligned) > 0:\n",
    "    print(df_aligned.head(3).to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2827b62",
   "metadata": {},
   "source": [
    "## 5. Add SLO Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e076e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get SLO values (should be constant throughout experiment)\n",
    "if len(df_slo) > 0:\n",
    "    SLO_ITL = df_slo['slo_itl'].iloc[0]\n",
    "    SLO_TTFT = df_slo['slo_ttft'].iloc[0]\n",
    "else:\n",
    "    # Default values\n",
    "    SLO_ITL = 10\n",
    "    SLO_TTFT = 1000\n",
    "\n",
    "print(f\"SLO ITL: {SLO_ITL} ms\")\n",
    "print(f\"SLO TTFT: {SLO_TTFT} ms\")\n",
    "\n",
    "# Add SLO violation flags to aligned data\n",
    "if len(df_aligned) > 0:\n",
    "    df_aligned['slo_itl_violated'] = df_aligned['observed_itl'] > SLO_ITL\n",
    "    df_aligned['slo_ttft_violated'] = df_aligned['observed_ttft'] > SLO_TTFT\n",
    "    df_aligned['any_slo_violated'] = df_aligned['slo_itl_violated'] | df_aligned['slo_ttft_violated']\n",
    "    \n",
    "    violation_count = df_aligned['any_slo_violated'].sum()\n",
    "    violation_pct = (violation_count / len(df_aligned)) * 100\n",
    "    \n",
    "    print(f\"\\nSLO Violations: {violation_count}/{len(df_aligned)} ({violation_pct:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54d46765",
   "metadata": {},
   "source": [
    "## 6. Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bc9e5c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(df_aligned) > 0:\n",
    "    print(\"=\"*60)\n",
    "    print(\"PREDICTION ACCURACY SUMMARY\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    print(\"\\nITL (Inter-Token Latency) Predictions:\")\n",
    "    print(f\"  Mean Error: {df_aligned['pred_error_itl'].mean():.2f} ms\")\n",
    "    print(f\"  Abs Mean Error: {df_aligned['pred_error_itl'].abs().mean():.2f} ms\")\n",
    "    print(f\"  Std Dev: {df_aligned['pred_error_itl'].std():.2f} ms\")\n",
    "    print(f\"  Min Error: {df_aligned['pred_error_itl'].min():.2f} ms\")\n",
    "    print(f\"  Max Error: {df_aligned['pred_error_itl'].max():.2f} ms\")\n",
    "    \n",
    "    print(\"\\nTTFT (Time to First Token) Predictions:\")\n",
    "    print(f\"  Mean Error: {df_aligned['pred_error_ttft'].mean():.2f} ms\")\n",
    "    print(f\"  Abs Mean Error: {df_aligned['pred_error_ttft'].abs().mean():.2f} ms\")\n",
    "    print(f\"  Std Dev: {df_aligned['pred_error_ttft'].std():.2f} ms\")\n",
    "    print(f\"  Min Error: {df_aligned['pred_error_ttft'].min():.2f} ms\")\n",
    "    print(f\"  Max Error: {df_aligned['pred_error_ttft'].max():.2f} ms\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"SCALING BEHAVIOR SUMMARY\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    if len(df_scaling) > 0:\n",
    "        action_counts = df_scaling['action'].value_counts()\n",
    "        print(\"\\nScaling Actions:\")\n",
    "        for action, count in action_counts.items():\n",
    "            pct = (count / len(df_scaling)) * 100\n",
    "            print(f\"  {action}: {count} ({pct:.1f}%)\")\n",
    "        \n",
    "        print(f\"\\nReplica Range: {df_scaling['current_replicas'].min()} - {df_scaling['current_replicas'].max()}\")\n",
    "else:\n",
    "    print(\"No aligned data available for statistics\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e12fbfbb",
   "metadata": {},
   "source": [
    "## 7. Visualization: Prediction vs Observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "806e1983",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create plots directory if it doesn't exist\n",
    "plots_dir = EXPERIMENT_DIR / 'plots'\n",
    "plots_dir.mkdir(exist_ok=True)\n",
    "print(f\"✓ Plots directory ready: {plots_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "974bf8dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(df_aligned) > 0:\n",
    "    fig, axes = plt.subplots(2, 1, figsize=(14, 10))\n",
    "    \n",
    "    # Calculate relative times (minutes from start)\n",
    "    start_time = df_aligned['prediction_time'].min()\n",
    "    df_aligned['time_minutes'] = (df_aligned['prediction_time'] - start_time).dt.total_seconds() / 60\n",
    "    \n",
    "    # ITL Plot\n",
    "    ax = axes[0]\n",
    "    ax.plot(df_aligned['time_minutes'], df_aligned['predicted_itl'], \n",
    "            'b-o', label='Predicted ITL', markersize=4, linewidth=1.5)\n",
    "    ax.plot(df_aligned['time_minutes'], df_aligned['observed_itl'], \n",
    "            'r-s', label='Observed ITL', markersize=4, linewidth=1.5)\n",
    "    ax.axhline(y=SLO_ITL, color='orange', linestyle='--', label=f'SLO ITL ({SLO_ITL}ms)', linewidth=2)\n",
    "    \n",
    "    # Highlight SLO violations\n",
    "    violations = df_aligned[df_aligned['slo_itl_violated']]\n",
    "    if len(violations) > 0:\n",
    "        ax.scatter(violations['time_minutes'], violations['observed_itl'], \n",
    "                   color='red', s=100, marker='x', linewidth=3, label='SLO Violation', zorder=5)\n",
    "    \n",
    "    ax.set_xlabel('Time (minutes)', fontsize=12)\n",
    "    ax.set_ylabel('ITL (ms)', fontsize=12)\n",
    "    ax.set_title('Inter-Token Latency: Predicted vs Observed', fontsize=14, fontweight='bold')\n",
    "    ax.legend(loc='best', fontsize=10)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # TTFT Plot\n",
    "    ax = axes[1]\n",
    "    ax.plot(df_aligned['time_minutes'], df_aligned['predicted_ttft'], \n",
    "            'b-o', label='Predicted TTFT', markersize=4, linewidth=1.5)\n",
    "    ax.plot(df_aligned['time_minutes'], df_aligned['observed_ttft'], \n",
    "            'r-s', label='Observed TTFT', markersize=4, linewidth=1.5)\n",
    "    ax.axhline(y=SLO_TTFT, color='orange', linestyle='--', label=f'SLO TTFT ({SLO_TTFT}ms)', linewidth=2)\n",
    "    \n",
    "    # Highlight SLO violations\n",
    "    violations = df_aligned[df_aligned['slo_ttft_violated']]\n",
    "    if len(violations) > 0:\n",
    "        ax.scatter(violations['time_minutes'], violations['observed_ttft'], \n",
    "                   color='red', s=100, marker='x', linewidth=3, label='SLO Violation', zorder=5)\n",
    "    \n",
    "    ax.set_xlabel('Time (minutes)', fontsize=12)\n",
    "    ax.set_ylabel('TTFT (ms)', fontsize=12)\n",
    "    ax.set_title('Time to First Token: Predicted vs Observed', fontsize=14, fontweight='bold')\n",
    "    ax.legend(loc='best', fontsize=10)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(EXPERIMENT_DIR / 'plots' / 'prediction_vs_observation.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No data to plot\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2511ffa",
   "metadata": {},
   "source": [
    "## 8. Visualization: Scaling Behavior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5217b61e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(df_scaling) > 0:\n",
    "    fig, ax = plt.subplots(figsize=(14, 6))\n",
    "    \n",
    "    # Calculate relative times\n",
    "    start_time = df_scaling['timestamp'].min()\n",
    "    df_scaling['time_minutes'] = (df_scaling['timestamp'] - start_time).dt.total_seconds() / 60\n",
    "    \n",
    "    # Plot current and target replicas\n",
    "    ax.plot(df_scaling['time_minutes'], df_scaling['current_replicas'], \n",
    "            'b-o', label='Current Replicas', markersize=6, linewidth=2)\n",
    "    ax.plot(df_scaling['time_minutes'], df_scaling['target_replicas'], \n",
    "            'r--s', label='Target Replicas', markersize=6, linewidth=2, alpha=0.7)\n",
    "    \n",
    "    # Annotate scaling actions\n",
    "    scale_up = df_scaling[df_scaling['action'] == 'scale-up']\n",
    "    scale_down = df_scaling[df_scaling['action'] == 'scale-down']\n",
    "    \n",
    "    if len(scale_up) > 0:\n",
    "        ax.scatter(scale_up['time_minutes'], scale_up['target_replicas'], \n",
    "                   color='green', s=150, marker='^', label='Scale Up', zorder=5)\n",
    "    \n",
    "    if len(scale_down) > 0:\n",
    "        ax.scatter(scale_down['time_minutes'], scale_down['target_replicas'], \n",
    "                   color='orange', s=150, marker='v', label='Scale Down', zorder=5)\n",
    "    \n",
    "    ax.set_xlabel('Time (minutes)', fontsize=12)\n",
    "    ax.set_ylabel('Number of Replicas', fontsize=12)\n",
    "    ax.set_title('WVA Model-Based Scaling Behavior', fontsize=14, fontweight='bold')\n",
    "    ax.legend(loc='best', fontsize=11)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.set_ylim(bottom=0)\n",
    "    \n",
    "    # Use integer y-axis\n",
    "    ax.yaxis.set_major_locator(plt.MaxNLocator(integer=True))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(EXPERIMENT_DIR / 'plots' / 'scaling_behavior.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No scaling data to plot\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbe57d3a",
   "metadata": {},
   "source": [
    "## 9. Visualization: Prediction Error Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5d7b0c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(df_aligned) > 0:\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # ITL Error Distribution\n",
    "    ax = axes[0]\n",
    "    errors = df_aligned['pred_error_itl'].dropna()\n",
    "    if len(errors) > 0:\n",
    "        ax.hist(errors, bins=20, color='steelblue', edgecolor='black', alpha=0.7)\n",
    "        ax.axvline(x=0, color='red', linestyle='--', linewidth=2, label='Perfect Prediction')\n",
    "        ax.axvline(x=errors.mean(), color='orange', linestyle='--', linewidth=2, \n",
    "                   label=f'Mean Error: {errors.mean():.2f}ms')\n",
    "        ax.set_xlabel('Prediction Error (ms)', fontsize=12)\n",
    "        ax.set_ylabel('Frequency', fontsize=12)\n",
    "        ax.set_title('ITL Prediction Error Distribution', fontsize=13, fontweight='bold')\n",
    "        ax.legend(fontsize=10)\n",
    "        ax.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # TTFT Error Distribution\n",
    "    ax = axes[1]\n",
    "    errors = df_aligned['pred_error_ttft'].dropna()\n",
    "    if len(errors) > 0:\n",
    "        ax.hist(errors, bins=20, color='steelblue', edgecolor='black', alpha=0.7)\n",
    "        ax.axvline(x=0, color='red', linestyle='--', linewidth=2, label='Perfect Prediction')\n",
    "        ax.axvline(x=errors.mean(), color='orange', linestyle='--', linewidth=2, \n",
    "                   label=f'Mean Error: {errors.mean():.2f}ms')\n",
    "        ax.set_xlabel('Prediction Error (ms)', fontsize=12)\n",
    "        ax.set_ylabel('Frequency', fontsize=12)\n",
    "        ax.set_title('TTFT Prediction Error Distribution', fontsize=13, fontweight='bold')\n",
    "        ax.legend(fontsize=10)\n",
    "        ax.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(EXPERIMENT_DIR / 'plots' / 'prediction_error_distribution.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No data to plot\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69174f0d",
   "metadata": {},
   "source": [
    "## 10. Export Processed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "249db14e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save processed data\n",
    "if len(df_aligned) > 0:\n",
    "    output_file = EXPERIMENT_DIR / 'processed_data.csv'\n",
    "    df_aligned.to_csv(output_file, index=False)\n",
    "    print(f\"✓ Saved processed data to: {output_file}\")\n",
    "\n",
    "# Create summary report\n",
    "if len(df_aligned) > 0:\n",
    "    summary_file = EXPERIMENT_DIR / 'ANALYSIS_SUMMARY.md'\n",
    "    \n",
    "    with open(summary_file, 'w') as f:\n",
    "        f.write(f\"# WVA Model-Based Experiment Analysis\\n\\n\")\n",
    "        f.write(f\"**Experiment:** {EXPERIMENT_DIR.name}\\n\\n\")\n",
    "        f.write(f\"**Generated:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\\n\")\n",
    "        f.write(f\"---\\n\\n\")\n",
    "        \n",
    "        f.write(f\"## Summary Statistics\\n\\n\")\n",
    "        f.write(f\"- **Total Predictions:** {len(df_predictions)}\\n\")\n",
    "        f.write(f\"- **Total Observations:** {len(df_observations)}\\n\")\n",
    "        f.write(f\"- **Aligned Pairs:** {len(df_aligned)}\\n\")\n",
    "        f.write(f\"- **Scaling Decisions:** {len(df_scaling)}\\n\\n\")\n",
    "        \n",
    "        f.write(f\"## SLO Configuration\\n\\n\")\n",
    "        f.write(f\"- **ITL SLO:** {SLO_ITL} ms\\n\")\n",
    "        f.write(f\"- **TTFT SLO:** {SLO_TTFT} ms\\n\\n\")\n",
    "        \n",
    "        violation_count = df_aligned['any_slo_violated'].sum()\n",
    "        violation_pct = (violation_count / len(df_aligned)) * 100\n",
    "        f.write(f\"- **SLO Violations:** {violation_count}/{len(df_aligned)} ({violation_pct:.1f}%)\\n\\n\")\n",
    "        \n",
    "        f.write(f\"## Prediction Accuracy\\n\\n\")\n",
    "        f.write(f\"### ITL Predictions\\n\\n\")\n",
    "        f.write(f\"- Mean Error: {df_aligned['pred_error_itl'].mean():.2f} ms\\n\")\n",
    "        f.write(f\"- Abs Mean Error: {df_aligned['pred_error_itl'].abs().mean():.2f} ms\\n\")\n",
    "        f.write(f\"- Std Dev: {df_aligned['pred_error_itl'].std():.2f} ms\\n\\n\")\n",
    "        \n",
    "        f.write(f\"### TTFT Predictions\\n\\n\")\n",
    "        f.write(f\"- Mean Error: {df_aligned['pred_error_ttft'].mean():.2f} ms\\n\")\n",
    "        f.write(f\"- Abs Mean Error: {df_aligned['pred_error_ttft'].abs().mean():.2f} ms\\n\")\n",
    "        f.write(f\"- Std Dev: {df_aligned['pred_error_ttft'].std():.2f} ms\\n\\n\")\n",
    "        \n",
    "        if len(df_scaling) > 0:\n",
    "            f.write(f\"## Scaling Behavior\\n\\n\")\n",
    "            action_counts = df_scaling['action'].value_counts()\n",
    "            for action, count in action_counts.items():\n",
    "                pct = (count / len(df_scaling)) * 100\n",
    "                f.write(f\"- {action}: {count} ({pct:.1f}%)\\n\")\n",
    "            f.write(f\"\\n- Replica Range: {df_scaling['current_replicas'].min()} - {df_scaling['current_replicas'].max()}\\n\\n\")\n",
    "        \n",
    "        f.write(f\"## Files Generated\\n\\n\")\n",
    "        f.write(f\"- `processed_data.csv` - Aligned prediction-observation pairs\\n\")\n",
    "        f.write(f\"- `plots/prediction_vs_observation.png` - ITL/TTFT over time\\n\")\n",
    "        f.write(f\"- `plots/scaling_behavior.png` - Replica scaling timeline\\n\")\n",
    "        f.write(f\"- `plots/prediction_error_distribution.png` - Error histograms\\n\")\n",
    "    \n",
    "    print(f\"✓ Saved summary report to: {summary_file}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ANALYSIS COMPLETE\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Results saved to: {EXPERIMENT_DIR}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
