{"level":"INFO","ts":"2025-12-10T13:10:13.809Z","msg":"Reconciling VariantAutoscaling: name=ms-inference-scheduling-llm-d-modelservice-decode, namespace=llm-d-inference-scheduler, modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-10T13:10:13.809Z","msg":"saturation scaling config cache updated: entries=1, has_default=true"}
{"level":"INFO","ts":"2025-12-10T13:10:13.809Z","msg":"saturation scaling configuration loaded successfully"}
{"level":"INFO","ts":"2025-12-10T13:10:13.809Z","msg":"Operating in MODEL-ONLY mode: model-based optimization only"}
{"level":"INFO","ts":"2025-12-10T13:10:13.809Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-12-10T13:10:13.809Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-12-10T13:10:13.822Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dgftqv, usage=0.000 (0.0%)"}
{"level":"DEBUG","ts":"2025-12-10T13:10:13.822Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"INFO","ts":"2025-12-10T13:10:13.822Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dgftqv, queueLength=0"}
{"level":"DEBUG","ts":"2025-12-10T13:10:13.822Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"DEBUG","ts":"2025-12-10T13:10:13.825Z","msg":"Pod-to-variant matching successful: totalPods=1, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"DEBUG","ts":"2025-12-10T13:10:13.825Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=1"}
{"level":"DEBUG","ts":"2025-12-10T13:10:13.825Z","msg":"Collected capacity metrics for observability in model-only mode: modelID=unsloth/Meta-Llama-3.1-8B, replicaCount=1"}
{"level":"DEBUG","ts":"2025-12-10T13:10:13.840Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=1, accelerator=H100, ttft=0.00ms, itl=0.00ms, cost=100.00, maxBatch=256, arrivalRate=0.00, avgInputTokens=0.00, avgOutputTokens=0.00"}
{"level":"DEBUG","ts":"2025-12-10T13:10:13.840Z","msg":"Experimental model tuner is enabled globally: tuning model performance parameters for active VAs"}
{"level":"DEBUG","ts":"2025-12-10T13:10:13.840Z","msg":"Using state vals from VA status to tune variant ms-inference-scheduling-llm-d-modelservice-decode: alpha= 6.921327, beta= 0.053938, gamma= 16.515985, delta= 0.000257"}
{"level":"DEBUG","ts":"2025-12-10T13:10:13.840Z","msg":"Skipping tuning for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler due to insufficient load. Using fallback parameters."}
{"level":"INFO","ts":"2025-12-10T13:10:13.840Z","msg":"Using tuned parameters from status for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler"}
{"level":"DEBUG","ts":"2025-12-10T13:10:13.840Z","msg":"Updated SystemData for model=unsloth/Meta-Llama-3.1-8B, accelerator=H100: alpha=6.921327, beta=0.053938, gamma=16.515985, delta=0.000257"}
{"level":"DEBUG","ts":"2025-12-10T13:10:13.840Z","msg":"Optimization solution - system: Solution: \ns=ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler; c=Premium; m=unsloth/Meta-Llama-3.1-8B; rate=0; inTk=0; outTk=0; sol=1, sat=false, alloc={acc=H100; numRep=1; maxBatch=512; cost=100, val=0, itl=6.975265, ttft=16.516243, rho=0, maxRPM=601717.9}; slo-itl=10, slo-ttft=1000, slo-tps=0 \nAllocationByType: \nname=NVIDIA-H100-80GB-HBM3, count=1, limit=0, cost=100 \ntotalCost=100 \n"}
{"level":"DEBUG","ts":"2025-12-10T13:10:13.840Z","msg":"Setting accelerator name: Name=H100, allocationData={H100 1 512 100 6.975265 16.516243 {0 0 0}}"}
{"level":"INFO","ts":"2025-12-10T13:10:13.840Z","msg":"Model-based optimization completed for model: unsloth/Meta-Llama-3.1-8B - model-based targets: map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"WARN","ts":"2025-12-10T13:10:13.840Z","msg":"saturation analysis unavailable, using model-based targets only: modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-10T13:10:13.840Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-12-10T13:10:13.840Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1→target=1"}
{"level":"INFO","ts":"2025-12-10T13:10:13.840Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 1, desired-replicas: 1, accelerator: H100"}
{"level":"INFO","ts":"2025-12-10T13:10:13.840Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=1, accelerator=H100, SaturationOnly=false"}
{"level":"INFO","ts":"2025-12-10T13:10:13.846Z","msg":"Applied Saturation decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1, target=1, reason=model-based only (Saturation unavailable)"}
{"level":"INFO","ts":"2025-12-10T13:10:13.846Z","msg":"Reconciliation completed successfully: mode=model-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-12-10T13:11:13.847Z","msg":"Reconciling VariantAutoscaling: name=ms-inference-scheduling-llm-d-modelservice-decode, namespace=llm-d-inference-scheduler, modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-10T13:11:13.847Z","msg":"saturation scaling config cache updated: entries=1, has_default=true"}
{"level":"INFO","ts":"2025-12-10T13:11:13.847Z","msg":"saturation scaling configuration loaded successfully"}
{"level":"INFO","ts":"2025-12-10T13:11:13.847Z","msg":"Operating in MODEL-ONLY mode: model-based optimization only"}
{"level":"INFO","ts":"2025-12-10T13:11:13.847Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-12-10T13:11:13.847Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-12-10T13:11:13.853Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dgftqv, usage=0.000 (0.0%)"}
{"level":"DEBUG","ts":"2025-12-10T13:11:13.853Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"INFO","ts":"2025-12-10T13:11:13.853Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dgftqv, queueLength=0"}
{"level":"DEBUG","ts":"2025-12-10T13:11:13.853Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"DEBUG","ts":"2025-12-10T13:11:13.855Z","msg":"Pod-to-variant matching successful: totalPods=1, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"DEBUG","ts":"2025-12-10T13:11:13.855Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=1"}
{"level":"DEBUG","ts":"2025-12-10T13:11:13.855Z","msg":"Collected capacity metrics for observability in model-only mode: modelID=unsloth/Meta-Llama-3.1-8B, replicaCount=1"}
{"level":"DEBUG","ts":"2025-12-10T13:11:13.867Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=1, accelerator=H100, ttft=0.00ms, itl=0.00ms, cost=100.00, maxBatch=256, arrivalRate=0.00, avgInputTokens=0.00, avgOutputTokens=0.00"}
{"level":"DEBUG","ts":"2025-12-10T13:11:13.867Z","msg":"Experimental model tuner is enabled globally: tuning model performance parameters for active VAs"}
{"level":"DEBUG","ts":"2025-12-10T13:11:13.867Z","msg":"Using state vals from VA status to tune variant ms-inference-scheduling-llm-d-modelservice-decode: alpha= 6.921327, beta= 0.053938, gamma= 16.515985, delta= 0.000257"}
{"level":"DEBUG","ts":"2025-12-10T13:11:13.867Z","msg":"Skipping tuning for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler due to insufficient load. Using fallback parameters."}
{"level":"INFO","ts":"2025-12-10T13:11:13.867Z","msg":"Using tuned parameters from status for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler"}
{"level":"DEBUG","ts":"2025-12-10T13:11:13.867Z","msg":"Updated SystemData for model=unsloth/Meta-Llama-3.1-8B, accelerator=H100: alpha=6.921327, beta=0.053938, gamma=16.515985, delta=0.000257"}
{"level":"DEBUG","ts":"2025-12-10T13:11:13.867Z","msg":"Optimization solution - system: Solution: \ns=ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler; c=Premium; m=unsloth/Meta-Llama-3.1-8B; rate=0; inTk=0; outTk=0; sol=1, sat=false, alloc={acc=H100; numRep=1; maxBatch=512; cost=100, val=0, itl=6.975265, ttft=16.516243, rho=0, maxRPM=601717.9}; slo-itl=10, slo-ttft=1000, slo-tps=0 \nAllocationByType: \nname=NVIDIA-H100-80GB-HBM3, count=1, limit=0, cost=100 \ntotalCost=100 \n"}
{"level":"DEBUG","ts":"2025-12-10T13:11:13.867Z","msg":"Setting accelerator name: Name=H100, allocationData={H100 1 512 100 6.975265 16.516243 {0 0 0}}"}
{"level":"INFO","ts":"2025-12-10T13:11:13.867Z","msg":"Model-based optimization completed for model: unsloth/Meta-Llama-3.1-8B - model-based targets: map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"WARN","ts":"2025-12-10T13:11:13.867Z","msg":"saturation analysis unavailable, using model-based targets only: modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-10T13:11:13.867Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-12-10T13:11:13.867Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1→target=1"}
{"level":"INFO","ts":"2025-12-10T13:11:13.867Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 1, desired-replicas: 1, accelerator: H100"}
{"level":"INFO","ts":"2025-12-10T13:11:13.867Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=1, accelerator=H100, SaturationOnly=false"}
{"level":"INFO","ts":"2025-12-10T13:11:13.886Z","msg":"Applied Saturation decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1, target=1, reason=model-based only (Saturation unavailable)"}
{"level":"INFO","ts":"2025-12-10T13:11:13.887Z","msg":"Reconciliation completed successfully: mode=model-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-12-10T13:12:13.887Z","msg":"Reconciling VariantAutoscaling: name=ms-inference-scheduling-llm-d-modelservice-decode, namespace=llm-d-inference-scheduler, modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-10T13:12:13.887Z","msg":"saturation scaling config cache updated: entries=1, has_default=true"}
{"level":"INFO","ts":"2025-12-10T13:12:13.887Z","msg":"saturation scaling configuration loaded successfully"}
{"level":"INFO","ts":"2025-12-10T13:12:13.887Z","msg":"Operating in MODEL-ONLY mode: model-based optimization only"}
{"level":"INFO","ts":"2025-12-10T13:12:13.887Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-12-10T13:12:13.887Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-12-10T13:12:13.899Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dgftqv, usage=0.000 (0.0%)"}
{"level":"DEBUG","ts":"2025-12-10T13:12:13.899Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"INFO","ts":"2025-12-10T13:12:13.899Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dgftqv, queueLength=0"}
{"level":"DEBUG","ts":"2025-12-10T13:12:13.899Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"DEBUG","ts":"2025-12-10T13:12:13.901Z","msg":"Pod-to-variant matching successful: totalPods=1, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"DEBUG","ts":"2025-12-10T13:12:13.901Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=1"}
{"level":"DEBUG","ts":"2025-12-10T13:12:13.901Z","msg":"Collected capacity metrics for observability in model-only mode: modelID=unsloth/Meta-Llama-3.1-8B, replicaCount=1"}
{"level":"DEBUG","ts":"2025-12-10T13:12:13.913Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=1, accelerator=H100, ttft=0.00ms, itl=0.00ms, cost=100.00, maxBatch=256, arrivalRate=0.00, avgInputTokens=0.00, avgOutputTokens=0.00"}
{"level":"DEBUG","ts":"2025-12-10T13:12:13.913Z","msg":"Experimental model tuner is enabled globally: tuning model performance parameters for active VAs"}
{"level":"DEBUG","ts":"2025-12-10T13:12:13.913Z","msg":"Using state vals from VA status to tune variant ms-inference-scheduling-llm-d-modelservice-decode: alpha= 6.921327, beta= 0.053938, gamma= 16.515985, delta= 0.000257"}
{"level":"DEBUG","ts":"2025-12-10T13:12:13.913Z","msg":"Skipping tuning for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler due to insufficient load. Using fallback parameters."}
{"level":"INFO","ts":"2025-12-10T13:12:13.913Z","msg":"Using tuned parameters from status for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler"}
{"level":"DEBUG","ts":"2025-12-10T13:12:13.913Z","msg":"Updated SystemData for model=unsloth/Meta-Llama-3.1-8B, accelerator=H100: alpha=6.921327, beta=0.053938, gamma=16.515985, delta=0.000257"}
{"level":"DEBUG","ts":"2025-12-10T13:12:13.913Z","msg":"Optimization solution - system: Solution: \ns=ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler; c=Premium; m=unsloth/Meta-Llama-3.1-8B; rate=0; inTk=0; outTk=0; sol=1, sat=false, alloc={acc=H100; numRep=1; maxBatch=512; cost=100, val=0, itl=6.975265, ttft=16.516243, rho=0, maxRPM=601717.9}; slo-itl=10, slo-ttft=1000, slo-tps=0 \nAllocationByType: \nname=NVIDIA-H100-80GB-HBM3, count=1, limit=0, cost=100 \ntotalCost=100 \n"}
{"level":"DEBUG","ts":"2025-12-10T13:12:13.913Z","msg":"Setting accelerator name: Name=H100, allocationData={H100 1 512 100 6.975265 16.516243 {0 0 0}}"}
{"level":"INFO","ts":"2025-12-10T13:12:13.913Z","msg":"Model-based optimization completed for model: unsloth/Meta-Llama-3.1-8B - model-based targets: map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"WARN","ts":"2025-12-10T13:12:13.913Z","msg":"saturation analysis unavailable, using model-based targets only: modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-10T13:12:13.913Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-12-10T13:12:13.913Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1→target=1"}
{"level":"INFO","ts":"2025-12-10T13:12:13.913Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 1, desired-replicas: 1, accelerator: H100"}
{"level":"INFO","ts":"2025-12-10T13:12:13.913Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=1, accelerator=H100, SaturationOnly=false"}
{"level":"INFO","ts":"2025-12-10T13:12:13.920Z","msg":"Applied Saturation decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1, target=1, reason=model-based only (Saturation unavailable)"}
{"level":"INFO","ts":"2025-12-10T13:12:13.920Z","msg":"Reconciliation completed successfully: mode=model-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-12-10T13:13:13.921Z","msg":"Reconciling VariantAutoscaling: name=ms-inference-scheduling-llm-d-modelservice-decode, namespace=llm-d-inference-scheduler, modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-10T13:13:13.921Z","msg":"saturation scaling config cache updated: entries=1, has_default=true"}
{"level":"INFO","ts":"2025-12-10T13:13:13.921Z","msg":"saturation scaling configuration loaded successfully"}
{"level":"INFO","ts":"2025-12-10T13:13:13.921Z","msg":"Operating in MODEL-ONLY mode: model-based optimization only"}
{"level":"INFO","ts":"2025-12-10T13:13:13.921Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-12-10T13:13:13.921Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-12-10T13:13:13.927Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dgftqv, queueLength=0"}
{"level":"DEBUG","ts":"2025-12-10T13:13:13.927Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"INFO","ts":"2025-12-10T13:13:13.927Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dgftqv, usage=0.000 (0.0%)"}
{"level":"DEBUG","ts":"2025-12-10T13:13:13.927Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"DEBUG","ts":"2025-12-10T13:13:13.929Z","msg":"Pod-to-variant matching successful: totalPods=1, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"DEBUG","ts":"2025-12-10T13:13:13.929Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=1"}
{"level":"DEBUG","ts":"2025-12-10T13:13:13.929Z","msg":"Collected capacity metrics for observability in model-only mode: modelID=unsloth/Meta-Llama-3.1-8B, replicaCount=1"}
{"level":"DEBUG","ts":"2025-12-10T13:13:13.942Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=1, accelerator=H100, ttft=0.00ms, itl=0.00ms, cost=100.00, maxBatch=256, arrivalRate=0.00, avgInputTokens=0.00, avgOutputTokens=0.00"}
{"level":"DEBUG","ts":"2025-12-10T13:13:13.942Z","msg":"Experimental model tuner is enabled globally: tuning model performance parameters for active VAs"}
{"level":"DEBUG","ts":"2025-12-10T13:13:13.942Z","msg":"Using state vals from VA status to tune variant ms-inference-scheduling-llm-d-modelservice-decode: alpha= 6.921327, beta= 0.053938, gamma= 16.515985, delta= 0.000257"}
{"level":"DEBUG","ts":"2025-12-10T13:13:13.942Z","msg":"Skipping tuning for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler due to insufficient load. Using fallback parameters."}
{"level":"INFO","ts":"2025-12-10T13:13:13.942Z","msg":"Using tuned parameters from status for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler"}
{"level":"DEBUG","ts":"2025-12-10T13:13:13.942Z","msg":"Updated SystemData for model=unsloth/Meta-Llama-3.1-8B, accelerator=H100: alpha=6.921327, beta=0.053938, gamma=16.515985, delta=0.000257"}
{"level":"DEBUG","ts":"2025-12-10T13:13:13.942Z","msg":"Optimization solution - system: Solution: \ns=ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler; c=Premium; m=unsloth/Meta-Llama-3.1-8B; rate=0; inTk=0; outTk=0; sol=1, sat=false, alloc={acc=H100; numRep=1; maxBatch=512; cost=100, val=0, itl=6.975265, ttft=16.516243, rho=0, maxRPM=601717.9}; slo-itl=10, slo-ttft=1000, slo-tps=0 \nAllocationByType: \nname=NVIDIA-H100-80GB-HBM3, count=1, limit=0, cost=100 \ntotalCost=100 \n"}
{"level":"DEBUG","ts":"2025-12-10T13:13:13.943Z","msg":"Setting accelerator name: Name=H100, allocationData={H100 1 512 100 6.975265 16.516243 {0 0 0}}"}
{"level":"INFO","ts":"2025-12-10T13:13:13.943Z","msg":"Model-based optimization completed for model: unsloth/Meta-Llama-3.1-8B - model-based targets: map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"WARN","ts":"2025-12-10T13:13:13.943Z","msg":"saturation analysis unavailable, using model-based targets only: modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-10T13:13:13.943Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-12-10T13:13:13.943Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1→target=1"}
{"level":"INFO","ts":"2025-12-10T13:13:13.943Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 1, desired-replicas: 1, accelerator: H100"}
{"level":"INFO","ts":"2025-12-10T13:13:13.943Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=1, accelerator=H100, SaturationOnly=false"}
{"level":"INFO","ts":"2025-12-10T13:13:13.956Z","msg":"Applied Saturation decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1, target=1, reason=model-based only (Saturation unavailable)"}
{"level":"INFO","ts":"2025-12-10T13:13:13.956Z","msg":"Reconciliation completed successfully: mode=model-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-12-10T13:14:13.957Z","msg":"Reconciling VariantAutoscaling: name=ms-inference-scheduling-llm-d-modelservice-decode, namespace=llm-d-inference-scheduler, modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-10T13:14:13.957Z","msg":"saturation scaling config cache updated: entries=1, has_default=true"}
{"level":"INFO","ts":"2025-12-10T13:14:13.957Z","msg":"saturation scaling configuration loaded successfully"}
{"level":"INFO","ts":"2025-12-10T13:14:13.957Z","msg":"Operating in MODEL-ONLY mode: model-based optimization only"}
{"level":"INFO","ts":"2025-12-10T13:14:13.957Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-12-10T13:14:13.957Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-12-10T13:14:13.965Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dgftqv, usage=0.000 (0.0%)"}
{"level":"INFO","ts":"2025-12-10T13:14:13.965Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dgftqv, queueLength=0"}
{"level":"DEBUG","ts":"2025-12-10T13:14:13.965Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"DEBUG","ts":"2025-12-10T13:14:13.965Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"DEBUG","ts":"2025-12-10T13:14:13.967Z","msg":"Pod-to-variant matching successful: totalPods=1, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"DEBUG","ts":"2025-12-10T13:14:13.967Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=1"}
{"level":"DEBUG","ts":"2025-12-10T13:14:13.967Z","msg":"Collected capacity metrics for observability in model-only mode: modelID=unsloth/Meta-Llama-3.1-8B, replicaCount=1"}
{"level":"DEBUG","ts":"2025-12-10T13:14:13.977Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=1, accelerator=H100, ttft=0.00ms, itl=0.00ms, cost=100.00, maxBatch=256, arrivalRate=0.00, avgInputTokens=0.00, avgOutputTokens=0.00"}
{"level":"DEBUG","ts":"2025-12-10T13:14:13.977Z","msg":"Experimental model tuner is enabled globally: tuning model performance parameters for active VAs"}
{"level":"DEBUG","ts":"2025-12-10T13:14:13.977Z","msg":"Using state vals from VA status to tune variant ms-inference-scheduling-llm-d-modelservice-decode: alpha= 6.921327, beta= 0.053938, gamma= 16.515985, delta= 0.000257"}
{"level":"DEBUG","ts":"2025-12-10T13:14:13.977Z","msg":"Skipping tuning for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler due to insufficient load. Using fallback parameters."}
{"level":"INFO","ts":"2025-12-10T13:14:13.977Z","msg":"Using tuned parameters from status for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler"}
{"level":"DEBUG","ts":"2025-12-10T13:14:13.977Z","msg":"Updated SystemData for model=unsloth/Meta-Llama-3.1-8B, accelerator=H100: alpha=6.921327, beta=0.053938, gamma=16.515985, delta=0.000257"}
{"level":"DEBUG","ts":"2025-12-10T13:14:13.977Z","msg":"Optimization solution - system: Solution: \ns=ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler; c=Premium; m=unsloth/Meta-Llama-3.1-8B; rate=0; inTk=0; outTk=0; sol=1, sat=false, alloc={acc=H100; numRep=1; maxBatch=512; cost=100, val=0, itl=6.975265, ttft=16.516243, rho=0, maxRPM=601717.9}; slo-itl=10, slo-ttft=1000, slo-tps=0 \nAllocationByType: \nname=NVIDIA-H100-80GB-HBM3, count=1, limit=0, cost=100 \ntotalCost=100 \n"}
{"level":"DEBUG","ts":"2025-12-10T13:14:13.977Z","msg":"Setting accelerator name: Name=H100, allocationData={H100 1 512 100 6.975265 16.516243 {0 0 0}}"}
{"level":"INFO","ts":"2025-12-10T13:14:13.977Z","msg":"Model-based optimization completed for model: unsloth/Meta-Llama-3.1-8B - model-based targets: map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"WARN","ts":"2025-12-10T13:14:13.977Z","msg":"saturation analysis unavailable, using model-based targets only: modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-10T13:14:13.977Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-12-10T13:14:13.977Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1→target=1"}
{"level":"INFO","ts":"2025-12-10T13:14:13.977Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 1, desired-replicas: 1, accelerator: H100"}
{"level":"INFO","ts":"2025-12-10T13:14:13.977Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=1, accelerator=H100, SaturationOnly=false"}
{"level":"INFO","ts":"2025-12-10T13:14:13.984Z","msg":"Applied Saturation decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1, target=1, reason=model-based only (Saturation unavailable)"}
{"level":"INFO","ts":"2025-12-10T13:14:13.984Z","msg":"Reconciliation completed successfully: mode=model-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-12-10T13:15:13.984Z","msg":"Reconciling VariantAutoscaling: name=ms-inference-scheduling-llm-d-modelservice-decode, namespace=llm-d-inference-scheduler, modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-10T13:15:13.984Z","msg":"saturation scaling config cache updated: entries=1, has_default=true"}
{"level":"INFO","ts":"2025-12-10T13:15:13.984Z","msg":"saturation scaling configuration loaded successfully"}
{"level":"INFO","ts":"2025-12-10T13:15:13.984Z","msg":"Operating in MODEL-ONLY mode: model-based optimization only"}
{"level":"INFO","ts":"2025-12-10T13:15:13.985Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-12-10T13:15:13.985Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-12-10T13:15:13.992Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dgftqv, queueLength=0"}
{"level":"DEBUG","ts":"2025-12-10T13:15:13.992Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"INFO","ts":"2025-12-10T13:15:13.992Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dgftqv, usage=0.000 (0.0%)"}
{"level":"DEBUG","ts":"2025-12-10T13:15:13.992Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"DEBUG","ts":"2025-12-10T13:15:13.996Z","msg":"Pod-to-variant matching successful: totalPods=1, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"DEBUG","ts":"2025-12-10T13:15:13.996Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=1"}
{"level":"DEBUG","ts":"2025-12-10T13:15:13.996Z","msg":"Collected capacity metrics for observability in model-only mode: modelID=unsloth/Meta-Llama-3.1-8B, replicaCount=1"}
{"level":"DEBUG","ts":"2025-12-10T13:15:14.007Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=1, accelerator=H100, ttft=0.00ms, itl=0.00ms, cost=100.00, maxBatch=256, arrivalRate=0.00, avgInputTokens=0.00, avgOutputTokens=0.00"}
{"level":"DEBUG","ts":"2025-12-10T13:15:14.007Z","msg":"Experimental model tuner is enabled globally: tuning model performance parameters for active VAs"}
{"level":"DEBUG","ts":"2025-12-10T13:15:14.007Z","msg":"Using state vals from VA status to tune variant ms-inference-scheduling-llm-d-modelservice-decode: alpha= 6.921327, beta= 0.053938, gamma= 16.515985, delta= 0.000257"}
{"level":"DEBUG","ts":"2025-12-10T13:15:14.007Z","msg":"Skipping tuning for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler due to insufficient load. Using fallback parameters."}
{"level":"INFO","ts":"2025-12-10T13:15:14.007Z","msg":"Using tuned parameters from status for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler"}
{"level":"DEBUG","ts":"2025-12-10T13:15:14.007Z","msg":"Updated SystemData for model=unsloth/Meta-Llama-3.1-8B, accelerator=H100: alpha=6.921327, beta=0.053938, gamma=16.515985, delta=0.000257"}
{"level":"DEBUG","ts":"2025-12-10T13:15:14.007Z","msg":"Optimization solution - system: Solution: \ns=ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler; c=Premium; m=unsloth/Meta-Llama-3.1-8B; rate=0; inTk=0; outTk=0; sol=1, sat=false, alloc={acc=H100; numRep=1; maxBatch=512; cost=100, val=0, itl=6.975265, ttft=16.516243, rho=0, maxRPM=601717.9}; slo-itl=10, slo-ttft=1000, slo-tps=0 \nAllocationByType: \nname=NVIDIA-H100-80GB-HBM3, count=1, limit=0, cost=100 \ntotalCost=100 \n"}
{"level":"DEBUG","ts":"2025-12-10T13:15:14.007Z","msg":"Setting accelerator name: Name=H100, allocationData={H100 1 512 100 6.975265 16.516243 {0 0 0}}"}
{"level":"INFO","ts":"2025-12-10T13:15:14.007Z","msg":"Model-based optimization completed for model: unsloth/Meta-Llama-3.1-8B - model-based targets: map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"WARN","ts":"2025-12-10T13:15:14.007Z","msg":"saturation analysis unavailable, using model-based targets only: modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-10T13:15:14.007Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-12-10T13:15:14.007Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1→target=1"}
{"level":"INFO","ts":"2025-12-10T13:15:14.007Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 1, desired-replicas: 1, accelerator: H100"}
{"level":"INFO","ts":"2025-12-10T13:15:14.007Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=1, accelerator=H100, SaturationOnly=false"}
{"level":"INFO","ts":"2025-12-10T13:15:14.030Z","msg":"Applied Saturation decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1, target=1, reason=model-based only (Saturation unavailable)"}
{"level":"INFO","ts":"2025-12-10T13:15:14.030Z","msg":"Reconciliation completed successfully: mode=model-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-12-10T13:16:14.030Z","msg":"Reconciling VariantAutoscaling: name=ms-inference-scheduling-llm-d-modelservice-decode, namespace=llm-d-inference-scheduler, modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-10T13:16:14.030Z","msg":"saturation scaling config cache updated: entries=1, has_default=true"}
{"level":"INFO","ts":"2025-12-10T13:16:14.030Z","msg":"saturation scaling configuration loaded successfully"}
{"level":"INFO","ts":"2025-12-10T13:16:14.030Z","msg":"Operating in MODEL-ONLY mode: model-based optimization only"}
{"level":"INFO","ts":"2025-12-10T13:16:14.030Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-12-10T13:16:14.030Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-12-10T13:16:14.040Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dgftqv, usage=0.036 (3.6%)"}
{"level":"DEBUG","ts":"2025-12-10T13:16:14.040Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"INFO","ts":"2025-12-10T13:16:14.040Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dgftqv, queueLength=0"}
{"level":"DEBUG","ts":"2025-12-10T13:16:14.040Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"DEBUG","ts":"2025-12-10T13:16:14.042Z","msg":"Pod-to-variant matching successful: totalPods=1, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"DEBUG","ts":"2025-12-10T13:16:14.042Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=1"}
{"level":"DEBUG","ts":"2025-12-10T13:16:14.042Z","msg":"Collected capacity metrics for observability in model-only mode: modelID=unsloth/Meta-Llama-3.1-8B, replicaCount=1"}
{"level":"DEBUG","ts":"2025-12-10T13:16:14.055Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=1, accelerator=H100, ttft=16.59ms, itl=7.92ms, cost=100.00, maxBatch=256, arrivalRate=121.80, avgInputTokens=298.78, avgOutputTokens=310.43"}
{"level":"DEBUG","ts":"2025-12-10T13:16:14.055Z","msg":"Experimental model tuner is enabled globally: tuning model performance parameters for active VAs"}
{"level":"DEBUG","ts":"2025-12-10T13:16:14.055Z","msg":"Using state vals from VA status to tune variant ms-inference-scheduling-llm-d-modelservice-decode: alpha= 6.921327, beta= 0.053938, gamma= 16.515985, delta= 0.000257"}
{"level":"DEBUG","ts":"2025-12-10T13:16:14.055Z","msg":"[Tuner Config] variant=ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler | Initial state: alpha=6.921327, beta=0.053938, gamma=16.515985, delta=0.000257 | Expected obs (for R): TTFT=16.59ms, ITL=7.92ms | Has covariance=true"}
{"level":"INFO","ts":"2025-12-10T13:16:14.055Z","msg":"Tuner validation succeeded for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler, server ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler - New NIS: 2.014826"}
{"level":"DEBUG","ts":"2025-12-10T13:16:14.055Z","msg":"Model tuner results: model=unsloth/Meta-Llama-3.1-8B, accelerator=H100, alpha=7.518454, beta=0.048728, gamma=16.197783, delta=0.000257, NIS=2.01"}
{"level":"DEBUG","ts":"2025-12-10T13:16:14.055Z","msg":"Updated tuner status for variant ms-inference-scheduling-llm-d-modelservice-decode: alpha=7.518454, beta=0.048728, gamma=16.197783, delta=0.000257, NIS=2.014826"}
{"level":"INFO","ts":"2025-12-10T13:16:14.055Z","msg":"Tuned performance parameters result: variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler - alpha: 7.518454, beta: 0.048728, gamma: 16.197783, delta: 0.000257"}
{"level":"DEBUG","ts":"2025-12-10T13:16:14.065Z","msg":"Optimization solution - system: Solution: \ns=ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler; c=Premium; m=unsloth/Meta-Llama-3.1-8B; rate=121.8; inTk=298; outTk=310; sol=1, sat=false, alloc={acc=H100; numRep=1; maxBatch=512; cost=100, val=0, itl=7.8074656, ttft=16.652475, rho=0.009631237, maxRPM=963.192}; slo-itl=10, slo-ttft=1000, slo-tps=0 \nAllocationByType: \nname=NVIDIA-H100-80GB-HBM3, count=1, limit=0, cost=100 \ntotalCost=100 \n"}
{"level":"DEBUG","ts":"2025-12-10T13:16:14.065Z","msg":"Setting accelerator name: Name=H100, allocationData={H100 1 512 100 7.8074656 16.652475 {121.8 298 310}}"}
{"level":"INFO","ts":"2025-12-10T13:16:14.065Z","msg":"Model-based optimization completed for model: unsloth/Meta-Llama-3.1-8B - model-based targets: map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"WARN","ts":"2025-12-10T13:16:14.065Z","msg":"saturation analysis unavailable, using model-based targets only: modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-10T13:16:14.065Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-12-10T13:16:14.065Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1→target=1"}
{"level":"INFO","ts":"2025-12-10T13:16:14.066Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 1, desired-replicas: 1, accelerator: H100"}
{"level":"INFO","ts":"2025-12-10T13:16:14.066Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=1, accelerator=H100, SaturationOnly=false"}
{"level":"INFO","ts":"2025-12-10T13:16:14.072Z","msg":"Applied Saturation decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1, target=1, reason=model-based only (Saturation unavailable)"}
{"level":"INFO","ts":"2025-12-10T13:16:14.072Z","msg":"Reconciliation completed successfully: mode=model-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-12-10T13:17:14.072Z","msg":"Reconciling VariantAutoscaling: name=ms-inference-scheduling-llm-d-modelservice-decode, namespace=llm-d-inference-scheduler, modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-10T13:17:14.072Z","msg":"saturation scaling config cache updated: entries=1, has_default=true"}
{"level":"INFO","ts":"2025-12-10T13:17:14.072Z","msg":"saturation scaling configuration loaded successfully"}
{"level":"INFO","ts":"2025-12-10T13:17:14.072Z","msg":"Operating in MODEL-ONLY mode: model-based optimization only"}
{"level":"INFO","ts":"2025-12-10T13:17:14.072Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-12-10T13:17:14.072Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-12-10T13:17:14.079Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dgftqv, queueLength=0"}
{"level":"DEBUG","ts":"2025-12-10T13:17:14.079Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"INFO","ts":"2025-12-10T13:17:14.079Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dgftqv, usage=0.073 (7.3%)"}
{"level":"DEBUG","ts":"2025-12-10T13:17:14.079Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"DEBUG","ts":"2025-12-10T13:17:14.081Z","msg":"Pod-to-variant matching successful: totalPods=1, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"DEBUG","ts":"2025-12-10T13:17:14.081Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=1"}
{"level":"DEBUG","ts":"2025-12-10T13:17:14.081Z","msg":"Collected capacity metrics for observability in model-only mode: modelID=unsloth/Meta-Llama-3.1-8B, replicaCount=1"}
{"level":"DEBUG","ts":"2025-12-10T13:17:14.097Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=1, accelerator=H100, ttft=17.41ms, itl=8.89ms, cost=100.00, maxBatch=256, arrivalRate=532.22, avgInputTokens=246.24, avgOutputTokens=431.14"}
{"level":"DEBUG","ts":"2025-12-10T13:17:14.097Z","msg":"Experimental model tuner is enabled globally: tuning model performance parameters for active VAs"}
{"level":"DEBUG","ts":"2025-12-10T13:17:14.097Z","msg":"Using state vals from VA status to tune variant ms-inference-scheduling-llm-d-modelservice-decode: alpha= 7.518454, beta= 0.048728, gamma= 16.197783, delta= 0.000257"}
{"level":"DEBUG","ts":"2025-12-10T13:17:14.097Z","msg":"[Tuner Config] variant=ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler | Initial state: alpha=7.518454, beta=0.048728, gamma=16.197783, delta=0.000257 | Expected obs (for R): TTFT=17.41ms, ITL=8.89ms | Has covariance=true"}
{"level":"INFO","ts":"2025-12-10T13:17:14.098Z","msg":"Tuner validation succeeded for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler, server ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler - New NIS: 1.472154"}
{"level":"DEBUG","ts":"2025-12-10T13:17:14.098Z","msg":"Model tuner results: model=unsloth/Meta-Llama-3.1-8B, accelerator=H100, alpha=7.304264, beta=0.046384, gamma=15.393015, delta=0.000256, NIS=1.47"}
{"level":"DEBUG","ts":"2025-12-10T13:17:14.098Z","msg":"Updated tuner status for variant ms-inference-scheduling-llm-d-modelservice-decode: alpha=7.304264, beta=0.046384, gamma=15.393015, delta=0.000256, NIS=1.472154"}
{"level":"INFO","ts":"2025-12-10T13:17:14.098Z","msg":"Tuned performance parameters result: variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler - alpha: 7.304264, beta: 0.046384, gamma: 15.393015, delta: 0.000256"}
{"level":"DEBUG","ts":"2025-12-10T13:17:14.107Z","msg":"Optimization solution - system: Solution: \ns=ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler; c=Premium; m=unsloth/Meta-Llama-3.1-8B; rate=532.22; inTk=246; outTk=431; sol=1, sat=false, alloc={acc=H100; numRep=1; maxBatch=512; cost=100, val=0, itl=8.939475, ttft=17.608877, rho=0.066901416, maxRPM=793.4734}; slo-itl=10, slo-ttft=1000, slo-tps=0 \nAllocationByType: \nname=NVIDIA-H100-80GB-HBM3, count=1, limit=0, cost=100 \ntotalCost=100 \n"}
{"level":"DEBUG","ts":"2025-12-10T13:17:14.107Z","msg":"Setting accelerator name: Name=H100, allocationData={H100 1 512 100 8.939475 17.608877 {532.22 246 431}}"}
{"level":"INFO","ts":"2025-12-10T13:17:14.107Z","msg":"Model-based optimization completed for model: unsloth/Meta-Llama-3.1-8B - model-based targets: map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"WARN","ts":"2025-12-10T13:17:14.107Z","msg":"saturation analysis unavailable, using model-based targets only: modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-10T13:17:14.107Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-12-10T13:17:14.107Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1→target=1"}
{"level":"INFO","ts":"2025-12-10T13:17:14.107Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 1, desired-replicas: 1, accelerator: H100"}
{"level":"INFO","ts":"2025-12-10T13:17:14.107Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=1, accelerator=H100, SaturationOnly=false"}
{"level":"INFO","ts":"2025-12-10T13:17:14.117Z","msg":"Applied Saturation decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1, target=1, reason=model-based only (Saturation unavailable)"}
{"level":"INFO","ts":"2025-12-10T13:17:14.117Z","msg":"Reconciliation completed successfully: mode=model-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-12-10T13:18:14.118Z","msg":"Reconciling VariantAutoscaling: name=ms-inference-scheduling-llm-d-modelservice-decode, namespace=llm-d-inference-scheduler, modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-10T13:18:14.118Z","msg":"saturation scaling config cache updated: entries=1, has_default=true"}
{"level":"INFO","ts":"2025-12-10T13:18:14.118Z","msg":"saturation scaling configuration loaded successfully"}
{"level":"INFO","ts":"2025-12-10T13:18:14.118Z","msg":"Operating in MODEL-ONLY mode: model-based optimization only"}
{"level":"INFO","ts":"2025-12-10T13:18:14.118Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-12-10T13:18:14.118Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-12-10T13:18:14.125Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dgftqv, usage=0.254 (25.4%)"}
{"level":"DEBUG","ts":"2025-12-10T13:18:14.125Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"INFO","ts":"2025-12-10T13:18:14.125Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dgftqv, queueLength=0"}
{"level":"DEBUG","ts":"2025-12-10T13:18:14.125Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"DEBUG","ts":"2025-12-10T13:18:14.127Z","msg":"Pod-to-variant matching successful: totalPods=1, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"DEBUG","ts":"2025-12-10T13:18:14.127Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=1"}
{"level":"DEBUG","ts":"2025-12-10T13:18:14.127Z","msg":"Collected capacity metrics for observability in model-only mode: modelID=unsloth/Meta-Llama-3.1-8B, replicaCount=1"}
{"level":"DEBUG","ts":"2025-12-10T13:18:14.139Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=1, accelerator=H100, ttft=26.76ms, itl=13.31ms, cost=100.00, maxBatch=256, arrivalRate=1026.04, avgInputTokens=260.13, avgOutputTokens=387.93"}
{"level":"DEBUG","ts":"2025-12-10T13:18:14.139Z","msg":"Experimental model tuner is enabled globally: tuning model performance parameters for active VAs"}
{"level":"DEBUG","ts":"2025-12-10T13:18:14.139Z","msg":"Using state vals from VA status to tune variant ms-inference-scheduling-llm-d-modelservice-decode: alpha= 7.304264, beta= 0.046384, gamma= 15.393015, delta= 0.000256"}
{"level":"DEBUG","ts":"2025-12-10T13:18:14.139Z","msg":"[Tuner Config] variant=ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler | Initial state: alpha=7.304264, beta=0.046384, gamma=15.393015, delta=0.000256 | Expected obs (for R): TTFT=26.76ms, ITL=13.31ms | Has covariance=true"}
{"level":"WARN","ts":"2025-12-10T13:18:14.140Z","msg":"Tuner validation failed (NIS=36.17), validation error: normalized innovation squared (NIS=36.17) exceeds threshold (7.38), rejecting update as outlier - returning previous state: alpha=7.304264, beta=0.046384, gamma=15.393015, delta=0.000256"}
{"level":"WARN","ts":"2025-12-10T13:18:14.140Z","msg":"Tuner NIS validation failed for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler, server ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler (NIS=36.17 exceeds threshold 7.38) - Keeping previous state: alpha=7.304264, beta=0.046384, gamma=15.393015, delta=0.000256"}
{"level":"INFO","ts":"2025-12-10T13:18:14.140Z","msg":"Keeping previous tuned parameters for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler due to NIS validation failure (NIS=36.166194)"}
{"level":"DEBUG","ts":"2025-12-10T13:18:14.140Z","msg":"Model tuner results: model=unsloth/Meta-Llama-3.1-8B, accelerator=H100, alpha=7.304264, beta=0.046384, gamma=15.393015, delta=0.000256, NIS=36.17"}
{"level":"DEBUG","ts":"2025-12-10T13:18:14.140Z","msg":"Updated tuner status for variant ms-inference-scheduling-llm-d-modelservice-decode: alpha=7.304264, beta=0.046384, gamma=15.393015, delta=0.000256, NIS=36.166194"}
{"level":"DEBUG","ts":"2025-12-10T13:18:14.150Z","msg":"Optimization solution - system: Solution: \ns=ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler; c=Premium; m=unsloth/Meta-Llama-3.1-8B; rate=1026.04; inTk=260; outTk=387; sol=1, sat=false, alloc={acc=H100; numRep=2; maxBatch=512; cost=200, val=100, itl=8.687481, ttft=17.3779, rho=0.056291018, maxRPM=883.43353}; slo-itl=10, slo-ttft=1000, slo-tps=0 \nAllocationByType: \nname=NVIDIA-H100-80GB-HBM3, count=2, limit=0, cost=200 \ntotalCost=200 \n"}
{"level":"DEBUG","ts":"2025-12-10T13:18:14.150Z","msg":"Setting accelerator name: Name=H100, allocationData={H100 2 512 200 8.687481 17.3779 {1026.04 260 387}}"}
{"level":"INFO","ts":"2025-12-10T13:18:14.150Z","msg":"Model-based optimization completed for model: unsloth/Meta-Llama-3.1-8B - model-based targets: map[ms-inference-scheduling-llm-d-modelservice-decode:2]"}
{"level":"WARN","ts":"2025-12-10T13:18:14.150Z","msg":"saturation analysis unavailable, using model-based targets only: modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-10T13:18:14.150Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-12-10T13:18:14.150Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=scale-up, current=1→target=2"}
{"level":"INFO","ts":"2025-12-10T13:18:14.150Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 1, desired-replicas: 2, accelerator: H100"}
{"level":"INFO","ts":"2025-12-10T13:18:14.150Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=2, accelerator=H100, SaturationOnly=false"}
{"level":"INFO","ts":"2025-12-10T13:18:14.157Z","msg":"Applied Saturation decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=scale-up, current=1, target=2, reason=model-based only (Saturation unavailable)"}
{"level":"INFO","ts":"2025-12-10T13:18:14.157Z","msg":"Reconciliation completed successfully: mode=model-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-12-10T13:19:14.157Z","msg":"Reconciling VariantAutoscaling: name=ms-inference-scheduling-llm-d-modelservice-decode, namespace=llm-d-inference-scheduler, modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-10T13:19:14.157Z","msg":"saturation scaling config cache updated: entries=1, has_default=true"}
{"level":"INFO","ts":"2025-12-10T13:19:14.157Z","msg":"saturation scaling configuration loaded successfully"}
{"level":"INFO","ts":"2025-12-10T13:19:14.157Z","msg":"Operating in MODEL-ONLY mode: model-based optimization only"}
{"level":"INFO","ts":"2025-12-10T13:19:14.157Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-12-10T13:19:14.157Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-12-10T13:19:14.161Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dgftqv, usage=0.478 (47.8%)"}
{"level":"DEBUG","ts":"2025-12-10T13:19:14.161Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"INFO","ts":"2025-12-10T13:19:14.161Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dgftqv, queueLength=0"}
{"level":"DEBUG","ts":"2025-12-10T13:19:14.161Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"DEBUG","ts":"2025-12-10T13:19:14.163Z","msg":"Pod-to-variant matching successful: totalPods=1, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"DEBUG","ts":"2025-12-10T13:19:14.163Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=1"}
{"level":"DEBUG","ts":"2025-12-10T13:19:14.163Z","msg":"Collected capacity metrics for observability in model-only mode: modelID=unsloth/Meta-Llama-3.1-8B, replicaCount=1"}
{"level":"DEBUG","ts":"2025-12-10T13:19:14.175Z","msg":"Replica count mismatch for llm-d-inference-scheduler/unsloth/Meta-Llama-3.1-8B: spec=2, reporting_metrics=1"}
{"level":"DEBUG","ts":"2025-12-10T13:19:14.175Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=1, accelerator=H100, ttft=44.07ms, itl=26.35ms, cost=100.00, maxBatch=256, arrivalRate=1519.86, avgInputTokens=238.11, avgOutputTokens=428.95"}
{"level":"DEBUG","ts":"2025-12-10T13:19:14.175Z","msg":"Experimental model tuner is enabled globally: tuning model performance parameters for active VAs"}
{"level":"DEBUG","ts":"2025-12-10T13:19:14.175Z","msg":"Using state vals from VA status to tune variant ms-inference-scheduling-llm-d-modelservice-decode: alpha= 7.304264, beta= 0.046384, gamma= 15.393015, delta= 0.000256"}
{"level":"DEBUG","ts":"2025-12-10T13:19:14.175Z","msg":"[Tuner Config] variant=ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler | Initial state: alpha=7.304264, beta=0.046384, gamma=15.393015, delta=0.000256 | Expected obs (for R): TTFT=44.07ms, ITL=26.35ms | Has covariance=true"}
{"level":"WARN","ts":"2025-12-10T13:19:14.176Z","msg":"Tuner validation failed (NIS=85.40), validation error: normalized innovation squared (NIS=85.40) exceeds threshold (7.38), rejecting update as outlier - returning previous state: alpha=7.304264, beta=0.046384, gamma=15.393015, delta=0.000256"}
{"level":"WARN","ts":"2025-12-10T13:19:14.176Z","msg":"Tuner NIS validation failed for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler, server ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler (NIS=85.40 exceeds threshold 7.38) - Keeping previous state: alpha=7.304264, beta=0.046384, gamma=15.393015, delta=0.000256"}
{"level":"INFO","ts":"2025-12-10T13:19:14.176Z","msg":"Keeping previous tuned parameters for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler due to NIS validation failure (NIS=85.402623)"}
{"level":"DEBUG","ts":"2025-12-10T13:19:14.176Z","msg":"Model tuner results: model=unsloth/Meta-Llama-3.1-8B, accelerator=H100, alpha=7.304264, beta=0.046384, gamma=15.393015, delta=0.000256, NIS=85.40"}
{"level":"DEBUG","ts":"2025-12-10T13:19:14.176Z","msg":"Updated tuner status for variant ms-inference-scheduling-llm-d-modelservice-decode: alpha=7.304264, beta=0.046384, gamma=15.393015, delta=0.000256, NIS=85.402623"}
{"level":"DEBUG","ts":"2025-12-10T13:19:14.185Z","msg":"Optimization solution - system: Solution: \ns=ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler; c=Premium; m=unsloth/Meta-Llama-3.1-8B; rate=1519.86; inTk=238; outTk=428; sol=1, sat=false, alloc={acc=H100; numRep=2; maxBatch=512; cost=200, val=100, itl=9.826687, ttft=18.70636, rho=0.104260355, maxRPM=799.0474}; slo-itl=10, slo-ttft=1000, slo-tps=0 \nAllocationByType: \nname=NVIDIA-H100-80GB-HBM3, count=2, limit=0, cost=200 \ntotalCost=200 \n"}
{"level":"DEBUG","ts":"2025-12-10T13:19:14.185Z","msg":"Setting accelerator name: Name=H100, allocationData={H100 2 512 200 9.826687 18.70636 {1519.86 238 428}}"}
{"level":"INFO","ts":"2025-12-10T13:19:14.185Z","msg":"Model-based optimization completed for model: unsloth/Meta-Llama-3.1-8B - model-based targets: map[ms-inference-scheduling-llm-d-modelservice-decode:2]"}
{"level":"WARN","ts":"2025-12-10T13:19:14.185Z","msg":"saturation analysis unavailable, using model-based targets only: modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-10T13:19:14.185Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-12-10T13:19:14.185Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=scale-up, current=1→target=2"}
{"level":"INFO","ts":"2025-12-10T13:19:14.185Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 1, desired-replicas: 2, accelerator: H100"}
{"level":"INFO","ts":"2025-12-10T13:19:14.185Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=2, accelerator=H100, SaturationOnly=false"}
{"level":"INFO","ts":"2025-12-10T13:19:14.193Z","msg":"Applied Saturation decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=scale-up, current=1, target=2, reason=model-based only (Saturation unavailable)"}
{"level":"INFO","ts":"2025-12-10T13:19:14.193Z","msg":"Reconciliation completed successfully: mode=model-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-12-10T13:20:14.194Z","msg":"Reconciling VariantAutoscaling: name=ms-inference-scheduling-llm-d-modelservice-decode, namespace=llm-d-inference-scheduler, modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-10T13:20:14.195Z","msg":"saturation scaling config cache updated: entries=1, has_default=true"}
{"level":"INFO","ts":"2025-12-10T13:20:14.195Z","msg":"saturation scaling configuration loaded successfully"}
{"level":"INFO","ts":"2025-12-10T13:20:14.195Z","msg":"Operating in MODEL-ONLY mode: model-based optimization only"}
{"level":"INFO","ts":"2025-12-10T13:20:14.195Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-12-10T13:20:14.195Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-12-10T13:20:14.209Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dgftqv, queueLength=0"}
{"level":"DEBUG","ts":"2025-12-10T13:20:14.209Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"INFO","ts":"2025-12-10T13:20:14.209Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dgftqv, usage=0.657 (65.7%)"}
{"level":"DEBUG","ts":"2025-12-10T13:20:14.209Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"DEBUG","ts":"2025-12-10T13:20:14.211Z","msg":"Pod-to-variant matching successful: totalPods=1, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"DEBUG","ts":"2025-12-10T13:20:14.211Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=1"}
{"level":"DEBUG","ts":"2025-12-10T13:20:14.211Z","msg":"Collected capacity metrics for observability in model-only mode: modelID=unsloth/Meta-Llama-3.1-8B, replicaCount=1"}
{"level":"DEBUG","ts":"2025-12-10T13:20:14.223Z","msg":"Replica count mismatch for llm-d-inference-scheduler/unsloth/Meta-Llama-3.1-8B: spec=2, reporting_metrics=1"}
{"level":"DEBUG","ts":"2025-12-10T13:20:14.223Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=1, accelerator=H100, ttft=58.70ms, itl=36.33ms, cost=100.00, maxBatch=256, arrivalRate=1599.29, avgInputTokens=240.16, avgOutputTokens=420.68"}
{"level":"DEBUG","ts":"2025-12-10T13:20:14.223Z","msg":"Experimental model tuner is enabled globally: tuning model performance parameters for active VAs"}
{"level":"DEBUG","ts":"2025-12-10T13:20:14.223Z","msg":"Using state vals from VA status to tune variant ms-inference-scheduling-llm-d-modelservice-decode: alpha= 7.304264, beta= 0.046384, gamma= 15.393015, delta= 0.000256"}
{"level":"DEBUG","ts":"2025-12-10T13:20:14.223Z","msg":"[Tuner Config] variant=ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler | Initial state: alpha=7.304264, beta=0.046384, gamma=15.393015, delta=0.000256 | Expected obs (for R): TTFT=58.70ms, ITL=36.33ms | Has covariance=true"}
{"level":"WARN","ts":"2025-12-10T13:20:14.224Z","msg":"Tuner validation failed (NIS=175.30), validation error: normalized innovation squared (NIS=175.30) exceeds threshold (7.38), rejecting update as outlier - returning previous state: alpha=7.304264, beta=0.046384, gamma=15.393015, delta=0.000256"}
{"level":"WARN","ts":"2025-12-10T13:20:14.224Z","msg":"Tuner NIS validation failed for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler, server ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler (NIS=175.30 exceeds threshold 7.38) - Keeping previous state: alpha=7.304264, beta=0.046384, gamma=15.393015, delta=0.000256"}
{"level":"INFO","ts":"2025-12-10T13:20:14.224Z","msg":"Keeping previous tuned parameters for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler due to NIS validation failure (NIS=175.299338)"}
{"level":"DEBUG","ts":"2025-12-10T13:20:14.224Z","msg":"Model tuner results: model=unsloth/Meta-Llama-3.1-8B, accelerator=H100, alpha=7.304264, beta=0.046384, gamma=15.393015, delta=0.000256, NIS=175.30"}
{"level":"DEBUG","ts":"2025-12-10T13:20:14.224Z","msg":"Updated tuner status for variant ms-inference-scheduling-llm-d-modelservice-decode: alpha=7.304264, beta=0.046384, gamma=15.393015, delta=0.000256, NIS=175.299338"}
{"level":"DEBUG","ts":"2025-12-10T13:20:14.234Z","msg":"Optimization solution - system: Solution: \ns=ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler; c=Premium; m=unsloth/Meta-Llama-3.1-8B; rate=1599.29; inTk=240; outTk=420; sol=1, sat=false, alloc={acc=H100; numRep=2; maxBatch=512; cost=200, val=100, itl=9.935881, ttft=18.87884, rho=0.10885825, maxRPM=814.233}; slo-itl=10, slo-ttft=1000, slo-tps=0 \nAllocationByType: \nname=NVIDIA-H100-80GB-HBM3, count=2, limit=0, cost=200 \ntotalCost=200 \n"}
{"level":"DEBUG","ts":"2025-12-10T13:20:14.234Z","msg":"Setting accelerator name: Name=H100, allocationData={H100 2 512 200 9.935881 18.87884 {1599.29 240 420}}"}
{"level":"INFO","ts":"2025-12-10T13:20:14.234Z","msg":"Model-based optimization completed for model: unsloth/Meta-Llama-3.1-8B - model-based targets: map[ms-inference-scheduling-llm-d-modelservice-decode:2]"}
{"level":"WARN","ts":"2025-12-10T13:20:14.234Z","msg":"saturation analysis unavailable, using model-based targets only: modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-10T13:20:14.234Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-12-10T13:20:14.234Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=scale-up, current=1→target=2"}
{"level":"INFO","ts":"2025-12-10T13:20:14.234Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 1, desired-replicas: 2, accelerator: H100"}
{"level":"INFO","ts":"2025-12-10T13:20:14.234Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=2, accelerator=H100, SaturationOnly=false"}
{"level":"INFO","ts":"2025-12-10T13:20:14.241Z","msg":"Applied Saturation decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=scale-up, current=1, target=2, reason=model-based only (Saturation unavailable)"}
{"level":"INFO","ts":"2025-12-10T13:20:14.241Z","msg":"Reconciliation completed successfully: mode=model-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-12-10T13:21:14.241Z","msg":"Reconciling VariantAutoscaling: name=ms-inference-scheduling-llm-d-modelservice-decode, namespace=llm-d-inference-scheduler, modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-10T13:21:14.241Z","msg":"saturation scaling config cache updated: entries=1, has_default=true"}
{"level":"INFO","ts":"2025-12-10T13:21:14.241Z","msg":"saturation scaling configuration loaded successfully"}
{"level":"INFO","ts":"2025-12-10T13:21:14.241Z","msg":"Operating in MODEL-ONLY mode: model-based optimization only"}
{"level":"INFO","ts":"2025-12-10T13:21:14.241Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-12-10T13:21:14.241Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-12-10T13:21:14.251Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dgftqv, queueLength=0"}
{"level":"INFO","ts":"2025-12-10T13:21:14.251Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984ddcmw2, queueLength=0"}
{"level":"DEBUG","ts":"2025-12-10T13:21:14.251Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=2"}
{"level":"INFO","ts":"2025-12-10T13:21:14.251Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dgftqv, usage=0.760 (76.0%)"}
{"level":"INFO","ts":"2025-12-10T13:21:14.251Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984ddcmw2, usage=0.053 (5.3%)"}
{"level":"DEBUG","ts":"2025-12-10T13:21:14.251Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=2"}
{"level":"DEBUG","ts":"2025-12-10T13:21:14.253Z","msg":"Pod-to-variant matching successful: totalPods=2, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:2]"}
{"level":"DEBUG","ts":"2025-12-10T13:21:14.253Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=2"}
{"level":"DEBUG","ts":"2025-12-10T13:21:14.253Z","msg":"Collected capacity metrics for observability in model-only mode: modelID=unsloth/Meta-Llama-3.1-8B, replicaCount=2"}
{"level":"DEBUG","ts":"2025-12-10T13:21:14.266Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=2, accelerator=H100, ttft=72.45ms, itl=45.79ms, cost=200.00, maxBatch=256, arrivalRate=1556.56, avgInputTokens=229.35, avgOutputTokens=438.04"}
{"level":"DEBUG","ts":"2025-12-10T13:21:14.266Z","msg":"Experimental model tuner is enabled globally: tuning model performance parameters for active VAs"}
{"level":"DEBUG","ts":"2025-12-10T13:21:14.266Z","msg":"Using state vals from VA status to tune variant ms-inference-scheduling-llm-d-modelservice-decode: alpha= 7.304264, beta= 0.046384, gamma= 15.393015, delta= 0.000256"}
{"level":"DEBUG","ts":"2025-12-10T13:21:14.266Z","msg":"[Tuner Config] variant=ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler | Initial state: alpha=7.304264, beta=0.046384, gamma=15.393015, delta=0.000256 | Expected obs (for R): TTFT=72.45ms, ITL=45.79ms | Has covariance=true"}
{"level":"WARN","ts":"2025-12-10T13:21:14.266Z","msg":"Tuner validation failed (NIS=1350.64), validation error: normalized innovation squared (NIS=1350.64) exceeds threshold (7.38), rejecting update as outlier - returning previous state: alpha=7.304264, beta=0.046384, gamma=15.393015, delta=0.000256"}
{"level":"WARN","ts":"2025-12-10T13:21:14.266Z","msg":"Tuner NIS validation failed for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler, server ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler (NIS=1350.64 exceeds threshold 7.38) - Keeping previous state: alpha=7.304264, beta=0.046384, gamma=15.393015, delta=0.000256"}
{"level":"INFO","ts":"2025-12-10T13:21:14.266Z","msg":"Keeping previous tuned parameters for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler due to NIS validation failure (NIS=1350.640772)"}
{"level":"DEBUG","ts":"2025-12-10T13:21:14.266Z","msg":"Model tuner results: model=unsloth/Meta-Llama-3.1-8B, accelerator=H100, alpha=7.304264, beta=0.046384, gamma=15.393015, delta=0.000256, NIS=1350.64"}
{"level":"DEBUG","ts":"2025-12-10T13:21:14.266Z","msg":"Updated tuner status for variant ms-inference-scheduling-llm-d-modelservice-decode: alpha=7.304264, beta=0.046384, gamma=15.393015, delta=0.000256, NIS=1350.640772"}
{"level":"DEBUG","ts":"2025-12-10T13:21:14.276Z","msg":"Optimization solution - system: Solution: \ns=ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler; c=Premium; m=unsloth/Meta-Llama-3.1-8B; rate=1556.56; inTk=229; outTk=438; sol=1, sat=false, alloc={acc=H100; numRep=2; maxBatch=512; cost=200, val=0, itl=9.98808, ttft=18.785048, rho=0.111056305, maxRPM=780.86707}; slo-itl=10, slo-ttft=1000, slo-tps=0 \nAllocationByType: \nname=NVIDIA-H100-80GB-HBM3, count=2, limit=0, cost=200 \ntotalCost=200 \n"}
{"level":"DEBUG","ts":"2025-12-10T13:21:14.276Z","msg":"Setting accelerator name: Name=H100, allocationData={H100 2 512 200 9.98808 18.785048 {1556.56 229 438}}"}
{"level":"INFO","ts":"2025-12-10T13:21:14.276Z","msg":"Model-based optimization completed for model: unsloth/Meta-Llama-3.1-8B - model-based targets: map[ms-inference-scheduling-llm-d-modelservice-decode:2]"}
{"level":"WARN","ts":"2025-12-10T13:21:14.276Z","msg":"saturation analysis unavailable, using model-based targets only: modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-10T13:21:14.276Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-12-10T13:21:14.276Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=2→target=2"}
{"level":"INFO","ts":"2025-12-10T13:21:14.276Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 2, desired-replicas: 2, accelerator: H100"}
{"level":"INFO","ts":"2025-12-10T13:21:14.276Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=2, accelerator=H100, SaturationOnly=false"}
{"level":"INFO","ts":"2025-12-10T13:21:14.283Z","msg":"Applied Saturation decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=2, target=2, reason=model-based only (Saturation unavailable)"}
{"level":"INFO","ts":"2025-12-10T13:21:14.283Z","msg":"Reconciliation completed successfully: mode=model-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-12-10T13:22:14.284Z","msg":"Reconciling VariantAutoscaling: name=ms-inference-scheduling-llm-d-modelservice-decode, namespace=llm-d-inference-scheduler, modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-10T13:22:14.284Z","msg":"saturation scaling config cache updated: entries=1, has_default=true"}
{"level":"INFO","ts":"2025-12-10T13:22:14.284Z","msg":"saturation scaling configuration loaded successfully"}
{"level":"INFO","ts":"2025-12-10T13:22:14.284Z","msg":"Operating in MODEL-ONLY mode: model-based optimization only"}
{"level":"INFO","ts":"2025-12-10T13:22:14.284Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-12-10T13:22:14.284Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-12-10T13:22:14.291Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dgftqv, usage=0.664 (66.4%)"}
{"level":"INFO","ts":"2025-12-10T13:22:14.291Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984ddcmw2, usage=0.055 (5.5%)"}
{"level":"DEBUG","ts":"2025-12-10T13:22:14.291Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=2"}
{"level":"INFO","ts":"2025-12-10T13:22:14.291Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dgftqv, queueLength=0"}
{"level":"INFO","ts":"2025-12-10T13:22:14.291Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984ddcmw2, queueLength=0"}
{"level":"DEBUG","ts":"2025-12-10T13:22:14.291Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=2"}
{"level":"DEBUG","ts":"2025-12-10T13:22:14.294Z","msg":"Pod-to-variant matching successful: totalPods=2, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:2]"}
{"level":"DEBUG","ts":"2025-12-10T13:22:14.294Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=2"}
{"level":"DEBUG","ts":"2025-12-10T13:22:14.294Z","msg":"Collected capacity metrics for observability in model-only mode: modelID=unsloth/Meta-Llama-3.1-8B, replicaCount=2"}
{"level":"DEBUG","ts":"2025-12-10T13:22:14.310Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=2, accelerator=H100, ttft=35.75ms, itl=21.05ms, cost=200.00, maxBatch=256, arrivalRate=1911.27, avgInputTokens=197.61, avgOutputTokens=578.44"}
{"level":"DEBUG","ts":"2025-12-10T13:22:14.310Z","msg":"Experimental model tuner is enabled globally: tuning model performance parameters for active VAs"}
{"level":"DEBUG","ts":"2025-12-10T13:22:14.310Z","msg":"Using state vals from VA status to tune variant ms-inference-scheduling-llm-d-modelservice-decode: alpha= 7.304264, beta= 0.046384, gamma= 15.393015, delta= 0.000256"}
{"level":"DEBUG","ts":"2025-12-10T13:22:14.310Z","msg":"[Tuner Config] variant=ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler | Initial state: alpha=7.304264, beta=0.046384, gamma=15.393015, delta=0.000256 | Expected obs (for R): TTFT=35.75ms, ITL=21.05ms | Has covariance=true"}
{"level":"WARN","ts":"2025-12-10T13:22:14.311Z","msg":"Tuner validation failed (NIS=103.17), validation error: normalized innovation squared (NIS=103.17) exceeds threshold (7.38), rejecting update as outlier - returning previous state: alpha=7.304264, beta=0.046384, gamma=15.393015, delta=0.000256"}
{"level":"WARN","ts":"2025-12-10T13:22:14.311Z","msg":"Tuner NIS validation failed for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler, server ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler (NIS=103.17 exceeds threshold 7.38) - Keeping previous state: alpha=7.304264, beta=0.046384, gamma=15.393015, delta=0.000256"}
{"level":"INFO","ts":"2025-12-10T13:22:14.311Z","msg":"Keeping previous tuned parameters for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler due to NIS validation failure (NIS=103.172393)"}
{"level":"DEBUG","ts":"2025-12-10T13:22:14.311Z","msg":"Model tuner results: model=unsloth/Meta-Llama-3.1-8B, accelerator=H100, alpha=7.304264, beta=0.046384, gamma=15.393015, delta=0.000256, NIS=103.17"}
{"level":"DEBUG","ts":"2025-12-10T13:22:14.311Z","msg":"Updated tuner status for variant ms-inference-scheduling-llm-d-modelservice-decode: alpha=7.304264, beta=0.046384, gamma=15.393015, delta=0.000256, NIS=103.172393"}
{"level":"DEBUG","ts":"2025-12-10T13:22:14.320Z","msg":"Optimization solution - system: Solution: \ns=ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler; c=Premium; m=unsloth/Meta-Llama-3.1-8B; rate=1911.27; inTk=197; outTk=578; sol=1, sat=false, alloc={acc=H100; numRep=4; maxBatch=512; cost=400, val=200, itl=9.349958, ttft=17.61724, rho=0.08418646, maxRPM=592.0647}; slo-itl=10, slo-ttft=1000, slo-tps=0 \nAllocationByType: \nname=NVIDIA-H100-80GB-HBM3, count=4, limit=0, cost=400 \ntotalCost=400 \n"}
{"level":"DEBUG","ts":"2025-12-10T13:22:14.320Z","msg":"Setting accelerator name: Name=H100, allocationData={H100 4 512 400 9.349958 17.61724 {1911.27 197 578}}"}
{"level":"INFO","ts":"2025-12-10T13:22:14.320Z","msg":"Model-based optimization completed for model: unsloth/Meta-Llama-3.1-8B - model-based targets: map[ms-inference-scheduling-llm-d-modelservice-decode:4]"}
{"level":"WARN","ts":"2025-12-10T13:22:14.320Z","msg":"saturation analysis unavailable, using model-based targets only: modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-10T13:22:14.320Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-12-10T13:22:14.320Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=scale-up, current=2→target=4"}
{"level":"INFO","ts":"2025-12-10T13:22:14.320Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 2, desired-replicas: 4, accelerator: H100"}
{"level":"INFO","ts":"2025-12-10T13:22:14.320Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=4, accelerator=H100, SaturationOnly=false"}
{"level":"INFO","ts":"2025-12-10T13:22:14.326Z","msg":"Applied Saturation decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=scale-up, current=2, target=4, reason=model-based only (Saturation unavailable)"}
{"level":"INFO","ts":"2025-12-10T13:22:14.326Z","msg":"Reconciliation completed successfully: mode=model-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-12-10T13:23:14.327Z","msg":"Reconciling VariantAutoscaling: name=ms-inference-scheduling-llm-d-modelservice-decode, namespace=llm-d-inference-scheduler, modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-10T13:23:14.327Z","msg":"saturation scaling config cache updated: entries=1, has_default=true"}
{"level":"INFO","ts":"2025-12-10T13:23:14.327Z","msg":"saturation scaling configuration loaded successfully"}
{"level":"INFO","ts":"2025-12-10T13:23:14.327Z","msg":"Operating in MODEL-ONLY mode: model-based optimization only"}
{"level":"INFO","ts":"2025-12-10T13:23:14.327Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-12-10T13:23:14.327Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-12-10T13:23:14.331Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dgftqv, queueLength=0"}
{"level":"INFO","ts":"2025-12-10T13:23:14.331Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984ddcmw2, queueLength=0"}
{"level":"DEBUG","ts":"2025-12-10T13:23:14.331Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=2"}
{"level":"INFO","ts":"2025-12-10T13:23:14.331Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dgftqv, usage=0.140 (14.0%)"}
{"level":"INFO","ts":"2025-12-10T13:23:14.331Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984ddcmw2, usage=0.080 (8.0%)"}
{"level":"DEBUG","ts":"2025-12-10T13:23:14.331Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=2"}
{"level":"DEBUG","ts":"2025-12-10T13:23:14.334Z","msg":"Pod-to-variant matching successful: totalPods=2, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:2]"}
{"level":"DEBUG","ts":"2025-12-10T13:23:14.334Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=2"}
{"level":"DEBUG","ts":"2025-12-10T13:23:14.334Z","msg":"Collected capacity metrics for observability in model-only mode: modelID=unsloth/Meta-Llama-3.1-8B, replicaCount=2"}
{"level":"DEBUG","ts":"2025-12-10T13:23:14.347Z","msg":"Replica count mismatch for llm-d-inference-scheduler/unsloth/Meta-Llama-3.1-8B: spec=4, reporting_metrics=2"}
{"level":"DEBUG","ts":"2025-12-10T13:23:14.347Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=2, accelerator=H100, ttft=22.58ms, itl=11.42ms, cost=200.00, maxBatch=256, arrivalRate=1493.40, avgInputTokens=232.37, avgOutputTokens=474.38"}
{"level":"DEBUG","ts":"2025-12-10T13:23:14.347Z","msg":"Experimental model tuner is enabled globally: tuning model performance parameters for active VAs"}
{"level":"DEBUG","ts":"2025-12-10T13:23:14.347Z","msg":"Using state vals from VA status to tune variant ms-inference-scheduling-llm-d-modelservice-decode: alpha= 7.304264, beta= 0.046384, gamma= 15.393015, delta= 0.000256"}
{"level":"DEBUG","ts":"2025-12-10T13:23:14.347Z","msg":"[Tuner Config] variant=ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler | Initial state: alpha=7.304264, beta=0.046384, gamma=15.393015, delta=0.000256 | Expected obs (for R): TTFT=22.58ms, ITL=11.42ms | Has covariance=true"}
{"level":"WARN","ts":"2025-12-10T13:23:14.347Z","msg":"Tuner validation failed (NIS=12.66), validation error: normalized innovation squared (NIS=12.66) exceeds threshold (7.38), rejecting update as outlier - returning previous state: alpha=7.304264, beta=0.046384, gamma=15.393015, delta=0.000256"}
{"level":"WARN","ts":"2025-12-10T13:23:14.347Z","msg":"Tuner NIS validation failed for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler, server ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler (NIS=12.66 exceeds threshold 7.38) - Keeping previous state: alpha=7.304264, beta=0.046384, gamma=15.393015, delta=0.000256"}
{"level":"INFO","ts":"2025-12-10T13:23:14.347Z","msg":"Keeping previous tuned parameters for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler due to NIS validation failure (NIS=12.662807)"}
{"level":"DEBUG","ts":"2025-12-10T13:23:14.347Z","msg":"Model tuner results: model=unsloth/Meta-Llama-3.1-8B, accelerator=H100, alpha=7.304264, beta=0.046384, gamma=15.393015, delta=0.000256, NIS=12.66"}
{"level":"DEBUG","ts":"2025-12-10T13:23:14.347Z","msg":"Updated tuner status for variant ms-inference-scheduling-llm-d-modelservice-decode: alpha=7.304264, beta=0.046384, gamma=15.393015, delta=0.000256, NIS=12.662807"}
{"level":"DEBUG","ts":"2025-12-10T13:23:14.357Z","msg":"Optimization solution - system: Solution: \ns=ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler; c=Premium; m=unsloth/Meta-Llama-3.1-8B; rate=1493.4; inTk=232; outTk=474; sol=1, sat=false, alloc={acc=H100; numRep=3; maxBatch=512; cost=300, val=100, itl=8.994665, ttft=17.557474, rho=0.06922587, maxRPM=721.6629}; slo-itl=10, slo-ttft=1000, slo-tps=0 \nAllocationByType: \nname=NVIDIA-H100-80GB-HBM3, count=3, limit=0, cost=300 \ntotalCost=300 \n"}
{"level":"DEBUG","ts":"2025-12-10T13:23:14.357Z","msg":"Setting accelerator name: Name=H100, allocationData={H100 3 512 300 8.994665 17.557474 {1493.4 232 474}}"}
{"level":"INFO","ts":"2025-12-10T13:23:14.357Z","msg":"Model-based optimization completed for model: unsloth/Meta-Llama-3.1-8B - model-based targets: map[ms-inference-scheduling-llm-d-modelservice-decode:3]"}
{"level":"WARN","ts":"2025-12-10T13:23:14.357Z","msg":"saturation analysis unavailable, using model-based targets only: modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-10T13:23:14.357Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-12-10T13:23:14.357Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=scale-up, current=2→target=3"}
{"level":"INFO","ts":"2025-12-10T13:23:14.357Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 2, desired-replicas: 3, accelerator: H100"}
{"level":"INFO","ts":"2025-12-10T13:23:14.357Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=3, accelerator=H100, SaturationOnly=false"}
{"level":"INFO","ts":"2025-12-10T13:23:14.365Z","msg":"Applied Saturation decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=scale-up, current=2, target=3, reason=model-based only (Saturation unavailable)"}
{"level":"INFO","ts":"2025-12-10T13:23:14.365Z","msg":"Reconciliation completed successfully: mode=model-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-12-10T13:24:14.366Z","msg":"Reconciling VariantAutoscaling: name=ms-inference-scheduling-llm-d-modelservice-decode, namespace=llm-d-inference-scheduler, modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-10T13:24:14.366Z","msg":"saturation scaling config cache updated: entries=1, has_default=true"}
{"level":"INFO","ts":"2025-12-10T13:24:14.366Z","msg":"saturation scaling configuration loaded successfully"}
{"level":"INFO","ts":"2025-12-10T13:24:14.366Z","msg":"Operating in MODEL-ONLY mode: model-based optimization only"}
{"level":"INFO","ts":"2025-12-10T13:24:14.366Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-12-10T13:24:14.366Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-12-10T13:24:14.373Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dgftqv, usage=0.102 (10.2%)"}
{"level":"INFO","ts":"2025-12-10T13:24:14.373Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984ddcmw2, usage=0.106 (10.6%)"}
{"level":"DEBUG","ts":"2025-12-10T13:24:14.373Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=2"}
{"level":"INFO","ts":"2025-12-10T13:24:14.373Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dgftqv, queueLength=0"}
{"level":"INFO","ts":"2025-12-10T13:24:14.373Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984ddcmw2, queueLength=0"}
{"level":"DEBUG","ts":"2025-12-10T13:24:14.373Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=2"}
{"level":"DEBUG","ts":"2025-12-10T13:24:14.376Z","msg":"Pod-to-variant matching successful: totalPods=2, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:2]"}
{"level":"DEBUG","ts":"2025-12-10T13:24:14.376Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=2"}
{"level":"DEBUG","ts":"2025-12-10T13:24:14.376Z","msg":"Collected capacity metrics for observability in model-only mode: modelID=unsloth/Meta-Llama-3.1-8B, replicaCount=2"}
{"level":"DEBUG","ts":"2025-12-10T13:24:14.392Z","msg":"Replica count mismatch for llm-d-inference-scheduler/unsloth/Meta-Llama-3.1-8B: spec=3, reporting_metrics=2"}
{"level":"DEBUG","ts":"2025-12-10T13:24:14.392Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=2, accelerator=H100, ttft=21.48ms, itl=10.41ms, cost=200.00, maxBatch=256, arrivalRate=1358.15, avgInputTokens=218.16, avgOutputTokens=479.76"}
{"level":"DEBUG","ts":"2025-12-10T13:24:14.392Z","msg":"Experimental model tuner is enabled globally: tuning model performance parameters for active VAs"}
{"level":"DEBUG","ts":"2025-12-10T13:24:14.392Z","msg":"Using state vals from VA status to tune variant ms-inference-scheduling-llm-d-modelservice-decode: alpha= 7.304264, beta= 0.046384, gamma= 15.393015, delta= 0.000256"}
{"level":"DEBUG","ts":"2025-12-10T13:24:14.392Z","msg":"[Tuner Config] variant=ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler | Initial state: alpha=7.304264, beta=0.046384, gamma=15.393015, delta=0.000256 | Expected obs (for R): TTFT=21.48ms, ITL=10.41ms | Has covariance=true"}
{"level":"WARN","ts":"2025-12-10T13:24:14.393Z","msg":"Tuner validation failed (NIS=8.78), validation error: normalized innovation squared (NIS=8.78) exceeds threshold (7.38), rejecting update as outlier - returning previous state: alpha=7.304264, beta=0.046384, gamma=15.393015, delta=0.000256"}
{"level":"WARN","ts":"2025-12-10T13:24:14.393Z","msg":"Tuner NIS validation failed for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler, server ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler (NIS=8.78 exceeds threshold 7.38) - Keeping previous state: alpha=7.304264, beta=0.046384, gamma=15.393015, delta=0.000256"}
{"level":"INFO","ts":"2025-12-10T13:24:14.393Z","msg":"Keeping previous tuned parameters for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler due to NIS validation failure (NIS=8.778074)"}
{"level":"DEBUG","ts":"2025-12-10T13:24:14.393Z","msg":"Model tuner results: model=unsloth/Meta-Llama-3.1-8B, accelerator=H100, alpha=7.304264, beta=0.046384, gamma=15.393015, delta=0.000256, NIS=8.78"}
{"level":"DEBUG","ts":"2025-12-10T13:24:14.393Z","msg":"Updated tuner status for variant ms-inference-scheduling-llm-d-modelservice-decode: alpha=7.304264, beta=0.046384, gamma=15.393015, delta=0.000256, NIS=8.778074"}
{"level":"DEBUG","ts":"2025-12-10T13:24:14.402Z","msg":"Optimization solution - system: Solution: \ns=ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler; c=Premium; m=unsloth/Meta-Llama-3.1-8B; rate=1358.15; inTk=218; outTk=479; sol=1, sat=false, alloc={acc=H100; numRep=2; maxBatch=512; cost=200, val=0, itl=9.826023, ttft=18.427128, rho=0.1042324, maxRPM=714.17413}; slo-itl=10, slo-ttft=1000, slo-tps=0 \nAllocationByType: \nname=NVIDIA-H100-80GB-HBM3, count=2, limit=0, cost=200 \ntotalCost=200 \n"}
{"level":"DEBUG","ts":"2025-12-10T13:24:14.402Z","msg":"Setting accelerator name: Name=H100, allocationData={H100 2 512 200 9.826023 18.427128 {1358.15 218 479}}"}
{"level":"INFO","ts":"2025-12-10T13:24:14.402Z","msg":"Model-based optimization completed for model: unsloth/Meta-Llama-3.1-8B - model-based targets: map[ms-inference-scheduling-llm-d-modelservice-decode:2]"}
{"level":"WARN","ts":"2025-12-10T13:24:14.402Z","msg":"saturation analysis unavailable, using model-based targets only: modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-10T13:24:14.402Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-12-10T13:24:14.402Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=2→target=2"}
{"level":"INFO","ts":"2025-12-10T13:24:14.402Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 2, desired-replicas: 2, accelerator: H100"}
{"level":"INFO","ts":"2025-12-10T13:24:14.402Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=2, accelerator=H100, SaturationOnly=false"}
{"level":"INFO","ts":"2025-12-10T13:24:14.408Z","msg":"Applied Saturation decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=2, target=2, reason=model-based only (Saturation unavailable)"}
{"level":"INFO","ts":"2025-12-10T13:24:14.408Z","msg":"Reconciliation completed successfully: mode=model-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-12-10T13:25:14.408Z","msg":"Reconciling VariantAutoscaling: name=ms-inference-scheduling-llm-d-modelservice-decode, namespace=llm-d-inference-scheduler, modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-10T13:25:14.409Z","msg":"saturation scaling config cache updated: entries=1, has_default=true"}
{"level":"INFO","ts":"2025-12-10T13:25:14.409Z","msg":"saturation scaling configuration loaded successfully"}
{"level":"INFO","ts":"2025-12-10T13:25:14.409Z","msg":"Operating in MODEL-ONLY mode: model-based optimization only"}
{"level":"INFO","ts":"2025-12-10T13:25:14.409Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-12-10T13:25:14.409Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-12-10T13:25:14.417Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dgftqv, usage=0.035 (3.5%)"}
{"level":"INFO","ts":"2025-12-10T13:25:14.417Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984ddcmw2, usage=0.028 (2.8%)"}
{"level":"DEBUG","ts":"2025-12-10T13:25:14.417Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=2"}
{"level":"INFO","ts":"2025-12-10T13:25:14.417Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dgftqv, queueLength=0"}
{"level":"INFO","ts":"2025-12-10T13:25:14.418Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984ddcmw2, queueLength=0"}
{"level":"DEBUG","ts":"2025-12-10T13:25:14.418Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=2"}
{"level":"DEBUG","ts":"2025-12-10T13:25:14.420Z","msg":"Pod-to-variant matching successful: totalPods=2, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:2]"}
{"level":"DEBUG","ts":"2025-12-10T13:25:14.420Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=2"}
{"level":"DEBUG","ts":"2025-12-10T13:25:14.420Z","msg":"Collected capacity metrics for observability in model-only mode: modelID=unsloth/Meta-Llama-3.1-8B, replicaCount=2"}
{"level":"DEBUG","ts":"2025-12-10T13:25:14.433Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=2, accelerator=H100, ttft=16.34ms, itl=8.07ms, cost=200.00, maxBatch=256, arrivalRate=726.14, avgInputTokens=240.58, avgOutputTokens=426.84"}
{"level":"DEBUG","ts":"2025-12-10T13:25:14.433Z","msg":"Experimental model tuner is enabled globally: tuning model performance parameters for active VAs"}
{"level":"DEBUG","ts":"2025-12-10T13:25:14.433Z","msg":"Using state vals from VA status to tune variant ms-inference-scheduling-llm-d-modelservice-decode: alpha= 7.304264, beta= 0.046384, gamma= 15.393015, delta= 0.000256"}
{"level":"DEBUG","ts":"2025-12-10T13:25:14.433Z","msg":"[Tuner Config] variant=ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler | Initial state: alpha=7.304264, beta=0.046384, gamma=15.393015, delta=0.000256 | Expected obs (for R): TTFT=16.34ms, ITL=8.07ms | Has covariance=true"}
{"level":"INFO","ts":"2025-12-10T13:25:14.434Z","msg":"Tuner validation succeeded for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler, server ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler - New NIS: 0.485068"}
{"level":"DEBUG","ts":"2025-12-10T13:25:14.434Z","msg":"Model tuner results: model=unsloth/Meta-Llama-3.1-8B, accelerator=H100, alpha=7.084064, beta=0.046863, gamma=15.066882, delta=0.000256, NIS=0.49"}
{"level":"DEBUG","ts":"2025-12-10T13:25:14.434Z","msg":"Updated tuner status for variant ms-inference-scheduling-llm-d-modelservice-decode: alpha=7.084064, beta=0.046863, gamma=15.066882, delta=0.000256, NIS=0.485068"}
{"level":"INFO","ts":"2025-12-10T13:25:14.434Z","msg":"Tuned performance parameters result: variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler - alpha: 7.084064, beta: 0.046863, gamma: 15.066882, delta: 0.000256"}
{"level":"DEBUG","ts":"2025-12-10T13:25:14.444Z","msg":"Optimization solution - system: Solution: \ns=ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler; c=Premium; m=unsloth/Meta-Llama-3.1-8B; rate=726.14; inTk=240; outTk=426; sol=1, sat=false, alloc={acc=H100; numRep=1; maxBatch=512; cost=100, val=-100, itl=9.4091835, ttft=18.115837, rho=0.09495177, maxRPM=860.49615}; slo-itl=10, slo-ttft=1000, slo-tps=0 \nAllocationByType: \nname=NVIDIA-H100-80GB-HBM3, count=1, limit=0, cost=100 \ntotalCost=100 \n"}
{"level":"DEBUG","ts":"2025-12-10T13:25:14.444Z","msg":"Setting accelerator name: Name=H100, allocationData={H100 1 512 100 9.4091835 18.115837 {726.14 240 426}}"}
{"level":"INFO","ts":"2025-12-10T13:25:14.444Z","msg":"Model-based optimization completed for model: unsloth/Meta-Llama-3.1-8B - model-based targets: map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"WARN","ts":"2025-12-10T13:25:14.444Z","msg":"saturation analysis unavailable, using model-based targets only: modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-10T13:25:14.444Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-12-10T13:25:14.444Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=scale-down, current=2→target=1"}
{"level":"INFO","ts":"2025-12-10T13:25:14.444Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 2, desired-replicas: 1, accelerator: H100"}
{"level":"INFO","ts":"2025-12-10T13:25:14.444Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=1, accelerator=H100, SaturationOnly=false"}
{"level":"INFO","ts":"2025-12-10T13:25:14.452Z","msg":"Applied Saturation decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=scale-down, current=2, target=1, reason=model-based only (Saturation unavailable)"}
{"level":"INFO","ts":"2025-12-10T13:25:14.452Z","msg":"Reconciliation completed successfully: mode=model-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-12-10T13:26:14.452Z","msg":"Reconciling VariantAutoscaling: name=ms-inference-scheduling-llm-d-modelservice-decode, namespace=llm-d-inference-scheduler, modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-10T13:26:14.452Z","msg":"saturation scaling config cache updated: entries=1, has_default=true"}
{"level":"INFO","ts":"2025-12-10T13:26:14.452Z","msg":"saturation scaling configuration loaded successfully"}
{"level":"INFO","ts":"2025-12-10T13:26:14.452Z","msg":"Operating in MODEL-ONLY mode: model-based optimization only"}
{"level":"INFO","ts":"2025-12-10T13:26:14.452Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-12-10T13:26:14.452Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-12-10T13:26:14.462Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dgftqv, queueLength=0"}
{"level":"DEBUG","ts":"2025-12-10T13:26:14.462Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"INFO","ts":"2025-12-10T13:26:14.472Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dgftqv, usage=0.100 (10.0%)"}
{"level":"DEBUG","ts":"2025-12-10T13:26:14.472Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"DEBUG","ts":"2025-12-10T13:26:14.474Z","msg":"Pod-to-variant matching successful: totalPods=1, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"DEBUG","ts":"2025-12-10T13:26:14.474Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=1"}
{"level":"DEBUG","ts":"2025-12-10T13:26:14.474Z","msg":"Collected capacity metrics for observability in model-only mode: modelID=unsloth/Meta-Llama-3.1-8B, replicaCount=1"}
{"level":"DEBUG","ts":"2025-12-10T13:26:14.487Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=1, accelerator=H100, ttft=20.29ms, itl=10.14ms, cost=100.00, maxBatch=256, arrivalRate=827.45, avgInputTokens=251.99, avgOutputTokens=393.36"}
{"level":"DEBUG","ts":"2025-12-10T13:26:14.487Z","msg":"Experimental model tuner is enabled globally: tuning model performance parameters for active VAs"}
{"level":"DEBUG","ts":"2025-12-10T13:26:14.487Z","msg":"Using state vals from VA status to tune variant ms-inference-scheduling-llm-d-modelservice-decode: alpha= 7.084064, beta= 0.046863, gamma= 15.066882, delta= 0.000256"}
{"level":"DEBUG","ts":"2025-12-10T13:26:14.487Z","msg":"[Tuner Config] variant=ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler | Initial state: alpha=7.084064, beta=0.046863, gamma=15.066882, delta=0.000256 | Expected obs (for R): TTFT=20.29ms, ITL=10.14ms | Has covariance=true"}
{"level":"INFO","ts":"2025-12-10T13:26:14.488Z","msg":"Tuner validation succeeded for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler, server ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler - New NIS: 3.480354"}
{"level":"DEBUG","ts":"2025-12-10T13:26:14.488Z","msg":"Model tuner results: model=unsloth/Meta-Llama-3.1-8B, accelerator=H100, alpha=7.283386, beta=0.050437, gamma=16.230640, delta=0.000258, NIS=3.48"}
{"level":"DEBUG","ts":"2025-12-10T13:26:14.488Z","msg":"Updated tuner status for variant ms-inference-scheduling-llm-d-modelservice-decode: alpha=7.283386, beta=0.050437, gamma=16.230640, delta=0.000258, NIS=3.480354"}
{"level":"INFO","ts":"2025-12-10T13:26:14.488Z","msg":"Tuned performance parameters result: variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler - alpha: 7.283386, beta: 0.050437, gamma: 16.230640, delta: 0.000258"}
{"level":"DEBUG","ts":"2025-12-10T13:26:14.497Z","msg":"Optimization solution - system: Solution: \ns=ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler; c=Premium; m=unsloth/Meta-Llama-3.1-8B; rate=827.45; inTk=251; outTk=393; sol=1, sat=false, alloc={acc=H100; numRep=2; maxBatch=512; cost=200, val=100, itl=8.498642, ttft=17.790768, rho=0.045106508, maxRPM=805.05444}; slo-itl=10, slo-ttft=1000, slo-tps=0 \nAllocationByType: \nname=NVIDIA-H100-80GB-HBM3, count=2, limit=0, cost=200 \ntotalCost=200 \n"}
{"level":"DEBUG","ts":"2025-12-10T13:26:14.497Z","msg":"Setting accelerator name: Name=H100, allocationData={H100 2 512 200 8.498642 17.790768 {827.45 251 393}}"}
{"level":"INFO","ts":"2025-12-10T13:26:14.497Z","msg":"Model-based optimization completed for model: unsloth/Meta-Llama-3.1-8B - model-based targets: map[ms-inference-scheduling-llm-d-modelservice-decode:2]"}
{"level":"WARN","ts":"2025-12-10T13:26:14.497Z","msg":"saturation analysis unavailable, using model-based targets only: modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-10T13:26:14.497Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-12-10T13:26:14.497Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=scale-up, current=1→target=2"}
{"level":"INFO","ts":"2025-12-10T13:26:14.497Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 1, desired-replicas: 2, accelerator: H100"}
{"level":"INFO","ts":"2025-12-10T13:26:14.497Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=2, accelerator=H100, SaturationOnly=false"}
{"level":"INFO","ts":"2025-12-10T13:26:14.504Z","msg":"Applied Saturation decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=scale-up, current=1, target=2, reason=model-based only (Saturation unavailable)"}
{"level":"INFO","ts":"2025-12-10T13:26:14.504Z","msg":"Reconciliation completed successfully: mode=model-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-12-10T13:27:14.505Z","msg":"Reconciling VariantAutoscaling: name=ms-inference-scheduling-llm-d-modelservice-decode, namespace=llm-d-inference-scheduler, modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-10T13:27:14.505Z","msg":"saturation scaling config cache updated: entries=1, has_default=true"}
{"level":"INFO","ts":"2025-12-10T13:27:14.505Z","msg":"saturation scaling configuration loaded successfully"}
{"level":"INFO","ts":"2025-12-10T13:27:14.505Z","msg":"Operating in MODEL-ONLY mode: model-based optimization only"}
{"level":"INFO","ts":"2025-12-10T13:27:14.505Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-12-10T13:27:14.505Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-12-10T13:27:14.509Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dgftqv, queueLength=0"}
{"level":"DEBUG","ts":"2025-12-10T13:27:14.509Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"INFO","ts":"2025-12-10T13:27:14.509Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dgftqv, usage=0.095 (9.5%)"}
{"level":"DEBUG","ts":"2025-12-10T13:27:14.509Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"DEBUG","ts":"2025-12-10T13:27:14.512Z","msg":"Pod-to-variant matching successful: totalPods=1, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"DEBUG","ts":"2025-12-10T13:27:14.512Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=1"}
{"level":"DEBUG","ts":"2025-12-10T13:27:14.512Z","msg":"Collected capacity metrics for observability in model-only mode: modelID=unsloth/Meta-Llama-3.1-8B, replicaCount=1"}
{"level":"DEBUG","ts":"2025-12-10T13:27:14.524Z","msg":"Replica count mismatch for llm-d-inference-scheduler/unsloth/Meta-Llama-3.1-8B: spec=2, reporting_metrics=1"}
{"level":"DEBUG","ts":"2025-12-10T13:27:14.524Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=1, accelerator=H100, ttft=22.09ms, itl=10.89ms, cost=100.00, maxBatch=256, arrivalRate=892.32, avgInputTokens=233.58, avgOutputTokens=441.47"}
{"level":"DEBUG","ts":"2025-12-10T13:27:14.525Z","msg":"Experimental model tuner is enabled globally: tuning model performance parameters for active VAs"}
{"level":"DEBUG","ts":"2025-12-10T13:27:14.525Z","msg":"Using state vals from VA status to tune variant ms-inference-scheduling-llm-d-modelservice-decode: alpha= 7.283386, beta= 0.050437, gamma= 16.230640, delta= 0.000258"}
{"level":"DEBUG","ts":"2025-12-10T13:27:14.525Z","msg":"[Tuner Config] variant=ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler | Initial state: alpha=7.283386, beta=0.050437, gamma=16.230640, delta=0.000258 | Expected obs (for R): TTFT=22.09ms, ITL=10.89ms | Has covariance=true"}
{"level":"INFO","ts":"2025-12-10T13:27:14.526Z","msg":"Tuner validation succeeded for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler, server ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler - New NIS: 1.886676"}
{"level":"DEBUG","ts":"2025-12-10T13:27:14.526Z","msg":"Model tuner results: model=unsloth/Meta-Llama-3.1-8B, accelerator=H100, alpha=7.270480, beta=0.050245, gamma=17.293163, delta=0.000259, NIS=1.89"}
{"level":"DEBUG","ts":"2025-12-10T13:27:14.526Z","msg":"Updated tuner status for variant ms-inference-scheduling-llm-d-modelservice-decode: alpha=7.270480, beta=0.050245, gamma=17.293163, delta=0.000259, NIS=1.886676"}
{"level":"INFO","ts":"2025-12-10T13:27:14.526Z","msg":"Tuned performance parameters result: variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler - alpha: 7.270480, beta: 0.050245, gamma: 17.293163, delta: 0.000259"}
{"level":"DEBUG","ts":"2025-12-10T13:27:14.530Z","msg":"Optimization solution - system: Solution: \ns=ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler; c=Premium; m=unsloth/Meta-Llama-3.1-8B; rate=892.32; inTk=233; outTk=441; sol=1, sat=false, alloc={acc=H100; numRep=2; maxBatch=512; cost=200, val=100, itl=8.7695055, ttft=19.09393, rho=0.056317188, maxRPM=723.7666}; slo-itl=10, slo-ttft=1000, slo-tps=0 \nAllocationByType: \nname=NVIDIA-H100-80GB-HBM3, count=2, limit=0, cost=200 \ntotalCost=200 \n"}
{"level":"DEBUG","ts":"2025-12-10T13:27:14.530Z","msg":"Setting accelerator name: Name=H100, allocationData={H100 2 512 200 8.7695055 19.09393 {892.32 233 441}}"}
{"level":"INFO","ts":"2025-12-10T13:27:14.530Z","msg":"Model-based optimization completed for model: unsloth/Meta-Llama-3.1-8B - model-based targets: map[ms-inference-scheduling-llm-d-modelservice-decode:2]"}
{"level":"WARN","ts":"2025-12-10T13:27:14.530Z","msg":"saturation analysis unavailable, using model-based targets only: modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-10T13:27:14.530Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-12-10T13:27:14.530Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=scale-up, current=1→target=2"}
{"level":"INFO","ts":"2025-12-10T13:27:14.530Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 1, desired-replicas: 2, accelerator: H100"}
{"level":"INFO","ts":"2025-12-10T13:27:14.530Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=2, accelerator=H100, SaturationOnly=false"}
{"level":"INFO","ts":"2025-12-10T13:27:14.538Z","msg":"Applied Saturation decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=scale-up, current=1, target=2, reason=model-based only (Saturation unavailable)"}
{"level":"INFO","ts":"2025-12-10T13:27:14.538Z","msg":"Reconciliation completed successfully: mode=model-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-12-10T13:28:14.538Z","msg":"Reconciling VariantAutoscaling: name=ms-inference-scheduling-llm-d-modelservice-decode, namespace=llm-d-inference-scheduler, modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-10T13:28:14.538Z","msg":"saturation scaling config cache updated: entries=1, has_default=true"}
{"level":"INFO","ts":"2025-12-10T13:28:14.538Z","msg":"saturation scaling configuration loaded successfully"}
{"level":"INFO","ts":"2025-12-10T13:28:14.538Z","msg":"Operating in MODEL-ONLY mode: model-based optimization only"}
{"level":"INFO","ts":"2025-12-10T13:28:14.539Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-12-10T13:28:14.539Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-12-10T13:28:14.549Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dgftqv, usage=0.103 (10.3%)"}
{"level":"DEBUG","ts":"2025-12-10T13:28:14.549Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"INFO","ts":"2025-12-10T13:28:14.549Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dgftqv, queueLength=0"}
{"level":"DEBUG","ts":"2025-12-10T13:28:14.549Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"DEBUG","ts":"2025-12-10T13:28:14.552Z","msg":"Pod-to-variant matching successful: totalPods=1, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"DEBUG","ts":"2025-12-10T13:28:14.552Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=1"}
{"level":"DEBUG","ts":"2025-12-10T13:28:14.552Z","msg":"Collected capacity metrics for observability in model-only mode: modelID=unsloth/Meta-Llama-3.1-8B, replicaCount=1"}
{"level":"DEBUG","ts":"2025-12-10T13:28:14.566Z","msg":"Replica count mismatch for llm-d-inference-scheduler/unsloth/Meta-Llama-3.1-8B: spec=2, reporting_metrics=1"}
{"level":"DEBUG","ts":"2025-12-10T13:28:14.566Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=1, accelerator=H100, ttft=20.90ms, itl=10.54ms, cost=100.00, maxBatch=256, arrivalRate=861.87, avgInputTokens=217.00, avgOutputTokens=453.28"}
{"level":"DEBUG","ts":"2025-12-10T13:28:14.566Z","msg":"Experimental model tuner is enabled globally: tuning model performance parameters for active VAs"}
{"level":"DEBUG","ts":"2025-12-10T13:28:14.566Z","msg":"Using state vals from VA status to tune variant ms-inference-scheduling-llm-d-modelservice-decode: alpha= 7.270480, beta= 0.050245, gamma= 17.293163, delta= 0.000259"}
{"level":"DEBUG","ts":"2025-12-10T13:28:14.566Z","msg":"[Tuner Config] variant=ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler | Initial state: alpha=7.270480, beta=0.050245, gamma=17.293163, delta=0.000259 | Expected obs (for R): TTFT=20.90ms, ITL=10.54ms | Has covariance=true"}
{"level":"INFO","ts":"2025-12-10T13:28:14.568Z","msg":"Tuner validation succeeded for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler, server ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler - New NIS: 0.333044"}
{"level":"DEBUG","ts":"2025-12-10T13:28:14.568Z","msg":"Model tuner results: model=unsloth/Meta-Llama-3.1-8B, accelerator=H100, alpha=7.114546, beta=0.049571, gamma=17.040157, delta=0.000259, NIS=0.33"}
{"level":"DEBUG","ts":"2025-12-10T13:28:14.568Z","msg":"Updated tuner status for variant ms-inference-scheduling-llm-d-modelservice-decode: alpha=7.114546, beta=0.049571, gamma=17.040157, delta=0.000259, NIS=0.333044"}
{"level":"INFO","ts":"2025-12-10T13:28:14.568Z","msg":"Tuned performance parameters result: variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler - alpha: 7.114546, beta: 0.049571, gamma: 17.040157, delta: 0.000259"}
{"level":"DEBUG","ts":"2025-12-10T13:28:14.578Z","msg":"Optimization solution - system: Solution: \ns=ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler; c=Premium; m=unsloth/Meta-Llama-3.1-8B; rate=861.87; inTk=217; outTk=453; sol=1, sat=false, alloc={acc=H100; numRep=2; maxBatch=512; cost=200, val=100, itl=8.546056, ttft=18.663824, rho=0.054448757, maxRPM=756.0038}; slo-itl=10, slo-ttft=1000, slo-tps=0 \nAllocationByType: \nname=NVIDIA-H100-80GB-HBM3, count=2, limit=0, cost=200 \ntotalCost=200 \n"}
{"level":"DEBUG","ts":"2025-12-10T13:28:14.578Z","msg":"Setting accelerator name: Name=H100, allocationData={H100 2 512 200 8.546056 18.663824 {861.87 217 453}}"}
{"level":"INFO","ts":"2025-12-10T13:28:14.578Z","msg":"Model-based optimization completed for model: unsloth/Meta-Llama-3.1-8B - model-based targets: map[ms-inference-scheduling-llm-d-modelservice-decode:2]"}
{"level":"WARN","ts":"2025-12-10T13:28:14.578Z","msg":"saturation analysis unavailable, using model-based targets only: modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-10T13:28:14.578Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-12-10T13:28:14.578Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=scale-up, current=1→target=2"}
{"level":"INFO","ts":"2025-12-10T13:28:14.578Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 1, desired-replicas: 2, accelerator: H100"}
{"level":"INFO","ts":"2025-12-10T13:28:14.578Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=2, accelerator=H100, SaturationOnly=false"}
{"level":"INFO","ts":"2025-12-10T13:28:14.586Z","msg":"Applied Saturation decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=scale-up, current=1, target=2, reason=model-based only (Saturation unavailable)"}
{"level":"INFO","ts":"2025-12-10T13:28:14.586Z","msg":"Reconciliation completed successfully: mode=model-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-12-10T13:29:14.587Z","msg":"Reconciling VariantAutoscaling: name=ms-inference-scheduling-llm-d-modelservice-decode, namespace=llm-d-inference-scheduler, modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-10T13:29:14.587Z","msg":"saturation scaling config cache updated: entries=1, has_default=true"}
{"level":"INFO","ts":"2025-12-10T13:29:14.587Z","msg":"saturation scaling configuration loaded successfully"}
{"level":"INFO","ts":"2025-12-10T13:29:14.587Z","msg":"Operating in MODEL-ONLY mode: model-based optimization only"}
{"level":"INFO","ts":"2025-12-10T13:29:14.587Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-12-10T13:29:14.587Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-12-10T13:29:14.594Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dgftqv, queueLength=0"}
{"level":"INFO","ts":"2025-12-10T13:29:14.594Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dlnxqm, queueLength=0"}
{"level":"DEBUG","ts":"2025-12-10T13:29:14.594Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=2"}
{"level":"INFO","ts":"2025-12-10T13:29:14.594Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dgftqv, usage=0.078 (7.8%)"}
{"level":"INFO","ts":"2025-12-10T13:29:14.594Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dlnxqm, usage=0.025 (2.5%)"}
{"level":"DEBUG","ts":"2025-12-10T13:29:14.594Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=2"}
{"level":"DEBUG","ts":"2025-12-10T13:29:14.597Z","msg":"Pod-to-variant matching successful: totalPods=2, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:2]"}
{"level":"DEBUG","ts":"2025-12-10T13:29:14.597Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=2"}
{"level":"DEBUG","ts":"2025-12-10T13:29:14.597Z","msg":"Collected capacity metrics for observability in model-only mode: modelID=unsloth/Meta-Llama-3.1-8B, replicaCount=2"}
{"level":"DEBUG","ts":"2025-12-10T13:29:14.609Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=2, accelerator=H100, ttft=20.07ms, itl=9.77ms, cost=200.00, maxBatch=256, arrivalRate=758.66, avgInputTokens=234.41, avgOutputTokens=454.00"}
{"level":"DEBUG","ts":"2025-12-10T13:29:14.609Z","msg":"Experimental model tuner is enabled globally: tuning model performance parameters for active VAs"}
{"level":"DEBUG","ts":"2025-12-10T13:29:14.609Z","msg":"Using state vals from VA status to tune variant ms-inference-scheduling-llm-d-modelservice-decode: alpha= 7.114546, beta= 0.049571, gamma= 17.040157, delta= 0.000259"}
{"level":"DEBUG","ts":"2025-12-10T13:29:14.609Z","msg":"[Tuner Config] variant=ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler | Initial state: alpha=7.114546, beta=0.049571, gamma=17.040157, delta=0.000259 | Expected obs (for R): TTFT=20.07ms, ITL=9.77ms | Has covariance=true"}
{"level":"INFO","ts":"2025-12-10T13:29:14.610Z","msg":"Tuner validation succeeded for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler, server ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler - New NIS: 6.380026"}
{"level":"DEBUG","ts":"2025-12-10T13:29:14.610Z","msg":"Model tuner results: model=unsloth/Meta-Llama-3.1-8B, accelerator=H100, alpha=8.355497, beta=0.041324, gamma=18.068615, delta=0.000257, NIS=6.38"}
{"level":"DEBUG","ts":"2025-12-10T13:29:14.610Z","msg":"Updated tuner status for variant ms-inference-scheduling-llm-d-modelservice-decode: alpha=8.355497, beta=0.041324, gamma=18.068615, delta=0.000257, NIS=6.380026"}
{"level":"INFO","ts":"2025-12-10T13:29:14.610Z","msg":"Tuned performance parameters result: variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler - alpha: 8.355497, beta: 0.041324, gamma: 18.068615, delta: 0.000257"}
{"level":"DEBUG","ts":"2025-12-10T13:29:14.619Z","msg":"Optimization solution - system: Solution: \ns=ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler; c=Premium; m=unsloth/Meta-Llama-3.1-8B; rate=758.66; inTk=234; outTk=454; sol=1, sat=false, alloc={acc=H100; numRep=2; maxBatch=512; cost=200, val=0, itl=9.529856, ttft=19.776237, rho=0.053550735, maxRPM=511.52756}; slo-itl=10, slo-ttft=1000, slo-tps=0 \nAllocationByType: \nname=NVIDIA-H100-80GB-HBM3, count=2, limit=0, cost=200 \ntotalCost=200 \n"}
{"level":"DEBUG","ts":"2025-12-10T13:29:14.619Z","msg":"Setting accelerator name: Name=H100, allocationData={H100 2 512 200 9.529856 19.776237 {758.66 234 454}}"}
{"level":"INFO","ts":"2025-12-10T13:29:14.619Z","msg":"Model-based optimization completed for model: unsloth/Meta-Llama-3.1-8B - model-based targets: map[ms-inference-scheduling-llm-d-modelservice-decode:2]"}
{"level":"WARN","ts":"2025-12-10T13:29:14.619Z","msg":"saturation analysis unavailable, using model-based targets only: modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-10T13:29:14.619Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-12-10T13:29:14.619Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=2→target=2"}
{"level":"INFO","ts":"2025-12-10T13:29:14.619Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 2, desired-replicas: 2, accelerator: H100"}
{"level":"INFO","ts":"2025-12-10T13:29:14.619Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=2, accelerator=H100, SaturationOnly=false"}
{"level":"INFO","ts":"2025-12-10T13:29:14.626Z","msg":"Applied Saturation decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=2, target=2, reason=model-based only (Saturation unavailable)"}
{"level":"INFO","ts":"2025-12-10T13:29:14.626Z","msg":"Reconciliation completed successfully: mode=model-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-12-10T13:30:14.626Z","msg":"Reconciling VariantAutoscaling: name=ms-inference-scheduling-llm-d-modelservice-decode, namespace=llm-d-inference-scheduler, modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-10T13:30:14.626Z","msg":"saturation scaling config cache updated: entries=1, has_default=true"}
{"level":"INFO","ts":"2025-12-10T13:30:14.626Z","msg":"saturation scaling configuration loaded successfully"}
{"level":"INFO","ts":"2025-12-10T13:30:14.626Z","msg":"Operating in MODEL-ONLY mode: model-based optimization only"}
{"level":"INFO","ts":"2025-12-10T13:30:14.626Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-12-10T13:30:14.626Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-12-10T13:30:14.637Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dgftqv, queueLength=0"}
{"level":"INFO","ts":"2025-12-10T13:30:14.637Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dlnxqm, queueLength=0"}
{"level":"DEBUG","ts":"2025-12-10T13:30:14.637Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=2"}
{"level":"INFO","ts":"2025-12-10T13:30:14.637Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dgftqv, usage=0.077 (7.7%)"}
{"level":"INFO","ts":"2025-12-10T13:30:14.637Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dlnxqm, usage=0.026 (2.6%)"}
{"level":"DEBUG","ts":"2025-12-10T13:30:14.637Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=2"}
{"level":"DEBUG","ts":"2025-12-10T13:30:14.640Z","msg":"Pod-to-variant matching successful: totalPods=2, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:2]"}
{"level":"DEBUG","ts":"2025-12-10T13:30:14.640Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=2"}
{"level":"DEBUG","ts":"2025-12-10T13:30:14.640Z","msg":"Collected capacity metrics for observability in model-only mode: modelID=unsloth/Meta-Llama-3.1-8B, replicaCount=2"}
{"level":"DEBUG","ts":"2025-12-10T13:30:14.653Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=2, accelerator=H100, ttft=17.38ms, itl=8.75ms, cost=200.00, maxBatch=256, arrivalRate=755.37, avgInputTokens=235.70, avgOutputTokens=489.95"}
{"level":"DEBUG","ts":"2025-12-10T13:30:14.653Z","msg":"Experimental model tuner is enabled globally: tuning model performance parameters for active VAs"}
{"level":"DEBUG","ts":"2025-12-10T13:30:14.653Z","msg":"Using state vals from VA status to tune variant ms-inference-scheduling-llm-d-modelservice-decode: alpha= 8.355497, beta= 0.041324, gamma= 18.068615, delta= 0.000257"}
{"level":"DEBUG","ts":"2025-12-10T13:30:14.653Z","msg":"[Tuner Config] variant=ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler | Initial state: alpha=8.355497, beta=0.041324, gamma=18.068615, delta=0.000257 | Expected obs (for R): TTFT=17.38ms, ITL=8.75ms | Has covariance=true"}
{"level":"INFO","ts":"2025-12-10T13:30:14.654Z","msg":"Tuner validation succeeded for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler, server ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler - New NIS: 7.084227"}
{"level":"DEBUG","ts":"2025-12-10T13:30:14.654Z","msg":"Model tuner results: model=unsloth/Meta-Llama-3.1-8B, accelerator=H100, alpha=7.702854, beta=0.041101, gamma=16.058758, delta=0.000257, NIS=7.08"}
{"level":"DEBUG","ts":"2025-12-10T13:30:14.654Z","msg":"Updated tuner status for variant ms-inference-scheduling-llm-d-modelservice-decode: alpha=7.702854, beta=0.041101, gamma=16.058758, delta=0.000257, NIS=7.084227"}
{"level":"INFO","ts":"2025-12-10T13:30:14.654Z","msg":"Tuned performance parameters result: variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler - alpha: 7.702854, beta: 0.041101, gamma: 16.058758, delta: 0.000257"}
{"level":"DEBUG","ts":"2025-12-10T13:30:14.663Z","msg":"Optimization solution - system: Solution: \ns=ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler; c=Premium; m=unsloth/Meta-Llama-3.1-8B; rate=755.37; inTk=235; outTk=489; sol=1, sat=false, alloc={acc=H100; numRep=2; maxBatch=512; cost=200, val=0, itl=8.8682165, ttft=17.768658, rho=0.05342495, maxRPM=672.2014}; slo-itl=10, slo-ttft=1000, slo-tps=0 \nAllocationByType: \nname=NVIDIA-H100-80GB-HBM3, count=2, limit=0, cost=200 \ntotalCost=200 \n"}
{"level":"DEBUG","ts":"2025-12-10T13:30:14.663Z","msg":"Setting accelerator name: Name=H100, allocationData={H100 2 512 200 8.8682165 17.768658 {755.37 235 489}}"}
{"level":"INFO","ts":"2025-12-10T13:30:14.663Z","msg":"Model-based optimization completed for model: unsloth/Meta-Llama-3.1-8B - model-based targets: map[ms-inference-scheduling-llm-d-modelservice-decode:2]"}
{"level":"WARN","ts":"2025-12-10T13:30:14.663Z","msg":"saturation analysis unavailable, using model-based targets only: modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-10T13:30:14.663Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-12-10T13:30:14.663Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=2→target=2"}
{"level":"INFO","ts":"2025-12-10T13:30:14.663Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 2, desired-replicas: 2, accelerator: H100"}
{"level":"INFO","ts":"2025-12-10T13:30:14.663Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=2, accelerator=H100, SaturationOnly=false"}
{"level":"INFO","ts":"2025-12-10T13:30:14.670Z","msg":"Applied Saturation decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=2, target=2, reason=model-based only (Saturation unavailable)"}
{"level":"INFO","ts":"2025-12-10T13:30:14.670Z","msg":"Reconciliation completed successfully: mode=model-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-12-10T13:31:14.671Z","msg":"Reconciling VariantAutoscaling: name=ms-inference-scheduling-llm-d-modelservice-decode, namespace=llm-d-inference-scheduler, modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-10T13:31:14.671Z","msg":"saturation scaling config cache updated: entries=1, has_default=true"}
{"level":"INFO","ts":"2025-12-10T13:31:14.671Z","msg":"saturation scaling configuration loaded successfully"}
{"level":"INFO","ts":"2025-12-10T13:31:14.671Z","msg":"Operating in MODEL-ONLY mode: model-based optimization only"}
{"level":"INFO","ts":"2025-12-10T13:31:14.671Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-12-10T13:31:14.671Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-12-10T13:31:14.679Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dgftqv, queueLength=0"}
{"level":"INFO","ts":"2025-12-10T13:31:14.679Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dlnxqm, queueLength=0"}
{"level":"DEBUG","ts":"2025-12-10T13:31:14.679Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=2"}
{"level":"INFO","ts":"2025-12-10T13:31:14.679Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dgftqv, usage=0.033 (3.3%)"}
{"level":"INFO","ts":"2025-12-10T13:31:14.679Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dlnxqm, usage=0.051 (5.1%)"}
{"level":"DEBUG","ts":"2025-12-10T13:31:14.679Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=2"}
{"level":"DEBUG","ts":"2025-12-10T13:31:14.682Z","msg":"Pod-to-variant matching successful: totalPods=2, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:2]"}
{"level":"DEBUG","ts":"2025-12-10T13:31:14.682Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=2"}
{"level":"DEBUG","ts":"2025-12-10T13:31:14.682Z","msg":"Collected capacity metrics for observability in model-only mode: modelID=unsloth/Meta-Llama-3.1-8B, replicaCount=2"}
{"level":"DEBUG","ts":"2025-12-10T13:31:14.694Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=2, accelerator=H100, ttft=15.80ms, itl=8.25ms, cost=200.00, maxBatch=256, arrivalRate=738.05, avgInputTokens=239.76, avgOutputTokens=453.46"}
{"level":"DEBUG","ts":"2025-12-10T13:31:14.694Z","msg":"Experimental model tuner is enabled globally: tuning model performance parameters for active VAs"}
{"level":"DEBUG","ts":"2025-12-10T13:31:14.694Z","msg":"Using state vals from VA status to tune variant ms-inference-scheduling-llm-d-modelservice-decode: alpha= 7.702854, beta= 0.041101, gamma= 16.058758, delta= 0.000257"}
{"level":"DEBUG","ts":"2025-12-10T13:31:14.694Z","msg":"[Tuner Config] variant=ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler | Initial state: alpha=7.702854, beta=0.041101, gamma=16.058758, delta=0.000257 | Expected obs (for R): TTFT=15.80ms, ITL=8.25ms | Has covariance=true"}
{"level":"INFO","ts":"2025-12-10T13:31:14.694Z","msg":"Tuner validation succeeded for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler, server ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler - New NIS: 4.062560"}
{"level":"DEBUG","ts":"2025-12-10T13:31:14.694Z","msg":"Model tuner results: model=unsloth/Meta-Llama-3.1-8B, accelerator=H100, alpha=7.317107, beta=0.041357, gamma=14.604880, delta=0.000257, NIS=4.06"}
{"level":"DEBUG","ts":"2025-12-10T13:31:14.694Z","msg":"Updated tuner status for variant ms-inference-scheduling-llm-d-modelservice-decode: alpha=7.317107, beta=0.041357, gamma=14.604880, delta=0.000257, NIS=4.062560"}
{"level":"INFO","ts":"2025-12-10T13:31:14.694Z","msg":"Tuned performance parameters result: variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler - alpha: 7.317107, beta: 0.041357, gamma: 14.604880, delta: 0.000257"}
{"level":"DEBUG","ts":"2025-12-10T13:31:14.703Z","msg":"Optimization solution - system: Solution: \ns=ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler; c=Premium; m=unsloth/Meta-Llama-3.1-8B; rate=738.05; inTk=239; outTk=453; sol=1, sat=false, alloc={acc=H100; numRep=1; maxBatch=512; cost=100, val=-100, itl=9.567637, ttft=17.94723, rho=0.10432931, maxRPM=844.3735}; slo-itl=10, slo-ttft=1000, slo-tps=0 \nAllocationByType: \nname=NVIDIA-H100-80GB-HBM3, count=1, limit=0, cost=100 \ntotalCost=100 \n"}
{"level":"DEBUG","ts":"2025-12-10T13:31:14.703Z","msg":"Setting accelerator name: Name=H100, allocationData={H100 1 512 100 9.567637 17.94723 {738.05 239 453}}"}
{"level":"INFO","ts":"2025-12-10T13:31:14.703Z","msg":"Model-based optimization completed for model: unsloth/Meta-Llama-3.1-8B - model-based targets: map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"WARN","ts":"2025-12-10T13:31:14.703Z","msg":"saturation analysis unavailable, using model-based targets only: modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-10T13:31:14.703Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-12-10T13:31:14.703Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=scale-down, current=2→target=1"}
{"level":"INFO","ts":"2025-12-10T13:31:14.703Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 2, desired-replicas: 1, accelerator: H100"}
{"level":"INFO","ts":"2025-12-10T13:31:14.703Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=1, accelerator=H100, SaturationOnly=false"}
{"level":"INFO","ts":"2025-12-10T13:31:14.711Z","msg":"Applied Saturation decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=scale-down, current=2, target=1, reason=model-based only (Saturation unavailable)"}
{"level":"INFO","ts":"2025-12-10T13:31:14.711Z","msg":"Reconciliation completed successfully: mode=model-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-12-10T13:32:14.711Z","msg":"Reconciling VariantAutoscaling: name=ms-inference-scheduling-llm-d-modelservice-decode, namespace=llm-d-inference-scheduler, modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-10T13:32:14.711Z","msg":"saturation scaling config cache updated: entries=1, has_default=true"}
{"level":"INFO","ts":"2025-12-10T13:32:14.711Z","msg":"saturation scaling configuration loaded successfully"}
{"level":"INFO","ts":"2025-12-10T13:32:14.711Z","msg":"Operating in MODEL-ONLY mode: model-based optimization only"}
{"level":"INFO","ts":"2025-12-10T13:32:14.711Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-12-10T13:32:14.711Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-12-10T13:32:14.722Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dgftqv, queueLength=0"}
{"level":"DEBUG","ts":"2025-12-10T13:32:14.722Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"INFO","ts":"2025-12-10T13:32:14.731Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dgftqv, usage=0.111 (11.1%)"}
{"level":"DEBUG","ts":"2025-12-10T13:32:14.732Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"DEBUG","ts":"2025-12-10T13:32:14.734Z","msg":"Pod-to-variant matching successful: totalPods=1, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"DEBUG","ts":"2025-12-10T13:32:14.734Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=1"}
{"level":"DEBUG","ts":"2025-12-10T13:32:14.734Z","msg":"Collected capacity metrics for observability in model-only mode: modelID=unsloth/Meta-Llama-3.1-8B, replicaCount=1"}
{"level":"DEBUG","ts":"2025-12-10T13:32:14.746Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=1, accelerator=H100, ttft=20.99ms, itl=10.38ms, cost=100.00, maxBatch=256, arrivalRate=803.62, avgInputTokens=258.18, avgOutputTokens=412.34"}
{"level":"DEBUG","ts":"2025-12-10T13:32:14.746Z","msg":"Experimental model tuner is enabled globally: tuning model performance parameters for active VAs"}
{"level":"DEBUG","ts":"2025-12-10T13:32:14.746Z","msg":"Using state vals from VA status to tune variant ms-inference-scheduling-llm-d-modelservice-decode: alpha= 7.317107, beta= 0.041357, gamma= 14.604880, delta= 0.000257"}
{"level":"DEBUG","ts":"2025-12-10T13:32:14.746Z","msg":"[Tuner Config] variant=ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler | Initial state: alpha=7.317107, beta=0.041357, gamma=14.604880, delta=0.000257 | Expected obs (for R): TTFT=20.99ms, ITL=10.38ms | Has covariance=true"}
{"level":"WARN","ts":"2025-12-10T13:32:14.747Z","msg":"Tuner validation failed (NIS=8.46), validation error: normalized innovation squared (NIS=8.46) exceeds threshold (7.38), rejecting update as outlier - returning previous state: alpha=7.317107, beta=0.041357, gamma=14.604880, delta=0.000257"}
{"level":"WARN","ts":"2025-12-10T13:32:14.747Z","msg":"Tuner NIS validation failed for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler, server ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler (NIS=8.46 exceeds threshold 7.38) - Keeping previous state: alpha=7.317107, beta=0.041357, gamma=14.604880, delta=0.000257"}
{"level":"INFO","ts":"2025-12-10T13:32:14.747Z","msg":"Keeping previous tuned parameters for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler due to NIS validation failure (NIS=8.458242)"}
{"level":"DEBUG","ts":"2025-12-10T13:32:14.747Z","msg":"Model tuner results: model=unsloth/Meta-Llama-3.1-8B, accelerator=H100, alpha=7.317107, beta=0.041357, gamma=14.604880, delta=0.000257, NIS=8.46"}
{"level":"DEBUG","ts":"2025-12-10T13:32:14.747Z","msg":"Updated tuner status for variant ms-inference-scheduling-llm-d-modelservice-decode: alpha=7.317107, beta=0.041357, gamma=14.604880, delta=0.000257, NIS=8.458242"}
{"level":"DEBUG","ts":"2025-12-10T13:32:14.757Z","msg":"Optimization solution - system: Solution: \ns=ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler; c=Premium; m=unsloth/Meta-Llama-3.1-8B; rate=803.62; inTk=258; outTk=412; sol=1, sat=false, alloc={acc=H100; numRep=1; maxBatch=512; cost=100, val=0, itl=9.540547, ttft=18.169632, rho=0.103051044, maxRPM=928.1597}; slo-itl=10, slo-ttft=1000, slo-tps=0 \nAllocationByType: \nname=NVIDIA-H100-80GB-HBM3, count=1, limit=0, cost=100 \ntotalCost=100 \n"}
{"level":"DEBUG","ts":"2025-12-10T13:32:14.757Z","msg":"Setting accelerator name: Name=H100, allocationData={H100 1 512 100 9.540547 18.169632 {803.62 258 412}}"}
{"level":"INFO","ts":"2025-12-10T13:32:14.757Z","msg":"Model-based optimization completed for model: unsloth/Meta-Llama-3.1-8B - model-based targets: map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"WARN","ts":"2025-12-10T13:32:14.757Z","msg":"saturation analysis unavailable, using model-based targets only: modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-10T13:32:14.757Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-12-10T13:32:14.757Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1→target=1"}
{"level":"INFO","ts":"2025-12-10T13:32:14.757Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 1, desired-replicas: 1, accelerator: H100"}
{"level":"INFO","ts":"2025-12-10T13:32:14.757Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=1, accelerator=H100, SaturationOnly=false"}
{"level":"INFO","ts":"2025-12-10T13:32:14.766Z","msg":"Applied Saturation decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1, target=1, reason=model-based only (Saturation unavailable)"}
{"level":"INFO","ts":"2025-12-10T13:32:14.766Z","msg":"Reconciliation completed successfully: mode=model-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-12-10T13:33:14.767Z","msg":"Reconciling VariantAutoscaling: name=ms-inference-scheduling-llm-d-modelservice-decode, namespace=llm-d-inference-scheduler, modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-10T13:33:14.767Z","msg":"saturation scaling config cache updated: entries=1, has_default=true"}
{"level":"INFO","ts":"2025-12-10T13:33:14.767Z","msg":"saturation scaling configuration loaded successfully"}
{"level":"INFO","ts":"2025-12-10T13:33:14.767Z","msg":"Operating in MODEL-ONLY mode: model-based optimization only"}
{"level":"INFO","ts":"2025-12-10T13:33:14.767Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-12-10T13:33:14.767Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-12-10T13:33:14.773Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dgftqv, queueLength=0"}
{"level":"DEBUG","ts":"2025-12-10T13:33:14.773Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"INFO","ts":"2025-12-10T13:33:14.773Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dgftqv, usage=0.088 (8.8%)"}
{"level":"DEBUG","ts":"2025-12-10T13:33:14.773Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"DEBUG","ts":"2025-12-10T13:33:14.775Z","msg":"Pod-to-variant matching successful: totalPods=1, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"DEBUG","ts":"2025-12-10T13:33:14.776Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=1"}
{"level":"DEBUG","ts":"2025-12-10T13:33:14.776Z","msg":"Collected capacity metrics for observability in model-only mode: modelID=unsloth/Meta-Llama-3.1-8B, replicaCount=1"}
{"level":"DEBUG","ts":"2025-12-10T13:33:14.787Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=1, accelerator=H100, ttft=20.57ms, itl=9.87ms, cost=100.00, maxBatch=256, arrivalRate=766.55, avgInputTokens=245.58, avgOutputTokens=418.04"}
{"level":"DEBUG","ts":"2025-12-10T13:33:14.787Z","msg":"Experimental model tuner is enabled globally: tuning model performance parameters for active VAs"}
{"level":"DEBUG","ts":"2025-12-10T13:33:14.787Z","msg":"Using state vals from VA status to tune variant ms-inference-scheduling-llm-d-modelservice-decode: alpha= 7.317107, beta= 0.041357, gamma= 14.604880, delta= 0.000257"}
{"level":"DEBUG","ts":"2025-12-10T13:33:14.787Z","msg":"[Tuner Config] variant=ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler | Initial state: alpha=7.317107, beta=0.041357, gamma=14.604880, delta=0.000257 | Expected obs (for R): TTFT=20.57ms, ITL=9.87ms | Has covariance=true"}
{"level":"WARN","ts":"2025-12-10T13:33:14.788Z","msg":"Tuner validation failed (NIS=7.52), validation error: normalized innovation squared (NIS=7.52) exceeds threshold (7.38), rejecting update as outlier - returning previous state: alpha=7.317107, beta=0.041357, gamma=14.604880, delta=0.000257"}
{"level":"WARN","ts":"2025-12-10T13:33:14.788Z","msg":"Tuner NIS validation failed for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler, server ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler (NIS=7.52 exceeds threshold 7.38) - Keeping previous state: alpha=7.317107, beta=0.041357, gamma=14.604880, delta=0.000257"}
{"level":"INFO","ts":"2025-12-10T13:33:14.788Z","msg":"Keeping previous tuned parameters for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler due to NIS validation failure (NIS=7.519505)"}
{"level":"DEBUG","ts":"2025-12-10T13:33:14.788Z","msg":"Model tuner results: model=unsloth/Meta-Llama-3.1-8B, accelerator=H100, alpha=7.317107, beta=0.041357, gamma=14.604880, delta=0.000257, NIS=7.52"}
{"level":"DEBUG","ts":"2025-12-10T13:33:14.788Z","msg":"Updated tuner status for variant ms-inference-scheduling-llm-d-modelservice-decode: alpha=7.317107, beta=0.041357, gamma=14.604880, delta=0.000257, NIS=7.519505"}
{"level":"DEBUG","ts":"2025-12-10T13:33:14.797Z","msg":"Optimization solution - system: Solution: \ns=ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler; c=Premium; m=unsloth/Meta-Llama-3.1-8B; rate=766.55; inTk=245; outTk=418; sol=1, sat=false, alloc={acc=H100; numRep=1; maxBatch=512; cost=100, val=0, itl=9.450024, ttft=17.852192, rho=0.09877595, maxRPM=914.91693}; slo-itl=10, slo-ttft=1000, slo-tps=0 \nAllocationByType: \nname=NVIDIA-H100-80GB-HBM3, count=1, limit=0, cost=100 \ntotalCost=100 \n"}
{"level":"DEBUG","ts":"2025-12-10T13:33:14.797Z","msg":"Setting accelerator name: Name=H100, allocationData={H100 1 512 100 9.450024 17.852192 {766.55 245 418}}"}
{"level":"INFO","ts":"2025-12-10T13:33:14.797Z","msg":"Model-based optimization completed for model: unsloth/Meta-Llama-3.1-8B - model-based targets: map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"WARN","ts":"2025-12-10T13:33:14.797Z","msg":"saturation analysis unavailable, using model-based targets only: modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-10T13:33:14.797Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-12-10T13:33:14.797Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1→target=1"}
{"level":"INFO","ts":"2025-12-10T13:33:14.797Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 1, desired-replicas: 1, accelerator: H100"}
{"level":"INFO","ts":"2025-12-10T13:33:14.797Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=1, accelerator=H100, SaturationOnly=false"}
{"level":"INFO","ts":"2025-12-10T13:33:14.804Z","msg":"Applied Saturation decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1, target=1, reason=model-based only (Saturation unavailable)"}
{"level":"INFO","ts":"2025-12-10T13:33:14.804Z","msg":"Reconciliation completed successfully: mode=model-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-12-10T13:34:14.805Z","msg":"Reconciling VariantAutoscaling: name=ms-inference-scheduling-llm-d-modelservice-decode, namespace=llm-d-inference-scheduler, modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-10T13:34:14.805Z","msg":"saturation scaling config cache updated: entries=1, has_default=true"}
{"level":"INFO","ts":"2025-12-10T13:34:14.805Z","msg":"saturation scaling configuration loaded successfully"}
{"level":"INFO","ts":"2025-12-10T13:34:14.805Z","msg":"Operating in MODEL-ONLY mode: model-based optimization only"}
{"level":"INFO","ts":"2025-12-10T13:34:14.805Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-12-10T13:34:14.805Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-12-10T13:34:14.816Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dgftqv, usage=0.092 (9.2%)"}
{"level":"DEBUG","ts":"2025-12-10T13:34:14.816Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"INFO","ts":"2025-12-10T13:34:14.816Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dgftqv, queueLength=0"}
{"level":"DEBUG","ts":"2025-12-10T13:34:14.816Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"DEBUG","ts":"2025-12-10T13:34:14.818Z","msg":"Pod-to-variant matching successful: totalPods=1, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"DEBUG","ts":"2025-12-10T13:34:14.819Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=1"}
{"level":"DEBUG","ts":"2025-12-10T13:34:14.819Z","msg":"Collected capacity metrics for observability in model-only mode: modelID=unsloth/Meta-Llama-3.1-8B, replicaCount=1"}
{"level":"DEBUG","ts":"2025-12-10T13:34:14.831Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=1, accelerator=H100, ttft=20.21ms, itl=9.90ms, cost=100.00, maxBatch=256, arrivalRate=800.97, avgInputTokens=232.83, avgOutputTokens=424.68"}
{"level":"DEBUG","ts":"2025-12-10T13:34:14.831Z","msg":"Experimental model tuner is enabled globally: tuning model performance parameters for active VAs"}
{"level":"DEBUG","ts":"2025-12-10T13:34:14.831Z","msg":"Using state vals from VA status to tune variant ms-inference-scheduling-llm-d-modelservice-decode: alpha= 7.317107, beta= 0.041357, gamma= 14.604880, delta= 0.000257"}
{"level":"DEBUG","ts":"2025-12-10T13:34:14.831Z","msg":"[Tuner Config] variant=ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler | Initial state: alpha=7.317107, beta=0.041357, gamma=14.604880, delta=0.000257 | Expected obs (for R): TTFT=20.21ms, ITL=9.90ms | Has covariance=true"}
{"level":"INFO","ts":"2025-12-10T13:34:14.831Z","msg":"Tuner validation succeeded for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler, server ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler - New NIS: 5.359843"}
{"level":"DEBUG","ts":"2025-12-10T13:34:14.831Z","msg":"Model tuner results: model=unsloth/Meta-Llama-3.1-8B, accelerator=H100, alpha=7.437888, beta=0.043105, gamma=16.147730, delta=0.000260, NIS=5.36"}
{"level":"DEBUG","ts":"2025-12-10T13:34:14.831Z","msg":"Updated tuner status for variant ms-inference-scheduling-llm-d-modelservice-decode: alpha=7.437888, beta=0.043105, gamma=16.147730, delta=0.000260, NIS=5.359843"}
{"level":"INFO","ts":"2025-12-10T13:34:14.831Z","msg":"Tuned performance parameters result: variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler - alpha: 7.437888, beta: 0.043105, gamma: 16.147730, delta: 0.000260"}
{"level":"DEBUG","ts":"2025-12-10T13:34:14.835Z","msg":"Optimization solution - system: Solution: \ns=ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler; c=Premium; m=unsloth/Meta-Llama-3.1-8B; rate=800.97; inTk=232; outTk=424; sol=1, sat=false, alloc={acc=H100; numRep=1; maxBatch=512; cost=100, val=0, itl=9.902638, ttft=19.591969, rho=0.109726846, maxRPM=825.0729}; slo-itl=10, slo-ttft=1000, slo-tps=0 \nAllocationByType: \nname=NVIDIA-H100-80GB-HBM3, count=1, limit=0, cost=100 \ntotalCost=100 \n"}
{"level":"DEBUG","ts":"2025-12-10T13:34:14.835Z","msg":"Setting accelerator name: Name=H100, allocationData={H100 1 512 100 9.902638 19.591969 {800.97 232 424}}"}
{"level":"INFO","ts":"2025-12-10T13:34:14.835Z","msg":"Model-based optimization completed for model: unsloth/Meta-Llama-3.1-8B - model-based targets: map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"WARN","ts":"2025-12-10T13:34:14.835Z","msg":"saturation analysis unavailable, using model-based targets only: modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-10T13:34:14.835Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-12-10T13:34:14.835Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1→target=1"}
{"level":"INFO","ts":"2025-12-10T13:34:14.835Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 1, desired-replicas: 1, accelerator: H100"}
{"level":"INFO","ts":"2025-12-10T13:34:14.835Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=1, accelerator=H100, SaturationOnly=false"}
{"level":"INFO","ts":"2025-12-10T13:34:14.843Z","msg":"Applied Saturation decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1, target=1, reason=model-based only (Saturation unavailable)"}
{"level":"INFO","ts":"2025-12-10T13:34:14.843Z","msg":"Reconciliation completed successfully: mode=model-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-12-10T13:35:14.844Z","msg":"Reconciling VariantAutoscaling: name=ms-inference-scheduling-llm-d-modelservice-decode, namespace=llm-d-inference-scheduler, modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-10T13:35:14.844Z","msg":"saturation scaling config cache updated: entries=1, has_default=true"}
{"level":"INFO","ts":"2025-12-10T13:35:14.844Z","msg":"saturation scaling configuration loaded successfully"}
{"level":"INFO","ts":"2025-12-10T13:35:14.844Z","msg":"Operating in MODEL-ONLY mode: model-based optimization only"}
{"level":"INFO","ts":"2025-12-10T13:35:14.844Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-12-10T13:35:14.844Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-12-10T13:35:14.853Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dgftqv, queueLength=0"}
{"level":"INFO","ts":"2025-12-10T13:35:14.853Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dgftqv, usage=0.089 (8.9%)"}
{"level":"DEBUG","ts":"2025-12-10T13:35:14.853Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"DEBUG","ts":"2025-12-10T13:35:14.853Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"DEBUG","ts":"2025-12-10T13:35:14.855Z","msg":"Pod-to-variant matching successful: totalPods=1, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"DEBUG","ts":"2025-12-10T13:35:14.855Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=1"}
{"level":"DEBUG","ts":"2025-12-10T13:35:14.855Z","msg":"Collected capacity metrics for observability in model-only mode: modelID=unsloth/Meta-Llama-3.1-8B, replicaCount=1"}
{"level":"DEBUG","ts":"2025-12-10T13:35:14.867Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=1, accelerator=H100, ttft=20.46ms, itl=10.10ms, cost=100.00, maxBatch=256, arrivalRate=744.04, avgInputTokens=220.75, avgOutputTokens=475.17"}
{"level":"DEBUG","ts":"2025-12-10T13:35:14.867Z","msg":"Experimental model tuner is enabled globally: tuning model performance parameters for active VAs"}
{"level":"DEBUG","ts":"2025-12-10T13:35:14.867Z","msg":"Using state vals from VA status to tune variant ms-inference-scheduling-llm-d-modelservice-decode: alpha= 7.437888, beta= 0.043105, gamma= 16.147730, delta= 0.000260"}
{"level":"DEBUG","ts":"2025-12-10T13:35:14.867Z","msg":"[Tuner Config] variant=ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler | Initial state: alpha=7.437888, beta=0.043105, gamma=16.147730, delta=0.000260 | Expected obs (for R): TTFT=20.46ms, ITL=10.10ms | Has covariance=true"}
{"level":"INFO","ts":"2025-12-10T13:35:14.868Z","msg":"Tuner validation succeeded for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler, server ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler - New NIS: 0.658215"}
{"level":"DEBUG","ts":"2025-12-10T13:35:14.868Z","msg":"Model tuner results: model=unsloth/Meta-Llama-3.1-8B, accelerator=H100, alpha=7.478476, beta=0.043268, gamma=16.782511, delta=0.000260, NIS=0.66"}
{"level":"DEBUG","ts":"2025-12-10T13:35:14.868Z","msg":"Updated tuner status for variant ms-inference-scheduling-llm-d-modelservice-decode: alpha=7.478476, beta=0.043268, gamma=16.782511, delta=0.000260, NIS=0.658215"}
{"level":"INFO","ts":"2025-12-10T13:35:14.868Z","msg":"Tuned performance parameters result: variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler - alpha: 7.478476, beta: 0.043268, gamma: 16.782511, delta: 0.000260"}
{"level":"DEBUG","ts":"2025-12-10T13:35:14.877Z","msg":"Optimization solution - system: Solution: \ns=ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler; c=Premium; m=unsloth/Meta-Llama-3.1-8B; rate=744.04; inTk=220; outTk=475; sol=1, sat=false, alloc={acc=H100; numRep=2; maxBatch=512; cost=200, val=100, itl=8.623198, ttft=18.29689, rho=0.04972004, maxRPM=721.9623}; slo-itl=10, slo-ttft=1000, slo-tps=0 \nAllocationByType: \nname=NVIDIA-H100-80GB-HBM3, count=2, limit=0, cost=200 \ntotalCost=200 \n"}
{"level":"DEBUG","ts":"2025-12-10T13:35:14.877Z","msg":"Setting accelerator name: Name=H100, allocationData={H100 2 512 200 8.623198 18.29689 {744.04 220 475}}"}
{"level":"INFO","ts":"2025-12-10T13:35:14.877Z","msg":"Model-based optimization completed for model: unsloth/Meta-Llama-3.1-8B - model-based targets: map[ms-inference-scheduling-llm-d-modelservice-decode:2]"}
{"level":"WARN","ts":"2025-12-10T13:35:14.877Z","msg":"saturation analysis unavailable, using model-based targets only: modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-10T13:35:14.877Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-12-10T13:35:14.877Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=scale-up, current=1→target=2"}
{"level":"INFO","ts":"2025-12-10T13:35:14.877Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 1, desired-replicas: 2, accelerator: H100"}
{"level":"INFO","ts":"2025-12-10T13:35:14.877Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=2, accelerator=H100, SaturationOnly=false"}
{"level":"INFO","ts":"2025-12-10T13:35:14.887Z","msg":"Applied Saturation decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=scale-up, current=1, target=2, reason=model-based only (Saturation unavailable)"}
{"level":"INFO","ts":"2025-12-10T13:35:14.887Z","msg":"Reconciliation completed successfully: mode=model-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-12-10T13:36:14.888Z","msg":"Reconciling VariantAutoscaling: name=ms-inference-scheduling-llm-d-modelservice-decode, namespace=llm-d-inference-scheduler, modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-10T13:36:14.888Z","msg":"saturation scaling config cache updated: entries=1, has_default=true"}
{"level":"INFO","ts":"2025-12-10T13:36:14.888Z","msg":"saturation scaling configuration loaded successfully"}
{"level":"INFO","ts":"2025-12-10T13:36:14.888Z","msg":"Operating in MODEL-ONLY mode: model-based optimization only"}
{"level":"INFO","ts":"2025-12-10T13:36:14.888Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-12-10T13:36:14.888Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-12-10T13:36:14.899Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dgftqv, queueLength=0"}
{"level":"DEBUG","ts":"2025-12-10T13:36:14.899Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"INFO","ts":"2025-12-10T13:36:14.899Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dgftqv, usage=0.047 (4.7%)"}
{"level":"DEBUG","ts":"2025-12-10T13:36:14.899Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"DEBUG","ts":"2025-12-10T13:36:14.901Z","msg":"Pod-to-variant matching successful: totalPods=1, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"DEBUG","ts":"2025-12-10T13:36:14.901Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=1"}
{"level":"DEBUG","ts":"2025-12-10T13:36:14.901Z","msg":"Collected capacity metrics for observability in model-only mode: modelID=unsloth/Meta-Llama-3.1-8B, replicaCount=1"}
{"level":"DEBUG","ts":"2025-12-10T13:36:14.913Z","msg":"Replica count mismatch for llm-d-inference-scheduler/unsloth/Meta-Llama-3.1-8B: spec=2, reporting_metrics=1"}
{"level":"DEBUG","ts":"2025-12-10T13:36:14.913Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=1, accelerator=H100, ttft=17.57ms, itl=8.30ms, cost=100.00, maxBatch=256, arrivalRate=431.60, avgInputTokens=238.23, avgOutputTokens=461.21"}
{"level":"DEBUG","ts":"2025-12-10T13:36:14.913Z","msg":"Experimental model tuner is enabled globally: tuning model performance parameters for active VAs"}
{"level":"DEBUG","ts":"2025-12-10T13:36:14.913Z","msg":"Using state vals from VA status to tune variant ms-inference-scheduling-llm-d-modelservice-decode: alpha= 7.478476, beta= 0.043268, gamma= 16.782511, delta= 0.000260"}
{"level":"DEBUG","ts":"2025-12-10T13:36:14.913Z","msg":"[Tuner Config] variant=ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler | Initial state: alpha=7.478476, beta=0.043268, gamma=16.782511, delta=0.000260 | Expected obs (for R): TTFT=17.57ms, ITL=8.30ms | Has covariance=true"}
{"level":"INFO","ts":"2025-12-10T13:36:14.914Z","msg":"Tuner validation succeeded for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler, server ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler - New NIS: 1.629297"}
{"level":"DEBUG","ts":"2025-12-10T13:36:14.914Z","msg":"Model tuner results: model=unsloth/Meta-Llama-3.1-8B, accelerator=H100, alpha=7.057176, beta=0.045175, gamma=15.964095, delta=0.000261, NIS=1.63"}
{"level":"DEBUG","ts":"2025-12-10T13:36:14.914Z","msg":"Updated tuner status for variant ms-inference-scheduling-llm-d-modelservice-decode: alpha=7.057176, beta=0.045175, gamma=15.964095, delta=0.000261, NIS=1.629297"}
{"level":"INFO","ts":"2025-12-10T13:36:14.914Z","msg":"Tuned performance parameters result: variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler - alpha: 7.057176, beta: 0.045175, gamma: 15.964095, delta: 0.000261"}
{"level":"DEBUG","ts":"2025-12-10T13:36:14.923Z","msg":"Optimization solution - system: Solution: \ns=ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler; c=Premium; m=unsloth/Meta-Llama-3.1-8B; rate=431.6; inTk=238; outTk=461; sol=1, sat=false, alloc={acc=H100; numRep=1; maxBatch=512; cost=100, val=0, itl=8.3574, ttft=17.749958, rho=0.054261252, maxRPM=833.0179}; slo-itl=10, slo-ttft=1000, slo-tps=0 \nAllocationByType: \nname=NVIDIA-H100-80GB-HBM3, count=1, limit=0, cost=100 \ntotalCost=100 \n"}
{"level":"DEBUG","ts":"2025-12-10T13:36:14.923Z","msg":"Setting accelerator name: Name=H100, allocationData={H100 1 512 100 8.3574 17.749958 {431.6 238 461}}"}
{"level":"INFO","ts":"2025-12-10T13:36:14.923Z","msg":"Model-based optimization completed for model: unsloth/Meta-Llama-3.1-8B - model-based targets: map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"WARN","ts":"2025-12-10T13:36:14.923Z","msg":"saturation analysis unavailable, using model-based targets only: modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-10T13:36:14.923Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-12-10T13:36:14.923Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1→target=1"}
{"level":"INFO","ts":"2025-12-10T13:36:14.923Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 1, desired-replicas: 1, accelerator: H100"}
{"level":"INFO","ts":"2025-12-10T13:36:14.923Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=1, accelerator=H100, SaturationOnly=false"}
{"level":"INFO","ts":"2025-12-10T13:36:14.931Z","msg":"Applied Saturation decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1, target=1, reason=model-based only (Saturation unavailable)"}
{"level":"INFO","ts":"2025-12-10T13:36:14.931Z","msg":"Reconciliation completed successfully: mode=model-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-12-10T13:37:14.932Z","msg":"Reconciling VariantAutoscaling: name=ms-inference-scheduling-llm-d-modelservice-decode, namespace=llm-d-inference-scheduler, modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-10T13:37:14.932Z","msg":"saturation scaling config cache updated: entries=1, has_default=true"}
{"level":"INFO","ts":"2025-12-10T13:37:14.932Z","msg":"saturation scaling configuration loaded successfully"}
{"level":"INFO","ts":"2025-12-10T13:37:14.932Z","msg":"Operating in MODEL-ONLY mode: model-based optimization only"}
{"level":"INFO","ts":"2025-12-10T13:37:14.932Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-12-10T13:37:14.932Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-12-10T13:37:14.938Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dgftqv, usage=0.043 (4.3%)"}
{"level":"DEBUG","ts":"2025-12-10T13:37:14.938Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"INFO","ts":"2025-12-10T13:37:14.938Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dgftqv, queueLength=0"}
{"level":"DEBUG","ts":"2025-12-10T13:37:14.938Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"DEBUG","ts":"2025-12-10T13:37:14.940Z","msg":"Pod-to-variant matching successful: totalPods=1, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"DEBUG","ts":"2025-12-10T13:37:14.940Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=1"}
{"level":"DEBUG","ts":"2025-12-10T13:37:14.940Z","msg":"Collected capacity metrics for observability in model-only mode: modelID=unsloth/Meta-Llama-3.1-8B, replicaCount=1"}
{"level":"DEBUG","ts":"2025-12-10T13:37:14.952Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=1, accelerator=H100, ttft=17.55ms, itl=8.13ms, cost=100.00, maxBatch=256, arrivalRate=411.74, avgInputTokens=251.05, avgOutputTokens=438.33"}
{"level":"DEBUG","ts":"2025-12-10T13:37:14.952Z","msg":"Experimental model tuner is enabled globally: tuning model performance parameters for active VAs"}
{"level":"DEBUG","ts":"2025-12-10T13:37:14.952Z","msg":"Using state vals from VA status to tune variant ms-inference-scheduling-llm-d-modelservice-decode: alpha= 7.057176, beta= 0.045175, gamma= 15.964095, delta= 0.000261"}
{"level":"DEBUG","ts":"2025-12-10T13:37:14.952Z","msg":"[Tuner Config] variant=ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler | Initial state: alpha=7.057176, beta=0.045175, gamma=15.964095, delta=0.000261 | Expected obs (for R): TTFT=17.55ms, ITL=8.13ms | Has covariance=true"}
{"level":"INFO","ts":"2025-12-10T13:37:14.953Z","msg":"Tuner validation succeeded for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler, server ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler - New NIS: 0.040993"}
{"level":"DEBUG","ts":"2025-12-10T13:37:14.953Z","msg":"Model tuner results: model=unsloth/Meta-Llama-3.1-8B, accelerator=H100, alpha=6.988644, beta=0.045263, gamma=15.894406, delta=0.000261, NIS=0.04"}
{"level":"DEBUG","ts":"2025-12-10T13:37:14.953Z","msg":"Updated tuner status for variant ms-inference-scheduling-llm-d-modelservice-decode: alpha=6.988644, beta=0.045263, gamma=15.894406, delta=0.000261, NIS=0.040993"}
{"level":"INFO","ts":"2025-12-10T13:37:14.953Z","msg":"Tuned performance parameters result: variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler - alpha: 6.988644, beta: 0.045263, gamma: 15.894406, delta: 0.000261"}
{"level":"DEBUG","ts":"2025-12-10T13:37:14.963Z","msg":"Optimization solution - system: Solution: \ns=ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler; c=Premium; m=unsloth/Meta-Llama-3.1-8B; rate=411.74; inTk=251; outTk=438; sol=1, sat=false, alloc={acc=H100; numRep=1; maxBatch=512; cost=100, val=0, itl=8.144916, ttft=17.567883, rho=0.04794112, maxRPM=895.5872}; slo-itl=10, slo-ttft=1000, slo-tps=0 \nAllocationByType: \nname=NVIDIA-H100-80GB-HBM3, count=1, limit=0, cost=100 \ntotalCost=100 \n"}
{"level":"DEBUG","ts":"2025-12-10T13:37:14.963Z","msg":"Setting accelerator name: Name=H100, allocationData={H100 1 512 100 8.144916 17.567883 {411.74 251 438}}"}
{"level":"INFO","ts":"2025-12-10T13:37:14.963Z","msg":"Model-based optimization completed for model: unsloth/Meta-Llama-3.1-8B - model-based targets: map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"WARN","ts":"2025-12-10T13:37:14.963Z","msg":"saturation analysis unavailable, using model-based targets only: modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-10T13:37:14.963Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-12-10T13:37:14.963Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1→target=1"}
{"level":"INFO","ts":"2025-12-10T13:37:14.963Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 1, desired-replicas: 1, accelerator: H100"}
{"level":"INFO","ts":"2025-12-10T13:37:14.963Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=1, accelerator=H100, SaturationOnly=false"}
{"level":"INFO","ts":"2025-12-10T13:37:15.015Z","msg":"Applied Saturation decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1, target=1, reason=model-based only (Saturation unavailable)"}
{"level":"INFO","ts":"2025-12-10T13:37:15.015Z","msg":"Reconciliation completed successfully: mode=model-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-12-10T13:38:15.016Z","msg":"Reconciling VariantAutoscaling: name=ms-inference-scheduling-llm-d-modelservice-decode, namespace=llm-d-inference-scheduler, modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-10T13:38:15.016Z","msg":"saturation scaling config cache updated: entries=1, has_default=true"}
{"level":"INFO","ts":"2025-12-10T13:38:15.016Z","msg":"saturation scaling configuration loaded successfully"}
{"level":"INFO","ts":"2025-12-10T13:38:15.016Z","msg":"Operating in MODEL-ONLY mode: model-based optimization only"}
{"level":"INFO","ts":"2025-12-10T13:38:15.016Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-12-10T13:38:15.016Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-12-10T13:38:15.029Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dgftqv, usage=0.038 (3.8%)"}
{"level":"DEBUG","ts":"2025-12-10T13:38:15.029Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"INFO","ts":"2025-12-10T13:38:15.041Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dgftqv, queueLength=0"}
{"level":"DEBUG","ts":"2025-12-10T13:38:15.041Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"DEBUG","ts":"2025-12-10T13:38:15.056Z","msg":"Pod-to-variant matching successful: totalPods=1, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"DEBUG","ts":"2025-12-10T13:38:15.056Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=1"}
{"level":"DEBUG","ts":"2025-12-10T13:38:15.056Z","msg":"Collected capacity metrics for observability in model-only mode: modelID=unsloth/Meta-Llama-3.1-8B, replicaCount=1"}
{"level":"DEBUG","ts":"2025-12-10T13:38:15.083Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=1, accelerator=H100, ttft=17.33ms, itl=8.18ms, cost=100.00, maxBatch=256, arrivalRate=385.26, avgInputTokens=250.30, avgOutputTokens=446.00"}
{"level":"DEBUG","ts":"2025-12-10T13:38:15.083Z","msg":"Experimental model tuner is enabled globally: tuning model performance parameters for active VAs"}
{"level":"DEBUG","ts":"2025-12-10T13:38:15.083Z","msg":"Using state vals from VA status to tune variant ms-inference-scheduling-llm-d-modelservice-decode: alpha= 6.988644, beta= 0.045263, gamma= 15.894406, delta= 0.000261"}
{"level":"DEBUG","ts":"2025-12-10T13:38:15.083Z","msg":"[Tuner Config] variant=ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler | Initial state: alpha=6.988644, beta=0.045263, gamma=15.894406, delta=0.000261 | Expected obs (for R): TTFT=17.33ms, ITL=8.18ms | Has covariance=true"}
{"level":"INFO","ts":"2025-12-10T13:38:15.084Z","msg":"Tuner validation succeeded for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler, server ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler - New NIS: 0.062066"}
{"level":"DEBUG","ts":"2025-12-10T13:38:15.084Z","msg":"Model tuner results: model=unsloth/Meta-Llama-3.1-8B, accelerator=H100, alpha=7.056028, beta=0.045268, gamma=15.766663, delta=0.000261, NIS=0.06"}
{"level":"DEBUG","ts":"2025-12-10T13:38:15.084Z","msg":"Updated tuner status for variant ms-inference-scheduling-llm-d-modelservice-decode: alpha=7.056028, beta=0.045268, gamma=15.766663, delta=0.000261, NIS=0.062066"}
{"level":"INFO","ts":"2025-12-10T13:38:15.084Z","msg":"Tuned performance parameters result: variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler - alpha: 7.056028, beta: 0.045268, gamma: 15.766663, delta: 0.000261"}
{"level":"DEBUG","ts":"2025-12-10T13:38:15.092Z","msg":"Optimization solution - system: Solution: \ns=ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler; c=Premium; m=unsloth/Meta-Llama-3.1-8B; rate=385.26; inTk=250; outTk=446; sol=1, sat=false, alloc={acc=H100; numRep=1; maxBatch=512; cost=100, val=0, itl=8.162091, ttft=17.360912, rho=0.04576833, maxRPM=859.5106}; slo-itl=10, slo-ttft=1000, slo-tps=0 \nAllocationByType: \nname=NVIDIA-H100-80GB-HBM3, count=1, limit=0, cost=100 \ntotalCost=100 \n"}
{"level":"DEBUG","ts":"2025-12-10T13:38:15.092Z","msg":"Setting accelerator name: Name=H100, allocationData={H100 1 512 100 8.162091 17.360912 {385.26 250 446}}"}
{"level":"INFO","ts":"2025-12-10T13:38:15.092Z","msg":"Model-based optimization completed for model: unsloth/Meta-Llama-3.1-8B - model-based targets: map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"WARN","ts":"2025-12-10T13:38:15.092Z","msg":"saturation analysis unavailable, using model-based targets only: modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-10T13:38:15.092Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-12-10T13:38:15.092Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1→target=1"}
{"level":"INFO","ts":"2025-12-10T13:38:15.092Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 1, desired-replicas: 1, accelerator: H100"}
{"level":"INFO","ts":"2025-12-10T13:38:15.092Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=1, accelerator=H100, SaturationOnly=false"}
{"level":"INFO","ts":"2025-12-10T13:38:15.098Z","msg":"Applied Saturation decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1, target=1, reason=model-based only (Saturation unavailable)"}
{"level":"INFO","ts":"2025-12-10T13:38:15.098Z","msg":"Reconciliation completed successfully: mode=model-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-12-10T13:39:15.099Z","msg":"Reconciling VariantAutoscaling: name=ms-inference-scheduling-llm-d-modelservice-decode, namespace=llm-d-inference-scheduler, modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-10T13:39:15.099Z","msg":"saturation scaling config cache updated: entries=1, has_default=true"}
{"level":"INFO","ts":"2025-12-10T13:39:15.099Z","msg":"saturation scaling configuration loaded successfully"}
{"level":"INFO","ts":"2025-12-10T13:39:15.099Z","msg":"Operating in MODEL-ONLY mode: model-based optimization only"}
{"level":"INFO","ts":"2025-12-10T13:39:15.099Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-12-10T13:39:15.099Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-12-10T13:39:15.106Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dgftqv, queueLength=0"}
{"level":"DEBUG","ts":"2025-12-10T13:39:15.106Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"INFO","ts":"2025-12-10T13:39:15.106Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dgftqv, usage=0.042 (4.2%)"}
{"level":"DEBUG","ts":"2025-12-10T13:39:15.106Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"DEBUG","ts":"2025-12-10T13:39:15.108Z","msg":"Pod-to-variant matching successful: totalPods=1, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"DEBUG","ts":"2025-12-10T13:39:15.108Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=1"}
{"level":"DEBUG","ts":"2025-12-10T13:39:15.108Z","msg":"Collected capacity metrics for observability in model-only mode: modelID=unsloth/Meta-Llama-3.1-8B, replicaCount=1"}
{"level":"DEBUG","ts":"2025-12-10T13:39:15.119Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=1, accelerator=H100, ttft=17.92ms, itl=8.43ms, cost=100.00, maxBatch=256, arrivalRate=427.63, avgInputTokens=250.26, avgOutputTokens=483.74"}
{"level":"DEBUG","ts":"2025-12-10T13:39:15.119Z","msg":"Experimental model tuner is enabled globally: tuning model performance parameters for active VAs"}
{"level":"DEBUG","ts":"2025-12-10T13:39:15.119Z","msg":"Using state vals from VA status to tune variant ms-inference-scheduling-llm-d-modelservice-decode: alpha= 7.056028, beta= 0.045268, gamma= 15.766663, delta= 0.000261"}
{"level":"DEBUG","ts":"2025-12-10T13:39:15.119Z","msg":"[Tuner Config] variant=ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler | Initial state: alpha=7.056028, beta=0.045268, gamma=15.766663, delta=0.000261 | Expected obs (for R): TTFT=17.92ms, ITL=8.43ms | Has covariance=true"}
{"level":"INFO","ts":"2025-12-10T13:39:15.120Z","msg":"Tuner validation succeeded for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler, server ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler - New NIS: 0.037606"}
{"level":"DEBUG","ts":"2025-12-10T13:39:15.120Z","msg":"Model tuner results: model=unsloth/Meta-Llama-3.1-8B, accelerator=H100, alpha=7.066104, beta=0.045304, gamma=15.916347, delta=0.000261, NIS=0.04"}
{"level":"DEBUG","ts":"2025-12-10T13:39:15.120Z","msg":"Updated tuner status for variant ms-inference-scheduling-llm-d-modelservice-decode: alpha=7.066104, beta=0.045304, gamma=15.916347, delta=0.000261, NIS=0.037606"}
{"level":"INFO","ts":"2025-12-10T13:39:15.120Z","msg":"Tuned performance parameters result: variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler - alpha: 7.066104, beta: 0.045304, gamma: 15.916347, delta: 0.000261"}
{"level":"DEBUG","ts":"2025-12-10T13:39:15.130Z","msg":"Optimization solution - system: Solution: \ns=ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler; c=Premium; m=unsloth/Meta-Llama-3.1-8B; rate=427.63; inTk=250; outTk=483; sol=1, sat=false, alloc={acc=H100; numRep=1; maxBatch=512; cost=100, val=0, itl=8.42901, ttft=17.879704, rho=0.056803837, maxRPM=790.3903}; slo-itl=10, slo-ttft=1000, slo-tps=0 \nAllocationByType: \nname=NVIDIA-H100-80GB-HBM3, count=1, limit=0, cost=100 \ntotalCost=100 \n"}
{"level":"DEBUG","ts":"2025-12-10T13:39:15.130Z","msg":"Setting accelerator name: Name=H100, allocationData={H100 1 512 100 8.42901 17.879704 {427.63 250 483}}"}
{"level":"INFO","ts":"2025-12-10T13:39:15.130Z","msg":"Model-based optimization completed for model: unsloth/Meta-Llama-3.1-8B - model-based targets: map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"WARN","ts":"2025-12-10T13:39:15.130Z","msg":"saturation analysis unavailable, using model-based targets only: modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-10T13:39:15.130Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-12-10T13:39:15.130Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1→target=1"}
{"level":"INFO","ts":"2025-12-10T13:39:15.130Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 1, desired-replicas: 1, accelerator: H100"}
{"level":"INFO","ts":"2025-12-10T13:39:15.130Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=1, accelerator=H100, SaturationOnly=false"}
{"level":"INFO","ts":"2025-12-10T13:39:15.139Z","msg":"Applied Saturation decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1, target=1, reason=model-based only (Saturation unavailable)"}
{"level":"INFO","ts":"2025-12-10T13:39:15.139Z","msg":"Reconciliation completed successfully: mode=model-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-12-10T13:40:15.140Z","msg":"Reconciling VariantAutoscaling: name=ms-inference-scheduling-llm-d-modelservice-decode, namespace=llm-d-inference-scheduler, modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-10T13:40:15.140Z","msg":"saturation scaling config cache updated: entries=1, has_default=true"}
{"level":"INFO","ts":"2025-12-10T13:40:15.140Z","msg":"saturation scaling configuration loaded successfully"}
{"level":"INFO","ts":"2025-12-10T13:40:15.140Z","msg":"Operating in MODEL-ONLY mode: model-based optimization only"}
{"level":"INFO","ts":"2025-12-10T13:40:15.140Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-12-10T13:40:15.140Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-12-10T13:40:15.150Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dgftqv, usage=0.027 (2.7%)"}
{"level":"DEBUG","ts":"2025-12-10T13:40:15.150Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"INFO","ts":"2025-12-10T13:40:15.150Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dgftqv, queueLength=0"}
{"level":"DEBUG","ts":"2025-12-10T13:40:15.150Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"DEBUG","ts":"2025-12-10T13:40:15.152Z","msg":"Pod-to-variant matching successful: totalPods=1, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"DEBUG","ts":"2025-12-10T13:40:15.152Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=1"}
{"level":"DEBUG","ts":"2025-12-10T13:40:15.152Z","msg":"Collected capacity metrics for observability in model-only mode: modelID=unsloth/Meta-Llama-3.1-8B, replicaCount=1"}
{"level":"DEBUG","ts":"2025-12-10T13:40:15.164Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=1, accelerator=H100, ttft=17.73ms, itl=8.28ms, cost=100.00, maxBatch=256, arrivalRate=434.25, avgInputTokens=228.64, avgOutputTokens=441.02"}
{"level":"DEBUG","ts":"2025-12-10T13:40:15.164Z","msg":"Experimental model tuner is enabled globally: tuning model performance parameters for active VAs"}
{"level":"DEBUG","ts":"2025-12-10T13:40:15.164Z","msg":"Using state vals from VA status to tune variant ms-inference-scheduling-llm-d-modelservice-decode: alpha= 7.066104, beta= 0.045304, gamma= 15.916347, delta= 0.000261"}
{"level":"DEBUG","ts":"2025-12-10T13:40:15.164Z","msg":"[Tuner Config] variant=ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler | Initial state: alpha=7.066104, beta=0.045304, gamma=15.916347, delta=0.000261 | Expected obs (for R): TTFT=17.73ms, ITL=8.28ms | Has covariance=true"}
{"level":"INFO","ts":"2025-12-10T13:40:15.165Z","msg":"Tuner validation succeeded for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler, server ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler - New NIS: 0.035831"}
{"level":"DEBUG","ts":"2025-12-10T13:40:15.165Z","msg":"Model tuner results: model=unsloth/Meta-Llama-3.1-8B, accelerator=H100, alpha=7.041122, beta=0.045289, gamma=16.056715, delta=0.000261, NIS=0.04"}
{"level":"DEBUG","ts":"2025-12-10T13:40:15.165Z","msg":"Updated tuner status for variant ms-inference-scheduling-llm-d-modelservice-decode: alpha=7.041122, beta=0.045289, gamma=16.056715, delta=0.000261, NIS=0.035831"}
{"level":"INFO","ts":"2025-12-10T13:40:15.165Z","msg":"Tuned performance parameters result: variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler - alpha: 7.041122, beta: 0.045289, gamma: 16.056715, delta: 0.000261"}
{"level":"DEBUG","ts":"2025-12-10T13:40:15.173Z","msg":"Optimization solution - system: Solution: \ns=ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler; c=Premium; m=unsloth/Meta-Llama-3.1-8B; rate=434.25; inTk=228; outTk=441; sol=1, sat=false, alloc={acc=H100; numRep=1; maxBatch=512; cost=100, val=0, itl=8.287441, ttft=17.69442, rho=0.05179574, maxRPM=873.3165}; slo-itl=10, slo-ttft=1000, slo-tps=0 \nAllocationByType: \nname=NVIDIA-H100-80GB-HBM3, count=1, limit=0, cost=100 \ntotalCost=100 \n"}
{"level":"DEBUG","ts":"2025-12-10T13:40:15.173Z","msg":"Setting accelerator name: Name=H100, allocationData={H100 1 512 100 8.287441 17.69442 {434.25 228 441}}"}
{"level":"INFO","ts":"2025-12-10T13:40:15.173Z","msg":"Model-based optimization completed for model: unsloth/Meta-Llama-3.1-8B - model-based targets: map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"WARN","ts":"2025-12-10T13:40:15.173Z","msg":"saturation analysis unavailable, using model-based targets only: modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-10T13:40:15.173Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-12-10T13:40:15.173Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1→target=1"}
{"level":"INFO","ts":"2025-12-10T13:40:15.173Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 1, desired-replicas: 1, accelerator: H100"}
{"level":"INFO","ts":"2025-12-10T13:40:15.173Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=1, accelerator=H100, SaturationOnly=false"}
{"level":"INFO","ts":"2025-12-10T13:40:15.181Z","msg":"Applied Saturation decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1, target=1, reason=model-based only (Saturation unavailable)"}
{"level":"INFO","ts":"2025-12-10T13:40:15.181Z","msg":"Reconciliation completed successfully: mode=model-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-12-10T13:41:15.182Z","msg":"Reconciling VariantAutoscaling: name=ms-inference-scheduling-llm-d-modelservice-decode, namespace=llm-d-inference-scheduler, modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-10T13:41:15.182Z","msg":"saturation scaling config cache updated: entries=1, has_default=true"}
{"level":"INFO","ts":"2025-12-10T13:41:15.182Z","msg":"saturation scaling configuration loaded successfully"}
{"level":"INFO","ts":"2025-12-10T13:41:15.182Z","msg":"Operating in MODEL-ONLY mode: model-based optimization only"}
{"level":"INFO","ts":"2025-12-10T13:41:15.182Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-12-10T13:41:15.182Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-12-10T13:41:15.191Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dgftqv, queueLength=0"}
{"level":"DEBUG","ts":"2025-12-10T13:41:15.191Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"INFO","ts":"2025-12-10T13:41:15.191Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dgftqv, usage=0.051 (5.1%)"}
{"level":"DEBUG","ts":"2025-12-10T13:41:15.191Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"DEBUG","ts":"2025-12-10T13:41:15.193Z","msg":"Pod-to-variant matching successful: totalPods=1, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"DEBUG","ts":"2025-12-10T13:41:15.193Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=1"}
{"level":"DEBUG","ts":"2025-12-10T13:41:15.193Z","msg":"Collected capacity metrics for observability in model-only mode: modelID=unsloth/Meta-Llama-3.1-8B, replicaCount=1"}
{"level":"DEBUG","ts":"2025-12-10T13:41:15.209Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=1, accelerator=H100, ttft=17.43ms, itl=8.52ms, cost=100.00, maxBatch=256, arrivalRate=436.89, avgInputTokens=235.20, avgOutputTokens=486.99"}
{"level":"DEBUG","ts":"2025-12-10T13:41:15.209Z","msg":"Experimental model tuner is enabled globally: tuning model performance parameters for active VAs"}
{"level":"DEBUG","ts":"2025-12-10T13:41:15.209Z","msg":"Using state vals from VA status to tune variant ms-inference-scheduling-llm-d-modelservice-decode: alpha= 7.041122, beta= 0.045289, gamma= 16.056715, delta= 0.000261"}
{"level":"DEBUG","ts":"2025-12-10T13:41:15.209Z","msg":"[Tuner Config] variant=ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler | Initial state: alpha=7.041122, beta=0.045289, gamma=16.056715, delta=0.000261 | Expected obs (for R): TTFT=17.43ms, ITL=8.52ms | Has covariance=true"}
{"level":"INFO","ts":"2025-12-10T13:41:15.209Z","msg":"Tuner validation succeeded for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler, server ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler - New NIS: 0.308002"}
{"level":"DEBUG","ts":"2025-12-10T13:41:15.209Z","msg":"Model tuner results: model=unsloth/Meta-Llama-3.1-8B, accelerator=H100, alpha=7.087841, beta=0.045408, gamma=15.625283, delta=0.000261, NIS=0.31"}
{"level":"DEBUG","ts":"2025-12-10T13:41:15.209Z","msg":"Updated tuner status for variant ms-inference-scheduling-llm-d-modelservice-decode: alpha=7.087841, beta=0.045408, gamma=15.625283, delta=0.000261, NIS=0.308002"}
{"level":"INFO","ts":"2025-12-10T13:41:15.209Z","msg":"Tuned performance parameters result: variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler - alpha: 7.087841, beta: 0.045408, gamma: 15.625283, delta: 0.000261"}
{"level":"DEBUG","ts":"2025-12-10T13:41:15.219Z","msg":"Optimization solution - system: Solution: \ns=ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler; c=Premium; m=unsloth/Meta-Llama-3.1-8B; rate=436.89; inTk=235; outTk=486; sol=1, sat=false, alloc={acc=H100; numRep=1; maxBatch=512; cost=100, val=0, itl=8.50249, ttft=17.535213, rho=0.058895428, maxRPM=777.89795}; slo-itl=10, slo-ttft=1000, slo-tps=0 \nAllocationByType: \nname=NVIDIA-H100-80GB-HBM3, count=1, limit=0, cost=100 \ntotalCost=100 \n"}
{"level":"DEBUG","ts":"2025-12-10T13:41:15.219Z","msg":"Setting accelerator name: Name=H100, allocationData={H100 1 512 100 8.50249 17.535213 {436.89 235 486}}"}
{"level":"INFO","ts":"2025-12-10T13:41:15.219Z","msg":"Model-based optimization completed for model: unsloth/Meta-Llama-3.1-8B - model-based targets: map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"WARN","ts":"2025-12-10T13:41:15.219Z","msg":"saturation analysis unavailable, using model-based targets only: modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-10T13:41:15.219Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-12-10T13:41:15.219Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1→target=1"}
{"level":"INFO","ts":"2025-12-10T13:41:15.219Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 1, desired-replicas: 1, accelerator: H100"}
{"level":"INFO","ts":"2025-12-10T13:41:15.219Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=1, accelerator=H100, SaturationOnly=false"}
{"level":"INFO","ts":"2025-12-10T13:41:15.228Z","msg":"Applied Saturation decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1, target=1, reason=model-based only (Saturation unavailable)"}
{"level":"INFO","ts":"2025-12-10T13:41:15.228Z","msg":"Reconciliation completed successfully: mode=model-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-12-10T13:42:15.228Z","msg":"Reconciling VariantAutoscaling: name=ms-inference-scheduling-llm-d-modelservice-decode, namespace=llm-d-inference-scheduler, modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-10T13:42:15.228Z","msg":"saturation scaling config cache updated: entries=1, has_default=true"}
{"level":"INFO","ts":"2025-12-10T13:42:15.228Z","msg":"saturation scaling configuration loaded successfully"}
{"level":"INFO","ts":"2025-12-10T13:42:15.228Z","msg":"Operating in MODEL-ONLY mode: model-based optimization only"}
{"level":"INFO","ts":"2025-12-10T13:42:15.228Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-12-10T13:42:15.228Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-12-10T13:42:15.237Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dgftqv, queueLength=0"}
{"level":"DEBUG","ts":"2025-12-10T13:42:15.237Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"INFO","ts":"2025-12-10T13:42:15.237Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dgftqv, usage=0.050 (5.0%)"}
{"level":"DEBUG","ts":"2025-12-10T13:42:15.237Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"DEBUG","ts":"2025-12-10T13:42:15.239Z","msg":"Pod-to-variant matching successful: totalPods=1, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"DEBUG","ts":"2025-12-10T13:42:15.239Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=1"}
{"level":"DEBUG","ts":"2025-12-10T13:42:15.239Z","msg":"Collected capacity metrics for observability in model-only mode: modelID=unsloth/Meta-Llama-3.1-8B, replicaCount=1"}
{"level":"DEBUG","ts":"2025-12-10T13:42:15.252Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=1, accelerator=H100, ttft=17.85ms, itl=8.38ms, cost=100.00, maxBatch=256, arrivalRate=439.54, avgInputTokens=247.48, avgOutputTokens=422.92"}
{"level":"DEBUG","ts":"2025-12-10T13:42:15.252Z","msg":"Experimental model tuner is enabled globally: tuning model performance parameters for active VAs"}
{"level":"DEBUG","ts":"2025-12-10T13:42:15.252Z","msg":"Using state vals from VA status to tune variant ms-inference-scheduling-llm-d-modelservice-decode: alpha= 7.087841, beta= 0.045408, gamma= 15.625283, delta= 0.000261"}
{"level":"DEBUG","ts":"2025-12-10T13:42:15.252Z","msg":"[Tuner Config] variant=ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler | Initial state: alpha=7.087841, beta=0.045408, gamma=15.625283, delta=0.000261 | Expected obs (for R): TTFT=17.85ms, ITL=8.38ms | Has covariance=true"}
{"level":"INFO","ts":"2025-12-10T13:42:15.252Z","msg":"Tuner validation succeeded for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler, server ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler - New NIS: 0.268982"}
{"level":"DEBUG","ts":"2025-12-10T13:42:15.252Z","msg":"Model tuner results: model=unsloth/Meta-Llama-3.1-8B, accelerator=H100, alpha=7.147014, beta=0.045367, gamma=16.009121, delta=0.000261, NIS=0.27"}
{"level":"DEBUG","ts":"2025-12-10T13:42:15.253Z","msg":"Updated tuner status for variant ms-inference-scheduling-llm-d-modelservice-decode: alpha=7.147014, beta=0.045367, gamma=16.009121, delta=0.000261, NIS=0.268982"}
{"level":"INFO","ts":"2025-12-10T13:42:15.253Z","msg":"Tuned performance parameters result: variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler - alpha: 7.147014, beta: 0.045367, gamma: 16.009121, delta: 0.000261"}
{"level":"DEBUG","ts":"2025-12-10T13:42:15.256Z","msg":"Optimization solution - system: Solution: \ns=ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler; c=Premium; m=unsloth/Meta-Llama-3.1-8B; rate=439.54; inTk=247; outTk=422; sol=1, sat=false, alloc={acc=H100; numRep=1; maxBatch=512; cost=100, val=0, itl=8.369274, ttft=17.746387, rho=0.050667446, maxRPM=877.8191}; slo-itl=10, slo-ttft=1000, slo-tps=0 \nAllocationByType: \nname=NVIDIA-H100-80GB-HBM3, count=1, limit=0, cost=100 \ntotalCost=100 \n"}
{"level":"DEBUG","ts":"2025-12-10T13:42:15.256Z","msg":"Setting accelerator name: Name=H100, allocationData={H100 1 512 100 8.369274 17.746387 {439.54 247 422}}"}
{"level":"INFO","ts":"2025-12-10T13:42:15.256Z","msg":"Model-based optimization completed for model: unsloth/Meta-Llama-3.1-8B - model-based targets: map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"WARN","ts":"2025-12-10T13:42:15.256Z","msg":"saturation analysis unavailable, using model-based targets only: modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-10T13:42:15.256Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-12-10T13:42:15.256Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1→target=1"}
{"level":"INFO","ts":"2025-12-10T13:42:15.256Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 1, desired-replicas: 1, accelerator: H100"}
{"level":"INFO","ts":"2025-12-10T13:42:15.256Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=1, accelerator=H100, SaturationOnly=false"}
{"level":"INFO","ts":"2025-12-10T13:42:15.262Z","msg":"Applied Saturation decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1, target=1, reason=model-based only (Saturation unavailable)"}
{"level":"INFO","ts":"2025-12-10T13:42:15.262Z","msg":"Reconciliation completed successfully: mode=model-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-12-10T13:43:15.269Z","msg":"Reconciling VariantAutoscaling: name=ms-inference-scheduling-llm-d-modelservice-decode, namespace=llm-d-inference-scheduler, modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-10T13:43:15.270Z","msg":"saturation scaling config cache updated: entries=1, has_default=true"}
{"level":"INFO","ts":"2025-12-10T13:43:15.270Z","msg":"saturation scaling configuration loaded successfully"}
{"level":"INFO","ts":"2025-12-10T13:43:15.270Z","msg":"Operating in MODEL-ONLY mode: model-based optimization only"}
{"level":"INFO","ts":"2025-12-10T13:43:15.270Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-12-10T13:43:15.270Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-12-10T13:43:15.277Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dgftqv, queueLength=0"}
{"level":"DEBUG","ts":"2025-12-10T13:43:15.277Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"INFO","ts":"2025-12-10T13:43:15.277Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dgftqv, usage=0.042 (4.2%)"}
{"level":"DEBUG","ts":"2025-12-10T13:43:15.277Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"DEBUG","ts":"2025-12-10T13:43:15.279Z","msg":"Pod-to-variant matching successful: totalPods=1, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"DEBUG","ts":"2025-12-10T13:43:15.279Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=1"}
{"level":"DEBUG","ts":"2025-12-10T13:43:15.279Z","msg":"Collected capacity metrics for observability in model-only mode: modelID=unsloth/Meta-Llama-3.1-8B, replicaCount=1"}
{"level":"DEBUG","ts":"2025-12-10T13:43:15.290Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=1, accelerator=H100, ttft=17.48ms, itl=8.50ms, cost=100.00, maxBatch=256, arrivalRate=473.96, avgInputTokens=216.60, avgOutputTokens=475.96"}
{"level":"DEBUG","ts":"2025-12-10T13:43:15.290Z","msg":"Experimental model tuner is enabled globally: tuning model performance parameters for active VAs"}
{"level":"DEBUG","ts":"2025-12-10T13:43:15.290Z","msg":"Using state vals from VA status to tune variant ms-inference-scheduling-llm-d-modelservice-decode: alpha= 7.147014, beta= 0.045367, gamma= 16.009121, delta= 0.000261"}
{"level":"DEBUG","ts":"2025-12-10T13:43:15.290Z","msg":"[Tuner Config] variant=ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler | Initial state: alpha=7.147014, beta=0.045367, gamma=16.009121, delta=0.000261 | Expected obs (for R): TTFT=17.48ms, ITL=8.50ms | Has covariance=true"}
{"level":"INFO","ts":"2025-12-10T13:43:15.291Z","msg":"Tuner validation succeeded for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler, server ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler - New NIS: 0.260116"}
{"level":"DEBUG","ts":"2025-12-10T13:43:15.291Z","msg":"Model tuner results: model=unsloth/Meta-Llama-3.1-8B, accelerator=H100, alpha=7.041991, beta=0.044878, gamma=15.695297, delta=0.000261, NIS=0.26"}
{"level":"DEBUG","ts":"2025-12-10T13:43:15.291Z","msg":"Updated tuner status for variant ms-inference-scheduling-llm-d-modelservice-decode: alpha=7.041991, beta=0.044878, gamma=15.695297, delta=0.000261, NIS=0.260116"}
{"level":"INFO","ts":"2025-12-10T13:43:15.291Z","msg":"Tuned performance parameters result: variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler - alpha: 7.041991, beta: 0.044878, gamma: 15.695297, delta: 0.000261"}
{"level":"DEBUG","ts":"2025-12-10T13:43:15.301Z","msg":"Optimization solution - system: Solution: \ns=ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler; c=Premium; m=unsloth/Meta-Llama-3.1-8B; rate=473.96; inTk=216; outTk=475; sol=1, sat=false, alloc={acc=H100; numRep=1; maxBatch=512; cost=100, val=0, itl=8.525741, ttft=17.558506, rho=0.0626201, maxRPM=818.31433}; slo-itl=10, slo-ttft=1000, slo-tps=0 \nAllocationByType: \nname=NVIDIA-H100-80GB-HBM3, count=1, limit=0, cost=100 \ntotalCost=100 \n"}
{"level":"DEBUG","ts":"2025-12-10T13:43:15.301Z","msg":"Setting accelerator name: Name=H100, allocationData={H100 1 512 100 8.525741 17.558506 {473.96 216 475}}"}
{"level":"INFO","ts":"2025-12-10T13:43:15.301Z","msg":"Model-based optimization completed for model: unsloth/Meta-Llama-3.1-8B - model-based targets: map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"WARN","ts":"2025-12-10T13:43:15.301Z","msg":"saturation analysis unavailable, using model-based targets only: modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-10T13:43:15.301Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-12-10T13:43:15.301Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1→target=1"}
{"level":"INFO","ts":"2025-12-10T13:43:15.301Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 1, desired-replicas: 1, accelerator: H100"}
{"level":"INFO","ts":"2025-12-10T13:43:15.301Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=1, accelerator=H100, SaturationOnly=false"}
{"level":"INFO","ts":"2025-12-10T13:43:15.311Z","msg":"Applied Saturation decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1, target=1, reason=model-based only (Saturation unavailable)"}
{"level":"INFO","ts":"2025-12-10T13:43:15.311Z","msg":"Reconciliation completed successfully: mode=model-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-12-10T13:44:15.311Z","msg":"Reconciling VariantAutoscaling: name=ms-inference-scheduling-llm-d-modelservice-decode, namespace=llm-d-inference-scheduler, modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-10T13:44:15.311Z","msg":"saturation scaling config cache updated: entries=1, has_default=true"}
{"level":"INFO","ts":"2025-12-10T13:44:15.311Z","msg":"saturation scaling configuration loaded successfully"}
{"level":"INFO","ts":"2025-12-10T13:44:15.311Z","msg":"Operating in MODEL-ONLY mode: model-based optimization only"}
{"level":"INFO","ts":"2025-12-10T13:44:15.311Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-12-10T13:44:15.311Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-12-10T13:44:15.319Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dgftqv, queueLength=0"}
{"level":"DEBUG","ts":"2025-12-10T13:44:15.319Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"INFO","ts":"2025-12-10T13:44:15.319Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dgftqv, usage=0.052 (5.2%)"}
{"level":"DEBUG","ts":"2025-12-10T13:44:15.319Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"DEBUG","ts":"2025-12-10T13:44:15.323Z","msg":"Pod-to-variant matching successful: totalPods=1, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"DEBUG","ts":"2025-12-10T13:44:15.323Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=1"}
{"level":"DEBUG","ts":"2025-12-10T13:44:15.323Z","msg":"Collected capacity metrics for observability in model-only mode: modelID=unsloth/Meta-Llama-3.1-8B, replicaCount=1"}
{"level":"DEBUG","ts":"2025-12-10T13:44:15.335Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=1, accelerator=H100, ttft=17.50ms, itl=8.22ms, cost=100.00, maxBatch=256, arrivalRate=383.94, avgInputTokens=260.13, avgOutputTokens=511.33"}
{"level":"DEBUG","ts":"2025-12-10T13:44:15.335Z","msg":"Experimental model tuner is enabled globally: tuning model performance parameters for active VAs"}
{"level":"DEBUG","ts":"2025-12-10T13:44:15.335Z","msg":"Using state vals from VA status to tune variant ms-inference-scheduling-llm-d-modelservice-decode: alpha= 7.041991, beta= 0.044878, gamma= 15.695297, delta= 0.000261"}
{"level":"DEBUG","ts":"2025-12-10T13:44:15.335Z","msg":"[Tuner Config] variant=ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler | Initial state: alpha=7.041991, beta=0.044878, gamma=15.695297, delta=0.000261 | Expected obs (for R): TTFT=17.50ms, ITL=8.22ms | Has covariance=true"}
{"level":"INFO","ts":"2025-12-10T13:44:15.336Z","msg":"Tuner validation succeeded for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler, server ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler - New NIS: 0.040107"}
{"level":"DEBUG","ts":"2025-12-10T13:44:15.336Z","msg":"Model tuner results: model=unsloth/Meta-Llama-3.1-8B, accelerator=H100, alpha=6.976676, beta=0.044943, gamma=15.620776, delta=0.000261, NIS=0.04"}
{"level":"DEBUG","ts":"2025-12-10T13:44:15.336Z","msg":"Updated tuner status for variant ms-inference-scheduling-llm-d-modelservice-decode: alpha=6.976676, beta=0.044943, gamma=15.620776, delta=0.000261, NIS=0.040107"}
{"level":"INFO","ts":"2025-12-10T13:44:15.336Z","msg":"Tuned performance parameters result: variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler - alpha: 6.976676, beta: 0.044943, gamma: 15.620776, delta: 0.000261"}
{"level":"DEBUG","ts":"2025-12-10T13:44:15.346Z","msg":"Optimization solution - system: Solution: \ns=ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler; c=Premium; m=unsloth/Meta-Llama-3.1-8B; rate=383.94; inTk=260; outTk=511; sol=1, sat=false, alloc={acc=H100; numRep=1; maxBatch=512; cost=100, val=0, itl=8.234419, ttft=17.51966, rho=0.05270518, maxRPM=776.5697}; slo-itl=10, slo-ttft=1000, slo-tps=0 \nAllocationByType: \nname=NVIDIA-H100-80GB-HBM3, count=1, limit=0, cost=100 \ntotalCost=100 \n"}
{"level":"DEBUG","ts":"2025-12-10T13:44:15.346Z","msg":"Setting accelerator name: Name=H100, allocationData={H100 1 512 100 8.234419 17.51966 {383.94 260 511}}"}
{"level":"INFO","ts":"2025-12-10T13:44:15.346Z","msg":"Model-based optimization completed for model: unsloth/Meta-Llama-3.1-8B - model-based targets: map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"WARN","ts":"2025-12-10T13:44:15.346Z","msg":"saturation analysis unavailable, using model-based targets only: modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-10T13:44:15.346Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-12-10T13:44:15.346Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1→target=1"}
{"level":"INFO","ts":"2025-12-10T13:44:15.346Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 1, desired-replicas: 1, accelerator: H100"}
{"level":"INFO","ts":"2025-12-10T13:44:15.346Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=1, accelerator=H100, SaturationOnly=false"}
{"level":"INFO","ts":"2025-12-10T13:44:15.352Z","msg":"Applied Saturation decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1, target=1, reason=model-based only (Saturation unavailable)"}
{"level":"INFO","ts":"2025-12-10T13:44:15.352Z","msg":"Reconciliation completed successfully: mode=model-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-12-10T13:45:15.353Z","msg":"Reconciling VariantAutoscaling: name=ms-inference-scheduling-llm-d-modelservice-decode, namespace=llm-d-inference-scheduler, modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-10T13:45:15.353Z","msg":"saturation scaling config cache updated: entries=1, has_default=true"}
{"level":"INFO","ts":"2025-12-10T13:45:15.353Z","msg":"saturation scaling configuration loaded successfully"}
{"level":"INFO","ts":"2025-12-10T13:45:15.353Z","msg":"Operating in MODEL-ONLY mode: model-based optimization only"}
{"level":"INFO","ts":"2025-12-10T13:45:15.353Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-12-10T13:45:15.353Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-12-10T13:45:15.360Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dgftqv, queueLength=0"}
{"level":"DEBUG","ts":"2025-12-10T13:45:15.360Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"INFO","ts":"2025-12-10T13:45:15.360Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dgftqv, usage=0.044 (4.4%)"}
{"level":"DEBUG","ts":"2025-12-10T13:45:15.360Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"DEBUG","ts":"2025-12-10T13:45:15.362Z","msg":"Pod-to-variant matching successful: totalPods=1, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"DEBUG","ts":"2025-12-10T13:45:15.362Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=1"}
{"level":"DEBUG","ts":"2025-12-10T13:45:15.362Z","msg":"Collected capacity metrics for observability in model-only mode: modelID=unsloth/Meta-Llama-3.1-8B, replicaCount=1"}
{"level":"DEBUG","ts":"2025-12-10T13:45:15.376Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=1, accelerator=H100, ttft=17.33ms, itl=8.23ms, cost=100.00, maxBatch=256, arrivalRate=418.36, avgInputTokens=240.44, avgOutputTokens=445.41"}
{"level":"DEBUG","ts":"2025-12-10T13:45:15.376Z","msg":"Experimental model tuner is enabled globally: tuning model performance parameters for active VAs"}
{"level":"DEBUG","ts":"2025-12-10T13:45:15.376Z","msg":"Using state vals from VA status to tune variant ms-inference-scheduling-llm-d-modelservice-decode: alpha= 6.976676, beta= 0.044943, gamma= 15.620776, delta= 0.000261"}
{"level":"DEBUG","ts":"2025-12-10T13:45:15.376Z","msg":"[Tuner Config] variant=ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler | Initial state: alpha=6.976676, beta=0.044943, gamma=15.620776, delta=0.000261 | Expected obs (for R): TTFT=17.33ms, ITL=8.23ms | Has covariance=true"}
{"level":"INFO","ts":"2025-12-10T13:45:15.376Z","msg":"Tuner validation succeeded for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler, server ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler - New NIS: 0.020398"}
{"level":"DEBUG","ts":"2025-12-10T13:45:15.376Z","msg":"Model tuner results: model=unsloth/Meta-Llama-3.1-8B, accelerator=H100, alpha=7.024860, beta=0.044931, gamma=15.656530, delta=0.000261, NIS=0.02"}
{"level":"DEBUG","ts":"2025-12-10T13:45:15.376Z","msg":"Updated tuner status for variant ms-inference-scheduling-llm-d-modelservice-decode: alpha=7.024860, beta=0.044931, gamma=15.656530, delta=0.000261, NIS=0.020398"}
{"level":"INFO","ts":"2025-12-10T13:45:15.376Z","msg":"Tuned performance parameters result: variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler - alpha: 7.024860, beta: 0.044931, gamma: 15.656530, delta: 0.000261"}
{"level":"DEBUG","ts":"2025-12-10T13:45:15.386Z","msg":"Optimization solution - system: Solution: \ns=ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler; c=Premium; m=unsloth/Meta-Llama-3.1-8B; rate=418.36; inTk=240; outTk=445; sol=1, sat=false, alloc={acc=H100; numRep=1; maxBatch=512; cost=100, val=0, itl=8.218405, ttft=17.320513, rho=0.04992936, maxRPM=877.377}; slo-itl=10, slo-ttft=1000, slo-tps=0 \nAllocationByType: \nname=NVIDIA-H100-80GB-HBM3, count=1, limit=0, cost=100 \ntotalCost=100 \n"}
{"level":"DEBUG","ts":"2025-12-10T13:45:15.386Z","msg":"Setting accelerator name: Name=H100, allocationData={H100 1 512 100 8.218405 17.320513 {418.36 240 445}}"}
{"level":"INFO","ts":"2025-12-10T13:45:15.386Z","msg":"Model-based optimization completed for model: unsloth/Meta-Llama-3.1-8B - model-based targets: map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"WARN","ts":"2025-12-10T13:45:15.386Z","msg":"saturation analysis unavailable, using model-based targets only: modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-10T13:45:15.386Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-12-10T13:45:15.386Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1→target=1"}
{"level":"INFO","ts":"2025-12-10T13:45:15.386Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 1, desired-replicas: 1, accelerator: H100"}
{"level":"INFO","ts":"2025-12-10T13:45:15.386Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=1, accelerator=H100, SaturationOnly=false"}
{"level":"INFO","ts":"2025-12-10T13:45:15.393Z","msg":"Applied Saturation decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1, target=1, reason=model-based only (Saturation unavailable)"}
{"level":"INFO","ts":"2025-12-10T13:45:15.393Z","msg":"Reconciliation completed successfully: mode=model-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-12-10T13:46:15.394Z","msg":"Reconciling VariantAutoscaling: name=ms-inference-scheduling-llm-d-modelservice-decode, namespace=llm-d-inference-scheduler, modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-10T13:46:15.394Z","msg":"saturation scaling config cache updated: entries=1, has_default=true"}
{"level":"INFO","ts":"2025-12-10T13:46:15.394Z","msg":"saturation scaling configuration loaded successfully"}
{"level":"INFO","ts":"2025-12-10T13:46:15.394Z","msg":"Operating in MODEL-ONLY mode: model-based optimization only"}
{"level":"INFO","ts":"2025-12-10T13:46:15.394Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-12-10T13:46:15.394Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-12-10T13:46:15.404Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dgftqv, queueLength=0"}
{"level":"DEBUG","ts":"2025-12-10T13:46:15.404Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"INFO","ts":"2025-12-10T13:46:15.404Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dgftqv, usage=0.035 (3.5%)"}
{"level":"DEBUG","ts":"2025-12-10T13:46:15.404Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"DEBUG","ts":"2025-12-10T13:46:15.406Z","msg":"Pod-to-variant matching successful: totalPods=1, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"DEBUG","ts":"2025-12-10T13:46:15.406Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=1"}
{"level":"DEBUG","ts":"2025-12-10T13:46:15.406Z","msg":"Collected capacity metrics for observability in model-only mode: modelID=unsloth/Meta-Llama-3.1-8B, replicaCount=1"}
{"level":"DEBUG","ts":"2025-12-10T13:46:15.418Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=1, accelerator=H100, ttft=17.09ms, itl=8.04ms, cost=100.00, maxBatch=256, arrivalRate=271.40, avgInputTokens=219.56, avgOutputTokens=528.09"}
{"level":"DEBUG","ts":"2025-12-10T13:46:15.418Z","msg":"Experimental model tuner is enabled globally: tuning model performance parameters for active VAs"}
{"level":"DEBUG","ts":"2025-12-10T13:46:15.418Z","msg":"Using state vals from VA status to tune variant ms-inference-scheduling-llm-d-modelservice-decode: alpha= 7.024860, beta= 0.044931, gamma= 15.656530, delta= 0.000261"}
{"level":"DEBUG","ts":"2025-12-10T13:46:15.418Z","msg":"[Tuner Config] variant=ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler | Initial state: alpha=7.024860, beta=0.044931, gamma=15.656530, delta=0.000261 | Expected obs (for R): TTFT=17.09ms, ITL=8.04ms | Has covariance=true"}
{"level":"INFO","ts":"2025-12-10T13:46:15.418Z","msg":"Tuner validation succeeded for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler, server ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler - New NIS: 0.141705"}
{"level":"DEBUG","ts":"2025-12-10T13:46:15.418Z","msg":"Model tuner results: model=unsloth/Meta-Llama-3.1-8B, accelerator=H100, alpha=7.117926, beta=0.044671, gamma=15.880216, delta=0.000261, NIS=0.14"}
{"level":"DEBUG","ts":"2025-12-10T13:46:15.418Z","msg":"Updated tuner status for variant ms-inference-scheduling-llm-d-modelservice-decode: alpha=7.117926, beta=0.044671, gamma=15.880216, delta=0.000261, NIS=0.141705"}
{"level":"INFO","ts":"2025-12-10T13:46:15.418Z","msg":"Tuned performance parameters result: variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler - alpha: 7.117926, beta: 0.044671, gamma: 15.880216, delta: 0.000261"}
{"level":"DEBUG","ts":"2025-12-10T13:46:15.428Z","msg":"Optimization solution - system: Solution: \ns=ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler; c=Premium; m=unsloth/Meta-Llama-3.1-8B; rate=271.4; inTk=219; outTk=528; sol=1, sat=false, alloc={acc=H100; numRep=1; maxBatch=512; cost=100, val=0, itl=8.020073, ttft=17.034225, rho=0.037490766, maxRPM=720.4855}; slo-itl=10, slo-ttft=1000, slo-tps=0 \nAllocationByType: \nname=NVIDIA-H100-80GB-HBM3, count=1, limit=0, cost=100 \ntotalCost=100 \n"}
{"level":"DEBUG","ts":"2025-12-10T13:46:15.428Z","msg":"Setting accelerator name: Name=H100, allocationData={H100 1 512 100 8.020073 17.034225 {271.4 219 528}}"}
{"level":"INFO","ts":"2025-12-10T13:46:15.428Z","msg":"Model-based optimization completed for model: unsloth/Meta-Llama-3.1-8B - model-based targets: map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"WARN","ts":"2025-12-10T13:46:15.428Z","msg":"saturation analysis unavailable, using model-based targets only: modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-10T13:46:15.428Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-12-10T13:46:15.428Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1→target=1"}
{"level":"INFO","ts":"2025-12-10T13:46:15.428Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 1, desired-replicas: 1, accelerator: H100"}
{"level":"INFO","ts":"2025-12-10T13:46:15.428Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=1, accelerator=H100, SaturationOnly=false"}
{"level":"INFO","ts":"2025-12-10T13:46:15.434Z","msg":"Applied Saturation decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1, target=1, reason=model-based only (Saturation unavailable)"}
{"level":"INFO","ts":"2025-12-10T13:46:15.434Z","msg":"Reconciliation completed successfully: mode=model-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-12-10T13:47:15.434Z","msg":"Reconciling VariantAutoscaling: name=ms-inference-scheduling-llm-d-modelservice-decode, namespace=llm-d-inference-scheduler, modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-10T13:47:15.434Z","msg":"saturation scaling config cache updated: entries=1, has_default=true"}
{"level":"INFO","ts":"2025-12-10T13:47:15.434Z","msg":"saturation scaling configuration loaded successfully"}
{"level":"INFO","ts":"2025-12-10T13:47:15.434Z","msg":"Operating in MODEL-ONLY mode: model-based optimization only"}
{"level":"INFO","ts":"2025-12-10T13:47:15.434Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-12-10T13:47:15.434Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-12-10T13:47:15.438Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dgftqv, queueLength=0"}
{"level":"DEBUG","ts":"2025-12-10T13:47:15.438Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"INFO","ts":"2025-12-10T13:47:15.438Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dgftqv, usage=0.000 (0.0%)"}
{"level":"DEBUG","ts":"2025-12-10T13:47:15.438Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"DEBUG","ts":"2025-12-10T13:47:15.441Z","msg":"Pod-to-variant matching successful: totalPods=1, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"DEBUG","ts":"2025-12-10T13:47:15.441Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=1"}
{"level":"DEBUG","ts":"2025-12-10T13:47:15.441Z","msg":"Collected capacity metrics for observability in model-only mode: modelID=unsloth/Meta-Llama-3.1-8B, replicaCount=1"}
{"level":"DEBUG","ts":"2025-12-10T13:47:15.452Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=1, accelerator=H100, ttft=0.00ms, itl=0.00ms, cost=100.00, maxBatch=256, arrivalRate=0.00, avgInputTokens=0.00, avgOutputTokens=0.00"}
{"level":"DEBUG","ts":"2025-12-10T13:47:15.452Z","msg":"Experimental model tuner is enabled globally: tuning model performance parameters for active VAs"}
{"level":"DEBUG","ts":"2025-12-10T13:47:15.452Z","msg":"Using state vals from VA status to tune variant ms-inference-scheduling-llm-d-modelservice-decode: alpha= 7.117926, beta= 0.044671, gamma= 15.880216, delta= 0.000261"}
{"level":"DEBUG","ts":"2025-12-10T13:47:15.452Z","msg":"Skipping tuning for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler due to insufficient load. Using fallback parameters."}
{"level":"INFO","ts":"2025-12-10T13:47:15.452Z","msg":"Using tuned parameters from status for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler"}
{"level":"DEBUG","ts":"2025-12-10T13:47:15.452Z","msg":"Updated SystemData for model=unsloth/Meta-Llama-3.1-8B, accelerator=H100: alpha=7.117926, beta=0.044671, gamma=15.880216, delta=0.000261"}
{"level":"DEBUG","ts":"2025-12-10T13:47:15.452Z","msg":"Optimization solution - system: Solution: \ns=ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler; c=Premium; m=unsloth/Meta-Llama-3.1-8B; rate=0; inTk=0; outTk=0; sol=1, sat=false, alloc={acc=H100; numRep=1; maxBatch=512; cost=100, val=0, itl=7.162597, ttft=15.880477, rho=0, maxRPM=669719.5}; slo-itl=10, slo-ttft=1000, slo-tps=0 \nAllocationByType: \nname=NVIDIA-H100-80GB-HBM3, count=1, limit=0, cost=100 \ntotalCost=100 \n"}
{"level":"DEBUG","ts":"2025-12-10T13:47:15.452Z","msg":"Setting accelerator name: Name=H100, allocationData={H100 1 512 100 7.162597 15.880477 {0 0 0}}"}
{"level":"INFO","ts":"2025-12-10T13:47:15.452Z","msg":"Model-based optimization completed for model: unsloth/Meta-Llama-3.1-8B - model-based targets: map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"WARN","ts":"2025-12-10T13:47:15.452Z","msg":"saturation analysis unavailable, using model-based targets only: modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-10T13:47:15.452Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-12-10T13:47:15.452Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1→target=1"}
{"level":"INFO","ts":"2025-12-10T13:47:15.452Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 1, desired-replicas: 1, accelerator: H100"}
{"level":"INFO","ts":"2025-12-10T13:47:15.452Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=1, accelerator=H100, SaturationOnly=false"}
{"level":"INFO","ts":"2025-12-10T13:47:15.459Z","msg":"Applied Saturation decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1, target=1, reason=model-based only (Saturation unavailable)"}
{"level":"INFO","ts":"2025-12-10T13:47:15.459Z","msg":"Reconciliation completed successfully: mode=model-only, modelsProcessed=1, decisionsApplied=1"}
