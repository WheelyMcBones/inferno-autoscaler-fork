{"level":"INFO","ts":"2025-12-06T02:39:24.682Z","msg":"saturation scaling config cache updated: entries=1, has_default=true"}
{"level":"INFO","ts":"2025-12-06T02:39:24.682Z","msg":"saturation scaling configuration loaded successfully"}
{"level":"INFO","ts":"2025-12-06T02:39:24.682Z","msg":"Operating in MODEL-ONLY mode: model-based optimization only"}
{"level":"INFO","ts":"2025-12-06T02:39:24.682Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-12-06T02:39:24.682Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-12-06T02:39:24.686Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dczzts, usage=0.000 (0.0%)"}
{"level":"INFO","ts":"2025-12-06T02:39:24.686Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dczzts, queueLength=0"}
{"level":"DEBUG","ts":"2025-12-06T02:39:24.686Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"DEBUG","ts":"2025-12-06T02:39:24.686Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"DEBUG","ts":"2025-12-06T02:39:24.688Z","msg":"Pod-to-variant matching successful: totalPods=1, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"DEBUG","ts":"2025-12-06T02:39:24.688Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=1"}
{"level":"DEBUG","ts":"2025-12-06T02:39:24.688Z","msg":"Collected capacity metrics for observability in model-only mode: modelID=unsloth/Meta-Llama-3.1-8B, replicaCount=1"}
{"level":"INFO","ts":"2025-12-06T02:39:24.688Z","msg":"Found SLO for model: model=unsloth/Meta-Llama-3.1-8B, class=Premium, slo-tpot=10, slo-ttft=1000"}
{"level":"DEBUG","ts":"2025-12-06T02:39:24.696Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=1, accelerator=H100, ttft=0.00ms, itl=0.00ms, cost=100.00, maxBatch=256, arrivalRate=0.00, avgInputTokens=0.00, avgOutputTokens=0.00"}
{"level":"DEBUG","ts":"2025-12-06T02:39:24.696Z","msg":"Experimental model tuner is enabled globally (EXPERIMENTAL_MODEL_TUNER_ENABLED=true) tuning model performance parameters for active VAs"}
{"level":"DEBUG","ts":"2025-12-06T02:39:24.696Z","msg":"Using state vals from VA status to tune variant ms-inference-scheduling-llm-d-modelservice-decode: alpha= 7.350417, beta= 0.045946, gamma= 17.152187, delta= 0.000321"}
{"level":"DEBUG","ts":"2025-12-06T02:39:24.696Z","msg":"[Tuner Config] variant=ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler | Initial state: alpha=7.350417, beta=0.045946, gamma=17.152187, delta=0.000321 | SLO targets: TTFT=1000.00ms, ITL=10.00ms | Has covariance=true"}
{"level":"WARN","ts":"2025-12-06T02:39:24.696Z","msg":"Tuner failed completely for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler: failed to get/create tuner: failed to create tuner: invalid environment: &{0 0 0 256 0 0 1}. Using fallback parameters."}
{"level":"INFO","ts":"2025-12-06T02:39:24.696Z","msg":"Using tuned parameters from status for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler"}
{"level":"DEBUG","ts":"2025-12-06T02:39:24.696Z","msg":"Updated SystemData for model=unsloth/Meta-Llama-3.1-8B, accelerator=H100: alpha=7.350417, beta=0.045946, gamma=17.152187, delta=0.000321"}
{"level":"DEBUG","ts":"2025-12-06T02:39:24.696Z","msg":"Optimization solution - system: Solution: \ns=ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler; c=Premium; m=unsloth/Meta-Llama-3.1-8B; rate=0; inTk=0; outTk=0; sol=1, sat=false, alloc={acc=H100; numRep=1; maxBatch=512; cost=100, val=0, itl=7.3963633, ttft=17.152508, rho=0, maxRPM=639636.56}; slo-itl=10, slo-ttft=1000, slo-tps=0 \nAllocationByType: \nname=NVIDIA-H100-80GB-HBM3, count=1, limit=0, cost=100 \ntotalCost=100 \n"}
{"level":"DEBUG","ts":"2025-12-06T02:39:24.696Z","msg":"Setting accelerator name: Name=H100, allocationData={H100 1 512 100 7.3963633 17.152508 {0 0 0}}"}
{"level":"INFO","ts":"2025-12-06T02:39:24.696Z","msg":"Model-based optimization completed for model: unsloth/Meta-Llama-3.1-8B - model-based targets: map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"WARN","ts":"2025-12-06T02:39:24.696Z","msg":"saturation analysis unavailable, using model-based targets only: modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-06T02:39:24.696Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-12-06T02:39:24.696Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1→target=1"}
{"level":"DEBUG","ts":"2025-12-06T02:39:24.696Z","msg":"Found VA in map: variant=ms-inference-scheduling-llm-d-modelservice-decode, hasCurrentAlloc=true, accelerator=H100"}
{"level":"INFO","ts":"2025-12-06T02:39:24.696Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 1, desired-replicas: 1, accelerator: H100"}
{"level":"INFO","ts":"2025-12-06T02:39:24.696Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=1, accelerator=H100, SaturationOnly=false"}
{"level":"INFO","ts":"2025-12-06T02:39:24.704Z","msg":"Applied Saturation decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1, target=1, reason=model-based only (Saturation unavailable)"}
{"level":"INFO","ts":"2025-12-06T02:39:24.704Z","msg":"Reconciliation completed successfully: mode=model-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-12-06T02:39:24.704Z","msg":"saturation scaling config cache updated: entries=1, has_default=true"}
{"level":"INFO","ts":"2025-12-06T02:39:24.704Z","msg":"saturation scaling configuration loaded successfully"}
{"level":"INFO","ts":"2025-12-06T02:39:24.704Z","msg":"Operating in MODEL-ONLY mode: model-based optimization only"}
{"level":"INFO","ts":"2025-12-06T02:39:24.704Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-12-06T02:39:24.704Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-12-06T02:39:24.705Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dczzts, usage=0.000 (0.0%)"}
{"level":"DEBUG","ts":"2025-12-06T02:39:24.705Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"INFO","ts":"2025-12-06T02:39:24.705Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dczzts, queueLength=0"}
{"level":"DEBUG","ts":"2025-12-06T02:39:24.705Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"DEBUG","ts":"2025-12-06T02:39:24.706Z","msg":"Pod-to-variant matching successful: totalPods=1, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"DEBUG","ts":"2025-12-06T02:39:24.706Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=1"}
{"level":"DEBUG","ts":"2025-12-06T02:39:24.706Z","msg":"Collected capacity metrics for observability in model-only mode: modelID=unsloth/Meta-Llama-3.1-8B, replicaCount=1"}
{"level":"INFO","ts":"2025-12-06T02:39:24.706Z","msg":"Found SLO for model: model=unsloth/Meta-Llama-3.1-8B, class=Premium, slo-tpot=10, slo-ttft=1000"}
{"level":"DEBUG","ts":"2025-12-06T02:39:24.719Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=1, accelerator=H100, ttft=0.00ms, itl=0.00ms, cost=100.00, maxBatch=256, arrivalRate=0.00, avgInputTokens=0.00, avgOutputTokens=0.00"}
{"level":"DEBUG","ts":"2025-12-06T02:39:24.719Z","msg":"Experimental model tuner is enabled globally (EXPERIMENTAL_MODEL_TUNER_ENABLED=true) tuning model performance parameters for active VAs"}
{"level":"DEBUG","ts":"2025-12-06T02:39:24.719Z","msg":"Using state vals from VA status to tune variant ms-inference-scheduling-llm-d-modelservice-decode: alpha= 7.350417, beta= 0.045946, gamma= 17.152187, delta= 0.000321"}
{"level":"DEBUG","ts":"2025-12-06T02:39:24.719Z","msg":"[Tuner Config] variant=ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler | Initial state: alpha=7.350417, beta=0.045946, gamma=17.152187, delta=0.000321 | SLO targets: TTFT=1000.00ms, ITL=10.00ms | Has covariance=true"}
{"level":"WARN","ts":"2025-12-06T02:39:24.719Z","msg":"Tuner failed completely for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler: failed to get/create tuner: failed to create tuner: invalid environment: &{0 0 0 256 0 0 1}. Using fallback parameters."}
{"level":"INFO","ts":"2025-12-06T02:39:24.719Z","msg":"Using tuned parameters from status for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler"}
{"level":"DEBUG","ts":"2025-12-06T02:39:24.719Z","msg":"Updated SystemData for model=unsloth/Meta-Llama-3.1-8B, accelerator=H100: alpha=7.350417, beta=0.045946, gamma=17.152187, delta=0.000321"}
{"level":"DEBUG","ts":"2025-12-06T02:39:24.719Z","msg":"Optimization solution - system: Solution: \ns=ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler; c=Premium; m=unsloth/Meta-Llama-3.1-8B; rate=0; inTk=0; outTk=0; sol=1, sat=false, alloc={acc=H100; numRep=1; maxBatch=512; cost=100, val=0, itl=7.3963633, ttft=17.152508, rho=0, maxRPM=639636.56}; slo-itl=10, slo-ttft=1000, slo-tps=0 \nAllocationByType: \nname=NVIDIA-H100-80GB-HBM3, count=1, limit=0, cost=100 \ntotalCost=100 \n"}
{"level":"DEBUG","ts":"2025-12-06T02:39:24.719Z","msg":"Setting accelerator name: Name=H100, allocationData={H100 1 512 100 7.3963633 17.152508 {0 0 0}}"}
{"level":"INFO","ts":"2025-12-06T02:39:24.719Z","msg":"Model-based optimization completed for model: unsloth/Meta-Llama-3.1-8B - model-based targets: map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"WARN","ts":"2025-12-06T02:39:24.719Z","msg":"saturation analysis unavailable, using model-based targets only: modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-06T02:39:24.719Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-12-06T02:39:24.719Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1→target=1"}
{"level":"DEBUG","ts":"2025-12-06T02:39:24.719Z","msg":"Found VA in map: variant=ms-inference-scheduling-llm-d-modelservice-decode, hasCurrentAlloc=true, accelerator=H100"}
{"level":"INFO","ts":"2025-12-06T02:39:24.719Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 1, desired-replicas: 1, accelerator: H100"}
{"level":"INFO","ts":"2025-12-06T02:39:24.719Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=1, accelerator=H100, SaturationOnly=false"}
{"level":"INFO","ts":"2025-12-06T02:39:24.724Z","msg":"Applied Saturation decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1, target=1, reason=model-based only (Saturation unavailable)"}
{"level":"INFO","ts":"2025-12-06T02:39:24.724Z","msg":"Reconciliation completed successfully: mode=model-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-12-06T02:40:24.704Z","msg":"saturation scaling config cache updated: entries=1, has_default=true"}
{"level":"INFO","ts":"2025-12-06T02:40:24.704Z","msg":"saturation scaling configuration loaded successfully"}
{"level":"INFO","ts":"2025-12-06T02:40:24.704Z","msg":"Operating in MODEL-ONLY mode: model-based optimization only"}
{"level":"INFO","ts":"2025-12-06T02:40:24.704Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-12-06T02:40:24.704Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-12-06T02:40:24.710Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dczzts, queueLength=0"}
{"level":"DEBUG","ts":"2025-12-06T02:40:24.710Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"INFO","ts":"2025-12-06T02:40:24.710Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dczzts, usage=0.000 (0.0%)"}
{"level":"DEBUG","ts":"2025-12-06T02:40:24.710Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"DEBUG","ts":"2025-12-06T02:40:24.712Z","msg":"Pod-to-variant matching successful: totalPods=1, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"DEBUG","ts":"2025-12-06T02:40:24.712Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=1"}
{"level":"DEBUG","ts":"2025-12-06T02:40:24.712Z","msg":"Collected capacity metrics for observability in model-only mode: modelID=unsloth/Meta-Llama-3.1-8B, replicaCount=1"}
{"level":"INFO","ts":"2025-12-06T02:40:24.712Z","msg":"Found SLO for model: model=unsloth/Meta-Llama-3.1-8B, class=Premium, slo-tpot=10, slo-ttft=1000"}
{"level":"DEBUG","ts":"2025-12-06T02:40:24.720Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=1, accelerator=H100, ttft=0.00ms, itl=0.00ms, cost=100.00, maxBatch=256, arrivalRate=0.00, avgInputTokens=0.00, avgOutputTokens=0.00"}
{"level":"DEBUG","ts":"2025-12-06T02:40:24.720Z","msg":"Experimental model tuner is enabled globally (EXPERIMENTAL_MODEL_TUNER_ENABLED=true) tuning model performance parameters for active VAs"}
{"level":"DEBUG","ts":"2025-12-06T02:40:24.720Z","msg":"Using state vals from VA status to tune variant ms-inference-scheduling-llm-d-modelservice-decode: alpha= 7.350417, beta= 0.045946, gamma= 17.152187, delta= 0.000321"}
{"level":"DEBUG","ts":"2025-12-06T02:40:24.720Z","msg":"[Tuner Config] variant=ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler | Initial state: alpha=7.350417, beta=0.045946, gamma=17.152187, delta=0.000321 | SLO targets: TTFT=1000.00ms, ITL=10.00ms | Has covariance=true"}
{"level":"WARN","ts":"2025-12-06T02:40:24.720Z","msg":"Tuner failed completely for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler: failed to get/create tuner: failed to create tuner: invalid environment: &{0 0 0 256 0 0 1}. Using fallback parameters."}
{"level":"INFO","ts":"2025-12-06T02:40:24.720Z","msg":"Using tuned parameters from status for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler"}
{"level":"DEBUG","ts":"2025-12-06T02:40:24.720Z","msg":"Updated SystemData for model=unsloth/Meta-Llama-3.1-8B, accelerator=H100: alpha=7.350417, beta=0.045946, gamma=17.152187, delta=0.000321"}
{"level":"DEBUG","ts":"2025-12-06T02:40:24.721Z","msg":"Optimization solution - system: Solution: \ns=ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler; c=Premium; m=unsloth/Meta-Llama-3.1-8B; rate=0; inTk=0; outTk=0; sol=1, sat=false, alloc={acc=H100; numRep=1; maxBatch=512; cost=100, val=0, itl=7.3963633, ttft=17.152508, rho=0, maxRPM=639636.56}; slo-itl=10, slo-ttft=1000, slo-tps=0 \nAllocationByType: \nname=NVIDIA-H100-80GB-HBM3, count=1, limit=0, cost=100 \ntotalCost=100 \n"}
{"level":"DEBUG","ts":"2025-12-06T02:40:24.721Z","msg":"Setting accelerator name: Name=H100, allocationData={H100 1 512 100 7.3963633 17.152508 {0 0 0}}"}
{"level":"INFO","ts":"2025-12-06T02:40:24.721Z","msg":"Model-based optimization completed for model: unsloth/Meta-Llama-3.1-8B - model-based targets: map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"WARN","ts":"2025-12-06T02:40:24.721Z","msg":"saturation analysis unavailable, using model-based targets only: modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-06T02:40:24.721Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-12-06T02:40:24.721Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1→target=1"}
{"level":"DEBUG","ts":"2025-12-06T02:40:24.721Z","msg":"Found VA in map: variant=ms-inference-scheduling-llm-d-modelservice-decode, hasCurrentAlloc=true, accelerator=H100"}
{"level":"INFO","ts":"2025-12-06T02:40:24.721Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 1, desired-replicas: 1, accelerator: H100"}
{"level":"INFO","ts":"2025-12-06T02:40:24.721Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=1, accelerator=H100, SaturationOnly=false"}
{"level":"INFO","ts":"2025-12-06T02:40:24.727Z","msg":"Applied Saturation decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1, target=1, reason=model-based only (Saturation unavailable)"}
{"level":"INFO","ts":"2025-12-06T02:40:24.727Z","msg":"Reconciliation completed successfully: mode=model-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-12-06T02:40:24.727Z","msg":"saturation scaling config cache updated: entries=1, has_default=true"}
{"level":"INFO","ts":"2025-12-06T02:40:24.727Z","msg":"saturation scaling configuration loaded successfully"}
{"level":"INFO","ts":"2025-12-06T02:40:24.727Z","msg":"Operating in MODEL-ONLY mode: model-based optimization only"}
{"level":"INFO","ts":"2025-12-06T02:40:24.727Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-12-06T02:40:24.727Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-12-06T02:40:24.728Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dczzts, queueLength=0"}
{"level":"DEBUG","ts":"2025-12-06T02:40:24.728Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"INFO","ts":"2025-12-06T02:40:24.728Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dczzts, usage=0.000 (0.0%)"}
{"level":"DEBUG","ts":"2025-12-06T02:40:24.728Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"DEBUG","ts":"2025-12-06T02:40:24.730Z","msg":"Pod-to-variant matching successful: totalPods=1, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"DEBUG","ts":"2025-12-06T02:40:24.730Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=1"}
{"level":"DEBUG","ts":"2025-12-06T02:40:24.730Z","msg":"Collected capacity metrics for observability in model-only mode: modelID=unsloth/Meta-Llama-3.1-8B, replicaCount=1"}
{"level":"INFO","ts":"2025-12-06T02:40:24.730Z","msg":"Found SLO for model: model=unsloth/Meta-Llama-3.1-8B, class=Premium, slo-tpot=10, slo-ttft=1000"}
{"level":"DEBUG","ts":"2025-12-06T02:40:24.738Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=1, accelerator=H100, ttft=0.00ms, itl=0.00ms, cost=100.00, maxBatch=256, arrivalRate=0.00, avgInputTokens=0.00, avgOutputTokens=0.00"}
{"level":"DEBUG","ts":"2025-12-06T02:40:24.738Z","msg":"Experimental model tuner is enabled globally (EXPERIMENTAL_MODEL_TUNER_ENABLED=true) tuning model performance parameters for active VAs"}
{"level":"DEBUG","ts":"2025-12-06T02:40:24.738Z","msg":"Using state vals from VA status to tune variant ms-inference-scheduling-llm-d-modelservice-decode: alpha= 7.350417, beta= 0.045946, gamma= 17.152187, delta= 0.000321"}
{"level":"DEBUG","ts":"2025-12-06T02:40:24.738Z","msg":"[Tuner Config] variant=ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler | Initial state: alpha=7.350417, beta=0.045946, gamma=17.152187, delta=0.000321 | SLO targets: TTFT=1000.00ms, ITL=10.00ms | Has covariance=true"}
{"level":"WARN","ts":"2025-12-06T02:40:24.738Z","msg":"Tuner failed completely for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler: failed to get/create tuner: failed to create tuner: invalid environment: &{0 0 0 256 0 0 1}. Using fallback parameters."}
{"level":"INFO","ts":"2025-12-06T02:40:24.738Z","msg":"Using tuned parameters from status for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler"}
{"level":"DEBUG","ts":"2025-12-06T02:40:24.738Z","msg":"Updated SystemData for model=unsloth/Meta-Llama-3.1-8B, accelerator=H100: alpha=7.350417, beta=0.045946, gamma=17.152187, delta=0.000321"}
{"level":"DEBUG","ts":"2025-12-06T02:40:24.738Z","msg":"Optimization solution - system: Solution: \ns=ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler; c=Premium; m=unsloth/Meta-Llama-3.1-8B; rate=0; inTk=0; outTk=0; sol=1, sat=false, alloc={acc=H100; numRep=1; maxBatch=512; cost=100, val=0, itl=7.3963633, ttft=17.152508, rho=0, maxRPM=639636.56}; slo-itl=10, slo-ttft=1000, slo-tps=0 \nAllocationByType: \nname=NVIDIA-H100-80GB-HBM3, count=1, limit=0, cost=100 \ntotalCost=100 \n"}
{"level":"DEBUG","ts":"2025-12-06T02:40:24.738Z","msg":"Setting accelerator name: Name=H100, allocationData={H100 1 512 100 7.3963633 17.152508 {0 0 0}}"}
{"level":"INFO","ts":"2025-12-06T02:40:24.738Z","msg":"Model-based optimization completed for model: unsloth/Meta-Llama-3.1-8B - model-based targets: map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"WARN","ts":"2025-12-06T02:40:24.738Z","msg":"saturation analysis unavailable, using model-based targets only: modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-06T02:40:24.738Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-12-06T02:40:24.738Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1→target=1"}
{"level":"DEBUG","ts":"2025-12-06T02:40:24.738Z","msg":"Found VA in map: variant=ms-inference-scheduling-llm-d-modelservice-decode, hasCurrentAlloc=true, accelerator=H100"}
{"level":"INFO","ts":"2025-12-06T02:40:24.738Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 1, desired-replicas: 1, accelerator: H100"}
{"level":"INFO","ts":"2025-12-06T02:40:24.738Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=1, accelerator=H100, SaturationOnly=false"}
{"level":"INFO","ts":"2025-12-06T02:40:24.743Z","msg":"Applied Saturation decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1, target=1, reason=model-based only (Saturation unavailable)"}
{"level":"INFO","ts":"2025-12-06T02:40:24.743Z","msg":"Reconciliation completed successfully: mode=model-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-12-06T02:41:24.728Z","msg":"saturation scaling config cache updated: entries=1, has_default=true"}
{"level":"INFO","ts":"2025-12-06T02:41:24.728Z","msg":"saturation scaling configuration loaded successfully"}
{"level":"INFO","ts":"2025-12-06T02:41:24.728Z","msg":"Operating in MODEL-ONLY mode: model-based optimization only"}
{"level":"INFO","ts":"2025-12-06T02:41:24.728Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-12-06T02:41:24.728Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-12-06T02:41:24.733Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dczzts, queueLength=0"}
{"level":"DEBUG","ts":"2025-12-06T02:41:24.733Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"INFO","ts":"2025-12-06T02:41:24.733Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dczzts, usage=0.000 (0.0%)"}
{"level":"DEBUG","ts":"2025-12-06T02:41:24.733Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"DEBUG","ts":"2025-12-06T02:41:24.735Z","msg":"Pod-to-variant matching successful: totalPods=1, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"DEBUG","ts":"2025-12-06T02:41:24.735Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=1"}
{"level":"DEBUG","ts":"2025-12-06T02:41:24.735Z","msg":"Collected capacity metrics for observability in model-only mode: modelID=unsloth/Meta-Llama-3.1-8B, replicaCount=1"}
{"level":"INFO","ts":"2025-12-06T02:41:24.736Z","msg":"Found SLO for model: model=unsloth/Meta-Llama-3.1-8B, class=Premium, slo-tpot=10, slo-ttft=1000"}
{"level":"DEBUG","ts":"2025-12-06T02:41:24.744Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=1, accelerator=H100, ttft=0.00ms, itl=0.00ms, cost=100.00, maxBatch=256, arrivalRate=0.00, avgInputTokens=0.00, avgOutputTokens=0.00"}
{"level":"DEBUG","ts":"2025-12-06T02:41:24.744Z","msg":"Experimental model tuner is enabled globally (EXPERIMENTAL_MODEL_TUNER_ENABLED=true) tuning model performance parameters for active VAs"}
{"level":"DEBUG","ts":"2025-12-06T02:41:24.744Z","msg":"Using state vals from VA status to tune variant ms-inference-scheduling-llm-d-modelservice-decode: alpha= 7.350417, beta= 0.045946, gamma= 17.152187, delta= 0.000321"}
{"level":"DEBUG","ts":"2025-12-06T02:41:24.744Z","msg":"[Tuner Config] variant=ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler | Initial state: alpha=7.350417, beta=0.045946, gamma=17.152187, delta=0.000321 | SLO targets: TTFT=1000.00ms, ITL=10.00ms | Has covariance=true"}
{"level":"WARN","ts":"2025-12-06T02:41:24.744Z","msg":"Tuner failed completely for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler: failed to get/create tuner: failed to create tuner: invalid environment: &{0 0 0 256 0 0 1}. Using fallback parameters."}
{"level":"INFO","ts":"2025-12-06T02:41:24.744Z","msg":"Using tuned parameters from status for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler"}
{"level":"DEBUG","ts":"2025-12-06T02:41:24.744Z","msg":"Updated SystemData for model=unsloth/Meta-Llama-3.1-8B, accelerator=H100: alpha=7.350417, beta=0.045946, gamma=17.152187, delta=0.000321"}
{"level":"DEBUG","ts":"2025-12-06T02:41:24.744Z","msg":"Optimization solution - system: Solution: \ns=ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler; c=Premium; m=unsloth/Meta-Llama-3.1-8B; rate=0; inTk=0; outTk=0; sol=1, sat=false, alloc={acc=H100; numRep=1; maxBatch=512; cost=100, val=0, itl=7.3963633, ttft=17.152508, rho=0, maxRPM=639636.56}; slo-itl=10, slo-ttft=1000, slo-tps=0 \nAllocationByType: \nname=NVIDIA-H100-80GB-HBM3, count=1, limit=0, cost=100 \ntotalCost=100 \n"}
{"level":"DEBUG","ts":"2025-12-06T02:41:24.744Z","msg":"Setting accelerator name: Name=H100, allocationData={H100 1 512 100 7.3963633 17.152508 {0 0 0}}"}
{"level":"INFO","ts":"2025-12-06T02:41:24.744Z","msg":"Model-based optimization completed for model: unsloth/Meta-Llama-3.1-8B - model-based targets: map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"WARN","ts":"2025-12-06T02:41:24.744Z","msg":"saturation analysis unavailable, using model-based targets only: modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-06T02:41:24.744Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-12-06T02:41:24.744Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1→target=1"}
{"level":"DEBUG","ts":"2025-12-06T02:41:24.744Z","msg":"Found VA in map: variant=ms-inference-scheduling-llm-d-modelservice-decode, hasCurrentAlloc=true, accelerator=H100"}
{"level":"INFO","ts":"2025-12-06T02:41:24.744Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 1, desired-replicas: 1, accelerator: H100"}
{"level":"INFO","ts":"2025-12-06T02:41:24.744Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=1, accelerator=H100, SaturationOnly=false"}
{"level":"INFO","ts":"2025-12-06T02:41:24.751Z","msg":"Applied Saturation decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1, target=1, reason=model-based only (Saturation unavailable)"}
{"level":"INFO","ts":"2025-12-06T02:41:24.751Z","msg":"Reconciliation completed successfully: mode=model-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-12-06T02:41:24.751Z","msg":"saturation scaling config cache updated: entries=1, has_default=true"}
{"level":"INFO","ts":"2025-12-06T02:41:24.751Z","msg":"saturation scaling configuration loaded successfully"}
{"level":"INFO","ts":"2025-12-06T02:41:24.751Z","msg":"Operating in MODEL-ONLY mode: model-based optimization only"}
{"level":"INFO","ts":"2025-12-06T02:41:24.751Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-12-06T02:41:24.751Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-12-06T02:41:24.753Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dczzts, queueLength=0"}
{"level":"DEBUG","ts":"2025-12-06T02:41:24.753Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"INFO","ts":"2025-12-06T02:41:24.753Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dczzts, usage=0.000 (0.0%)"}
{"level":"DEBUG","ts":"2025-12-06T02:41:24.753Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"DEBUG","ts":"2025-12-06T02:41:24.754Z","msg":"Pod-to-variant matching successful: totalPods=1, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"DEBUG","ts":"2025-12-06T02:41:24.754Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=1"}
{"level":"DEBUG","ts":"2025-12-06T02:41:24.754Z","msg":"Collected capacity metrics for observability in model-only mode: modelID=unsloth/Meta-Llama-3.1-8B, replicaCount=1"}
{"level":"INFO","ts":"2025-12-06T02:41:24.754Z","msg":"Found SLO for model: model=unsloth/Meta-Llama-3.1-8B, class=Premium, slo-tpot=10, slo-ttft=1000"}
{"level":"DEBUG","ts":"2025-12-06T02:41:24.761Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=1, accelerator=H100, ttft=0.00ms, itl=0.00ms, cost=100.00, maxBatch=256, arrivalRate=0.00, avgInputTokens=0.00, avgOutputTokens=0.00"}
{"level":"DEBUG","ts":"2025-12-06T02:41:24.761Z","msg":"Experimental model tuner is enabled globally (EXPERIMENTAL_MODEL_TUNER_ENABLED=true) tuning model performance parameters for active VAs"}
{"level":"DEBUG","ts":"2025-12-06T02:41:24.761Z","msg":"Using state vals from VA status to tune variant ms-inference-scheduling-llm-d-modelservice-decode: alpha= 7.350417, beta= 0.045946, gamma= 17.152187, delta= 0.000321"}
{"level":"DEBUG","ts":"2025-12-06T02:41:24.761Z","msg":"[Tuner Config] variant=ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler | Initial state: alpha=7.350417, beta=0.045946, gamma=17.152187, delta=0.000321 | SLO targets: TTFT=1000.00ms, ITL=10.00ms | Has covariance=true"}
{"level":"WARN","ts":"2025-12-06T02:41:24.761Z","msg":"Tuner failed completely for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler: failed to get/create tuner: failed to create tuner: invalid environment: &{0 0 0 256 0 0 1}. Using fallback parameters."}
{"level":"INFO","ts":"2025-12-06T02:41:24.761Z","msg":"Using tuned parameters from status for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler"}
{"level":"DEBUG","ts":"2025-12-06T02:41:24.761Z","msg":"Updated SystemData for model=unsloth/Meta-Llama-3.1-8B, accelerator=H100: alpha=7.350417, beta=0.045946, gamma=17.152187, delta=0.000321"}
{"level":"DEBUG","ts":"2025-12-06T02:41:24.761Z","msg":"Optimization solution - system: Solution: \ns=ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler; c=Premium; m=unsloth/Meta-Llama-3.1-8B; rate=0; inTk=0; outTk=0; sol=1, sat=false, alloc={acc=H100; numRep=1; maxBatch=512; cost=100, val=0, itl=7.3963633, ttft=17.152508, rho=0, maxRPM=639636.56}; slo-itl=10, slo-ttft=1000, slo-tps=0 \nAllocationByType: \nname=NVIDIA-H100-80GB-HBM3, count=1, limit=0, cost=100 \ntotalCost=100 \n"}
{"level":"DEBUG","ts":"2025-12-06T02:41:24.761Z","msg":"Setting accelerator name: Name=H100, allocationData={H100 1 512 100 7.3963633 17.152508 {0 0 0}}"}
{"level":"INFO","ts":"2025-12-06T02:41:24.761Z","msg":"Model-based optimization completed for model: unsloth/Meta-Llama-3.1-8B - model-based targets: map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"WARN","ts":"2025-12-06T02:41:24.761Z","msg":"saturation analysis unavailable, using model-based targets only: modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-06T02:41:24.762Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-12-06T02:41:24.762Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1→target=1"}
{"level":"DEBUG","ts":"2025-12-06T02:41:24.762Z","msg":"Found VA in map: variant=ms-inference-scheduling-llm-d-modelservice-decode, hasCurrentAlloc=true, accelerator=H100"}
{"level":"INFO","ts":"2025-12-06T02:41:24.762Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 1, desired-replicas: 1, accelerator: H100"}
{"level":"INFO","ts":"2025-12-06T02:41:24.762Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=1, accelerator=H100, SaturationOnly=false"}
{"level":"INFO","ts":"2025-12-06T02:41:24.767Z","msg":"Applied Saturation decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1, target=1, reason=model-based only (Saturation unavailable)"}
{"level":"INFO","ts":"2025-12-06T02:41:24.767Z","msg":"Reconciliation completed successfully: mode=model-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-12-06T02:42:24.751Z","msg":"saturation scaling config cache updated: entries=1, has_default=true"}
{"level":"INFO","ts":"2025-12-06T02:42:24.751Z","msg":"saturation scaling configuration loaded successfully"}
{"level":"INFO","ts":"2025-12-06T02:42:24.751Z","msg":"Operating in MODEL-ONLY mode: model-based optimization only"}
{"level":"INFO","ts":"2025-12-06T02:42:24.751Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-12-06T02:42:24.751Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-12-06T02:42:24.760Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dczzts, queueLength=0"}
{"level":"DEBUG","ts":"2025-12-06T02:42:24.760Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"INFO","ts":"2025-12-06T02:42:24.760Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dczzts, usage=0.000 (0.0%)"}
{"level":"DEBUG","ts":"2025-12-06T02:42:24.760Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"DEBUG","ts":"2025-12-06T02:42:24.762Z","msg":"Pod-to-variant matching successful: totalPods=1, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"DEBUG","ts":"2025-12-06T02:42:24.762Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=1"}
{"level":"DEBUG","ts":"2025-12-06T02:42:24.762Z","msg":"Collected capacity metrics for observability in model-only mode: modelID=unsloth/Meta-Llama-3.1-8B, replicaCount=1"}
{"level":"INFO","ts":"2025-12-06T02:42:24.762Z","msg":"Found SLO for model: model=unsloth/Meta-Llama-3.1-8B, class=Premium, slo-tpot=10, slo-ttft=1000"}
{"level":"DEBUG","ts":"2025-12-06T02:42:24.771Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=1, accelerator=H100, ttft=0.00ms, itl=0.00ms, cost=100.00, maxBatch=256, arrivalRate=0.00, avgInputTokens=0.00, avgOutputTokens=0.00"}
{"level":"DEBUG","ts":"2025-12-06T02:42:24.771Z","msg":"Experimental model tuner is enabled globally (EXPERIMENTAL_MODEL_TUNER_ENABLED=true) tuning model performance parameters for active VAs"}
{"level":"DEBUG","ts":"2025-12-06T02:42:24.771Z","msg":"Using state vals from VA status to tune variant ms-inference-scheduling-llm-d-modelservice-decode: alpha= 7.350417, beta= 0.045946, gamma= 17.152187, delta= 0.000321"}
{"level":"DEBUG","ts":"2025-12-06T02:42:24.771Z","msg":"[Tuner Config] variant=ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler | Initial state: alpha=7.350417, beta=0.045946, gamma=17.152187, delta=0.000321 | SLO targets: TTFT=1000.00ms, ITL=10.00ms | Has covariance=true"}
{"level":"WARN","ts":"2025-12-06T02:42:24.771Z","msg":"Tuner failed completely for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler: failed to get/create tuner: failed to create tuner: invalid environment: &{0 0 0 256 0 0 1}. Using fallback parameters."}
{"level":"INFO","ts":"2025-12-06T02:42:24.771Z","msg":"Using tuned parameters from status for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler"}
{"level":"DEBUG","ts":"2025-12-06T02:42:24.771Z","msg":"Updated SystemData for model=unsloth/Meta-Llama-3.1-8B, accelerator=H100: alpha=7.350417, beta=0.045946, gamma=17.152187, delta=0.000321"}
{"level":"DEBUG","ts":"2025-12-06T02:42:24.771Z","msg":"Optimization solution - system: Solution: \ns=ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler; c=Premium; m=unsloth/Meta-Llama-3.1-8B; rate=0; inTk=0; outTk=0; sol=1, sat=false, alloc={acc=H100; numRep=1; maxBatch=512; cost=100, val=0, itl=7.3963633, ttft=17.152508, rho=0, maxRPM=639636.56}; slo-itl=10, slo-ttft=1000, slo-tps=0 \nAllocationByType: \nname=NVIDIA-H100-80GB-HBM3, count=1, limit=0, cost=100 \ntotalCost=100 \n"}
{"level":"DEBUG","ts":"2025-12-06T02:42:24.771Z","msg":"Setting accelerator name: Name=H100, allocationData={H100 1 512 100 7.3963633 17.152508 {0 0 0}}"}
{"level":"INFO","ts":"2025-12-06T02:42:24.771Z","msg":"Model-based optimization completed for model: unsloth/Meta-Llama-3.1-8B - model-based targets: map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"WARN","ts":"2025-12-06T02:42:24.771Z","msg":"saturation analysis unavailable, using model-based targets only: modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-06T02:42:24.771Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-12-06T02:42:24.771Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1→target=1"}
{"level":"DEBUG","ts":"2025-12-06T02:42:24.771Z","msg":"Found VA in map: variant=ms-inference-scheduling-llm-d-modelservice-decode, hasCurrentAlloc=true, accelerator=H100"}
{"level":"INFO","ts":"2025-12-06T02:42:24.771Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 1, desired-replicas: 1, accelerator: H100"}
{"level":"INFO","ts":"2025-12-06T02:42:24.771Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=1, accelerator=H100, SaturationOnly=false"}
{"level":"INFO","ts":"2025-12-06T02:42:24.776Z","msg":"Applied Saturation decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1, target=1, reason=model-based only (Saturation unavailable)"}
{"level":"INFO","ts":"2025-12-06T02:42:24.776Z","msg":"Reconciliation completed successfully: mode=model-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-12-06T02:42:24.777Z","msg":"saturation scaling config cache updated: entries=1, has_default=true"}
{"level":"INFO","ts":"2025-12-06T02:42:24.777Z","msg":"saturation scaling configuration loaded successfully"}
{"level":"INFO","ts":"2025-12-06T02:42:24.777Z","msg":"Operating in MODEL-ONLY mode: model-based optimization only"}
{"level":"INFO","ts":"2025-12-06T02:42:24.777Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-12-06T02:42:24.777Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-12-06T02:42:24.781Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dczzts, queueLength=0"}
{"level":"DEBUG","ts":"2025-12-06T02:42:24.781Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"INFO","ts":"2025-12-06T02:42:24.782Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dczzts, usage=0.000 (0.0%)"}
{"level":"DEBUG","ts":"2025-12-06T02:42:24.782Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"DEBUG","ts":"2025-12-06T02:42:24.784Z","msg":"Pod-to-variant matching successful: totalPods=1, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"DEBUG","ts":"2025-12-06T02:42:24.784Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=1"}
{"level":"DEBUG","ts":"2025-12-06T02:42:24.784Z","msg":"Collected capacity metrics for observability in model-only mode: modelID=unsloth/Meta-Llama-3.1-8B, replicaCount=1"}
{"level":"INFO","ts":"2025-12-06T02:42:24.784Z","msg":"Found SLO for model: model=unsloth/Meta-Llama-3.1-8B, class=Premium, slo-tpot=10, slo-ttft=1000"}
{"level":"DEBUG","ts":"2025-12-06T02:42:24.799Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=1, accelerator=H100, ttft=0.00ms, itl=0.00ms, cost=100.00, maxBatch=256, arrivalRate=0.00, avgInputTokens=0.00, avgOutputTokens=0.00"}
{"level":"DEBUG","ts":"2025-12-06T02:42:24.799Z","msg":"Experimental model tuner is enabled globally (EXPERIMENTAL_MODEL_TUNER_ENABLED=true) tuning model performance parameters for active VAs"}
{"level":"DEBUG","ts":"2025-12-06T02:42:24.799Z","msg":"Using state vals from VA status to tune variant ms-inference-scheduling-llm-d-modelservice-decode: alpha= 7.350417, beta= 0.045946, gamma= 17.152187, delta= 0.000321"}
{"level":"DEBUG","ts":"2025-12-06T02:42:24.799Z","msg":"[Tuner Config] variant=ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler | Initial state: alpha=7.350417, beta=0.045946, gamma=17.152187, delta=0.000321 | SLO targets: TTFT=1000.00ms, ITL=10.00ms | Has covariance=true"}
{"level":"WARN","ts":"2025-12-06T02:42:24.799Z","msg":"Tuner failed completely for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler: failed to get/create tuner: failed to create tuner: invalid environment: &{0 0 0 256 0 0 1}. Using fallback parameters."}
{"level":"INFO","ts":"2025-12-06T02:42:24.799Z","msg":"Using tuned parameters from status for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler"}
{"level":"DEBUG","ts":"2025-12-06T02:42:24.799Z","msg":"Updated SystemData for model=unsloth/Meta-Llama-3.1-8B, accelerator=H100: alpha=7.350417, beta=0.045946, gamma=17.152187, delta=0.000321"}
{"level":"DEBUG","ts":"2025-12-06T02:42:24.799Z","msg":"Optimization solution - system: Solution: \ns=ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler; c=Premium; m=unsloth/Meta-Llama-3.1-8B; rate=0; inTk=0; outTk=0; sol=1, sat=false, alloc={acc=H100; numRep=1; maxBatch=512; cost=100, val=0, itl=7.3963633, ttft=17.152508, rho=0, maxRPM=639636.56}; slo-itl=10, slo-ttft=1000, slo-tps=0 \nAllocationByType: \nname=NVIDIA-H100-80GB-HBM3, count=1, limit=0, cost=100 \ntotalCost=100 \n"}
{"level":"DEBUG","ts":"2025-12-06T02:42:24.799Z","msg":"Setting accelerator name: Name=H100, allocationData={H100 1 512 100 7.3963633 17.152508 {0 0 0}}"}
{"level":"INFO","ts":"2025-12-06T02:42:24.799Z","msg":"Model-based optimization completed for model: unsloth/Meta-Llama-3.1-8B - model-based targets: map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"WARN","ts":"2025-12-06T02:42:24.799Z","msg":"saturation analysis unavailable, using model-based targets only: modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-06T02:42:24.799Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-12-06T02:42:24.799Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1→target=1"}
{"level":"DEBUG","ts":"2025-12-06T02:42:24.799Z","msg":"Found VA in map: variant=ms-inference-scheduling-llm-d-modelservice-decode, hasCurrentAlloc=true, accelerator=H100"}
{"level":"INFO","ts":"2025-12-06T02:42:24.799Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 1, desired-replicas: 1, accelerator: H100"}
{"level":"INFO","ts":"2025-12-06T02:42:24.799Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=1, accelerator=H100, SaturationOnly=false"}
{"level":"INFO","ts":"2025-12-06T02:42:24.805Z","msg":"Applied Saturation decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1, target=1, reason=model-based only (Saturation unavailable)"}
{"level":"INFO","ts":"2025-12-06T02:42:24.805Z","msg":"Reconciliation completed successfully: mode=model-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-12-06T02:43:24.777Z","msg":"saturation scaling config cache updated: entries=1, has_default=true"}
{"level":"INFO","ts":"2025-12-06T02:43:24.778Z","msg":"saturation scaling configuration loaded successfully"}
{"level":"INFO","ts":"2025-12-06T02:43:24.778Z","msg":"Operating in MODEL-ONLY mode: model-based optimization only"}
{"level":"INFO","ts":"2025-12-06T02:43:24.778Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-12-06T02:43:24.778Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-12-06T02:43:24.782Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dczzts, queueLength=0"}
{"level":"DEBUG","ts":"2025-12-06T02:43:24.782Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"INFO","ts":"2025-12-06T02:43:24.782Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dczzts, usage=0.000 (0.0%)"}
{"level":"DEBUG","ts":"2025-12-06T02:43:24.782Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"DEBUG","ts":"2025-12-06T02:43:24.784Z","msg":"Pod-to-variant matching successful: totalPods=1, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"DEBUG","ts":"2025-12-06T02:43:24.784Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=1"}
{"level":"DEBUG","ts":"2025-12-06T02:43:24.784Z","msg":"Collected capacity metrics for observability in model-only mode: modelID=unsloth/Meta-Llama-3.1-8B, replicaCount=1"}
{"level":"INFO","ts":"2025-12-06T02:43:24.784Z","msg":"Found SLO for model: model=unsloth/Meta-Llama-3.1-8B, class=Premium, slo-tpot=10, slo-ttft=1000"}
{"level":"DEBUG","ts":"2025-12-06T02:43:24.793Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=1, accelerator=H100, ttft=0.00ms, itl=0.00ms, cost=100.00, maxBatch=256, arrivalRate=0.00, avgInputTokens=0.00, avgOutputTokens=0.00"}
{"level":"DEBUG","ts":"2025-12-06T02:43:24.793Z","msg":"Experimental model tuner is enabled globally (EXPERIMENTAL_MODEL_TUNER_ENABLED=true) tuning model performance parameters for active VAs"}
{"level":"DEBUG","ts":"2025-12-06T02:43:24.793Z","msg":"Using state vals from VA status to tune variant ms-inference-scheduling-llm-d-modelservice-decode: alpha= 7.350417, beta= 0.045946, gamma= 17.152187, delta= 0.000321"}
{"level":"DEBUG","ts":"2025-12-06T02:43:24.793Z","msg":"[Tuner Config] variant=ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler | Initial state: alpha=7.350417, beta=0.045946, gamma=17.152187, delta=0.000321 | SLO targets: TTFT=1000.00ms, ITL=10.00ms | Has covariance=true"}
{"level":"WARN","ts":"2025-12-06T02:43:24.793Z","msg":"Tuner failed completely for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler: failed to get/create tuner: failed to create tuner: invalid environment: &{0 0 0 256 0 0 1}. Using fallback parameters."}
{"level":"INFO","ts":"2025-12-06T02:43:24.793Z","msg":"Using tuned parameters from status for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler"}
{"level":"DEBUG","ts":"2025-12-06T02:43:24.793Z","msg":"Updated SystemData for model=unsloth/Meta-Llama-3.1-8B, accelerator=H100: alpha=7.350417, beta=0.045946, gamma=17.152187, delta=0.000321"}
{"level":"DEBUG","ts":"2025-12-06T02:43:24.793Z","msg":"Optimization solution - system: Solution: \ns=ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler; c=Premium; m=unsloth/Meta-Llama-3.1-8B; rate=0; inTk=0; outTk=0; sol=1, sat=false, alloc={acc=H100; numRep=1; maxBatch=512; cost=100, val=0, itl=7.3963633, ttft=17.152508, rho=0, maxRPM=639636.56}; slo-itl=10, slo-ttft=1000, slo-tps=0 \nAllocationByType: \nname=NVIDIA-H100-80GB-HBM3, count=1, limit=0, cost=100 \ntotalCost=100 \n"}
{"level":"DEBUG","ts":"2025-12-06T02:43:24.793Z","msg":"Setting accelerator name: Name=H100, allocationData={H100 1 512 100 7.3963633 17.152508 {0 0 0}}"}
{"level":"INFO","ts":"2025-12-06T02:43:24.793Z","msg":"Model-based optimization completed for model: unsloth/Meta-Llama-3.1-8B - model-based targets: map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"WARN","ts":"2025-12-06T02:43:24.793Z","msg":"saturation analysis unavailable, using model-based targets only: modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-06T02:43:24.793Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-12-06T02:43:24.793Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1→target=1"}
{"level":"DEBUG","ts":"2025-12-06T02:43:24.793Z","msg":"Found VA in map: variant=ms-inference-scheduling-llm-d-modelservice-decode, hasCurrentAlloc=true, accelerator=H100"}
{"level":"INFO","ts":"2025-12-06T02:43:24.793Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 1, desired-replicas: 1, accelerator: H100"}
{"level":"INFO","ts":"2025-12-06T02:43:24.793Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=1, accelerator=H100, SaturationOnly=false"}
{"level":"INFO","ts":"2025-12-06T02:43:24.798Z","msg":"Applied Saturation decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1, target=1, reason=model-based only (Saturation unavailable)"}
{"level":"INFO","ts":"2025-12-06T02:43:24.798Z","msg":"Reconciliation completed successfully: mode=model-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-12-06T02:43:24.806Z","msg":"saturation scaling config cache updated: entries=1, has_default=true"}
{"level":"INFO","ts":"2025-12-06T02:43:24.806Z","msg":"saturation scaling configuration loaded successfully"}
{"level":"INFO","ts":"2025-12-06T02:43:24.806Z","msg":"Operating in MODEL-ONLY mode: model-based optimization only"}
{"level":"INFO","ts":"2025-12-06T02:43:24.806Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-12-06T02:43:24.806Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-12-06T02:43:24.807Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dczzts, usage=0.000 (0.0%)"}
{"level":"DEBUG","ts":"2025-12-06T02:43:24.807Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"INFO","ts":"2025-12-06T02:43:24.807Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dczzts, queueLength=0"}
{"level":"DEBUG","ts":"2025-12-06T02:43:24.807Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"DEBUG","ts":"2025-12-06T02:43:24.809Z","msg":"Pod-to-variant matching successful: totalPods=1, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"DEBUG","ts":"2025-12-06T02:43:24.809Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=1"}
{"level":"DEBUG","ts":"2025-12-06T02:43:24.809Z","msg":"Collected capacity metrics for observability in model-only mode: modelID=unsloth/Meta-Llama-3.1-8B, replicaCount=1"}
{"level":"INFO","ts":"2025-12-06T02:43:24.809Z","msg":"Found SLO for model: model=unsloth/Meta-Llama-3.1-8B, class=Premium, slo-tpot=10, slo-ttft=1000"}
{"level":"DEBUG","ts":"2025-12-06T02:43:24.817Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=1, accelerator=H100, ttft=0.00ms, itl=0.00ms, cost=100.00, maxBatch=256, arrivalRate=0.00, avgInputTokens=0.00, avgOutputTokens=0.00"}
{"level":"DEBUG","ts":"2025-12-06T02:43:24.817Z","msg":"Experimental model tuner is enabled globally (EXPERIMENTAL_MODEL_TUNER_ENABLED=true) tuning model performance parameters for active VAs"}
{"level":"DEBUG","ts":"2025-12-06T02:43:24.817Z","msg":"Using state vals from VA status to tune variant ms-inference-scheduling-llm-d-modelservice-decode: alpha= 7.350417, beta= 0.045946, gamma= 17.152187, delta= 0.000321"}
{"level":"DEBUG","ts":"2025-12-06T02:43:24.817Z","msg":"[Tuner Config] variant=ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler | Initial state: alpha=7.350417, beta=0.045946, gamma=17.152187, delta=0.000321 | SLO targets: TTFT=1000.00ms, ITL=10.00ms | Has covariance=true"}
{"level":"WARN","ts":"2025-12-06T02:43:24.817Z","msg":"Tuner failed completely for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler: failed to get/create tuner: failed to create tuner: invalid environment: &{0 0 0 256 0 0 1}. Using fallback parameters."}
{"level":"INFO","ts":"2025-12-06T02:43:24.817Z","msg":"Using tuned parameters from status for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler"}
{"level":"DEBUG","ts":"2025-12-06T02:43:24.817Z","msg":"Updated SystemData for model=unsloth/Meta-Llama-3.1-8B, accelerator=H100: alpha=7.350417, beta=0.045946, gamma=17.152187, delta=0.000321"}
{"level":"DEBUG","ts":"2025-12-06T02:43:24.817Z","msg":"Optimization solution - system: Solution: \ns=ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler; c=Premium; m=unsloth/Meta-Llama-3.1-8B; rate=0; inTk=0; outTk=0; sol=1, sat=false, alloc={acc=H100; numRep=1; maxBatch=512; cost=100, val=0, itl=7.3963633, ttft=17.152508, rho=0, maxRPM=639636.56}; slo-itl=10, slo-ttft=1000, slo-tps=0 \nAllocationByType: \nname=NVIDIA-H100-80GB-HBM3, count=1, limit=0, cost=100 \ntotalCost=100 \n"}
{"level":"DEBUG","ts":"2025-12-06T02:43:24.817Z","msg":"Setting accelerator name: Name=H100, allocationData={H100 1 512 100 7.3963633 17.152508 {0 0 0}}"}
{"level":"INFO","ts":"2025-12-06T02:43:24.817Z","msg":"Model-based optimization completed for model: unsloth/Meta-Llama-3.1-8B - model-based targets: map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"WARN","ts":"2025-12-06T02:43:24.817Z","msg":"saturation analysis unavailable, using model-based targets only: modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-06T02:43:24.817Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-12-06T02:43:24.817Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1→target=1"}
{"level":"DEBUG","ts":"2025-12-06T02:43:24.817Z","msg":"Found VA in map: variant=ms-inference-scheduling-llm-d-modelservice-decode, hasCurrentAlloc=true, accelerator=H100"}
{"level":"INFO","ts":"2025-12-06T02:43:24.817Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 1, desired-replicas: 1, accelerator: H100"}
{"level":"INFO","ts":"2025-12-06T02:43:24.817Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=1, accelerator=H100, SaturationOnly=false"}
{"level":"INFO","ts":"2025-12-06T02:43:24.821Z","msg":"Applied Saturation decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1, target=1, reason=model-based only (Saturation unavailable)"}
{"level":"INFO","ts":"2025-12-06T02:43:24.821Z","msg":"Reconciliation completed successfully: mode=model-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-12-06T02:44:24.799Z","msg":"saturation scaling config cache updated: entries=1, has_default=true"}
{"level":"INFO","ts":"2025-12-06T02:44:24.799Z","msg":"saturation scaling configuration loaded successfully"}
{"level":"INFO","ts":"2025-12-06T02:44:24.799Z","msg":"Operating in MODEL-ONLY mode: model-based optimization only"}
{"level":"INFO","ts":"2025-12-06T02:44:24.799Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-12-06T02:44:24.799Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-12-06T02:44:24.815Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dczzts, usage=0.000 (0.0%)"}
{"level":"DEBUG","ts":"2025-12-06T02:44:24.815Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"INFO","ts":"2025-12-06T02:44:24.815Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dczzts, queueLength=0"}
{"level":"DEBUG","ts":"2025-12-06T02:44:24.815Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"DEBUG","ts":"2025-12-06T02:44:24.831Z","msg":"Pod-to-variant matching successful: totalPods=1, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"DEBUG","ts":"2025-12-06T02:44:24.831Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=1"}
{"level":"DEBUG","ts":"2025-12-06T02:44:24.831Z","msg":"Collected capacity metrics for observability in model-only mode: modelID=unsloth/Meta-Llama-3.1-8B, replicaCount=1"}
{"level":"INFO","ts":"2025-12-06T02:44:24.831Z","msg":"Found SLO for model: model=unsloth/Meta-Llama-3.1-8B, class=Premium, slo-tpot=10, slo-ttft=1000"}
{"level":"DEBUG","ts":"2025-12-06T02:44:24.855Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=1, accelerator=H100, ttft=0.00ms, itl=0.00ms, cost=100.00, maxBatch=256, arrivalRate=0.00, avgInputTokens=0.00, avgOutputTokens=0.00"}
{"level":"DEBUG","ts":"2025-12-06T02:44:24.855Z","msg":"Experimental model tuner is enabled globally (EXPERIMENTAL_MODEL_TUNER_ENABLED=true) tuning model performance parameters for active VAs"}
{"level":"DEBUG","ts":"2025-12-06T02:44:24.855Z","msg":"Using state vals from VA status to tune variant ms-inference-scheduling-llm-d-modelservice-decode: alpha= 7.350417, beta= 0.045946, gamma= 17.152187, delta= 0.000321"}
{"level":"DEBUG","ts":"2025-12-06T02:44:24.855Z","msg":"[Tuner Config] variant=ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler | Initial state: alpha=7.350417, beta=0.045946, gamma=17.152187, delta=0.000321 | SLO targets: TTFT=1000.00ms, ITL=10.00ms | Has covariance=true"}
{"level":"WARN","ts":"2025-12-06T02:44:24.855Z","msg":"Tuner failed completely for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler: failed to get/create tuner: failed to create tuner: invalid environment: &{0 0 0 256 0 0 1}. Using fallback parameters."}
{"level":"INFO","ts":"2025-12-06T02:44:24.855Z","msg":"Using tuned parameters from status for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler"}
{"level":"DEBUG","ts":"2025-12-06T02:44:24.855Z","msg":"Updated SystemData for model=unsloth/Meta-Llama-3.1-8B, accelerator=H100: alpha=7.350417, beta=0.045946, gamma=17.152187, delta=0.000321"}
{"level":"DEBUG","ts":"2025-12-06T02:44:24.855Z","msg":"Optimization solution - system: Solution: \ns=ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler; c=Premium; m=unsloth/Meta-Llama-3.1-8B; rate=0; inTk=0; outTk=0; sol=1, sat=false, alloc={acc=H100; numRep=1; maxBatch=512; cost=100, val=0, itl=7.3963633, ttft=17.152508, rho=0, maxRPM=639636.56}; slo-itl=10, slo-ttft=1000, slo-tps=0 \nAllocationByType: \nname=NVIDIA-H100-80GB-HBM3, count=1, limit=0, cost=100 \ntotalCost=100 \n"}
{"level":"DEBUG","ts":"2025-12-06T02:44:24.855Z","msg":"Setting accelerator name: Name=H100, allocationData={H100 1 512 100 7.3963633 17.152508 {0 0 0}}"}
{"level":"INFO","ts":"2025-12-06T02:44:24.855Z","msg":"Model-based optimization completed for model: unsloth/Meta-Llama-3.1-8B - model-based targets: map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"WARN","ts":"2025-12-06T02:44:24.855Z","msg":"saturation analysis unavailable, using model-based targets only: modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-06T02:44:24.855Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-12-06T02:44:24.855Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1→target=1"}
{"level":"DEBUG","ts":"2025-12-06T02:44:24.855Z","msg":"Found VA in map: variant=ms-inference-scheduling-llm-d-modelservice-decode, hasCurrentAlloc=true, accelerator=H100"}
{"level":"INFO","ts":"2025-12-06T02:44:24.855Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 1, desired-replicas: 1, accelerator: H100"}
{"level":"INFO","ts":"2025-12-06T02:44:24.855Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=1, accelerator=H100, SaturationOnly=false"}
{"level":"INFO","ts":"2025-12-06T02:44:24.862Z","msg":"Applied Saturation decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1, target=1, reason=model-based only (Saturation unavailable)"}
{"level":"INFO","ts":"2025-12-06T02:44:24.862Z","msg":"Reconciliation completed successfully: mode=model-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-12-06T02:44:24.862Z","msg":"saturation scaling config cache updated: entries=1, has_default=true"}
{"level":"INFO","ts":"2025-12-06T02:44:24.862Z","msg":"saturation scaling configuration loaded successfully"}
{"level":"INFO","ts":"2025-12-06T02:44:24.862Z","msg":"Operating in MODEL-ONLY mode: model-based optimization only"}
{"level":"INFO","ts":"2025-12-06T02:44:24.862Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-12-06T02:44:24.862Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-12-06T02:44:24.863Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dczzts, queueLength=0"}
{"level":"DEBUG","ts":"2025-12-06T02:44:24.863Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"INFO","ts":"2025-12-06T02:44:24.863Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dczzts, usage=0.000 (0.0%)"}
{"level":"DEBUG","ts":"2025-12-06T02:44:24.863Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"DEBUG","ts":"2025-12-06T02:44:24.865Z","msg":"Pod-to-variant matching successful: totalPods=1, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"DEBUG","ts":"2025-12-06T02:44:24.865Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=1"}
{"level":"DEBUG","ts":"2025-12-06T02:44:24.865Z","msg":"Collected capacity metrics for observability in model-only mode: modelID=unsloth/Meta-Llama-3.1-8B, replicaCount=1"}
{"level":"INFO","ts":"2025-12-06T02:44:24.865Z","msg":"Found SLO for model: model=unsloth/Meta-Llama-3.1-8B, class=Premium, slo-tpot=10, slo-ttft=1000"}
{"level":"DEBUG","ts":"2025-12-06T02:44:24.872Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=1, accelerator=H100, ttft=0.00ms, itl=0.00ms, cost=100.00, maxBatch=256, arrivalRate=0.00, avgInputTokens=0.00, avgOutputTokens=0.00"}
{"level":"DEBUG","ts":"2025-12-06T02:44:24.872Z","msg":"Experimental model tuner is enabled globally (EXPERIMENTAL_MODEL_TUNER_ENABLED=true) tuning model performance parameters for active VAs"}
{"level":"DEBUG","ts":"2025-12-06T02:44:24.873Z","msg":"Using state vals from VA status to tune variant ms-inference-scheduling-llm-d-modelservice-decode: alpha= 7.350417, beta= 0.045946, gamma= 17.152187, delta= 0.000321"}
{"level":"DEBUG","ts":"2025-12-06T02:44:24.873Z","msg":"[Tuner Config] variant=ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler | Initial state: alpha=7.350417, beta=0.045946, gamma=17.152187, delta=0.000321 | SLO targets: TTFT=1000.00ms, ITL=10.00ms | Has covariance=true"}
{"level":"WARN","ts":"2025-12-06T02:44:24.873Z","msg":"Tuner failed completely for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler: failed to get/create tuner: failed to create tuner: invalid environment: &{0 0 0 256 0 0 1}. Using fallback parameters."}
{"level":"INFO","ts":"2025-12-06T02:44:24.873Z","msg":"Using tuned parameters from status for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler"}
{"level":"DEBUG","ts":"2025-12-06T02:44:24.873Z","msg":"Updated SystemData for model=unsloth/Meta-Llama-3.1-8B, accelerator=H100: alpha=7.350417, beta=0.045946, gamma=17.152187, delta=0.000321"}
{"level":"DEBUG","ts":"2025-12-06T02:44:24.873Z","msg":"Optimization solution - system: Solution: \ns=ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler; c=Premium; m=unsloth/Meta-Llama-3.1-8B; rate=0; inTk=0; outTk=0; sol=1, sat=false, alloc={acc=H100; numRep=1; maxBatch=512; cost=100, val=0, itl=7.3963633, ttft=17.152508, rho=0, maxRPM=639636.56}; slo-itl=10, slo-ttft=1000, slo-tps=0 \nAllocationByType: \nname=NVIDIA-H100-80GB-HBM3, count=1, limit=0, cost=100 \ntotalCost=100 \n"}
{"level":"DEBUG","ts":"2025-12-06T02:44:24.873Z","msg":"Setting accelerator name: Name=H100, allocationData={H100 1 512 100 7.3963633 17.152508 {0 0 0}}"}
{"level":"INFO","ts":"2025-12-06T02:44:24.873Z","msg":"Model-based optimization completed for model: unsloth/Meta-Llama-3.1-8B - model-based targets: map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"WARN","ts":"2025-12-06T02:44:24.873Z","msg":"saturation analysis unavailable, using model-based targets only: modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-06T02:44:24.873Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-12-06T02:44:24.873Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1→target=1"}
{"level":"DEBUG","ts":"2025-12-06T02:44:24.873Z","msg":"Found VA in map: variant=ms-inference-scheduling-llm-d-modelservice-decode, hasCurrentAlloc=true, accelerator=H100"}
{"level":"INFO","ts":"2025-12-06T02:44:24.873Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 1, desired-replicas: 1, accelerator: H100"}
{"level":"INFO","ts":"2025-12-06T02:44:24.873Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=1, accelerator=H100, SaturationOnly=false"}
{"level":"INFO","ts":"2025-12-06T02:44:24.877Z","msg":"Applied Saturation decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1, target=1, reason=model-based only (Saturation unavailable)"}
{"level":"INFO","ts":"2025-12-06T02:44:24.877Z","msg":"Reconciliation completed successfully: mode=model-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-12-06T02:45:24.862Z","msg":"saturation scaling config cache updated: entries=1, has_default=true"}
{"level":"INFO","ts":"2025-12-06T02:45:24.862Z","msg":"saturation scaling configuration loaded successfully"}
{"level":"INFO","ts":"2025-12-06T02:45:24.862Z","msg":"Operating in MODEL-ONLY mode: model-based optimization only"}
{"level":"INFO","ts":"2025-12-06T02:45:24.862Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-12-06T02:45:24.862Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-12-06T02:45:24.867Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dczzts, queueLength=0"}
{"level":"DEBUG","ts":"2025-12-06T02:45:24.867Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"INFO","ts":"2025-12-06T02:45:24.867Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dczzts, usage=0.063 (6.3%)"}
{"level":"DEBUG","ts":"2025-12-06T02:45:24.867Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"DEBUG","ts":"2025-12-06T02:45:24.869Z","msg":"Pod-to-variant matching successful: totalPods=1, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"DEBUG","ts":"2025-12-06T02:45:24.869Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=1"}
{"level":"DEBUG","ts":"2025-12-06T02:45:24.869Z","msg":"Collected capacity metrics for observability in model-only mode: modelID=unsloth/Meta-Llama-3.1-8B, replicaCount=1"}
{"level":"INFO","ts":"2025-12-06T02:45:24.869Z","msg":"Found SLO for model: model=unsloth/Meta-Llama-3.1-8B, class=Premium, slo-tpot=10, slo-ttft=1000"}
{"level":"DEBUG","ts":"2025-12-06T02:45:24.878Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=1, accelerator=H100, ttft=18.92ms, itl=9.15ms, cost=100.00, maxBatch=256, arrivalRate=545.87, avgInputTokens=261.80, avgOutputTokens=422.68"}
{"level":"DEBUG","ts":"2025-12-06T02:45:24.878Z","msg":"Experimental model tuner is enabled globally (EXPERIMENTAL_MODEL_TUNER_ENABLED=true) tuning model performance parameters for active VAs"}
{"level":"DEBUG","ts":"2025-12-06T02:45:24.878Z","msg":"Using state vals from VA status to tune variant ms-inference-scheduling-llm-d-modelservice-decode: alpha= 7.350417, beta= 0.045946, gamma= 17.152187, delta= 0.000321"}
{"level":"DEBUG","ts":"2025-12-06T02:45:24.878Z","msg":"[Tuner Config] variant=ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler | Initial state: alpha=7.350417, beta=0.045946, gamma=17.152187, delta=0.000321 | SLO targets: TTFT=1000.00ms, ITL=10.00ms | Has covariance=true"}
{"level":"INFO","ts":"2025-12-06T02:45:24.878Z","msg":"Tuner validation succeeded for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler, server ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler - New NIS: 0.083842"}
{"level":"DEBUG","ts":"2025-12-06T02:45:24.878Z","msg":"Model tuner results: model=unsloth/Meta-Llama-3.1-8B, accelerator=H100, alpha=7.468441, beta=0.045672, gamma=17.123430, delta=0.000321, NIS=0.08"}
{"level":"DEBUG","ts":"2025-12-06T02:45:24.878Z","msg":"Updated tuner status for variant ms-inference-scheduling-llm-d-modelservice-decode: alpha=7.468441, beta=0.045672, gamma=17.123430, delta=0.000321, NIS=0.083842"}
{"level":"INFO","ts":"2025-12-06T02:45:24.878Z","msg":"Tuned performance parameters result: variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler - alpha: 7.468441, beta: 0.045672, gamma: 17.123430, delta: 0.000321"}
{"level":"DEBUG","ts":"2025-12-06T02:45:24.886Z","msg":"Optimization solution - system: Solution: \ns=ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler; c=Premium; m=unsloth/Meta-Llama-3.1-8B; rate=545.87; inTk=261; outTk=422; sol=1, sat=false, alloc={acc=H100; numRep=1; maxBatch=512; cost=100, val=0, itl=9.117436, ttft=20.148289, rho=0.068564065, maxRPM=771.7117}; slo-itl=10, slo-ttft=1000, slo-tps=0 \nAllocationByType: \nname=NVIDIA-H100-80GB-HBM3, count=1, limit=0, cost=100 \ntotalCost=100 \n"}
{"level":"DEBUG","ts":"2025-12-06T02:45:24.886Z","msg":"Setting accelerator name: Name=H100, allocationData={H100 1 512 100 9.117436 20.148289 {545.87 261 422}}"}
{"level":"INFO","ts":"2025-12-06T02:45:24.886Z","msg":"Model-based optimization completed for model: unsloth/Meta-Llama-3.1-8B - model-based targets: map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"WARN","ts":"2025-12-06T02:45:24.886Z","msg":"saturation analysis unavailable, using model-based targets only: modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-06T02:45:24.886Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-12-06T02:45:24.886Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1→target=1"}
{"level":"DEBUG","ts":"2025-12-06T02:45:24.886Z","msg":"Found VA in map: variant=ms-inference-scheduling-llm-d-modelservice-decode, hasCurrentAlloc=true, accelerator=H100"}
{"level":"INFO","ts":"2025-12-06T02:45:24.886Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 1, desired-replicas: 1, accelerator: H100"}
{"level":"INFO","ts":"2025-12-06T02:45:24.886Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=1, accelerator=H100, SaturationOnly=false"}
{"level":"INFO","ts":"2025-12-06T02:45:24.893Z","msg":"Applied Saturation decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1, target=1, reason=model-based only (Saturation unavailable)"}
{"level":"INFO","ts":"2025-12-06T02:45:24.893Z","msg":"Reconciliation completed successfully: mode=model-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-12-06T02:45:24.894Z","msg":"saturation scaling config cache updated: entries=1, has_default=true"}
{"level":"INFO","ts":"2025-12-06T02:45:24.894Z","msg":"saturation scaling configuration loaded successfully"}
{"level":"INFO","ts":"2025-12-06T02:45:24.894Z","msg":"Operating in MODEL-ONLY mode: model-based optimization only"}
{"level":"INFO","ts":"2025-12-06T02:45:24.894Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-12-06T02:45:24.894Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-12-06T02:45:24.895Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dczzts, usage=0.063 (6.3%)"}
{"level":"DEBUG","ts":"2025-12-06T02:45:24.895Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"INFO","ts":"2025-12-06T02:45:24.895Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dczzts, queueLength=0"}
{"level":"DEBUG","ts":"2025-12-06T02:45:24.895Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"DEBUG","ts":"2025-12-06T02:45:24.897Z","msg":"Pod-to-variant matching successful: totalPods=1, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"DEBUG","ts":"2025-12-06T02:45:24.897Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=1"}
{"level":"DEBUG","ts":"2025-12-06T02:45:24.897Z","msg":"Collected capacity metrics for observability in model-only mode: modelID=unsloth/Meta-Llama-3.1-8B, replicaCount=1"}
{"level":"INFO","ts":"2025-12-06T02:45:24.897Z","msg":"Found SLO for model: model=unsloth/Meta-Llama-3.1-8B, class=Premium, slo-tpot=10, slo-ttft=1000"}
{"level":"DEBUG","ts":"2025-12-06T02:45:24.908Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=1, accelerator=H100, ttft=18.92ms, itl=9.15ms, cost=100.00, maxBatch=256, arrivalRate=545.87, avgInputTokens=261.80, avgOutputTokens=422.68"}
{"level":"DEBUG","ts":"2025-12-06T02:45:24.908Z","msg":"Experimental model tuner is enabled globally (EXPERIMENTAL_MODEL_TUNER_ENABLED=true) tuning model performance parameters for active VAs"}
{"level":"DEBUG","ts":"2025-12-06T02:45:24.908Z","msg":"Using state vals from VA status to tune variant ms-inference-scheduling-llm-d-modelservice-decode: alpha= 7.468441, beta= 0.045672, gamma= 17.123430, delta= 0.000321"}
{"level":"DEBUG","ts":"2025-12-06T02:45:24.908Z","msg":"[Tuner Config] variant=ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler | Initial state: alpha=7.468441, beta=0.045672, gamma=17.123430, delta=0.000321 | SLO targets: TTFT=1000.00ms, ITL=10.00ms | Has covariance=true"}
{"level":"INFO","ts":"2025-12-06T02:45:24.908Z","msg":"Tuner validation succeeded for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler, server ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler - New NIS: 0.005500"}
{"level":"DEBUG","ts":"2025-12-06T02:45:24.908Z","msg":"Model tuner results: model=unsloth/Meta-Llama-3.1-8B, accelerator=H100, alpha=7.489491, beta=0.045687, gamma=17.094530, delta=0.000321, NIS=0.01"}
{"level":"DEBUG","ts":"2025-12-06T02:45:24.908Z","msg":"Updated tuner status for variant ms-inference-scheduling-llm-d-modelservice-decode: alpha=7.489491, beta=0.045687, gamma=17.094530, delta=0.000321, NIS=0.005500"}
{"level":"INFO","ts":"2025-12-06T02:45:24.908Z","msg":"Tuned performance parameters result: variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler - alpha: 7.489491, beta: 0.045687, gamma: 17.094530, delta: 0.000321"}
{"level":"DEBUG","ts":"2025-12-06T02:45:24.918Z","msg":"Optimization solution - system: Solution: \ns=ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler; c=Premium; m=unsloth/Meta-Llama-3.1-8B; rate=545.87; inTk=261; outTk=422; sol=1, sat=false, alloc={acc=H100; numRep=1; maxBatch=512; cost=100, val=0, itl=9.143571, ttft=20.127756, rho=0.0687592, maxRPM=764.94745}; slo-itl=10, slo-ttft=1000, slo-tps=0 \nAllocationByType: \nname=NVIDIA-H100-80GB-HBM3, count=1, limit=0, cost=100 \ntotalCost=100 \n"}
{"level":"DEBUG","ts":"2025-12-06T02:45:24.918Z","msg":"Setting accelerator name: Name=H100, allocationData={H100 1 512 100 9.143571 20.127756 {545.87 261 422}}"}
{"level":"INFO","ts":"2025-12-06T02:45:24.918Z","msg":"Model-based optimization completed for model: unsloth/Meta-Llama-3.1-8B - model-based targets: map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"WARN","ts":"2025-12-06T02:45:24.918Z","msg":"saturation analysis unavailable, using model-based targets only: modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-06T02:45:24.918Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-12-06T02:45:24.918Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1→target=1"}
{"level":"DEBUG","ts":"2025-12-06T02:45:24.918Z","msg":"Found VA in map: variant=ms-inference-scheduling-llm-d-modelservice-decode, hasCurrentAlloc=true, accelerator=H100"}
{"level":"INFO","ts":"2025-12-06T02:45:24.918Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 1, desired-replicas: 1, accelerator: H100"}
{"level":"INFO","ts":"2025-12-06T02:45:24.918Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=1, accelerator=H100, SaturationOnly=false"}
{"level":"INFO","ts":"2025-12-06T02:45:24.924Z","msg":"Applied Saturation decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1, target=1, reason=model-based only (Saturation unavailable)"}
{"level":"INFO","ts":"2025-12-06T02:45:24.924Z","msg":"Reconciliation completed successfully: mode=model-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-12-06T02:46:24.895Z","msg":"saturation scaling config cache updated: entries=1, has_default=true"}
{"level":"INFO","ts":"2025-12-06T02:46:24.895Z","msg":"saturation scaling configuration loaded successfully"}
{"level":"INFO","ts":"2025-12-06T02:46:24.895Z","msg":"Operating in MODEL-ONLY mode: model-based optimization only"}
{"level":"INFO","ts":"2025-12-06T02:46:24.895Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-12-06T02:46:24.895Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-12-06T02:46:24.903Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dczzts, usage=0.056 (5.6%)"}
{"level":"DEBUG","ts":"2025-12-06T02:46:24.903Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"INFO","ts":"2025-12-06T02:46:24.903Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dczzts, queueLength=0"}
{"level":"DEBUG","ts":"2025-12-06T02:46:24.904Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"DEBUG","ts":"2025-12-06T02:46:24.905Z","msg":"Pod-to-variant matching successful: totalPods=1, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"DEBUG","ts":"2025-12-06T02:46:24.905Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=1"}
{"level":"DEBUG","ts":"2025-12-06T02:46:24.905Z","msg":"Collected capacity metrics for observability in model-only mode: modelID=unsloth/Meta-Llama-3.1-8B, replicaCount=1"}
{"level":"INFO","ts":"2025-12-06T02:46:24.906Z","msg":"Found SLO for model: model=unsloth/Meta-Llama-3.1-8B, class=Premium, slo-tpot=10, slo-ttft=1000"}
{"level":"DEBUG","ts":"2025-12-06T02:46:24.914Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=1, accelerator=H100, ttft=18.71ms, itl=8.98ms, cost=100.00, maxBatch=256, arrivalRate=589.89, avgInputTokens=241.67, avgOutputTokens=457.39"}
{"level":"DEBUG","ts":"2025-12-06T02:46:24.914Z","msg":"Experimental model tuner is enabled globally (EXPERIMENTAL_MODEL_TUNER_ENABLED=true) tuning model performance parameters for active VAs"}
{"level":"DEBUG","ts":"2025-12-06T02:46:24.914Z","msg":"Using state vals from VA status to tune variant ms-inference-scheduling-llm-d-modelservice-decode: alpha= 7.489491, beta= 0.045687, gamma= 17.094530, delta= 0.000321"}
{"level":"DEBUG","ts":"2025-12-06T02:46:24.914Z","msg":"[Tuner Config] variant=ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler | Initial state: alpha=7.489491, beta=0.045687, gamma=17.094530, delta=0.000321 | SLO targets: TTFT=1000.00ms, ITL=10.00ms | Has covariance=true"}
{"level":"INFO","ts":"2025-12-06T02:46:24.915Z","msg":"Tuner validation succeeded for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler, server ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler - New NIS: 0.706947"}
{"level":"DEBUG","ts":"2025-12-06T02:46:24.915Z","msg":"Model tuner results: model=unsloth/Meta-Llama-3.1-8B, accelerator=H100, alpha=7.222897, beta=0.044192, gamma=17.055386, delta=0.000321, NIS=0.71"}
{"level":"DEBUG","ts":"2025-12-06T02:46:24.915Z","msg":"Updated tuner status for variant ms-inference-scheduling-llm-d-modelservice-decode: alpha=7.222897, beta=0.044192, gamma=17.055386, delta=0.000321, NIS=0.706947"}
{"level":"INFO","ts":"2025-12-06T02:46:24.915Z","msg":"Tuned performance parameters result: variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler - alpha: 7.222897, beta: 0.044192, gamma: 17.055386, delta: 0.000321"}
{"level":"DEBUG","ts":"2025-12-06T02:46:24.919Z","msg":"Optimization solution - system: Solution: \ns=ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler; c=Premium; m=unsloth/Meta-Llama-3.1-8B; rate=589.89; inTk=241; outTk=457; sol=1, sat=false, alloc={acc=H100; numRep=1; maxBatch=512; cost=100, val=0, itl=9.073566, ttft=20.295008, rho=0.07983949, maxRPM=809.8122}; slo-itl=10, slo-ttft=1000, slo-tps=0 \nAllocationByType: \nname=NVIDIA-H100-80GB-HBM3, count=1, limit=0, cost=100 \ntotalCost=100 \n"}
{"level":"DEBUG","ts":"2025-12-06T02:46:24.919Z","msg":"Setting accelerator name: Name=H100, allocationData={H100 1 512 100 9.073566 20.295008 {589.89 241 457}}"}
{"level":"INFO","ts":"2025-12-06T02:46:24.919Z","msg":"Model-based optimization completed for model: unsloth/Meta-Llama-3.1-8B - model-based targets: map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"WARN","ts":"2025-12-06T02:46:24.919Z","msg":"saturation analysis unavailable, using model-based targets only: modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-06T02:46:24.919Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-12-06T02:46:24.919Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1→target=1"}
{"level":"DEBUG","ts":"2025-12-06T02:46:24.919Z","msg":"Found VA in map: variant=ms-inference-scheduling-llm-d-modelservice-decode, hasCurrentAlloc=true, accelerator=H100"}
{"level":"INFO","ts":"2025-12-06T02:46:24.919Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 1, desired-replicas: 1, accelerator: H100"}
{"level":"INFO","ts":"2025-12-06T02:46:24.919Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=1, accelerator=H100, SaturationOnly=false"}
{"level":"INFO","ts":"2025-12-06T02:46:24.924Z","msg":"Applied Saturation decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1, target=1, reason=model-based only (Saturation unavailable)"}
{"level":"INFO","ts":"2025-12-06T02:46:24.924Z","msg":"Reconciliation completed successfully: mode=model-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-12-06T02:46:24.925Z","msg":"saturation scaling config cache updated: entries=1, has_default=true"}
{"level":"INFO","ts":"2025-12-06T02:46:24.925Z","msg":"saturation scaling configuration loaded successfully"}
{"level":"INFO","ts":"2025-12-06T02:46:24.925Z","msg":"Operating in MODEL-ONLY mode: model-based optimization only"}
{"level":"INFO","ts":"2025-12-06T02:46:24.925Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-12-06T02:46:24.925Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-12-06T02:46:24.926Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dczzts, queueLength=0"}
{"level":"DEBUG","ts":"2025-12-06T02:46:24.926Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"INFO","ts":"2025-12-06T02:46:24.926Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dczzts, usage=0.056 (5.6%)"}
{"level":"DEBUG","ts":"2025-12-06T02:46:24.926Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"DEBUG","ts":"2025-12-06T02:46:24.927Z","msg":"Pod-to-variant matching successful: totalPods=1, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"DEBUG","ts":"2025-12-06T02:46:24.927Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=1"}
{"level":"DEBUG","ts":"2025-12-06T02:46:24.927Z","msg":"Collected capacity metrics for observability in model-only mode: modelID=unsloth/Meta-Llama-3.1-8B, replicaCount=1"}
{"level":"INFO","ts":"2025-12-06T02:46:24.928Z","msg":"Found SLO for model: model=unsloth/Meta-Llama-3.1-8B, class=Premium, slo-tpot=10, slo-ttft=1000"}
{"level":"DEBUG","ts":"2025-12-06T02:46:24.935Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=1, accelerator=H100, ttft=18.71ms, itl=8.98ms, cost=100.00, maxBatch=256, arrivalRate=589.89, avgInputTokens=241.67, avgOutputTokens=457.39"}
{"level":"DEBUG","ts":"2025-12-06T02:46:24.935Z","msg":"Experimental model tuner is enabled globally (EXPERIMENTAL_MODEL_TUNER_ENABLED=true) tuning model performance parameters for active VAs"}
{"level":"DEBUG","ts":"2025-12-06T02:46:24.935Z","msg":"Using state vals from VA status to tune variant ms-inference-scheduling-llm-d-modelservice-decode: alpha= 7.222897, beta= 0.044192, gamma= 17.055386, delta= 0.000321"}
{"level":"DEBUG","ts":"2025-12-06T02:46:24.935Z","msg":"[Tuner Config] variant=ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler | Initial state: alpha=7.222897, beta=0.044192, gamma=17.055386, delta=0.000321 | SLO targets: TTFT=1000.00ms, ITL=10.00ms | Has covariance=true"}
{"level":"INFO","ts":"2025-12-06T02:46:24.936Z","msg":"Tuner validation succeeded for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler, server ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler - New NIS: 0.029901"}
{"level":"DEBUG","ts":"2025-12-06T02:46:24.936Z","msg":"Model tuner results: model=unsloth/Meta-Llama-3.1-8B, accelerator=H100, alpha=7.165479, beta=0.044121, gamma=17.017385, delta=0.000321, NIS=0.03"}
{"level":"DEBUG","ts":"2025-12-06T02:46:24.936Z","msg":"Updated tuner status for variant ms-inference-scheduling-llm-d-modelservice-decode: alpha=7.165479, beta=0.044121, gamma=17.017385, delta=0.000321, NIS=0.029901"}
{"level":"INFO","ts":"2025-12-06T02:46:24.936Z","msg":"Tuned performance parameters result: variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler - alpha: 7.165479, beta: 0.044121, gamma: 17.017385, delta: 0.000321"}
{"level":"DEBUG","ts":"2025-12-06T02:46:24.939Z","msg":"Optimization solution - system: Solution: \ns=ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler; c=Premium; m=unsloth/Meta-Llama-3.1-8B; rate=589.89; inTk=241; outTk=457; sol=1, sat=false, alloc={acc=H100; numRep=1; maxBatch=512; cost=100, val=0, itl=8.998251, ttft=20.230862, rho=0.07917877, maxRPM=828.1651}; slo-itl=10, slo-ttft=1000, slo-tps=0 \nAllocationByType: \nname=NVIDIA-H100-80GB-HBM3, count=1, limit=0, cost=100 \ntotalCost=100 \n"}
{"level":"DEBUG","ts":"2025-12-06T02:46:24.939Z","msg":"Setting accelerator name: Name=H100, allocationData={H100 1 512 100 8.998251 20.230862 {589.89 241 457}}"}
{"level":"INFO","ts":"2025-12-06T02:46:24.939Z","msg":"Model-based optimization completed for model: unsloth/Meta-Llama-3.1-8B - model-based targets: map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"WARN","ts":"2025-12-06T02:46:24.939Z","msg":"saturation analysis unavailable, using model-based targets only: modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-06T02:46:24.939Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-12-06T02:46:24.939Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1→target=1"}
{"level":"DEBUG","ts":"2025-12-06T02:46:24.939Z","msg":"Found VA in map: variant=ms-inference-scheduling-llm-d-modelservice-decode, hasCurrentAlloc=true, accelerator=H100"}
{"level":"INFO","ts":"2025-12-06T02:46:24.939Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 1, desired-replicas: 1, accelerator: H100"}
{"level":"INFO","ts":"2025-12-06T02:46:24.939Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=1, accelerator=H100, SaturationOnly=false"}
{"level":"INFO","ts":"2025-12-06T02:46:24.944Z","msg":"Applied Saturation decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1, target=1, reason=model-based only (Saturation unavailable)"}
{"level":"INFO","ts":"2025-12-06T02:46:24.944Z","msg":"Reconciliation completed successfully: mode=model-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-12-06T02:47:24.925Z","msg":"saturation scaling config cache updated: entries=1, has_default=true"}
{"level":"INFO","ts":"2025-12-06T02:47:24.925Z","msg":"saturation scaling configuration loaded successfully"}
{"level":"INFO","ts":"2025-12-06T02:47:24.925Z","msg":"Operating in MODEL-ONLY mode: model-based optimization only"}
{"level":"INFO","ts":"2025-12-06T02:47:24.925Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-12-06T02:47:24.925Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-12-06T02:47:24.929Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dczzts, queueLength=0"}
{"level":"DEBUG","ts":"2025-12-06T02:47:24.929Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"INFO","ts":"2025-12-06T02:47:24.929Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dczzts, usage=0.064 (6.4%)"}
{"level":"DEBUG","ts":"2025-12-06T02:47:24.929Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"DEBUG","ts":"2025-12-06T02:47:24.931Z","msg":"Pod-to-variant matching successful: totalPods=1, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"DEBUG","ts":"2025-12-06T02:47:24.931Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=1"}
{"level":"DEBUG","ts":"2025-12-06T02:47:24.931Z","msg":"Collected capacity metrics for observability in model-only mode: modelID=unsloth/Meta-Llama-3.1-8B, replicaCount=1"}
{"level":"INFO","ts":"2025-12-06T02:47:24.931Z","msg":"Found SLO for model: model=unsloth/Meta-Llama-3.1-8B, class=Premium, slo-tpot=10, slo-ttft=1000"}
{"level":"DEBUG","ts":"2025-12-06T02:47:24.940Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=1, accelerator=H100, ttft=19.11ms, itl=9.16ms, cost=100.00, maxBatch=256, arrivalRate=655.92, avgInputTokens=228.15, avgOutputTokens=448.79"}
{"level":"DEBUG","ts":"2025-12-06T02:47:24.940Z","msg":"Experimental model tuner is enabled globally (EXPERIMENTAL_MODEL_TUNER_ENABLED=true) tuning model performance parameters for active VAs"}
{"level":"DEBUG","ts":"2025-12-06T02:47:24.940Z","msg":"Using state vals from VA status to tune variant ms-inference-scheduling-llm-d-modelservice-decode: alpha= 7.165479, beta= 0.044121, gamma= 17.017385, delta= 0.000321"}
{"level":"DEBUG","ts":"2025-12-06T02:47:24.940Z","msg":"[Tuner Config] variant=ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler | Initial state: alpha=7.165479, beta=0.044121, gamma=17.017385, delta=0.000321 | SLO targets: TTFT=1000.00ms, ITL=10.00ms | Has covariance=true"}
{"level":"INFO","ts":"2025-12-06T02:47:24.941Z","msg":"Tuner validation succeeded for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler, server ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler - New NIS: 0.007923"}
{"level":"DEBUG","ts":"2025-12-06T02:47:24.941Z","msg":"Model tuner results: model=unsloth/Meta-Llama-3.1-8B, accelerator=H100, alpha=7.142333, beta=0.044017, gamma=16.985559, delta=0.000321, NIS=0.01"}
{"level":"DEBUG","ts":"2025-12-06T02:47:24.941Z","msg":"Updated tuner status for variant ms-inference-scheduling-llm-d-modelservice-decode: alpha=7.142333, beta=0.044017, gamma=16.985559, delta=0.000321, NIS=0.007923"}
{"level":"INFO","ts":"2025-12-06T02:47:24.941Z","msg":"Tuned performance parameters result: variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler - alpha: 7.142333, beta: 0.044017, gamma: 16.985559, delta: 0.000321"}
{"level":"DEBUG","ts":"2025-12-06T02:47:24.950Z","msg":"Optimization solution - system: Solution: \ns=ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler; c=Premium; m=unsloth/Meta-Llama-3.1-8B; rate=655.92; inTk=228; outTk=448; sol=1, sat=false, alloc={acc=H100; numRep=1; maxBatch=512; cost=100, val=0, itl=9.168137, ttft=20.353874, rho=0.08793665, maxRPM=853.86804}; slo-itl=10, slo-ttft=1000, slo-tps=0 \nAllocationByType: \nname=NVIDIA-H100-80GB-HBM3, count=1, limit=0, cost=100 \ntotalCost=100 \n"}
{"level":"DEBUG","ts":"2025-12-06T02:47:24.950Z","msg":"Setting accelerator name: Name=H100, allocationData={H100 1 512 100 9.168137 20.353874 {655.92 228 448}}"}
{"level":"INFO","ts":"2025-12-06T02:47:24.950Z","msg":"Model-based optimization completed for model: unsloth/Meta-Llama-3.1-8B - model-based targets: map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"WARN","ts":"2025-12-06T02:47:24.950Z","msg":"saturation analysis unavailable, using model-based targets only: modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-06T02:47:24.950Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-12-06T02:47:24.950Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1→target=1"}
{"level":"DEBUG","ts":"2025-12-06T02:47:24.950Z","msg":"Found VA in map: variant=ms-inference-scheduling-llm-d-modelservice-decode, hasCurrentAlloc=true, accelerator=H100"}
{"level":"INFO","ts":"2025-12-06T02:47:24.950Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 1, desired-replicas: 1, accelerator: H100"}
{"level":"INFO","ts":"2025-12-06T02:47:24.950Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=1, accelerator=H100, SaturationOnly=false"}
{"level":"INFO","ts":"2025-12-06T02:47:24.956Z","msg":"Applied Saturation decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1, target=1, reason=model-based only (Saturation unavailable)"}
{"level":"INFO","ts":"2025-12-06T02:47:24.956Z","msg":"Reconciliation completed successfully: mode=model-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-12-06T02:47:24.956Z","msg":"saturation scaling config cache updated: entries=1, has_default=true"}
{"level":"INFO","ts":"2025-12-06T02:47:24.956Z","msg":"saturation scaling configuration loaded successfully"}
{"level":"INFO","ts":"2025-12-06T02:47:24.956Z","msg":"Operating in MODEL-ONLY mode: model-based optimization only"}
{"level":"INFO","ts":"2025-12-06T02:47:24.956Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-12-06T02:47:24.956Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-12-06T02:47:24.960Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dczzts, queueLength=0"}
{"level":"DEBUG","ts":"2025-12-06T02:47:24.960Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"INFO","ts":"2025-12-06T02:47:24.960Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dczzts, usage=0.064 (6.4%)"}
{"level":"DEBUG","ts":"2025-12-06T02:47:24.960Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"DEBUG","ts":"2025-12-06T02:47:24.962Z","msg":"Pod-to-variant matching successful: totalPods=1, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"DEBUG","ts":"2025-12-06T02:47:24.962Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=1"}
{"level":"DEBUG","ts":"2025-12-06T02:47:24.962Z","msg":"Collected capacity metrics for observability in model-only mode: modelID=unsloth/Meta-Llama-3.1-8B, replicaCount=1"}
{"level":"INFO","ts":"2025-12-06T02:47:24.962Z","msg":"Found SLO for model: model=unsloth/Meta-Llama-3.1-8B, class=Premium, slo-tpot=10, slo-ttft=1000"}
{"level":"DEBUG","ts":"2025-12-06T02:47:24.970Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=1, accelerator=H100, ttft=19.11ms, itl=9.16ms, cost=100.00, maxBatch=256, arrivalRate=655.92, avgInputTokens=228.15, avgOutputTokens=448.79"}
{"level":"DEBUG","ts":"2025-12-06T02:47:24.970Z","msg":"Experimental model tuner is enabled globally (EXPERIMENTAL_MODEL_TUNER_ENABLED=true) tuning model performance parameters for active VAs"}
{"level":"DEBUG","ts":"2025-12-06T02:47:24.970Z","msg":"Using state vals from VA status to tune variant ms-inference-scheduling-llm-d-modelservice-decode: alpha= 7.142333, beta= 0.044017, gamma= 16.985559, delta= 0.000321"}
{"level":"DEBUG","ts":"2025-12-06T02:47:24.970Z","msg":"[Tuner Config] variant=ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler | Initial state: alpha=7.142333, beta=0.044017, gamma=16.985559, delta=0.000321 | SLO targets: TTFT=1000.00ms, ITL=10.00ms | Has covariance=true"}
{"level":"INFO","ts":"2025-12-06T02:47:24.971Z","msg":"Tuner validation succeeded for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler, server ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler - New NIS: 0.002503"}
{"level":"DEBUG","ts":"2025-12-06T02:47:24.971Z","msg":"Model tuner results: model=unsloth/Meta-Llama-3.1-8B, accelerator=H100, alpha=7.137604, beta=0.044007, gamma=16.954107, delta=0.000321, NIS=0.00"}
{"level":"DEBUG","ts":"2025-12-06T02:47:24.971Z","msg":"Updated tuner status for variant ms-inference-scheduling-llm-d-modelservice-decode: alpha=7.137604, beta=0.044007, gamma=16.954107, delta=0.000321, NIS=0.002503"}
{"level":"INFO","ts":"2025-12-06T02:47:24.971Z","msg":"Tuned performance parameters result: variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler - alpha: 7.137604, beta: 0.044007, gamma: 16.954107, delta: 0.000321"}
{"level":"DEBUG","ts":"2025-12-06T02:47:25.120Z","msg":"Optimization solution - system: Solution: \ns=ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler; c=Premium; m=unsloth/Meta-Llama-3.1-8B; rate=655.92; inTk=228; outTk=448; sol=1, sat=false, alloc={acc=H100; numRep=1; maxBatch=512; cost=100, val=0, itl=9.161517, ttft=20.320032, rho=0.08787274, maxRPM=855.4987}; slo-itl=10, slo-ttft=1000, slo-tps=0 \nAllocationByType: \nname=NVIDIA-H100-80GB-HBM3, count=1, limit=0, cost=100 \ntotalCost=100 \n"}
{"level":"DEBUG","ts":"2025-12-06T02:47:25.121Z","msg":"Setting accelerator name: Name=H100, allocationData={H100 1 512 100 9.161517 20.320032 {655.92 228 448}}"}
{"level":"INFO","ts":"2025-12-06T02:47:25.121Z","msg":"Model-based optimization completed for model: unsloth/Meta-Llama-3.1-8B - model-based targets: map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"WARN","ts":"2025-12-06T02:47:25.121Z","msg":"saturation analysis unavailable, using model-based targets only: modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-06T02:47:25.121Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-12-06T02:47:25.121Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1→target=1"}
{"level":"DEBUG","ts":"2025-12-06T02:47:25.121Z","msg":"Found VA in map: variant=ms-inference-scheduling-llm-d-modelservice-decode, hasCurrentAlloc=true, accelerator=H100"}
{"level":"INFO","ts":"2025-12-06T02:47:25.121Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 1, desired-replicas: 1, accelerator: H100"}
{"level":"INFO","ts":"2025-12-06T02:47:25.121Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=1, accelerator=H100, SaturationOnly=false"}
{"level":"INFO","ts":"2025-12-06T02:47:25.127Z","msg":"Applied Saturation decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1, target=1, reason=model-based only (Saturation unavailable)"}
{"level":"INFO","ts":"2025-12-06T02:47:25.127Z","msg":"Reconciliation completed successfully: mode=model-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-12-06T02:48:24.957Z","msg":"saturation scaling config cache updated: entries=1, has_default=true"}
{"level":"INFO","ts":"2025-12-06T02:48:24.957Z","msg":"saturation scaling configuration loaded successfully"}
{"level":"INFO","ts":"2025-12-06T02:48:24.957Z","msg":"Operating in MODEL-ONLY mode: model-based optimization only"}
{"level":"INFO","ts":"2025-12-06T02:48:24.957Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-12-06T02:48:24.957Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-12-06T02:48:24.965Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dczzts, queueLength=0"}
{"level":"DEBUG","ts":"2025-12-06T02:48:24.965Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"INFO","ts":"2025-12-06T02:48:24.975Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dczzts, usage=0.227 (22.7%)"}
{"level":"DEBUG","ts":"2025-12-06T02:48:24.975Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"DEBUG","ts":"2025-12-06T02:48:24.977Z","msg":"Pod-to-variant matching successful: totalPods=1, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"DEBUG","ts":"2025-12-06T02:48:24.977Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=1"}
{"level":"DEBUG","ts":"2025-12-06T02:48:24.977Z","msg":"Collected capacity metrics for observability in model-only mode: modelID=unsloth/Meta-Llama-3.1-8B, replicaCount=1"}
{"level":"INFO","ts":"2025-12-06T02:48:24.977Z","msg":"Found SLO for model: model=unsloth/Meta-Llama-3.1-8B, class=Premium, slo-tpot=10, slo-ttft=1000"}
{"level":"DEBUG","ts":"2025-12-06T02:48:24.987Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=1, accelerator=H100, ttft=27.56ms, itl=14.11ms, cost=100.00, maxBatch=256, arrivalRate=1059.45, avgInputTokens=265.45, avgOutputTokens=381.87"}
{"level":"DEBUG","ts":"2025-12-06T02:48:24.987Z","msg":"Experimental model tuner is enabled globally (EXPERIMENTAL_MODEL_TUNER_ENABLED=true) tuning model performance parameters for active VAs"}
{"level":"DEBUG","ts":"2025-12-06T02:48:24.987Z","msg":"Using state vals from VA status to tune variant ms-inference-scheduling-llm-d-modelservice-decode: alpha= 7.137604, beta= 0.044007, gamma= 16.954107, delta= 0.000321"}
{"level":"DEBUG","ts":"2025-12-06T02:48:24.987Z","msg":"[Tuner Config] variant=ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler | Initial state: alpha=7.137604, beta=0.044007, gamma=16.954107, delta=0.000321 | SLO targets: TTFT=1000.00ms, ITL=10.00ms | Has covariance=true"}
{"level":"WARN","ts":"2025-12-06T02:48:24.987Z","msg":"Tuner validation failed (NIS=27.73), validation error: normalized innovation squared (NIS=27.73) exceeds threshold (7.38), rejecting update as outlier - returning previous state: alpha=7.137604, beta=0.044007, gamma=16.954107, delta=0.000321"}
{"level":"WARN","ts":"2025-12-06T02:48:24.988Z","msg":"Tuner NIS validation failed for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler, server ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler (NIS=27.73 exceeds threshold 7.38) - Keeping previous state: alpha=7.137604, beta=0.044007, gamma=16.954107, delta=0.000321"}
{"level":"INFO","ts":"2025-12-06T02:48:24.988Z","msg":"Keeping previous tuned parameters for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler due to NIS validation failure (NIS=27.731012)"}
{"level":"DEBUG","ts":"2025-12-06T02:48:24.988Z","msg":"Model tuner results: model=unsloth/Meta-Llama-3.1-8B, accelerator=H100, alpha=7.137604, beta=0.044007, gamma=16.954107, delta=0.000321, NIS=27.73"}
{"level":"DEBUG","ts":"2025-12-06T02:48:24.988Z","msg":"Updated tuner status for variant ms-inference-scheduling-llm-d-modelservice-decode: alpha=7.137604, beta=0.044007, gamma=16.954107, delta=0.000321, NIS=27.731012"}
{"level":"DEBUG","ts":"2025-12-06T02:48:24.997Z","msg":"Optimization solution - system: Solution: \ns=ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler; c=Premium; m=unsloth/Meta-Llama-3.1-8B; rate=1059.45; inTk=265; outTk=381; sol=1, sat=false, alloc={acc=H100; numRep=2; maxBatch=512; cost=200, val=100, itl=8.434434, ttft=19.460863, rho=0.05560294, maxRPM=1005.27155}; slo-itl=10, slo-ttft=1000, slo-tps=0 \nAllocationByType: \nname=NVIDIA-H100-80GB-HBM3, count=2, limit=0, cost=200 \ntotalCost=200 \n"}
{"level":"DEBUG","ts":"2025-12-06T02:48:24.997Z","msg":"Setting accelerator name: Name=H100, allocationData={H100 2 512 200 8.434434 19.460863 {1059.45 265 381}}"}
{"level":"INFO","ts":"2025-12-06T02:48:24.997Z","msg":"Model-based optimization completed for model: unsloth/Meta-Llama-3.1-8B - model-based targets: map[ms-inference-scheduling-llm-d-modelservice-decode:2]"}
{"level":"WARN","ts":"2025-12-06T02:48:24.997Z","msg":"saturation analysis unavailable, using model-based targets only: modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-06T02:48:24.997Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-12-06T02:48:24.997Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=scale-up, current=1→target=2"}
{"level":"DEBUG","ts":"2025-12-06T02:48:24.997Z","msg":"Found VA in map: variant=ms-inference-scheduling-llm-d-modelservice-decode, hasCurrentAlloc=true, accelerator=H100"}
{"level":"INFO","ts":"2025-12-06T02:48:24.997Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 1, desired-replicas: 2, accelerator: H100"}
{"level":"INFO","ts":"2025-12-06T02:48:24.997Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=2, accelerator=H100, SaturationOnly=false"}
{"level":"INFO","ts":"2025-12-06T02:48:25.002Z","msg":"Applied Saturation decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=scale-up, current=1, target=2, reason=model-based only (Saturation unavailable)"}
{"level":"INFO","ts":"2025-12-06T02:48:25.002Z","msg":"Reconciliation completed successfully: mode=model-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-12-06T02:48:25.127Z","msg":"saturation scaling config cache updated: entries=1, has_default=true"}
{"level":"INFO","ts":"2025-12-06T02:48:25.127Z","msg":"saturation scaling configuration loaded successfully"}
{"level":"INFO","ts":"2025-12-06T02:48:25.127Z","msg":"Operating in MODEL-ONLY mode: model-based optimization only"}
{"level":"INFO","ts":"2025-12-06T02:48:25.127Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-12-06T02:48:25.127Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-12-06T02:48:25.129Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dczzts, queueLength=0"}
{"level":"DEBUG","ts":"2025-12-06T02:48:25.129Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"INFO","ts":"2025-12-06T02:48:25.129Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dczzts, usage=0.227 (22.7%)"}
{"level":"DEBUG","ts":"2025-12-06T02:48:25.129Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"DEBUG","ts":"2025-12-06T02:48:25.131Z","msg":"Pod-to-variant matching successful: totalPods=1, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"DEBUG","ts":"2025-12-06T02:48:25.131Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=1"}
{"level":"DEBUG","ts":"2025-12-06T02:48:25.131Z","msg":"Collected capacity metrics for observability in model-only mode: modelID=unsloth/Meta-Llama-3.1-8B, replicaCount=1"}
{"level":"INFO","ts":"2025-12-06T02:48:25.131Z","msg":"Found SLO for model: model=unsloth/Meta-Llama-3.1-8B, class=Premium, slo-tpot=10, slo-ttft=1000"}
{"level":"DEBUG","ts":"2025-12-06T02:48:25.140Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=1, accelerator=H100, ttft=27.56ms, itl=14.11ms, cost=100.00, maxBatch=256, arrivalRate=1059.45, avgInputTokens=265.45, avgOutputTokens=381.87"}
{"level":"DEBUG","ts":"2025-12-06T02:48:25.140Z","msg":"Experimental model tuner is enabled globally (EXPERIMENTAL_MODEL_TUNER_ENABLED=true) tuning model performance parameters for active VAs"}
{"level":"DEBUG","ts":"2025-12-06T02:48:25.140Z","msg":"Using state vals from VA status to tune variant ms-inference-scheduling-llm-d-modelservice-decode: alpha= 7.137604, beta= 0.044007, gamma= 16.954107, delta= 0.000321"}
{"level":"DEBUG","ts":"2025-12-06T02:48:25.140Z","msg":"[Tuner Config] variant=ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler | Initial state: alpha=7.137604, beta=0.044007, gamma=16.954107, delta=0.000321 | SLO targets: TTFT=1000.00ms, ITL=10.00ms | Has covariance=true"}
{"level":"WARN","ts":"2025-12-06T02:48:25.140Z","msg":"Tuner validation failed (NIS=27.73), validation error: normalized innovation squared (NIS=27.73) exceeds threshold (7.38), rejecting update as outlier - returning previous state: alpha=7.137604, beta=0.044007, gamma=16.954107, delta=0.000321"}
{"level":"WARN","ts":"2025-12-06T02:48:25.140Z","msg":"Tuner NIS validation failed for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler, server ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler (NIS=27.73 exceeds threshold 7.38) - Keeping previous state: alpha=7.137604, beta=0.044007, gamma=16.954107, delta=0.000321"}
{"level":"INFO","ts":"2025-12-06T02:48:25.140Z","msg":"Keeping previous tuned parameters for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler due to NIS validation failure (NIS=27.731012)"}
{"level":"DEBUG","ts":"2025-12-06T02:48:25.140Z","msg":"Model tuner results: model=unsloth/Meta-Llama-3.1-8B, accelerator=H100, alpha=7.137604, beta=0.044007, gamma=16.954107, delta=0.000321, NIS=27.73"}
{"level":"DEBUG","ts":"2025-12-06T02:48:25.140Z","msg":"Tuned parameters unchanged for variant ms-inference-scheduling-llm-d-modelservice-decode, skipping status update"}
{"level":"DEBUG","ts":"2025-12-06T02:48:25.150Z","msg":"Optimization solution - system: Solution: \ns=ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler; c=Premium; m=unsloth/Meta-Llama-3.1-8B; rate=1059.45; inTk=265; outTk=381; sol=1, sat=false, alloc={acc=H100; numRep=2; maxBatch=512; cost=200, val=100, itl=8.434434, ttft=19.460863, rho=0.05560294, maxRPM=1005.27155}; slo-itl=10, slo-ttft=1000, slo-tps=0 \nAllocationByType: \nname=NVIDIA-H100-80GB-HBM3, count=2, limit=0, cost=200 \ntotalCost=200 \n"}
{"level":"DEBUG","ts":"2025-12-06T02:48:25.150Z","msg":"Setting accelerator name: Name=H100, allocationData={H100 2 512 200 8.434434 19.460863 {1059.45 265 381}}"}
{"level":"INFO","ts":"2025-12-06T02:48:25.150Z","msg":"Model-based optimization completed for model: unsloth/Meta-Llama-3.1-8B - model-based targets: map[ms-inference-scheduling-llm-d-modelservice-decode:2]"}
{"level":"WARN","ts":"2025-12-06T02:48:25.150Z","msg":"saturation analysis unavailable, using model-based targets only: modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-06T02:48:25.150Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-12-06T02:48:25.150Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=scale-up, current=1→target=2"}
{"level":"DEBUG","ts":"2025-12-06T02:48:25.150Z","msg":"Found VA in map: variant=ms-inference-scheduling-llm-d-modelservice-decode, hasCurrentAlloc=true, accelerator=H100"}
{"level":"INFO","ts":"2025-12-06T02:48:25.150Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 1, desired-replicas: 2, accelerator: H100"}
{"level":"INFO","ts":"2025-12-06T02:48:25.150Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=2, accelerator=H100, SaturationOnly=false"}
{"level":"INFO","ts":"2025-12-06T02:48:25.155Z","msg":"Applied Saturation decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=scale-up, current=1, target=2, reason=model-based only (Saturation unavailable)"}
{"level":"INFO","ts":"2025-12-06T02:48:25.155Z","msg":"Reconciliation completed successfully: mode=model-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-12-06T02:49:25.003Z","msg":"saturation scaling config cache updated: entries=1, has_default=true"}
{"level":"INFO","ts":"2025-12-06T02:49:25.003Z","msg":"saturation scaling configuration loaded successfully"}
{"level":"INFO","ts":"2025-12-06T02:49:25.003Z","msg":"Operating in MODEL-ONLY mode: model-based optimization only"}
{"level":"INFO","ts":"2025-12-06T02:49:25.003Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-12-06T02:49:25.003Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-12-06T02:49:25.007Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dczzts, usage=0.230 (23.0%)"}
{"level":"DEBUG","ts":"2025-12-06T02:49:25.007Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"INFO","ts":"2025-12-06T02:49:25.007Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dczzts, queueLength=0"}
{"level":"DEBUG","ts":"2025-12-06T02:49:25.007Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"DEBUG","ts":"2025-12-06T02:49:25.009Z","msg":"Pod-to-variant matching successful: totalPods=1, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"DEBUG","ts":"2025-12-06T02:49:25.009Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=1"}
{"level":"DEBUG","ts":"2025-12-06T02:49:25.009Z","msg":"Collected capacity metrics for observability in model-only mode: modelID=unsloth/Meta-Llama-3.1-8B, replicaCount=1"}
{"level":"INFO","ts":"2025-12-06T02:49:25.010Z","msg":"Found SLO for model: model=unsloth/Meta-Llama-3.1-8B, class=Premium, slo-tpot=10, slo-ttft=1000"}
{"level":"DEBUG","ts":"2025-12-06T02:49:25.018Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=2, accelerator=H100, ttft=30.96ms, itl=16.12ms, cost=200.00, maxBatch=256, arrivalRate=1295.70, avgInputTokens=230.56, avgOutputTokens=472.57"}
{"level":"DEBUG","ts":"2025-12-06T02:49:25.018Z","msg":"Experimental model tuner is enabled globally (EXPERIMENTAL_MODEL_TUNER_ENABLED=true) tuning model performance parameters for active VAs"}
{"level":"DEBUG","ts":"2025-12-06T02:49:25.018Z","msg":"Using state vals from VA status to tune variant ms-inference-scheduling-llm-d-modelservice-decode: alpha= 7.137604, beta= 0.044007, gamma= 16.954107, delta= 0.000321"}
{"level":"DEBUG","ts":"2025-12-06T02:49:25.018Z","msg":"[Tuner Config] variant=ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler | Initial state: alpha=7.137604, beta=0.044007, gamma=16.954107, delta=0.000321 | SLO targets: TTFT=1000.00ms, ITL=10.00ms | Has covariance=true"}
{"level":"WARN","ts":"2025-12-06T02:49:25.019Z","msg":"Tuner validation failed (NIS=134.61), validation error: normalized innovation squared (NIS=134.61) exceeds threshold (7.38), rejecting update as outlier - returning previous state: alpha=7.137604, beta=0.044007, gamma=16.954107, delta=0.000321"}
{"level":"WARN","ts":"2025-12-06T02:49:25.019Z","msg":"Tuner NIS validation failed for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler, server ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler (NIS=134.61 exceeds threshold 7.38) - Keeping previous state: alpha=7.137604, beta=0.044007, gamma=16.954107, delta=0.000321"}
{"level":"INFO","ts":"2025-12-06T02:49:25.019Z","msg":"Keeping previous tuned parameters for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler due to NIS validation failure (NIS=134.607489)"}
{"level":"DEBUG","ts":"2025-12-06T02:49:25.019Z","msg":"Model tuner results: model=unsloth/Meta-Llama-3.1-8B, accelerator=H100, alpha=7.137604, beta=0.044007, gamma=16.954107, delta=0.000321, NIS=134.61"}
{"level":"DEBUG","ts":"2025-12-06T02:49:25.019Z","msg":"Updated tuner status for variant ms-inference-scheduling-llm-d-modelservice-decode: alpha=7.137604, beta=0.044007, gamma=16.954107, delta=0.000321, NIS=134.607489"}
{"level":"DEBUG","ts":"2025-12-06T02:49:25.022Z","msg":"Optimization solution - system: Solution: \ns=ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler; c=Premium; m=unsloth/Meta-Llama-3.1-8B; rate=1295.7; inTk=230; outTk=472; sol=1, sat=false, alloc={acc=H100; numRep=2; maxBatch=512; cost=200, val=0, itl=9.264868, ttft=20.522991, rho=0.09245941, maxRPM=812.0963}; slo-itl=10, slo-ttft=1000, slo-tps=0 \nAllocationByType: \nname=NVIDIA-H100-80GB-HBM3, count=2, limit=0, cost=200 \ntotalCost=200 \n"}
{"level":"DEBUG","ts":"2025-12-06T02:49:25.022Z","msg":"Setting accelerator name: Name=H100, allocationData={H100 2 512 200 9.264868 20.522991 {1295.7 230 472}}"}
{"level":"INFO","ts":"2025-12-06T02:49:25.022Z","msg":"Model-based optimization completed for model: unsloth/Meta-Llama-3.1-8B - model-based targets: map[ms-inference-scheduling-llm-d-modelservice-decode:2]"}
{"level":"WARN","ts":"2025-12-06T02:49:25.022Z","msg":"saturation analysis unavailable, using model-based targets only: modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-06T02:49:25.022Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-12-06T02:49:25.022Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=2→target=2"}
{"level":"DEBUG","ts":"2025-12-06T02:49:25.022Z","msg":"Found VA in map: variant=ms-inference-scheduling-llm-d-modelservice-decode, hasCurrentAlloc=true, accelerator=H100"}
{"level":"INFO","ts":"2025-12-06T02:49:25.022Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 2, desired-replicas: 2, accelerator: H100"}
{"level":"INFO","ts":"2025-12-06T02:49:25.022Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=2, accelerator=H100, SaturationOnly=false"}
{"level":"INFO","ts":"2025-12-06T02:49:25.029Z","msg":"Applied Saturation decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=2, target=2, reason=model-based only (Saturation unavailable)"}
{"level":"INFO","ts":"2025-12-06T02:49:25.029Z","msg":"Reconciliation completed successfully: mode=model-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-12-06T02:49:25.156Z","msg":"saturation scaling config cache updated: entries=1, has_default=true"}
{"level":"INFO","ts":"2025-12-06T02:49:25.156Z","msg":"saturation scaling configuration loaded successfully"}
{"level":"INFO","ts":"2025-12-06T02:49:25.156Z","msg":"Operating in MODEL-ONLY mode: model-based optimization only"}
{"level":"INFO","ts":"2025-12-06T02:49:25.156Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-12-06T02:49:25.156Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-12-06T02:49:25.158Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dczzts, usage=0.230 (23.0%)"}
{"level":"DEBUG","ts":"2025-12-06T02:49:25.158Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"INFO","ts":"2025-12-06T02:49:25.159Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dczzts, queueLength=0"}
{"level":"DEBUG","ts":"2025-12-06T02:49:25.159Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"DEBUG","ts":"2025-12-06T02:49:25.161Z","msg":"Pod-to-variant matching successful: totalPods=1, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"DEBUG","ts":"2025-12-06T02:49:25.161Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=1"}
{"level":"DEBUG","ts":"2025-12-06T02:49:25.161Z","msg":"Collected capacity metrics for observability in model-only mode: modelID=unsloth/Meta-Llama-3.1-8B, replicaCount=1"}
{"level":"INFO","ts":"2025-12-06T02:49:25.161Z","msg":"Found SLO for model: model=unsloth/Meta-Llama-3.1-8B, class=Premium, slo-tpot=10, slo-ttft=1000"}
{"level":"DEBUG","ts":"2025-12-06T02:49:25.169Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=2, accelerator=H100, ttft=30.96ms, itl=16.12ms, cost=200.00, maxBatch=256, arrivalRate=1295.70, avgInputTokens=230.56, avgOutputTokens=472.57"}
{"level":"DEBUG","ts":"2025-12-06T02:49:25.169Z","msg":"Experimental model tuner is enabled globally (EXPERIMENTAL_MODEL_TUNER_ENABLED=true) tuning model performance parameters for active VAs"}
{"level":"DEBUG","ts":"2025-12-06T02:49:25.169Z","msg":"Using state vals from VA status to tune variant ms-inference-scheduling-llm-d-modelservice-decode: alpha= 7.137604, beta= 0.044007, gamma= 16.954107, delta= 0.000321"}
{"level":"DEBUG","ts":"2025-12-06T02:49:25.169Z","msg":"[Tuner Config] variant=ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler | Initial state: alpha=7.137604, beta=0.044007, gamma=16.954107, delta=0.000321 | SLO targets: TTFT=1000.00ms, ITL=10.00ms | Has covariance=true"}
{"level":"WARN","ts":"2025-12-06T02:49:25.170Z","msg":"Tuner validation failed (NIS=134.61), validation error: normalized innovation squared (NIS=134.61) exceeds threshold (7.38), rejecting update as outlier - returning previous state: alpha=7.137604, beta=0.044007, gamma=16.954107, delta=0.000321"}
{"level":"WARN","ts":"2025-12-06T02:49:25.170Z","msg":"Tuner NIS validation failed for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler, server ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler (NIS=134.61 exceeds threshold 7.38) - Keeping previous state: alpha=7.137604, beta=0.044007, gamma=16.954107, delta=0.000321"}
{"level":"INFO","ts":"2025-12-06T02:49:25.170Z","msg":"Keeping previous tuned parameters for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler due to NIS validation failure (NIS=134.607489)"}
{"level":"DEBUG","ts":"2025-12-06T02:49:25.170Z","msg":"Model tuner results: model=unsloth/Meta-Llama-3.1-8B, accelerator=H100, alpha=7.137604, beta=0.044007, gamma=16.954107, delta=0.000321, NIS=134.61"}
{"level":"DEBUG","ts":"2025-12-06T02:49:25.170Z","msg":"Tuned parameters unchanged for variant ms-inference-scheduling-llm-d-modelservice-decode, skipping status update"}
{"level":"DEBUG","ts":"2025-12-06T02:49:25.174Z","msg":"Optimization solution - system: Solution: \ns=ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler; c=Premium; m=unsloth/Meta-Llama-3.1-8B; rate=1295.7; inTk=230; outTk=472; sol=1, sat=false, alloc={acc=H100; numRep=2; maxBatch=512; cost=200, val=0, itl=9.264868, ttft=20.522991, rho=0.09245941, maxRPM=812.0963}; slo-itl=10, slo-ttft=1000, slo-tps=0 \nAllocationByType: \nname=NVIDIA-H100-80GB-HBM3, count=2, limit=0, cost=200 \ntotalCost=200 \n"}
{"level":"DEBUG","ts":"2025-12-06T02:49:25.174Z","msg":"Setting accelerator name: Name=H100, allocationData={H100 2 512 200 9.264868 20.522991 {1295.7 230 472}}"}
{"level":"INFO","ts":"2025-12-06T02:49:25.174Z","msg":"Model-based optimization completed for model: unsloth/Meta-Llama-3.1-8B - model-based targets: map[ms-inference-scheduling-llm-d-modelservice-decode:2]"}
{"level":"WARN","ts":"2025-12-06T02:49:25.174Z","msg":"saturation analysis unavailable, using model-based targets only: modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-06T02:49:25.174Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-12-06T02:49:25.174Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=2→target=2"}
{"level":"DEBUG","ts":"2025-12-06T02:49:25.174Z","msg":"Found VA in map: variant=ms-inference-scheduling-llm-d-modelservice-decode, hasCurrentAlloc=true, accelerator=H100"}
{"level":"INFO","ts":"2025-12-06T02:49:25.174Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 2, desired-replicas: 2, accelerator: H100"}
{"level":"INFO","ts":"2025-12-06T02:49:25.174Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=2, accelerator=H100, SaturationOnly=false"}
{"level":"INFO","ts":"2025-12-06T02:49:25.179Z","msg":"Applied Saturation decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=2, target=2, reason=model-based only (Saturation unavailable)"}
{"level":"INFO","ts":"2025-12-06T02:49:25.179Z","msg":"Reconciliation completed successfully: mode=model-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-12-06T02:50:25.030Z","msg":"saturation scaling config cache updated: entries=1, has_default=true"}
{"level":"INFO","ts":"2025-12-06T02:50:25.030Z","msg":"saturation scaling configuration loaded successfully"}
{"level":"INFO","ts":"2025-12-06T02:50:25.030Z","msg":"Operating in MODEL-ONLY mode: model-based optimization only"}
{"level":"INFO","ts":"2025-12-06T02:50:25.030Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-12-06T02:50:25.030Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-12-06T02:50:25.036Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dczzts, usage=0.255 (25.5%)"}
{"level":"DEBUG","ts":"2025-12-06T02:50:25.036Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"INFO","ts":"2025-12-06T02:50:25.036Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dczzts, queueLength=0"}
{"level":"DEBUG","ts":"2025-12-06T02:50:25.036Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"DEBUG","ts":"2025-12-06T02:50:25.038Z","msg":"Pod-to-variant matching successful: totalPods=1, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"DEBUG","ts":"2025-12-06T02:50:25.038Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=1"}
{"level":"DEBUG","ts":"2025-12-06T02:50:25.038Z","msg":"Collected capacity metrics for observability in model-only mode: modelID=unsloth/Meta-Llama-3.1-8B, replicaCount=1"}
{"level":"INFO","ts":"2025-12-06T02:50:25.039Z","msg":"Found SLO for model: model=unsloth/Meta-Llama-3.1-8B, class=Premium, slo-tpot=10, slo-ttft=1000"}
{"level":"DEBUG","ts":"2025-12-06T02:50:25.047Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=2, accelerator=H100, ttft=31.21ms, itl=16.57ms, cost=200.00, maxBatch=256, arrivalRate=1388.15, avgInputTokens=227.78, avgOutputTokens=462.93"}
{"level":"DEBUG","ts":"2025-12-06T02:50:25.047Z","msg":"Experimental model tuner is enabled globally (EXPERIMENTAL_MODEL_TUNER_ENABLED=true) tuning model performance parameters for active VAs"}
{"level":"DEBUG","ts":"2025-12-06T02:50:25.047Z","msg":"Using state vals from VA status to tune variant ms-inference-scheduling-llm-d-modelservice-decode: alpha= 7.137604, beta= 0.044007, gamma= 16.954107, delta= 0.000321"}
{"level":"DEBUG","ts":"2025-12-06T02:50:25.047Z","msg":"[Tuner Config] variant=ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler | Initial state: alpha=7.137604, beta=0.044007, gamma=16.954107, delta=0.000321 | SLO targets: TTFT=1000.00ms, ITL=10.00ms | Has covariance=true"}
{"level":"WARN","ts":"2025-12-06T02:50:25.048Z","msg":"Tuner validation failed (NIS=141.34), validation error: normalized innovation squared (NIS=141.34) exceeds threshold (7.38), rejecting update as outlier - returning previous state: alpha=7.137604, beta=0.044007, gamma=16.954107, delta=0.000321"}
{"level":"WARN","ts":"2025-12-06T02:50:25.048Z","msg":"Tuner NIS validation failed for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler, server ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler (NIS=141.34 exceeds threshold 7.38) - Keeping previous state: alpha=7.137604, beta=0.044007, gamma=16.954107, delta=0.000321"}
{"level":"INFO","ts":"2025-12-06T02:50:25.048Z","msg":"Keeping previous tuned parameters for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler due to NIS validation failure (NIS=141.339386)"}
{"level":"DEBUG","ts":"2025-12-06T02:50:25.048Z","msg":"Model tuner results: model=unsloth/Meta-Llama-3.1-8B, accelerator=H100, alpha=7.137604, beta=0.044007, gamma=16.954107, delta=0.000321, NIS=141.34"}
{"level":"DEBUG","ts":"2025-12-06T02:50:25.048Z","msg":"Updated tuner status for variant ms-inference-scheduling-llm-d-modelservice-decode: alpha=7.137604, beta=0.044007, gamma=16.954107, delta=0.000321, NIS=141.339386"}
{"level":"DEBUG","ts":"2025-12-06T02:50:25.058Z","msg":"Optimization solution - system: Solution: \ns=ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler; c=Premium; m=unsloth/Meta-Llama-3.1-8B; rate=1388.15; inTk=227; outTk=462; sol=1, sat=false, alloc={acc=H100; numRep=2; maxBatch=512; cost=200, val=0, itl=9.39758, ttft=20.696186, rho=0.098349445, maxRPM=829.64215}; slo-itl=10, slo-ttft=1000, slo-tps=0 \nAllocationByType: \nname=NVIDIA-H100-80GB-HBM3, count=2, limit=0, cost=200 \ntotalCost=200 \n"}
{"level":"DEBUG","ts":"2025-12-06T02:50:25.058Z","msg":"Setting accelerator name: Name=H100, allocationData={H100 2 512 200 9.39758 20.696186 {1388.15 227 462}}"}
{"level":"INFO","ts":"2025-12-06T02:50:25.058Z","msg":"Model-based optimization completed for model: unsloth/Meta-Llama-3.1-8B - model-based targets: map[ms-inference-scheduling-llm-d-modelservice-decode:2]"}
{"level":"WARN","ts":"2025-12-06T02:50:25.058Z","msg":"saturation analysis unavailable, using model-based targets only: modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-06T02:50:25.058Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-12-06T02:50:25.058Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=2→target=2"}
{"level":"DEBUG","ts":"2025-12-06T02:50:25.058Z","msg":"Found VA in map: variant=ms-inference-scheduling-llm-d-modelservice-decode, hasCurrentAlloc=true, accelerator=H100"}
{"level":"INFO","ts":"2025-12-06T02:50:25.058Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 2, desired-replicas: 2, accelerator: H100"}
{"level":"INFO","ts":"2025-12-06T02:50:25.058Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=2, accelerator=H100, SaturationOnly=false"}
{"level":"INFO","ts":"2025-12-06T02:50:25.065Z","msg":"Applied Saturation decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=2, target=2, reason=model-based only (Saturation unavailable)"}
{"level":"INFO","ts":"2025-12-06T02:50:25.065Z","msg":"Reconciliation completed successfully: mode=model-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-12-06T02:50:25.180Z","msg":"saturation scaling config cache updated: entries=1, has_default=true"}
{"level":"INFO","ts":"2025-12-06T02:50:25.180Z","msg":"saturation scaling configuration loaded successfully"}
{"level":"INFO","ts":"2025-12-06T02:50:25.180Z","msg":"Operating in MODEL-ONLY mode: model-based optimization only"}
{"level":"INFO","ts":"2025-12-06T02:50:25.180Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-12-06T02:50:25.180Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-12-06T02:50:25.183Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dczzts, queueLength=0"}
{"level":"DEBUG","ts":"2025-12-06T02:50:25.183Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"INFO","ts":"2025-12-06T02:50:25.183Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dczzts, usage=0.255 (25.5%)"}
{"level":"DEBUG","ts":"2025-12-06T02:50:25.183Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"DEBUG","ts":"2025-12-06T02:50:25.185Z","msg":"Pod-to-variant matching successful: totalPods=1, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"DEBUG","ts":"2025-12-06T02:50:25.185Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=1"}
{"level":"DEBUG","ts":"2025-12-06T02:50:25.185Z","msg":"Collected capacity metrics for observability in model-only mode: modelID=unsloth/Meta-Llama-3.1-8B, replicaCount=1"}
{"level":"INFO","ts":"2025-12-06T02:50:25.185Z","msg":"Found SLO for model: model=unsloth/Meta-Llama-3.1-8B, class=Premium, slo-tpot=10, slo-ttft=1000"}
{"level":"DEBUG","ts":"2025-12-06T02:50:25.194Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=2, accelerator=H100, ttft=31.21ms, itl=16.57ms, cost=200.00, maxBatch=256, arrivalRate=1388.15, avgInputTokens=227.78, avgOutputTokens=462.93"}
{"level":"DEBUG","ts":"2025-12-06T02:50:25.194Z","msg":"Experimental model tuner is enabled globally (EXPERIMENTAL_MODEL_TUNER_ENABLED=true) tuning model performance parameters for active VAs"}
{"level":"DEBUG","ts":"2025-12-06T02:50:25.194Z","msg":"Using state vals from VA status to tune variant ms-inference-scheduling-llm-d-modelservice-decode: alpha= 7.137604, beta= 0.044007, gamma= 16.954107, delta= 0.000321"}
{"level":"DEBUG","ts":"2025-12-06T02:50:25.194Z","msg":"[Tuner Config] variant=ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler | Initial state: alpha=7.137604, beta=0.044007, gamma=16.954107, delta=0.000321 | SLO targets: TTFT=1000.00ms, ITL=10.00ms | Has covariance=true"}
{"level":"WARN","ts":"2025-12-06T02:50:25.194Z","msg":"Tuner validation failed (NIS=141.34), validation error: normalized innovation squared (NIS=141.34) exceeds threshold (7.38), rejecting update as outlier - returning previous state: alpha=7.137604, beta=0.044007, gamma=16.954107, delta=0.000321"}
{"level":"WARN","ts":"2025-12-06T02:50:25.194Z","msg":"Tuner NIS validation failed for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler, server ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler (NIS=141.34 exceeds threshold 7.38) - Keeping previous state: alpha=7.137604, beta=0.044007, gamma=16.954107, delta=0.000321"}
{"level":"INFO","ts":"2025-12-06T02:50:25.194Z","msg":"Keeping previous tuned parameters for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler due to NIS validation failure (NIS=141.339386)"}
{"level":"DEBUG","ts":"2025-12-06T02:50:25.194Z","msg":"Model tuner results: model=unsloth/Meta-Llama-3.1-8B, accelerator=H100, alpha=7.137604, beta=0.044007, gamma=16.954107, delta=0.000321, NIS=141.34"}
{"level":"DEBUG","ts":"2025-12-06T02:50:25.194Z","msg":"Tuned parameters unchanged for variant ms-inference-scheduling-llm-d-modelservice-decode, skipping status update"}
{"level":"DEBUG","ts":"2025-12-06T02:50:25.204Z","msg":"Optimization solution - system: Solution: \ns=ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler; c=Premium; m=unsloth/Meta-Llama-3.1-8B; rate=1388.15; inTk=227; outTk=462; sol=1, sat=false, alloc={acc=H100; numRep=2; maxBatch=512; cost=200, val=0, itl=9.39758, ttft=20.696186, rho=0.098349445, maxRPM=829.64215}; slo-itl=10, slo-ttft=1000, slo-tps=0 \nAllocationByType: \nname=NVIDIA-H100-80GB-HBM3, count=2, limit=0, cost=200 \ntotalCost=200 \n"}
{"level":"DEBUG","ts":"2025-12-06T02:50:25.204Z","msg":"Setting accelerator name: Name=H100, allocationData={H100 2 512 200 9.39758 20.696186 {1388.15 227 462}}"}
{"level":"INFO","ts":"2025-12-06T02:50:25.204Z","msg":"Model-based optimization completed for model: unsloth/Meta-Llama-3.1-8B - model-based targets: map[ms-inference-scheduling-llm-d-modelservice-decode:2]"}
{"level":"WARN","ts":"2025-12-06T02:50:25.204Z","msg":"saturation analysis unavailable, using model-based targets only: modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-06T02:50:25.204Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-12-06T02:50:25.204Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=2→target=2"}
{"level":"DEBUG","ts":"2025-12-06T02:50:25.204Z","msg":"Found VA in map: variant=ms-inference-scheduling-llm-d-modelservice-decode, hasCurrentAlloc=true, accelerator=H100"}
{"level":"INFO","ts":"2025-12-06T02:50:25.204Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 2, desired-replicas: 2, accelerator: H100"}
{"level":"INFO","ts":"2025-12-06T02:50:25.204Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=2, accelerator=H100, SaturationOnly=false"}
{"level":"INFO","ts":"2025-12-06T02:50:25.210Z","msg":"Applied Saturation decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=2, target=2, reason=model-based only (Saturation unavailable)"}
{"level":"INFO","ts":"2025-12-06T02:50:25.210Z","msg":"Reconciliation completed successfully: mode=model-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-12-06T02:51:25.065Z","msg":"saturation scaling config cache updated: entries=1, has_default=true"}
{"level":"INFO","ts":"2025-12-06T02:51:25.065Z","msg":"saturation scaling configuration loaded successfully"}
{"level":"INFO","ts":"2025-12-06T02:51:25.065Z","msg":"Operating in MODEL-ONLY mode: model-based optimization only"}
{"level":"INFO","ts":"2025-12-06T02:51:25.065Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-12-06T02:51:25.065Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-12-06T02:51:25.072Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dczzts, usage=0.321 (32.1%)"}
{"level":"INFO","ts":"2025-12-06T02:51:25.072Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984ds7q94, usage=0.009 (0.9%)"}
{"level":"INFO","ts":"2025-12-06T02:51:25.072Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dczzts, queueLength=0"}
{"level":"INFO","ts":"2025-12-06T02:51:25.072Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984ds7q94, queueLength=0"}
{"level":"DEBUG","ts":"2025-12-06T02:51:25.072Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=2"}
{"level":"DEBUG","ts":"2025-12-06T02:51:25.072Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=2"}
{"level":"DEBUG","ts":"2025-12-06T02:51:25.075Z","msg":"Pod-to-variant matching successful: totalPods=2, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:2]"}
{"level":"DEBUG","ts":"2025-12-06T02:51:25.075Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=2"}
{"level":"DEBUG","ts":"2025-12-06T02:51:25.075Z","msg":"Collected capacity metrics for observability in model-only mode: modelID=unsloth/Meta-Llama-3.1-8B, replicaCount=2"}
{"level":"INFO","ts":"2025-12-06T02:51:25.076Z","msg":"Found SLO for model: model=unsloth/Meta-Llama-3.1-8B, class=Premium, slo-tpot=10, slo-ttft=1000"}
{"level":"DEBUG","ts":"2025-12-06T02:51:25.087Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=2, accelerator=H100, ttft=32.60ms, itl=18.26ms, cost=200.00, maxBatch=256, arrivalRate=1405.01, avgInputTokens=242.10, avgOutputTokens=422.00"}
{"level":"DEBUG","ts":"2025-12-06T02:51:25.087Z","msg":"Experimental model tuner is enabled globally (EXPERIMENTAL_MODEL_TUNER_ENABLED=true) tuning model performance parameters for active VAs"}
{"level":"DEBUG","ts":"2025-12-06T02:51:25.088Z","msg":"Using state vals from VA status to tune variant ms-inference-scheduling-llm-d-modelservice-decode: alpha= 7.137604, beta= 0.044007, gamma= 16.954107, delta= 0.000321"}
{"level":"DEBUG","ts":"2025-12-06T02:51:25.088Z","msg":"[Tuner Config] variant=ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler | Initial state: alpha=7.137604, beta=0.044007, gamma=16.954107, delta=0.000321 | SLO targets: TTFT=1000.00ms, ITL=10.00ms | Has covariance=true"}
{"level":"WARN","ts":"2025-12-06T02:51:25.088Z","msg":"Tuner validation failed (NIS=240.98), validation error: normalized innovation squared (NIS=240.98) exceeds threshold (7.38), rejecting update as outlier - returning previous state: alpha=7.137604, beta=0.044007, gamma=16.954107, delta=0.000321"}
{"level":"WARN","ts":"2025-12-06T02:51:25.088Z","msg":"Tuner NIS validation failed for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler, server ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler (NIS=240.98 exceeds threshold 7.38) - Keeping previous state: alpha=7.137604, beta=0.044007, gamma=16.954107, delta=0.000321"}
{"level":"INFO","ts":"2025-12-06T02:51:25.088Z","msg":"Keeping previous tuned parameters for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler due to NIS validation failure (NIS=240.980538)"}
{"level":"DEBUG","ts":"2025-12-06T02:51:25.088Z","msg":"Model tuner results: model=unsloth/Meta-Llama-3.1-8B, accelerator=H100, alpha=7.137604, beta=0.044007, gamma=16.954107, delta=0.000321, NIS=240.98"}
{"level":"DEBUG","ts":"2025-12-06T02:51:25.088Z","msg":"Updated tuner status for variant ms-inference-scheduling-llm-d-modelservice-decode: alpha=7.137604, beta=0.044007, gamma=16.954107, delta=0.000321, NIS=240.980538"}
{"level":"DEBUG","ts":"2025-12-06T02:51:25.093Z","msg":"Optimization solution - system: Solution: \ns=ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler; c=Premium; m=unsloth/Meta-Llama-3.1-8B; rate=1405.01; inTk=242; outTk=422; sol=1, sat=false, alloc={acc=H100; numRep=2; maxBatch=512; cost=200, val=0, itl=9.184527, ttft=20.567375, rho=0.08889372, maxRPM=907.9951}; slo-itl=10, slo-ttft=1000, slo-tps=0 \nAllocationByType: \nname=NVIDIA-H100-80GB-HBM3, count=2, limit=0, cost=200 \ntotalCost=200 \n"}
{"level":"DEBUG","ts":"2025-12-06T02:51:25.093Z","msg":"Setting accelerator name: Name=H100, allocationData={H100 2 512 200 9.184527 20.567375 {1405.01 242 422}}"}
{"level":"INFO","ts":"2025-12-06T02:51:25.093Z","msg":"Model-based optimization completed for model: unsloth/Meta-Llama-3.1-8B - model-based targets: map[ms-inference-scheduling-llm-d-modelservice-decode:2]"}
{"level":"WARN","ts":"2025-12-06T02:51:25.093Z","msg":"saturation analysis unavailable, using model-based targets only: modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-06T02:51:25.093Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-12-06T02:51:25.093Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=2→target=2"}
{"level":"DEBUG","ts":"2025-12-06T02:51:25.093Z","msg":"Found VA in map: variant=ms-inference-scheduling-llm-d-modelservice-decode, hasCurrentAlloc=true, accelerator=H100"}
{"level":"INFO","ts":"2025-12-06T02:51:25.093Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 2, desired-replicas: 2, accelerator: H100"}
{"level":"INFO","ts":"2025-12-06T02:51:25.093Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=2, accelerator=H100, SaturationOnly=false"}
{"level":"INFO","ts":"2025-12-06T02:51:25.098Z","msg":"Applied Saturation decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=2, target=2, reason=model-based only (Saturation unavailable)"}
{"level":"INFO","ts":"2025-12-06T02:51:25.098Z","msg":"Reconciliation completed successfully: mode=model-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-12-06T02:51:25.210Z","msg":"saturation scaling config cache updated: entries=1, has_default=true"}
{"level":"INFO","ts":"2025-12-06T02:51:25.210Z","msg":"saturation scaling configuration loaded successfully"}
{"level":"INFO","ts":"2025-12-06T02:51:25.210Z","msg":"Operating in MODEL-ONLY mode: model-based optimization only"}
{"level":"INFO","ts":"2025-12-06T02:51:25.210Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-12-06T02:51:25.210Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-12-06T02:51:25.213Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dczzts, usage=0.321 (32.1%)"}
{"level":"INFO","ts":"2025-12-06T02:51:25.213Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984ds7q94, usage=0.009 (0.9%)"}
{"level":"DEBUG","ts":"2025-12-06T02:51:25.213Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=2"}
{"level":"INFO","ts":"2025-12-06T02:51:25.213Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dczzts, queueLength=0"}
{"level":"INFO","ts":"2025-12-06T02:51:25.213Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984ds7q94, queueLength=0"}
{"level":"DEBUG","ts":"2025-12-06T02:51:25.213Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=2"}
{"level":"DEBUG","ts":"2025-12-06T02:51:25.215Z","msg":"Pod-to-variant matching successful: totalPods=2, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:2]"}
{"level":"DEBUG","ts":"2025-12-06T02:51:25.215Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=2"}
{"level":"DEBUG","ts":"2025-12-06T02:51:25.215Z","msg":"Collected capacity metrics for observability in model-only mode: modelID=unsloth/Meta-Llama-3.1-8B, replicaCount=2"}
{"level":"INFO","ts":"2025-12-06T02:51:25.215Z","msg":"Found SLO for model: model=unsloth/Meta-Llama-3.1-8B, class=Premium, slo-tpot=10, slo-ttft=1000"}
{"level":"DEBUG","ts":"2025-12-06T02:51:25.223Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=2, accelerator=H100, ttft=32.60ms, itl=18.26ms, cost=200.00, maxBatch=256, arrivalRate=1405.01, avgInputTokens=242.10, avgOutputTokens=422.00"}
{"level":"DEBUG","ts":"2025-12-06T02:51:25.223Z","msg":"Experimental model tuner is enabled globally (EXPERIMENTAL_MODEL_TUNER_ENABLED=true) tuning model performance parameters for active VAs"}
{"level":"DEBUG","ts":"2025-12-06T02:51:25.223Z","msg":"Using state vals from VA status to tune variant ms-inference-scheduling-llm-d-modelservice-decode: alpha= 7.137604, beta= 0.044007, gamma= 16.954107, delta= 0.000321"}
{"level":"DEBUG","ts":"2025-12-06T02:51:25.223Z","msg":"[Tuner Config] variant=ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler | Initial state: alpha=7.137604, beta=0.044007, gamma=16.954107, delta=0.000321 | SLO targets: TTFT=1000.00ms, ITL=10.00ms | Has covariance=true"}
{"level":"WARN","ts":"2025-12-06T02:51:25.224Z","msg":"Tuner validation failed (NIS=240.98), validation error: normalized innovation squared (NIS=240.98) exceeds threshold (7.38), rejecting update as outlier - returning previous state: alpha=7.137604, beta=0.044007, gamma=16.954107, delta=0.000321"}
{"level":"WARN","ts":"2025-12-06T02:51:25.224Z","msg":"Tuner NIS validation failed for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler, server ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler (NIS=240.98 exceeds threshold 7.38) - Keeping previous state: alpha=7.137604, beta=0.044007, gamma=16.954107, delta=0.000321"}
{"level":"INFO","ts":"2025-12-06T02:51:25.224Z","msg":"Keeping previous tuned parameters for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler due to NIS validation failure (NIS=240.980538)"}
{"level":"DEBUG","ts":"2025-12-06T02:51:25.224Z","msg":"Model tuner results: model=unsloth/Meta-Llama-3.1-8B, accelerator=H100, alpha=7.137604, beta=0.044007, gamma=16.954107, delta=0.000321, NIS=240.98"}
{"level":"DEBUG","ts":"2025-12-06T02:51:25.224Z","msg":"Tuned parameters unchanged for variant ms-inference-scheduling-llm-d-modelservice-decode, skipping status update"}
{"level":"DEBUG","ts":"2025-12-06T02:51:25.228Z","msg":"Optimization solution - system: Solution: \ns=ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler; c=Premium; m=unsloth/Meta-Llama-3.1-8B; rate=1405.01; inTk=242; outTk=422; sol=1, sat=false, alloc={acc=H100; numRep=2; maxBatch=512; cost=200, val=0, itl=9.184527, ttft=20.567375, rho=0.08889372, maxRPM=907.9951}; slo-itl=10, slo-ttft=1000, slo-tps=0 \nAllocationByType: \nname=NVIDIA-H100-80GB-HBM3, count=2, limit=0, cost=200 \ntotalCost=200 \n"}
{"level":"DEBUG","ts":"2025-12-06T02:51:25.228Z","msg":"Setting accelerator name: Name=H100, allocationData={H100 2 512 200 9.184527 20.567375 {1405.01 242 422}}"}
{"level":"INFO","ts":"2025-12-06T02:51:25.228Z","msg":"Model-based optimization completed for model: unsloth/Meta-Llama-3.1-8B - model-based targets: map[ms-inference-scheduling-llm-d-modelservice-decode:2]"}
{"level":"WARN","ts":"2025-12-06T02:51:25.228Z","msg":"saturation analysis unavailable, using model-based targets only: modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-06T02:51:25.228Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-12-06T02:51:25.228Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=2→target=2"}
{"level":"DEBUG","ts":"2025-12-06T02:51:25.228Z","msg":"Found VA in map: variant=ms-inference-scheduling-llm-d-modelservice-decode, hasCurrentAlloc=true, accelerator=H100"}
{"level":"INFO","ts":"2025-12-06T02:51:25.228Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 2, desired-replicas: 2, accelerator: H100"}
{"level":"INFO","ts":"2025-12-06T02:51:25.228Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=2, accelerator=H100, SaturationOnly=false"}
{"level":"INFO","ts":"2025-12-06T02:51:25.234Z","msg":"Applied Saturation decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=2, target=2, reason=model-based only (Saturation unavailable)"}
{"level":"INFO","ts":"2025-12-06T02:51:25.234Z","msg":"Reconciliation completed successfully: mode=model-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-12-06T02:52:25.099Z","msg":"saturation scaling config cache updated: entries=1, has_default=true"}
{"level":"INFO","ts":"2025-12-06T02:52:25.099Z","msg":"saturation scaling configuration loaded successfully"}
{"level":"INFO","ts":"2025-12-06T02:52:25.099Z","msg":"Operating in MODEL-ONLY mode: model-based optimization only"}
{"level":"INFO","ts":"2025-12-06T02:52:25.099Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-12-06T02:52:25.099Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-12-06T02:52:25.109Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dczzts, queueLength=0"}
{"level":"INFO","ts":"2025-12-06T02:52:25.109Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984ds7q94, queueLength=0"}
{"level":"DEBUG","ts":"2025-12-06T02:52:25.109Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=2"}
{"level":"INFO","ts":"2025-12-06T02:52:25.109Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dczzts, usage=0.140 (14.0%)"}
{"level":"INFO","ts":"2025-12-06T02:52:25.109Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984ds7q94, usage=0.122 (12.2%)"}
{"level":"DEBUG","ts":"2025-12-06T02:52:25.109Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=2"}
{"level":"DEBUG","ts":"2025-12-06T02:52:25.112Z","msg":"Pod-to-variant matching successful: totalPods=2, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:2]"}
{"level":"DEBUG","ts":"2025-12-06T02:52:25.112Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=2"}
{"level":"DEBUG","ts":"2025-12-06T02:52:25.112Z","msg":"Collected capacity metrics for observability in model-only mode: modelID=unsloth/Meta-Llama-3.1-8B, replicaCount=2"}
{"level":"INFO","ts":"2025-12-06T02:52:25.112Z","msg":"Found SLO for model: model=unsloth/Meta-Llama-3.1-8B, class=Premium, slo-tpot=10, slo-ttft=1000"}
{"level":"DEBUG","ts":"2025-12-06T02:52:25.122Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=2, accelerator=H100, ttft=23.37ms, itl=12.02ms, cost=200.00, maxBatch=256, arrivalRate=1976.64, avgInputTokens=228.68, avgOutputTokens=471.24"}
{"level":"DEBUG","ts":"2025-12-06T02:52:25.122Z","msg":"Experimental model tuner is enabled globally (EXPERIMENTAL_MODEL_TUNER_ENABLED=true) tuning model performance parameters for active VAs"}
{"level":"DEBUG","ts":"2025-12-06T02:52:25.122Z","msg":"Using state vals from VA status to tune variant ms-inference-scheduling-llm-d-modelservice-decode: alpha= 7.137604, beta= 0.044007, gamma= 16.954107, delta= 0.000321"}
{"level":"DEBUG","ts":"2025-12-06T02:52:25.122Z","msg":"[Tuner Config] variant=ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler | Initial state: alpha=7.137604, beta=0.044007, gamma=16.954107, delta=0.000321 | SLO targets: TTFT=1000.00ms, ITL=10.00ms | Has covariance=true"}
{"level":"INFO","ts":"2025-12-06T02:52:25.124Z","msg":"Tuner validation succeeded for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler, server ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler - New NIS: 1.399441"}
{"level":"DEBUG","ts":"2025-12-06T02:52:25.124Z","msg":"Model tuner results: model=unsloth/Meta-Llama-3.1-8B, accelerator=H100, alpha=7.111185, beta=0.052130, gamma=16.947632, delta=0.000321, NIS=1.40"}
{"level":"DEBUG","ts":"2025-12-06T02:52:25.124Z","msg":"Updated tuner status for variant ms-inference-scheduling-llm-d-modelservice-decode: alpha=7.111185, beta=0.052130, gamma=16.947632, delta=0.000321, NIS=1.399441"}
{"level":"INFO","ts":"2025-12-06T02:52:25.124Z","msg":"Tuned performance parameters result: variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler - alpha: 7.111185, beta: 0.052130, gamma: 16.947632, delta: 0.000321"}
{"level":"DEBUG","ts":"2025-12-06T02:52:25.133Z","msg":"Optimization solution - system: Solution: \ns=ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler; c=Premium; m=unsloth/Meta-Llama-3.1-8B; rate=1976.64; inTk=228; outTk=471; sol=1, sat=false, alloc={acc=H100; numRep=3; maxBatch=512; cost=300, val=100, itl=9.816331, ttft=20.745556, rho=0.09939868, maxRPM=691.5737}; slo-itl=10, slo-ttft=1000, slo-tps=0 \nAllocationByType: \nname=NVIDIA-H100-80GB-HBM3, count=3, limit=0, cost=300 \ntotalCost=300 \n"}
{"level":"DEBUG","ts":"2025-12-06T02:52:25.133Z","msg":"Setting accelerator name: Name=H100, allocationData={H100 3 512 300 9.816331 20.745556 {1976.64 228 471}}"}
{"level":"INFO","ts":"2025-12-06T02:52:25.133Z","msg":"Model-based optimization completed for model: unsloth/Meta-Llama-3.1-8B - model-based targets: map[ms-inference-scheduling-llm-d-modelservice-decode:3]"}
{"level":"WARN","ts":"2025-12-06T02:52:25.133Z","msg":"saturation analysis unavailable, using model-based targets only: modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-06T02:52:25.133Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-12-06T02:52:25.133Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=scale-up, current=2→target=3"}
{"level":"DEBUG","ts":"2025-12-06T02:52:25.133Z","msg":"Found VA in map: variant=ms-inference-scheduling-llm-d-modelservice-decode, hasCurrentAlloc=true, accelerator=H100"}
{"level":"INFO","ts":"2025-12-06T02:52:25.134Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 2, desired-replicas: 3, accelerator: H100"}
{"level":"INFO","ts":"2025-12-06T02:52:25.134Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=3, accelerator=H100, SaturationOnly=false"}
{"level":"INFO","ts":"2025-12-06T02:52:25.139Z","msg":"Applied Saturation decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=scale-up, current=2, target=3, reason=model-based only (Saturation unavailable)"}
{"level":"INFO","ts":"2025-12-06T02:52:25.139Z","msg":"Reconciliation completed successfully: mode=model-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-12-06T02:52:25.234Z","msg":"saturation scaling config cache updated: entries=1, has_default=true"}
{"level":"INFO","ts":"2025-12-06T02:52:25.234Z","msg":"saturation scaling configuration loaded successfully"}
{"level":"INFO","ts":"2025-12-06T02:52:25.234Z","msg":"Operating in MODEL-ONLY mode: model-based optimization only"}
{"level":"INFO","ts":"2025-12-06T02:52:25.234Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-12-06T02:52:25.234Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-12-06T02:52:25.237Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dczzts, usage=0.140 (14.0%)"}
{"level":"INFO","ts":"2025-12-06T02:52:25.237Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984ds7q94, usage=0.122 (12.2%)"}
{"level":"DEBUG","ts":"2025-12-06T02:52:25.237Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=2"}
{"level":"INFO","ts":"2025-12-06T02:52:25.237Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dczzts, queueLength=0"}
{"level":"INFO","ts":"2025-12-06T02:52:25.237Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984ds7q94, queueLength=0"}
{"level":"DEBUG","ts":"2025-12-06T02:52:25.237Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=2"}
{"level":"DEBUG","ts":"2025-12-06T02:52:25.239Z","msg":"Pod-to-variant matching successful: totalPods=2, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:2]"}
{"level":"DEBUG","ts":"2025-12-06T02:52:25.239Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=2"}
{"level":"DEBUG","ts":"2025-12-06T02:52:25.239Z","msg":"Collected capacity metrics for observability in model-only mode: modelID=unsloth/Meta-Llama-3.1-8B, replicaCount=2"}
{"level":"INFO","ts":"2025-12-06T02:52:25.239Z","msg":"Found SLO for model: model=unsloth/Meta-Llama-3.1-8B, class=Premium, slo-tpot=10, slo-ttft=1000"}
{"level":"DEBUG","ts":"2025-12-06T02:52:25.248Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=2, accelerator=H100, ttft=23.37ms, itl=12.02ms, cost=200.00, maxBatch=256, arrivalRate=1976.64, avgInputTokens=228.68, avgOutputTokens=471.24"}
{"level":"DEBUG","ts":"2025-12-06T02:52:25.248Z","msg":"Experimental model tuner is enabled globally (EXPERIMENTAL_MODEL_TUNER_ENABLED=true) tuning model performance parameters for active VAs"}
{"level":"DEBUG","ts":"2025-12-06T02:52:25.248Z","msg":"Using state vals from VA status to tune variant ms-inference-scheduling-llm-d-modelservice-decode: alpha= 7.111185, beta= 0.052130, gamma= 16.947632, delta= 0.000321"}
{"level":"DEBUG","ts":"2025-12-06T02:52:25.248Z","msg":"[Tuner Config] variant=ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler | Initial state: alpha=7.111185, beta=0.052130, gamma=16.947632, delta=0.000321 | SLO targets: TTFT=1000.00ms, ITL=10.00ms | Has covariance=true"}
{"level":"INFO","ts":"2025-12-06T02:52:25.249Z","msg":"Tuner validation succeeded for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler, server ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler - New NIS: 0.001249"}
{"level":"DEBUG","ts":"2025-12-06T02:52:25.249Z","msg":"Model tuner results: model=unsloth/Meta-Llama-3.1-8B, accelerator=H100, alpha=7.105848, beta=0.052044, gamma=16.934669, delta=0.000321, NIS=0.00"}
{"level":"DEBUG","ts":"2025-12-06T02:52:25.249Z","msg":"Updated tuner status for variant ms-inference-scheduling-llm-d-modelservice-decode: alpha=7.105848, beta=0.052044, gamma=16.934669, delta=0.000321, NIS=0.001249"}
{"level":"INFO","ts":"2025-12-06T02:52:25.249Z","msg":"Tuned performance parameters result: variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler - alpha: 7.105848, beta: 0.052044, gamma: 16.934669, delta: 0.000321"}
{"level":"DEBUG","ts":"2025-12-06T02:52:25.259Z","msg":"Optimization solution - system: Solution: \ns=ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler; c=Premium; m=unsloth/Meta-Llama-3.1-8B; rate=1976.64; inTk=228; outTk=471; sol=1, sat=false, alloc={acc=H100; numRep=3; maxBatch=512; cost=300, val=100, itl=9.8029, ttft=20.727411, rho=0.09926289, maxRPM=694.04504}; slo-itl=10, slo-ttft=1000, slo-tps=0 \nAllocationByType: \nname=NVIDIA-H100-80GB-HBM3, count=3, limit=0, cost=300 \ntotalCost=300 \n"}
{"level":"DEBUG","ts":"2025-12-06T02:52:25.259Z","msg":"Setting accelerator name: Name=H100, allocationData={H100 3 512 300 9.8029 20.727411 {1976.64 228 471}}"}
{"level":"INFO","ts":"2025-12-06T02:52:25.259Z","msg":"Model-based optimization completed for model: unsloth/Meta-Llama-3.1-8B - model-based targets: map[ms-inference-scheduling-llm-d-modelservice-decode:3]"}
{"level":"WARN","ts":"2025-12-06T02:52:25.259Z","msg":"saturation analysis unavailable, using model-based targets only: modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-06T02:52:25.259Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-12-06T02:52:25.259Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=scale-up, current=2→target=3"}
{"level":"DEBUG","ts":"2025-12-06T02:52:25.259Z","msg":"Found VA in map: variant=ms-inference-scheduling-llm-d-modelservice-decode, hasCurrentAlloc=true, accelerator=H100"}
{"level":"INFO","ts":"2025-12-06T02:52:25.259Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 2, desired-replicas: 3, accelerator: H100"}
{"level":"INFO","ts":"2025-12-06T02:52:25.259Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=3, accelerator=H100, SaturationOnly=false"}
{"level":"INFO","ts":"2025-12-06T02:52:25.265Z","msg":"Applied Saturation decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=scale-up, current=2, target=3, reason=model-based only (Saturation unavailable)"}
{"level":"INFO","ts":"2025-12-06T02:52:25.265Z","msg":"Reconciliation completed successfully: mode=model-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-12-06T02:53:25.140Z","msg":"saturation scaling config cache updated: entries=1, has_default=true"}
{"level":"INFO","ts":"2025-12-06T02:53:25.140Z","msg":"saturation scaling configuration loaded successfully"}
{"level":"INFO","ts":"2025-12-06T02:53:25.140Z","msg":"Operating in MODEL-ONLY mode: model-based optimization only"}
{"level":"INFO","ts":"2025-12-06T02:53:25.140Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-12-06T02:53:25.140Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-12-06T02:53:25.145Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dczzts, queueLength=0"}
{"level":"INFO","ts":"2025-12-06T02:53:25.145Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984ds7q94, queueLength=0"}
{"level":"DEBUG","ts":"2025-12-06T02:53:25.145Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=2"}
{"level":"INFO","ts":"2025-12-06T02:53:25.155Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dczzts, usage=0.123 (12.3%)"}
{"level":"INFO","ts":"2025-12-06T02:53:25.155Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984ds7q94, usage=0.106 (10.6%)"}
{"level":"DEBUG","ts":"2025-12-06T02:53:25.155Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=2"}
{"level":"DEBUG","ts":"2025-12-06T02:53:25.158Z","msg":"Pod-to-variant matching successful: totalPods=2, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:2]"}
{"level":"DEBUG","ts":"2025-12-06T02:53:25.158Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=2"}
{"level":"DEBUG","ts":"2025-12-06T02:53:25.158Z","msg":"Collected capacity metrics for observability in model-only mode: modelID=unsloth/Meta-Llama-3.1-8B, replicaCount=2"}
{"level":"INFO","ts":"2025-12-06T02:53:25.158Z","msg":"Found SLO for model: model=unsloth/Meta-Llama-3.1-8B, class=Premium, slo-tpot=10, slo-ttft=1000"}
{"level":"DEBUG","ts":"2025-12-06T02:53:25.167Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=3, accelerator=H100, ttft=22.44ms, itl=11.13ms, cost=300.00, maxBatch=256, arrivalRate=1855.94, avgInputTokens=232.05, avgOutputTokens=440.75"}
{"level":"DEBUG","ts":"2025-12-06T02:53:25.167Z","msg":"Experimental model tuner is enabled globally (EXPERIMENTAL_MODEL_TUNER_ENABLED=true) tuning model performance parameters for active VAs"}
{"level":"DEBUG","ts":"2025-12-06T02:53:25.167Z","msg":"Using state vals from VA status to tune variant ms-inference-scheduling-llm-d-modelservice-decode: alpha= 7.105848, beta= 0.052044, gamma= 16.934669, delta= 0.000321"}
{"level":"DEBUG","ts":"2025-12-06T02:53:25.167Z","msg":"[Tuner Config] variant=ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler | Initial state: alpha=7.105848, beta=0.052044, gamma=16.934669, delta=0.000321 | SLO targets: TTFT=1000.00ms, ITL=10.00ms | Has covariance=true"}
{"level":"INFO","ts":"2025-12-06T02:53:25.168Z","msg":"Tuner validation succeeded for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler, server ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler - New NIS: 5.700098"}
{"level":"DEBUG","ts":"2025-12-06T02:53:25.168Z","msg":"Model tuner results: model=unsloth/Meta-Llama-3.1-8B, accelerator=H100, alpha=8.684816, beta=0.042845, gamma=16.976051, delta=0.000321, NIS=5.70"}
{"level":"DEBUG","ts":"2025-12-06T02:53:25.168Z","msg":"Updated tuner status for variant ms-inference-scheduling-llm-d-modelservice-decode: alpha=8.684816, beta=0.042845, gamma=16.976051, delta=0.000321, NIS=5.700098"}
{"level":"INFO","ts":"2025-12-06T02:53:25.168Z","msg":"Tuned performance parameters result: variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler - alpha: 8.684816, beta: 0.042845, gamma: 16.976051, delta: 0.000321"}
{"level":"DEBUG","ts":"2025-12-06T02:53:25.172Z","msg":"Optimization solution - system: Solution: \ns=ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler; c=Premium; m=unsloth/Meta-Llama-3.1-8B; rate=1855.94; inTk=232; outTk=440; sol=1, sat=false, alloc={acc=H100; numRep=5; maxBatch=512; cost=500, val=200, itl=9.882666, ttft=19.05821, rho=0.052652005, maxRPM=404.09857}; slo-itl=10, slo-ttft=1000, slo-tps=0 \nAllocationByType: \nname=NVIDIA-H100-80GB-HBM3, count=5, limit=0, cost=500 \ntotalCost=500 \n"}
{"level":"DEBUG","ts":"2025-12-06T02:53:25.172Z","msg":"Setting accelerator name: Name=H100, allocationData={H100 5 512 500 9.882666 19.05821 {1855.94 232 440}}"}
{"level":"INFO","ts":"2025-12-06T02:53:25.172Z","msg":"Model-based optimization completed for model: unsloth/Meta-Llama-3.1-8B - model-based targets: map[ms-inference-scheduling-llm-d-modelservice-decode:5]"}
{"level":"WARN","ts":"2025-12-06T02:53:25.172Z","msg":"saturation analysis unavailable, using model-based targets only: modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-06T02:53:25.172Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-12-06T02:53:25.172Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=scale-up, current=3→target=5"}
{"level":"DEBUG","ts":"2025-12-06T02:53:25.172Z","msg":"Found VA in map: variant=ms-inference-scheduling-llm-d-modelservice-decode, hasCurrentAlloc=true, accelerator=H100"}
{"level":"INFO","ts":"2025-12-06T02:53:25.172Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 3, desired-replicas: 5, accelerator: H100"}
{"level":"INFO","ts":"2025-12-06T02:53:25.172Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=5, accelerator=H100, SaturationOnly=false"}
{"level":"INFO","ts":"2025-12-06T02:53:25.183Z","msg":"Applied Saturation decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=scale-up, current=3, target=5, reason=model-based only (Saturation unavailable)"}
{"level":"INFO","ts":"2025-12-06T02:53:25.183Z","msg":"Reconciliation completed successfully: mode=model-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-12-06T02:53:25.265Z","msg":"saturation scaling config cache updated: entries=1, has_default=true"}
{"level":"INFO","ts":"2025-12-06T02:53:25.265Z","msg":"saturation scaling configuration loaded successfully"}
{"level":"INFO","ts":"2025-12-06T02:53:25.265Z","msg":"Operating in MODEL-ONLY mode: model-based optimization only"}
{"level":"INFO","ts":"2025-12-06T02:53:25.265Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-12-06T02:53:25.265Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-12-06T02:53:25.267Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dczzts, queueLength=0"}
{"level":"INFO","ts":"2025-12-06T02:53:25.267Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984ds7q94, queueLength=0"}
{"level":"DEBUG","ts":"2025-12-06T02:53:25.267Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=2"}
{"level":"INFO","ts":"2025-12-06T02:53:25.267Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dczzts, usage=0.123 (12.3%)"}
{"level":"INFO","ts":"2025-12-06T02:53:25.267Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984ds7q94, usage=0.106 (10.6%)"}
{"level":"DEBUG","ts":"2025-12-06T02:53:25.267Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=2"}
{"level":"DEBUG","ts":"2025-12-06T02:53:25.270Z","msg":"Pod-to-variant matching successful: totalPods=2, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:2]"}
{"level":"DEBUG","ts":"2025-12-06T02:53:25.270Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=2"}
{"level":"DEBUG","ts":"2025-12-06T02:53:25.270Z","msg":"Collected capacity metrics for observability in model-only mode: modelID=unsloth/Meta-Llama-3.1-8B, replicaCount=2"}
{"level":"INFO","ts":"2025-12-06T02:53:25.270Z","msg":"Found SLO for model: model=unsloth/Meta-Llama-3.1-8B, class=Premium, slo-tpot=10, slo-ttft=1000"}
{"level":"DEBUG","ts":"2025-12-06T02:53:25.278Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=3, accelerator=H100, ttft=22.44ms, itl=11.13ms, cost=300.00, maxBatch=256, arrivalRate=1855.94, avgInputTokens=232.05, avgOutputTokens=440.75"}
{"level":"DEBUG","ts":"2025-12-06T02:53:25.278Z","msg":"Experimental model tuner is enabled globally (EXPERIMENTAL_MODEL_TUNER_ENABLED=true) tuning model performance parameters for active VAs"}
{"level":"DEBUG","ts":"2025-12-06T02:53:25.278Z","msg":"Using state vals from VA status to tune variant ms-inference-scheduling-llm-d-modelservice-decode: alpha= 8.684816, beta= 0.042845, gamma= 16.976051, delta= 0.000321"}
{"level":"DEBUG","ts":"2025-12-06T02:53:25.278Z","msg":"[Tuner Config] variant=ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler | Initial state: alpha=8.684816, beta=0.042845, gamma=16.976051, delta=0.000321 | SLO targets: TTFT=1000.00ms, ITL=10.00ms | Has covariance=true"}
{"level":"INFO","ts":"2025-12-06T02:53:25.279Z","msg":"Tuner validation succeeded for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler, server ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler - New NIS: 0.204635"}
{"level":"DEBUG","ts":"2025-12-06T02:53:25.279Z","msg":"Model tuner results: model=unsloth/Meta-Llama-3.1-8B, accelerator=H100, alpha=8.872552, beta=0.043060, gamma=17.018108, delta=0.000321, NIS=0.20"}
{"level":"DEBUG","ts":"2025-12-06T02:53:25.279Z","msg":"Updated tuner status for variant ms-inference-scheduling-llm-d-modelservice-decode: alpha=8.872552, beta=0.043060, gamma=17.018108, delta=0.000321, NIS=0.204635"}
{"level":"INFO","ts":"2025-12-06T02:53:25.279Z","msg":"Tuned performance parameters result: variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler - alpha: 8.872552, beta: 0.043060, gamma: 17.018108, delta: 0.000321"}
{"level":"DEBUG","ts":"2025-12-06T02:53:25.288Z","msg":"Optimization solution - system: Solution: \ns=ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler; c=Premium; m=unsloth/Meta-Llama-3.1-8B; rate=1855.94; inTk=232; outTk=440; sol=1, sat=false, alloc={acc=H100; numRep=6; maxBatch=512; cost=600, val=300, itl=9.882896, ttft=18.765556, rho=0.043874744, maxRPM=342.7125}; slo-itl=10, slo-ttft=1000, slo-tps=0 \nAllocationByType: \nname=NVIDIA-H100-80GB-HBM3, count=6, limit=0, cost=600 \ntotalCost=600 \n"}
{"level":"DEBUG","ts":"2025-12-06T02:53:25.288Z","msg":"Setting accelerator name: Name=H100, allocationData={H100 6 512 600 9.882896 18.765556 {1855.94 232 440}}"}
{"level":"INFO","ts":"2025-12-06T02:53:25.288Z","msg":"Model-based optimization completed for model: unsloth/Meta-Llama-3.1-8B - model-based targets: map[ms-inference-scheduling-llm-d-modelservice-decode:6]"}
{"level":"WARN","ts":"2025-12-06T02:53:25.288Z","msg":"saturation analysis unavailable, using model-based targets only: modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-06T02:53:25.288Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-12-06T02:53:25.288Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=scale-up, current=3→target=6"}
{"level":"DEBUG","ts":"2025-12-06T02:53:25.288Z","msg":"Found VA in map: variant=ms-inference-scheduling-llm-d-modelservice-decode, hasCurrentAlloc=true, accelerator=H100"}
{"level":"INFO","ts":"2025-12-06T02:53:25.288Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 3, desired-replicas: 6, accelerator: H100"}
{"level":"INFO","ts":"2025-12-06T02:53:25.288Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=6, accelerator=H100, SaturationOnly=false"}
{"level":"INFO","ts":"2025-12-06T02:53:25.301Z","msg":"Applied Saturation decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=scale-up, current=3, target=6, reason=model-based only (Saturation unavailable)"}
{"level":"INFO","ts":"2025-12-06T02:53:25.301Z","msg":"Reconciliation completed successfully: mode=model-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-12-06T02:54:25.184Z","msg":"saturation scaling config cache updated: entries=1, has_default=true"}
{"level":"INFO","ts":"2025-12-06T02:54:25.184Z","msg":"saturation scaling configuration loaded successfully"}
{"level":"INFO","ts":"2025-12-06T02:54:25.184Z","msg":"Operating in MODEL-ONLY mode: model-based optimization only"}
{"level":"INFO","ts":"2025-12-06T02:54:25.184Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-12-06T02:54:25.184Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-12-06T02:54:25.192Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dczzts, usage=0.242 (24.2%)"}
{"level":"INFO","ts":"2025-12-06T02:54:25.192Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984ds7q94, usage=0.185 (18.5%)"}
{"level":"INFO","ts":"2025-12-06T02:54:25.192Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dczzts, queueLength=0"}
{"level":"DEBUG","ts":"2025-12-06T02:54:25.192Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=2"}
{"level":"INFO","ts":"2025-12-06T02:54:25.192Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984ds7q94, queueLength=0"}
{"level":"DEBUG","ts":"2025-12-06T02:54:25.192Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=2"}
{"level":"DEBUG","ts":"2025-12-06T02:54:25.195Z","msg":"Pod-to-variant matching successful: totalPods=2, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:2]"}
{"level":"DEBUG","ts":"2025-12-06T02:54:25.195Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=2"}
{"level":"DEBUG","ts":"2025-12-06T02:54:25.195Z","msg":"Collected capacity metrics for observability in model-only mode: modelID=unsloth/Meta-Llama-3.1-8B, replicaCount=2"}
{"level":"INFO","ts":"2025-12-06T02:54:25.195Z","msg":"Found SLO for model: model=unsloth/Meta-Llama-3.1-8B, class=Premium, slo-tpot=10, slo-ttft=1000"}
{"level":"DEBUG","ts":"2025-12-06T02:54:25.205Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=6, accelerator=H100, ttft=26.86ms, itl=13.56ms, cost=600.00, maxBatch=256, arrivalRate=2098.64, avgInputTokens=240.91, avgOutputTokens=426.68"}
{"level":"DEBUG","ts":"2025-12-06T02:54:25.205Z","msg":"Experimental model tuner is enabled globally (EXPERIMENTAL_MODEL_TUNER_ENABLED=true) tuning model performance parameters for active VAs"}
{"level":"DEBUG","ts":"2025-12-06T02:54:25.205Z","msg":"Using state vals from VA status to tune variant ms-inference-scheduling-llm-d-modelservice-decode: alpha= 8.872552, beta= 0.043060, gamma= 17.018108, delta= 0.000321"}
{"level":"DEBUG","ts":"2025-12-06T02:54:25.205Z","msg":"[Tuner Config] variant=ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler | Initial state: alpha=8.872552, beta=0.043060, gamma=17.018108, delta=0.000321 | SLO targets: TTFT=1000.00ms, ITL=10.00ms | Has covariance=true"}
{"level":"WARN","ts":"2025-12-06T02:54:25.205Z","msg":"Tuner validation failed (NIS=32.64), validation error: normalized innovation squared (NIS=32.64) exceeds threshold (7.38), rejecting update as outlier - returning previous state: alpha=8.872552, beta=0.043060, gamma=17.018108, delta=0.000321"}
{"level":"WARN","ts":"2025-12-06T02:54:25.205Z","msg":"Tuner NIS validation failed for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler, server ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler (NIS=32.64 exceeds threshold 7.38) - Keeping previous state: alpha=8.872552, beta=0.043060, gamma=17.018108, delta=0.000321"}
{"level":"INFO","ts":"2025-12-06T02:54:25.205Z","msg":"Keeping previous tuned parameters for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler due to NIS validation failure (NIS=32.637395)"}
{"level":"DEBUG","ts":"2025-12-06T02:54:25.205Z","msg":"Model tuner results: model=unsloth/Meta-Llama-3.1-8B, accelerator=H100, alpha=8.872552, beta=0.043060, gamma=17.018108, delta=0.000321, NIS=32.64"}
{"level":"DEBUG","ts":"2025-12-06T02:54:25.205Z","msg":"Updated tuner status for variant ms-inference-scheduling-llm-d-modelservice-decode: alpha=8.872552, beta=0.043060, gamma=17.018108, delta=0.000321, NIS=32.637395"}
{"level":"DEBUG","ts":"2025-12-06T02:54:25.215Z","msg":"Optimization solution - system: Solution: \ns=ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler; c=Premium; m=unsloth/Meta-Llama-3.1-8B; rate=2098.64; inTk=240; outTk=426; sol=1, sat=false, alloc={acc=H100; numRep=6; maxBatch=512; cost=600, val=0, itl=9.985696, ttft=19.009668, rho=0.048537087, maxRPM=353.9436}; slo-itl=10, slo-ttft=1000, slo-tps=0 \nAllocationByType: \nname=NVIDIA-H100-80GB-HBM3, count=6, limit=0, cost=600 \ntotalCost=600 \n"}
{"level":"DEBUG","ts":"2025-12-06T02:54:25.215Z","msg":"Setting accelerator name: Name=H100, allocationData={H100 6 512 600 9.985696 19.009668 {2098.64 240 426}}"}
{"level":"INFO","ts":"2025-12-06T02:54:25.215Z","msg":"Model-based optimization completed for model: unsloth/Meta-Llama-3.1-8B - model-based targets: map[ms-inference-scheduling-llm-d-modelservice-decode:6]"}
{"level":"WARN","ts":"2025-12-06T02:54:25.215Z","msg":"saturation analysis unavailable, using model-based targets only: modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-06T02:54:25.215Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-12-06T02:54:25.215Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=6→target=6"}
{"level":"DEBUG","ts":"2025-12-06T02:54:25.215Z","msg":"Found VA in map: variant=ms-inference-scheduling-llm-d-modelservice-decode, hasCurrentAlloc=true, accelerator=H100"}
{"level":"INFO","ts":"2025-12-06T02:54:25.215Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 6, desired-replicas: 6, accelerator: H100"}
{"level":"INFO","ts":"2025-12-06T02:54:25.215Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=6, accelerator=H100, SaturationOnly=false"}
{"level":"INFO","ts":"2025-12-06T02:54:25.220Z","msg":"Applied Saturation decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=6, target=6, reason=model-based only (Saturation unavailable)"}
{"level":"INFO","ts":"2025-12-06T02:54:25.220Z","msg":"Reconciliation completed successfully: mode=model-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-12-06T02:54:25.302Z","msg":"saturation scaling config cache updated: entries=1, has_default=true"}
{"level":"INFO","ts":"2025-12-06T02:54:25.302Z","msg":"saturation scaling configuration loaded successfully"}
{"level":"INFO","ts":"2025-12-06T02:54:25.302Z","msg":"Operating in MODEL-ONLY mode: model-based optimization only"}
{"level":"INFO","ts":"2025-12-06T02:54:25.302Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-12-06T02:54:25.302Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-12-06T02:54:25.305Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dczzts, queueLength=0"}
{"level":"INFO","ts":"2025-12-06T02:54:25.305Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984ds7q94, queueLength=0"}
{"level":"DEBUG","ts":"2025-12-06T02:54:25.305Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=2"}
{"level":"INFO","ts":"2025-12-06T02:54:25.305Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dczzts, usage=0.242 (24.2%)"}
{"level":"INFO","ts":"2025-12-06T02:54:25.305Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984ds7q94, usage=0.185 (18.5%)"}
{"level":"DEBUG","ts":"2025-12-06T02:54:25.305Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=2"}
{"level":"DEBUG","ts":"2025-12-06T02:54:25.308Z","msg":"Pod-to-variant matching successful: totalPods=2, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:2]"}
{"level":"DEBUG","ts":"2025-12-06T02:54:25.308Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=2"}
{"level":"DEBUG","ts":"2025-12-06T02:54:25.308Z","msg":"Collected capacity metrics for observability in model-only mode: modelID=unsloth/Meta-Llama-3.1-8B, replicaCount=2"}
{"level":"INFO","ts":"2025-12-06T02:54:25.308Z","msg":"Found SLO for model: model=unsloth/Meta-Llama-3.1-8B, class=Premium, slo-tpot=10, slo-ttft=1000"}
{"level":"DEBUG","ts":"2025-12-06T02:54:25.319Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=6, accelerator=H100, ttft=26.86ms, itl=13.56ms, cost=600.00, maxBatch=256, arrivalRate=2098.64, avgInputTokens=240.91, avgOutputTokens=426.68"}
{"level":"DEBUG","ts":"2025-12-06T02:54:25.319Z","msg":"Experimental model tuner is enabled globally (EXPERIMENTAL_MODEL_TUNER_ENABLED=true) tuning model performance parameters for active VAs"}
{"level":"DEBUG","ts":"2025-12-06T02:54:25.319Z","msg":"Using state vals from VA status to tune variant ms-inference-scheduling-llm-d-modelservice-decode: alpha= 8.872552, beta= 0.043060, gamma= 17.018108, delta= 0.000321"}
{"level":"DEBUG","ts":"2025-12-06T02:54:25.319Z","msg":"[Tuner Config] variant=ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler | Initial state: alpha=8.872552, beta=0.043060, gamma=17.018108, delta=0.000321 | SLO targets: TTFT=1000.00ms, ITL=10.00ms | Has covariance=true"}
{"level":"WARN","ts":"2025-12-06T02:54:25.320Z","msg":"Tuner validation failed (NIS=32.64), validation error: normalized innovation squared (NIS=32.64) exceeds threshold (7.38), rejecting update as outlier - returning previous state: alpha=8.872552, beta=0.043060, gamma=17.018108, delta=0.000321"}
{"level":"WARN","ts":"2025-12-06T02:54:25.320Z","msg":"Tuner NIS validation failed for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler, server ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler (NIS=32.64 exceeds threshold 7.38) - Keeping previous state: alpha=8.872552, beta=0.043060, gamma=17.018108, delta=0.000321"}
{"level":"INFO","ts":"2025-12-06T02:54:25.320Z","msg":"Keeping previous tuned parameters for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler due to NIS validation failure (NIS=32.637395)"}
{"level":"DEBUG","ts":"2025-12-06T02:54:25.320Z","msg":"Model tuner results: model=unsloth/Meta-Llama-3.1-8B, accelerator=H100, alpha=8.872552, beta=0.043060, gamma=17.018108, delta=0.000321, NIS=32.64"}
{"level":"DEBUG","ts":"2025-12-06T02:54:25.320Z","msg":"Tuned parameters unchanged for variant ms-inference-scheduling-llm-d-modelservice-decode, skipping status update"}
{"level":"DEBUG","ts":"2025-12-06T02:54:25.330Z","msg":"Optimization solution - system: Solution: \ns=ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler; c=Premium; m=unsloth/Meta-Llama-3.1-8B; rate=2098.64; inTk=240; outTk=426; sol=1, sat=false, alloc={acc=H100; numRep=6; maxBatch=512; cost=600, val=0, itl=9.985696, ttft=19.009668, rho=0.048537087, maxRPM=353.9436}; slo-itl=10, slo-ttft=1000, slo-tps=0 \nAllocationByType: \nname=NVIDIA-H100-80GB-HBM3, count=6, limit=0, cost=600 \ntotalCost=600 \n"}
{"level":"DEBUG","ts":"2025-12-06T02:54:25.330Z","msg":"Setting accelerator name: Name=H100, allocationData={H100 6 512 600 9.985696 19.009668 {2098.64 240 426}}"}
{"level":"INFO","ts":"2025-12-06T02:54:25.330Z","msg":"Model-based optimization completed for model: unsloth/Meta-Llama-3.1-8B - model-based targets: map[ms-inference-scheduling-llm-d-modelservice-decode:6]"}
{"level":"WARN","ts":"2025-12-06T02:54:25.330Z","msg":"saturation analysis unavailable, using model-based targets only: modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-06T02:54:25.330Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-12-06T02:54:25.330Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=6→target=6"}
{"level":"DEBUG","ts":"2025-12-06T02:54:25.330Z","msg":"Found VA in map: variant=ms-inference-scheduling-llm-d-modelservice-decode, hasCurrentAlloc=true, accelerator=H100"}
{"level":"INFO","ts":"2025-12-06T02:54:25.330Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 6, desired-replicas: 6, accelerator: H100"}
{"level":"INFO","ts":"2025-12-06T02:54:25.330Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=6, accelerator=H100, SaturationOnly=false"}
{"level":"INFO","ts":"2025-12-06T02:54:25.336Z","msg":"Applied Saturation decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=6, target=6, reason=model-based only (Saturation unavailable)"}
{"level":"INFO","ts":"2025-12-06T02:54:25.336Z","msg":"Reconciliation completed successfully: mode=model-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-12-06T02:55:25.221Z","msg":"saturation scaling config cache updated: entries=1, has_default=true"}
{"level":"INFO","ts":"2025-12-06T02:55:25.221Z","msg":"saturation scaling configuration loaded successfully"}
{"level":"INFO","ts":"2025-12-06T02:55:25.221Z","msg":"Operating in MODEL-ONLY mode: model-based optimization only"}
{"level":"INFO","ts":"2025-12-06T02:55:25.221Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-12-06T02:55:25.221Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-12-06T02:55:25.225Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dgzznw, queueLength=0"}
{"level":"INFO","ts":"2025-12-06T02:55:25.225Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dczzts, queueLength=0"}
{"level":"INFO","ts":"2025-12-06T02:55:25.225Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984ds7q94, queueLength=0"}
{"level":"DEBUG","ts":"2025-12-06T02:55:25.225Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=3"}
{"level":"INFO","ts":"2025-12-06T02:55:25.225Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dgzznw, usage=0.037 (3.7%)"}
{"level":"INFO","ts":"2025-12-06T02:55:25.225Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dczzts, usage=0.134 (13.4%)"}
{"level":"INFO","ts":"2025-12-06T02:55:25.225Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984ds7q94, usage=0.114 (11.4%)"}
{"level":"DEBUG","ts":"2025-12-06T02:55:25.225Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=3"}
{"level":"DEBUG","ts":"2025-12-06T02:55:25.228Z","msg":"Pod-to-variant matching successful: totalPods=3, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:3]"}
{"level":"DEBUG","ts":"2025-12-06T02:55:25.228Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=3"}
{"level":"DEBUG","ts":"2025-12-06T02:55:25.228Z","msg":"Collected capacity metrics for observability in model-only mode: modelID=unsloth/Meta-Llama-3.1-8B, replicaCount=3"}
{"level":"INFO","ts":"2025-12-06T02:55:25.228Z","msg":"Found SLO for model: model=unsloth/Meta-Llama-3.1-8B, class=Premium, slo-tpot=10, slo-ttft=1000"}
{"level":"DEBUG","ts":"2025-12-06T02:55:25.237Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=6, accelerator=H100, ttft=19.93ms, itl=9.95ms, cost=600.00, maxBatch=256, arrivalRate=1928.16, avgInputTokens=219.45, avgOutputTokens=481.17"}
{"level":"DEBUG","ts":"2025-12-06T02:55:25.237Z","msg":"Experimental model tuner is enabled globally (EXPERIMENTAL_MODEL_TUNER_ENABLED=true) tuning model performance parameters for active VAs"}
{"level":"DEBUG","ts":"2025-12-06T02:55:25.237Z","msg":"Using state vals from VA status to tune variant ms-inference-scheduling-llm-d-modelservice-decode: alpha= 8.872552, beta= 0.043060, gamma= 17.018108, delta= 0.000321"}
{"level":"DEBUG","ts":"2025-12-06T02:55:25.237Z","msg":"[Tuner Config] variant=ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler | Initial state: alpha=8.872552, beta=0.043060, gamma=17.018108, delta=0.000321 | SLO targets: TTFT=1000.00ms, ITL=10.00ms | Has covariance=true"}
{"level":"INFO","ts":"2025-12-06T02:55:25.238Z","msg":"Tuner validation succeeded for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler, server ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler - New NIS: 0.018032"}
{"level":"DEBUG","ts":"2025-12-06T02:55:25.238Z","msg":"Model tuner results: model=unsloth/Meta-Llama-3.1-8B, accelerator=H100, alpha=8.807188, beta=0.043274, gamma=17.046793, delta=0.000321, NIS=0.02"}
{"level":"DEBUG","ts":"2025-12-06T02:55:25.238Z","msg":"Updated tuner status for variant ms-inference-scheduling-llm-d-modelservice-decode: alpha=8.807188, beta=0.043274, gamma=17.046793, delta=0.000321, NIS=0.018032"}
{"level":"INFO","ts":"2025-12-06T02:55:25.238Z","msg":"Tuned performance parameters result: variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler - alpha: 8.807188, beta: 0.043274, gamma: 17.046793, delta: 0.000321"}
{"level":"DEBUG","ts":"2025-12-06T02:55:25.247Z","msg":"Optimization solution - system: Solution: \ns=ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler; c=Premium; m=unsloth/Meta-Llama-3.1-8B; rate=1928.16; inTk=219; outTk=481; sol=1, sat=false, alloc={acc=H100; numRep=6; maxBatch=512; cost=600, val=0, itl=9.963282, ttft=18.924898, rho=0.050226104, maxRPM=330.74564}; slo-itl=10, slo-ttft=1000, slo-tps=0 \nAllocationByType: \nname=NVIDIA-H100-80GB-HBM3, count=6, limit=0, cost=600 \ntotalCost=600 \n"}
{"level":"DEBUG","ts":"2025-12-06T02:55:25.247Z","msg":"Setting accelerator name: Name=H100, allocationData={H100 6 512 600 9.963282 18.924898 {1928.16 219 481}}"}
{"level":"INFO","ts":"2025-12-06T02:55:25.247Z","msg":"Model-based optimization completed for model: unsloth/Meta-Llama-3.1-8B - model-based targets: map[ms-inference-scheduling-llm-d-modelservice-decode:6]"}
{"level":"WARN","ts":"2025-12-06T02:55:25.247Z","msg":"saturation analysis unavailable, using model-based targets only: modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-06T02:55:25.247Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-12-06T02:55:25.247Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=6→target=6"}
{"level":"DEBUG","ts":"2025-12-06T02:55:25.247Z","msg":"Found VA in map: variant=ms-inference-scheduling-llm-d-modelservice-decode, hasCurrentAlloc=true, accelerator=H100"}
{"level":"INFO","ts":"2025-12-06T02:55:25.247Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 6, desired-replicas: 6, accelerator: H100"}
{"level":"INFO","ts":"2025-12-06T02:55:25.247Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=6, accelerator=H100, SaturationOnly=false"}
{"level":"INFO","ts":"2025-12-06T02:55:25.253Z","msg":"Applied Saturation decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=6, target=6, reason=model-based only (Saturation unavailable)"}
{"level":"INFO","ts":"2025-12-06T02:55:25.253Z","msg":"Reconciliation completed successfully: mode=model-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-12-06T02:55:25.337Z","msg":"saturation scaling config cache updated: entries=1, has_default=true"}
{"level":"INFO","ts":"2025-12-06T02:55:25.337Z","msg":"saturation scaling configuration loaded successfully"}
{"level":"INFO","ts":"2025-12-06T02:55:25.337Z","msg":"Operating in MODEL-ONLY mode: model-based optimization only"}
{"level":"INFO","ts":"2025-12-06T02:55:25.337Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-12-06T02:55:25.337Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-12-06T02:55:25.339Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dgzznw, queueLength=0"}
{"level":"INFO","ts":"2025-12-06T02:55:25.339Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dczzts, queueLength=0"}
{"level":"INFO","ts":"2025-12-06T02:55:25.339Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984ds7q94, queueLength=0"}
{"level":"DEBUG","ts":"2025-12-06T02:55:25.339Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=3"}
{"level":"INFO","ts":"2025-12-06T02:55:25.339Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dgzznw, usage=0.037 (3.7%)"}
{"level":"INFO","ts":"2025-12-06T02:55:25.339Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dczzts, usage=0.134 (13.4%)"}
{"level":"INFO","ts":"2025-12-06T02:55:25.339Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984ds7q94, usage=0.114 (11.4%)"}
{"level":"DEBUG","ts":"2025-12-06T02:55:25.339Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=3"}
{"level":"DEBUG","ts":"2025-12-06T02:55:25.342Z","msg":"Pod-to-variant matching successful: totalPods=3, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:3]"}
{"level":"DEBUG","ts":"2025-12-06T02:55:25.342Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=3"}
{"level":"DEBUG","ts":"2025-12-06T02:55:25.342Z","msg":"Collected capacity metrics for observability in model-only mode: modelID=unsloth/Meta-Llama-3.1-8B, replicaCount=3"}
{"level":"INFO","ts":"2025-12-06T02:55:25.342Z","msg":"Found SLO for model: model=unsloth/Meta-Llama-3.1-8B, class=Premium, slo-tpot=10, slo-ttft=1000"}
{"level":"DEBUG","ts":"2025-12-06T02:55:25.351Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=6, accelerator=H100, ttft=19.93ms, itl=9.95ms, cost=600.00, maxBatch=256, arrivalRate=1928.96, avgInputTokens=219.45, avgOutputTokens=481.15"}
{"level":"DEBUG","ts":"2025-12-06T02:55:25.351Z","msg":"Experimental model tuner is enabled globally (EXPERIMENTAL_MODEL_TUNER_ENABLED=true) tuning model performance parameters for active VAs"}
{"level":"DEBUG","ts":"2025-12-06T02:55:25.351Z","msg":"Using state vals from VA status to tune variant ms-inference-scheduling-llm-d-modelservice-decode: alpha= 8.807188, beta= 0.043274, gamma= 17.046793, delta= 0.000321"}
{"level":"DEBUG","ts":"2025-12-06T02:55:25.351Z","msg":"[Tuner Config] variant=ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler | Initial state: alpha=8.807188, beta=0.043274, gamma=17.046793, delta=0.000321 | SLO targets: TTFT=1000.00ms, ITL=10.00ms | Has covariance=true"}
{"level":"INFO","ts":"2025-12-06T02:55:25.351Z","msg":"Tuner validation succeeded for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler, server ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler - New NIS: 0.002029"}
{"level":"DEBUG","ts":"2025-12-06T02:55:25.351Z","msg":"Model tuner results: model=unsloth/Meta-Llama-3.1-8B, accelerator=H100, alpha=8.797053, beta=0.043275, gamma=17.074556, delta=0.000321, NIS=0.00"}
{"level":"DEBUG","ts":"2025-12-06T02:55:25.351Z","msg":"Updated tuner status for variant ms-inference-scheduling-llm-d-modelservice-decode: alpha=8.797053, beta=0.043275, gamma=17.074556, delta=0.000321, NIS=0.002029"}
{"level":"INFO","ts":"2025-12-06T02:55:25.351Z","msg":"Tuned performance parameters result: variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler - alpha: 8.797053, beta: 0.043275, gamma: 17.074556, delta: 0.000321"}
{"level":"DEBUG","ts":"2025-12-06T02:55:25.360Z","msg":"Optimization solution - system: Solution: \ns=ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler; c=Premium; m=unsloth/Meta-Llama-3.1-8B; rate=1928.96; inTk=219; outTk=481; sol=1, sat=false, alloc={acc=H100; numRep=6; maxBatch=512; cost=600, val=0, itl=9.952447, ttft=18.951462, rho=0.050192792, maxRPM=333.64417}; slo-itl=10, slo-ttft=1000, slo-tps=0 \nAllocationByType: \nname=NVIDIA-H100-80GB-HBM3, count=6, limit=0, cost=600 \ntotalCost=600 \n"}
{"level":"DEBUG","ts":"2025-12-06T02:55:25.360Z","msg":"Setting accelerator name: Name=H100, allocationData={H100 6 512 600 9.952447 18.951462 {1928.96 219 481}}"}
{"level":"INFO","ts":"2025-12-06T02:55:25.360Z","msg":"Model-based optimization completed for model: unsloth/Meta-Llama-3.1-8B - model-based targets: map[ms-inference-scheduling-llm-d-modelservice-decode:6]"}
{"level":"WARN","ts":"2025-12-06T02:55:25.360Z","msg":"saturation analysis unavailable, using model-based targets only: modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-06T02:55:25.360Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-12-06T02:55:25.360Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=6→target=6"}
{"level":"DEBUG","ts":"2025-12-06T02:55:25.360Z","msg":"Found VA in map: variant=ms-inference-scheduling-llm-d-modelservice-decode, hasCurrentAlloc=true, accelerator=H100"}
{"level":"INFO","ts":"2025-12-06T02:55:25.360Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 6, desired-replicas: 6, accelerator: H100"}
{"level":"INFO","ts":"2025-12-06T02:55:25.360Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=6, accelerator=H100, SaturationOnly=false"}
{"level":"INFO","ts":"2025-12-06T02:55:25.367Z","msg":"Applied Saturation decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=6, target=6, reason=model-based only (Saturation unavailable)"}
{"level":"INFO","ts":"2025-12-06T02:55:25.367Z","msg":"Reconciliation completed successfully: mode=model-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-12-06T02:56:25.253Z","msg":"saturation scaling config cache updated: entries=1, has_default=true"}
{"level":"INFO","ts":"2025-12-06T02:56:25.253Z","msg":"saturation scaling configuration loaded successfully"}
{"level":"INFO","ts":"2025-12-06T02:56:25.253Z","msg":"Operating in MODEL-ONLY mode: model-based optimization only"}
{"level":"INFO","ts":"2025-12-06T02:56:25.253Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-12-06T02:56:25.254Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-12-06T02:56:25.266Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dgzznw, usage=0.051 (5.1%)"}
{"level":"INFO","ts":"2025-12-06T02:56:25.266Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984d5fntr, usage=0.001 (0.1%)"}
{"level":"INFO","ts":"2025-12-06T02:56:25.266Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984d2cbwp, usage=0.020 (2.0%)"}
{"level":"INFO","ts":"2025-12-06T02:56:25.266Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dczzts, usage=0.075 (7.5%)"}
{"level":"INFO","ts":"2025-12-06T02:56:25.266Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984ds7q94, usage=0.081 (8.1%)"}
{"level":"INFO","ts":"2025-12-06T02:56:25.266Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984d9wc6l, usage=0.024 (2.4%)"}
{"level":"DEBUG","ts":"2025-12-06T02:56:25.266Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=6"}
{"level":"INFO","ts":"2025-12-06T02:56:25.266Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dgzznw, queueLength=0"}
{"level":"INFO","ts":"2025-12-06T02:56:25.266Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984d5fntr, queueLength=0"}
{"level":"INFO","ts":"2025-12-06T02:56:25.266Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984d2cbwp, queueLength=0"}
{"level":"INFO","ts":"2025-12-06T02:56:25.266Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dczzts, queueLength=0"}
{"level":"INFO","ts":"2025-12-06T02:56:25.266Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984ds7q94, queueLength=0"}
{"level":"INFO","ts":"2025-12-06T02:56:25.266Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984d9wc6l, queueLength=0"}
{"level":"DEBUG","ts":"2025-12-06T02:56:25.266Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=6"}
{"level":"DEBUG","ts":"2025-12-06T02:56:25.269Z","msg":"Pod-to-variant matching successful: totalPods=6, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:6]"}
{"level":"DEBUG","ts":"2025-12-06T02:56:25.269Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=6"}
{"level":"DEBUG","ts":"2025-12-06T02:56:25.269Z","msg":"Collected capacity metrics for observability in model-only mode: modelID=unsloth/Meta-Llama-3.1-8B, replicaCount=6"}
{"level":"INFO","ts":"2025-12-06T02:56:25.269Z","msg":"Found SLO for model: model=unsloth/Meta-Llama-3.1-8B, class=Premium, slo-tpot=10, slo-ttft=1000"}
{"level":"DEBUG","ts":"2025-12-06T02:56:25.279Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=6, accelerator=H100, ttft=18.97ms, itl=9.41ms, cost=600.00, maxBatch=256, arrivalRate=1987.24, avgInputTokens=229.79, avgOutputTokens=466.76"}
{"level":"DEBUG","ts":"2025-12-06T02:56:25.279Z","msg":"Experimental model tuner is enabled globally (EXPERIMENTAL_MODEL_TUNER_ENABLED=true) tuning model performance parameters for active VAs"}
{"level":"DEBUG","ts":"2025-12-06T02:56:25.279Z","msg":"Using state vals from VA status to tune variant ms-inference-scheduling-llm-d-modelservice-decode: alpha= 8.797053, beta= 0.043275, gamma= 17.074556, delta= 0.000321"}
{"level":"DEBUG","ts":"2025-12-06T02:56:25.279Z","msg":"[Tuner Config] variant=ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler | Initial state: alpha=8.797053, beta=0.043275, gamma=17.074556, delta=0.000321 | SLO targets: TTFT=1000.00ms, ITL=10.00ms | Has covariance=true"}
{"level":"INFO","ts":"2025-12-06T02:56:25.280Z","msg":"Tuner validation succeeded for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler, server ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler - New NIS: 0.792780"}
{"level":"DEBUG","ts":"2025-12-06T02:56:25.280Z","msg":"Model tuner results: model=unsloth/Meta-Llama-3.1-8B, accelerator=H100, alpha=8.407248, beta=0.043072, gamma=17.075220, delta=0.000321, NIS=0.79"}
{"level":"DEBUG","ts":"2025-12-06T02:56:25.280Z","msg":"Updated tuner status for variant ms-inference-scheduling-llm-d-modelservice-decode: alpha=8.407248, beta=0.043072, gamma=17.075220, delta=0.000321, NIS=0.792780"}
{"level":"INFO","ts":"2025-12-06T02:56:25.280Z","msg":"Tuned performance parameters result: variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler - alpha: 8.407248, beta: 0.043072, gamma: 17.075220, delta: 0.000321"}
{"level":"DEBUG","ts":"2025-12-06T02:56:25.289Z","msg":"Optimization solution - system: Solution: \ns=ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler; c=Premium; m=unsloth/Meta-Llama-3.1-8B; rate=1987.24; inTk=229; outTk=466; sol=1, sat=false, alloc={acc=H100; numRep=5; maxBatch=512; cost=500, val=-100, itl=9.749295, ttft=19.365631, rho=0.05890288, maxRPM=462.27795}; slo-itl=10, slo-ttft=1000, slo-tps=0 \nAllocationByType: \nname=NVIDIA-H100-80GB-HBM3, count=5, limit=0, cost=500 \ntotalCost=500 \n"}
{"level":"DEBUG","ts":"2025-12-06T02:56:25.289Z","msg":"Setting accelerator name: Name=H100, allocationData={H100 5 512 500 9.749295 19.365631 {1987.24 229 466}}"}
{"level":"INFO","ts":"2025-12-06T02:56:25.289Z","msg":"Model-based optimization completed for model: unsloth/Meta-Llama-3.1-8B - model-based targets: map[ms-inference-scheduling-llm-d-modelservice-decode:5]"}
{"level":"WARN","ts":"2025-12-06T02:56:25.289Z","msg":"saturation analysis unavailable, using model-based targets only: modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-06T02:56:25.289Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-12-06T02:56:25.289Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=scale-down, current=6→target=5"}
{"level":"DEBUG","ts":"2025-12-06T02:56:25.289Z","msg":"Found VA in map: variant=ms-inference-scheduling-llm-d-modelservice-decode, hasCurrentAlloc=true, accelerator=H100"}
{"level":"INFO","ts":"2025-12-06T02:56:25.289Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 6, desired-replicas: 5, accelerator: H100"}
{"level":"INFO","ts":"2025-12-06T02:56:25.289Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=5, accelerator=H100, SaturationOnly=false"}
{"level":"INFO","ts":"2025-12-06T02:56:25.295Z","msg":"Applied Saturation decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=scale-down, current=6, target=5, reason=model-based only (Saturation unavailable)"}
{"level":"INFO","ts":"2025-12-06T02:56:25.295Z","msg":"Reconciliation completed successfully: mode=model-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-12-06T02:56:25.367Z","msg":"saturation scaling config cache updated: entries=1, has_default=true"}
{"level":"INFO","ts":"2025-12-06T02:56:25.367Z","msg":"saturation scaling configuration loaded successfully"}
{"level":"INFO","ts":"2025-12-06T02:56:25.367Z","msg":"Operating in MODEL-ONLY mode: model-based optimization only"}
{"level":"INFO","ts":"2025-12-06T02:56:25.367Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-12-06T02:56:25.367Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-12-06T02:56:25.370Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dgzznw, usage=0.051 (5.1%)"}
{"level":"INFO","ts":"2025-12-06T02:56:25.370Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984d5fntr, usage=0.001 (0.1%)"}
{"level":"INFO","ts":"2025-12-06T02:56:25.370Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984d2cbwp, usage=0.020 (2.0%)"}
{"level":"INFO","ts":"2025-12-06T02:56:25.370Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dczzts, usage=0.075 (7.5%)"}
{"level":"INFO","ts":"2025-12-06T02:56:25.370Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984ds7q94, usage=0.081 (8.1%)"}
{"level":"INFO","ts":"2025-12-06T02:56:25.370Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984d9wc6l, usage=0.024 (2.4%)"}
{"level":"DEBUG","ts":"2025-12-06T02:56:25.370Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=6"}
{"level":"INFO","ts":"2025-12-06T02:56:25.370Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dgzznw, queueLength=0"}
{"level":"INFO","ts":"2025-12-06T02:56:25.370Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984d5fntr, queueLength=0"}
{"level":"INFO","ts":"2025-12-06T02:56:25.370Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984d2cbwp, queueLength=0"}
{"level":"INFO","ts":"2025-12-06T02:56:25.370Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dczzts, queueLength=0"}
{"level":"INFO","ts":"2025-12-06T02:56:25.370Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984ds7q94, queueLength=0"}
{"level":"INFO","ts":"2025-12-06T02:56:25.370Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984d9wc6l, queueLength=0"}
{"level":"DEBUG","ts":"2025-12-06T02:56:25.370Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=6"}
{"level":"DEBUG","ts":"2025-12-06T02:56:25.372Z","msg":"Pod-to-variant matching successful: totalPods=6, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:6]"}
{"level":"DEBUG","ts":"2025-12-06T02:56:25.372Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=6"}
{"level":"DEBUG","ts":"2025-12-06T02:56:25.372Z","msg":"Collected capacity metrics for observability in model-only mode: modelID=unsloth/Meta-Llama-3.1-8B, replicaCount=6"}
{"level":"INFO","ts":"2025-12-06T02:56:25.373Z","msg":"Found SLO for model: model=unsloth/Meta-Llama-3.1-8B, class=Premium, slo-tpot=10, slo-ttft=1000"}
{"level":"DEBUG","ts":"2025-12-06T02:56:25.382Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=6, accelerator=H100, ttft=18.97ms, itl=9.41ms, cost=600.00, maxBatch=256, arrivalRate=1987.86, avgInputTokens=229.81, avgOutputTokens=466.71"}
{"level":"DEBUG","ts":"2025-12-06T02:56:25.382Z","msg":"Experimental model tuner is enabled globally (EXPERIMENTAL_MODEL_TUNER_ENABLED=true) tuning model performance parameters for active VAs"}
{"level":"DEBUG","ts":"2025-12-06T02:56:25.382Z","msg":"Using state vals from VA status to tune variant ms-inference-scheduling-llm-d-modelservice-decode: alpha= 8.407248, beta= 0.043072, gamma= 17.075220, delta= 0.000321"}
{"level":"DEBUG","ts":"2025-12-06T02:56:25.382Z","msg":"[Tuner Config] variant=ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler | Initial state: alpha=8.407248, beta=0.043072, gamma=17.075220, delta=0.000321 | SLO targets: TTFT=1000.00ms, ITL=10.00ms | Has covariance=true"}
{"level":"INFO","ts":"2025-12-06T02:56:25.383Z","msg":"Tuner validation succeeded for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler, server ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler - New NIS: 0.026710"}
{"level":"DEBUG","ts":"2025-12-06T02:56:25.383Z","msg":"Model tuner results: model=unsloth/Meta-Llama-3.1-8B, accelerator=H100, alpha=8.338484, beta=0.043046, gamma=17.076180, delta=0.000321, NIS=0.03"}
{"level":"DEBUG","ts":"2025-12-06T02:56:25.383Z","msg":"Updated tuner status for variant ms-inference-scheduling-llm-d-modelservice-decode: alpha=8.338484, beta=0.043046, gamma=17.076180, delta=0.000321, NIS=0.026710"}
{"level":"INFO","ts":"2025-12-06T02:56:25.383Z","msg":"Tuned performance parameters result: variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler - alpha: 8.338484, beta: 0.043046, gamma: 17.076180, delta: 0.000321"}
{"level":"DEBUG","ts":"2025-12-06T02:56:25.387Z","msg":"Optimization solution - system: Solution: \ns=ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler; c=Premium; m=unsloth/Meta-Llama-3.1-8B; rate=1987.86; inTk=229; outTk=466; sol=1, sat=false, alloc={acc=H100; numRep=5; maxBatch=512; cost=500, val=-100, itl=9.669537, ttft=19.34921, rho=0.058441065, maxRPM=483.07715}; slo-itl=10, slo-ttft=1000, slo-tps=0 \nAllocationByType: \nname=NVIDIA-H100-80GB-HBM3, count=5, limit=0, cost=500 \ntotalCost=500 \n"}
{"level":"DEBUG","ts":"2025-12-06T02:56:25.387Z","msg":"Setting accelerator name: Name=H100, allocationData={H100 5 512 500 9.669537 19.34921 {1987.86 229 466}}"}
{"level":"INFO","ts":"2025-12-06T02:56:25.387Z","msg":"Model-based optimization completed for model: unsloth/Meta-Llama-3.1-8B - model-based targets: map[ms-inference-scheduling-llm-d-modelservice-decode:5]"}
{"level":"WARN","ts":"2025-12-06T02:56:25.387Z","msg":"saturation analysis unavailable, using model-based targets only: modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-06T02:56:25.387Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-12-06T02:56:25.387Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=scale-down, current=6→target=5"}
{"level":"DEBUG","ts":"2025-12-06T02:56:25.387Z","msg":"Found VA in map: variant=ms-inference-scheduling-llm-d-modelservice-decode, hasCurrentAlloc=true, accelerator=H100"}
{"level":"INFO","ts":"2025-12-06T02:56:25.387Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 6, desired-replicas: 5, accelerator: H100"}
{"level":"INFO","ts":"2025-12-06T02:56:25.387Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=5, accelerator=H100, SaturationOnly=false"}
{"level":"INFO","ts":"2025-12-06T02:56:25.394Z","msg":"Applied Saturation decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=scale-down, current=6, target=5, reason=model-based only (Saturation unavailable)"}
{"level":"INFO","ts":"2025-12-06T02:56:25.394Z","msg":"Reconciliation completed successfully: mode=model-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-12-06T02:57:25.296Z","msg":"saturation scaling config cache updated: entries=1, has_default=true"}
{"level":"INFO","ts":"2025-12-06T02:57:25.296Z","msg":"saturation scaling configuration loaded successfully"}
{"level":"INFO","ts":"2025-12-06T02:57:25.296Z","msg":"Operating in MODEL-ONLY mode: model-based optimization only"}
{"level":"INFO","ts":"2025-12-06T02:57:25.296Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-12-06T02:57:25.296Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-12-06T02:57:25.301Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dgzznw, usage=0.040 (4.0%)"}
{"level":"INFO","ts":"2025-12-06T02:57:25.301Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984d5fntr, usage=0.031 (3.1%)"}
{"level":"INFO","ts":"2025-12-06T02:57:25.301Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dgzznw, queueLength=0"}
{"level":"INFO","ts":"2025-12-06T02:57:25.301Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984d5fntr, queueLength=0"}
{"level":"INFO","ts":"2025-12-06T02:57:25.301Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984d2cbwp, queueLength=0"}
{"level":"INFO","ts":"2025-12-06T02:57:25.301Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984d2cbwp, usage=0.035 (3.5%)"}
{"level":"INFO","ts":"2025-12-06T02:57:25.301Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dczzts, queueLength=0"}
{"level":"INFO","ts":"2025-12-06T02:57:25.301Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dczzts, usage=0.042 (4.2%)"}
{"level":"INFO","ts":"2025-12-06T02:57:25.301Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984ds7q94, usage=0.032 (3.2%)"}
{"level":"DEBUG","ts":"2025-12-06T02:57:25.301Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=5"}
{"level":"INFO","ts":"2025-12-06T02:57:25.301Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984ds7q94, queueLength=0"}
{"level":"DEBUG","ts":"2025-12-06T02:57:25.301Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=5"}
{"level":"DEBUG","ts":"2025-12-06T02:57:25.304Z","msg":"Pod-to-variant matching successful: totalPods=5, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:5]"}
{"level":"DEBUG","ts":"2025-12-06T02:57:25.304Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=5"}
{"level":"DEBUG","ts":"2025-12-06T02:57:25.304Z","msg":"Collected capacity metrics for observability in model-only mode: modelID=unsloth/Meta-Llama-3.1-8B, replicaCount=5"}
{"level":"INFO","ts":"2025-12-06T02:57:25.304Z","msg":"Found SLO for model: model=unsloth/Meta-Llama-3.1-8B, class=Premium, slo-tpot=10, slo-ttft=1000"}
{"level":"DEBUG","ts":"2025-12-06T02:57:25.313Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=5, accelerator=H100, ttft=17.36ms, itl=8.22ms, cost=500.00, maxBatch=256, arrivalRate=1832.63, avgInputTokens=237.40, avgOutputTokens=448.08"}
{"level":"DEBUG","ts":"2025-12-06T02:57:25.313Z","msg":"Experimental model tuner is enabled globally (EXPERIMENTAL_MODEL_TUNER_ENABLED=true) tuning model performance parameters for active VAs"}
{"level":"DEBUG","ts":"2025-12-06T02:57:25.313Z","msg":"Using state vals from VA status to tune variant ms-inference-scheduling-llm-d-modelservice-decode: alpha= 8.338484, beta= 0.043046, gamma= 17.076180, delta= 0.000321"}
{"level":"DEBUG","ts":"2025-12-06T02:57:25.313Z","msg":"[Tuner Config] variant=ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler | Initial state: alpha=8.338484, beta=0.043046, gamma=17.076180, delta=0.000321 | SLO targets: TTFT=1000.00ms, ITL=10.00ms | Has covariance=true"}
{"level":"INFO","ts":"2025-12-06T02:57:25.314Z","msg":"Tuner validation succeeded for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler, server ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler - New NIS: 4.752791"}
{"level":"DEBUG","ts":"2025-12-06T02:57:25.314Z","msg":"Model tuner results: model=unsloth/Meta-Llama-3.1-8B, accelerator=H100, alpha=7.446451, beta=0.042004, gamma=17.030674, delta=0.000321, NIS=4.75"}
{"level":"DEBUG","ts":"2025-12-06T02:57:25.314Z","msg":"Updated tuner status for variant ms-inference-scheduling-llm-d-modelservice-decode: alpha=7.446451, beta=0.042004, gamma=17.030674, delta=0.000321, NIS=4.752791"}
{"level":"INFO","ts":"2025-12-06T02:57:25.314Z","msg":"Tuned performance parameters result: variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler - alpha: 7.446451, beta: 0.042004, gamma: 17.030674, delta: 0.000321"}
{"level":"DEBUG","ts":"2025-12-06T02:57:25.323Z","msg":"Optimization solution - system: Solution: \ns=ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler; c=Premium; m=unsloth/Meta-Llama-3.1-8B; rate=1832.63; inTk=237; outTk=448; sol=1, sat=false, alloc={acc=H100; numRep=3; maxBatch=512; cost=300, val=-200, itl=9.269024, ttft=20.331644, rho=0.082794175, maxRPM=798.7248}; slo-itl=10, slo-ttft=1000, slo-tps=0 \nAllocationByType: \nname=NVIDIA-H100-80GB-HBM3, count=3, limit=0, cost=300 \ntotalCost=300 \n"}
{"level":"DEBUG","ts":"2025-12-06T02:57:25.323Z","msg":"Setting accelerator name: Name=H100, allocationData={H100 3 512 300 9.269024 20.331644 {1832.63 237 448}}"}
{"level":"INFO","ts":"2025-12-06T02:57:25.323Z","msg":"Model-based optimization completed for model: unsloth/Meta-Llama-3.1-8B - model-based targets: map[ms-inference-scheduling-llm-d-modelservice-decode:3]"}
{"level":"WARN","ts":"2025-12-06T02:57:25.323Z","msg":"saturation analysis unavailable, using model-based targets only: modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-06T02:57:25.323Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-12-06T02:57:25.323Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=scale-down, current=5→target=3"}
{"level":"DEBUG","ts":"2025-12-06T02:57:25.323Z","msg":"Found VA in map: variant=ms-inference-scheduling-llm-d-modelservice-decode, hasCurrentAlloc=true, accelerator=H100"}
{"level":"INFO","ts":"2025-12-06T02:57:25.323Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 5, desired-replicas: 3, accelerator: H100"}
{"level":"INFO","ts":"2025-12-06T02:57:25.323Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=3, accelerator=H100, SaturationOnly=false"}
{"level":"INFO","ts":"2025-12-06T02:57:25.328Z","msg":"Applied Saturation decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=scale-down, current=5, target=3, reason=model-based only (Saturation unavailable)"}
{"level":"INFO","ts":"2025-12-06T02:57:25.328Z","msg":"Reconciliation completed successfully: mode=model-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-12-06T02:57:25.394Z","msg":"saturation scaling config cache updated: entries=1, has_default=true"}
{"level":"INFO","ts":"2025-12-06T02:57:25.394Z","msg":"saturation scaling configuration loaded successfully"}
{"level":"INFO","ts":"2025-12-06T02:57:25.394Z","msg":"Operating in MODEL-ONLY mode: model-based optimization only"}
{"level":"INFO","ts":"2025-12-06T02:57:25.394Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-12-06T02:57:25.394Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-12-06T02:57:25.396Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dgzznw, usage=0.040 (4.0%)"}
{"level":"INFO","ts":"2025-12-06T02:57:25.396Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984d5fntr, usage=0.031 (3.1%)"}
{"level":"INFO","ts":"2025-12-06T02:57:25.396Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984d2cbwp, usage=0.035 (3.5%)"}
{"level":"INFO","ts":"2025-12-06T02:57:25.396Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dczzts, usage=0.042 (4.2%)"}
{"level":"INFO","ts":"2025-12-06T02:57:25.396Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984ds7q94, usage=0.032 (3.2%)"}
{"level":"DEBUG","ts":"2025-12-06T02:57:25.396Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=5"}
{"level":"INFO","ts":"2025-12-06T02:57:25.396Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dgzznw, queueLength=0"}
{"level":"INFO","ts":"2025-12-06T02:57:25.396Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984d5fntr, queueLength=0"}
{"level":"INFO","ts":"2025-12-06T02:57:25.396Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984d2cbwp, queueLength=0"}
{"level":"INFO","ts":"2025-12-06T02:57:25.396Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dczzts, queueLength=0"}
{"level":"INFO","ts":"2025-12-06T02:57:25.396Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984ds7q94, queueLength=0"}
{"level":"DEBUG","ts":"2025-12-06T02:57:25.396Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=5"}
{"level":"DEBUG","ts":"2025-12-06T02:57:25.398Z","msg":"Pod-to-variant matching successful: totalPods=5, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:5]"}
{"level":"DEBUG","ts":"2025-12-06T02:57:25.398Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=5"}
{"level":"DEBUG","ts":"2025-12-06T02:57:25.398Z","msg":"Collected capacity metrics for observability in model-only mode: modelID=unsloth/Meta-Llama-3.1-8B, replicaCount=5"}
{"level":"INFO","ts":"2025-12-06T02:57:25.399Z","msg":"Found SLO for model: model=unsloth/Meta-Llama-3.1-8B, class=Premium, slo-tpot=10, slo-ttft=1000"}
{"level":"DEBUG","ts":"2025-12-06T02:57:25.410Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=5, accelerator=H100, ttft=17.36ms, itl=8.22ms, cost=500.00, maxBatch=256, arrivalRate=1832.05, avgInputTokens=237.41, avgOutputTokens=448.07"}
{"level":"DEBUG","ts":"2025-12-06T02:57:25.410Z","msg":"Experimental model tuner is enabled globally (EXPERIMENTAL_MODEL_TUNER_ENABLED=true) tuning model performance parameters for active VAs"}
{"level":"DEBUG","ts":"2025-12-06T02:57:25.410Z","msg":"Using state vals from VA status to tune variant ms-inference-scheduling-llm-d-modelservice-decode: alpha= 7.446451, beta= 0.042004, gamma= 17.030674, delta= 0.000321"}
{"level":"DEBUG","ts":"2025-12-06T02:57:25.410Z","msg":"[Tuner Config] variant=ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler | Initial state: alpha=7.446451, beta=0.042004, gamma=17.030674, delta=0.000321 | SLO targets: TTFT=1000.00ms, ITL=10.00ms | Has covariance=true"}
{"level":"INFO","ts":"2025-12-06T02:57:25.410Z","msg":"Tuner validation succeeded for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler, server ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler - New NIS: 0.202744"}
{"level":"DEBUG","ts":"2025-12-06T02:57:25.410Z","msg":"Model tuner results: model=unsloth/Meta-Llama-3.1-8B, accelerator=H100, alpha=7.276710, beta=0.042049, gamma=16.988239, delta=0.000321, NIS=0.20"}
{"level":"DEBUG","ts":"2025-12-06T02:57:25.410Z","msg":"Updated tuner status for variant ms-inference-scheduling-llm-d-modelservice-decode: alpha=7.276710, beta=0.042049, gamma=16.988239, delta=0.000321, NIS=0.202744"}
{"level":"INFO","ts":"2025-12-06T02:57:25.410Z","msg":"Tuned performance parameters result: variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler - alpha: 7.276710, beta: 0.042049, gamma: 16.988239, delta: 0.000321"}
{"level":"DEBUG","ts":"2025-12-06T02:57:25.420Z","msg":"Optimization solution - system: Solution: \ns=ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler; c=Premium; m=unsloth/Meta-Llama-3.1-8B; rate=1832.05; inTk=237; outTk=448; sol=1, sat=false, alloc={acc=H100; numRep=3; maxBatch=512; cost=300, val=-200, itl=9.060793, ttft=20.216047, rho=0.080915354, maxRPM=851.7275}; slo-itl=10, slo-ttft=1000, slo-tps=0 \nAllocationByType: \nname=NVIDIA-H100-80GB-HBM3, count=3, limit=0, cost=300 \ntotalCost=300 \n"}
{"level":"DEBUG","ts":"2025-12-06T02:57:25.420Z","msg":"Setting accelerator name: Name=H100, allocationData={H100 3 512 300 9.060793 20.216047 {1832.05 237 448}}"}
{"level":"INFO","ts":"2025-12-06T02:57:25.420Z","msg":"Model-based optimization completed for model: unsloth/Meta-Llama-3.1-8B - model-based targets: map[ms-inference-scheduling-llm-d-modelservice-decode:3]"}
{"level":"WARN","ts":"2025-12-06T02:57:25.420Z","msg":"saturation analysis unavailable, using model-based targets only: modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-06T02:57:25.420Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-12-06T02:57:25.420Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=scale-down, current=5→target=3"}
{"level":"DEBUG","ts":"2025-12-06T02:57:25.420Z","msg":"Found VA in map: variant=ms-inference-scheduling-llm-d-modelservice-decode, hasCurrentAlloc=true, accelerator=H100"}
{"level":"INFO","ts":"2025-12-06T02:57:25.420Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 5, desired-replicas: 3, accelerator: H100"}
{"level":"INFO","ts":"2025-12-06T02:57:25.420Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=3, accelerator=H100, SaturationOnly=false"}
{"level":"INFO","ts":"2025-12-06T02:57:25.426Z","msg":"Applied Saturation decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=scale-down, current=5, target=3, reason=model-based only (Saturation unavailable)"}
{"level":"INFO","ts":"2025-12-06T02:57:25.426Z","msg":"Reconciliation completed successfully: mode=model-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-12-06T02:58:25.329Z","msg":"saturation scaling config cache updated: entries=1, has_default=true"}
{"level":"INFO","ts":"2025-12-06T02:58:25.329Z","msg":"saturation scaling configuration loaded successfully"}
{"level":"INFO","ts":"2025-12-06T02:58:25.329Z","msg":"Operating in MODEL-ONLY mode: model-based optimization only"}
{"level":"INFO","ts":"2025-12-06T02:58:25.329Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-12-06T02:58:25.329Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-12-06T02:58:25.339Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dgzznw, queueLength=0"}
{"level":"INFO","ts":"2025-12-06T02:58:25.339Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984d5fntr, queueLength=0"}
{"level":"INFO","ts":"2025-12-06T02:58:25.339Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dczzts, queueLength=0"}
{"level":"INFO","ts":"2025-12-06T02:58:25.339Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984ds7q94, queueLength=0"}
{"level":"DEBUG","ts":"2025-12-06T02:58:25.339Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=4"}
{"level":"INFO","ts":"2025-12-06T02:58:25.349Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dgzznw, usage=0.054 (5.4%)"}
{"level":"INFO","ts":"2025-12-06T02:58:25.349Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984d5fntr, usage=0.015 (1.5%)"}
{"level":"INFO","ts":"2025-12-06T02:58:25.349Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dczzts, usage=0.044 (4.4%)"}
{"level":"INFO","ts":"2025-12-06T02:58:25.349Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984ds7q94, usage=0.024 (2.4%)"}
{"level":"DEBUG","ts":"2025-12-06T02:58:25.349Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=4"}
{"level":"DEBUG","ts":"2025-12-06T02:58:25.352Z","msg":"Pod-to-variant matching successful: totalPods=4, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:4]"}
{"level":"DEBUG","ts":"2025-12-06T02:58:25.352Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=4"}
{"level":"DEBUG","ts":"2025-12-06T02:58:25.352Z","msg":"Collected capacity metrics for observability in model-only mode: modelID=unsloth/Meta-Llama-3.1-8B, replicaCount=4"}
{"level":"INFO","ts":"2025-12-06T02:58:25.352Z","msg":"Found SLO for model: model=unsloth/Meta-Llama-3.1-8B, class=Premium, slo-tpot=10, slo-ttft=1000"}
{"level":"DEBUG","ts":"2025-12-06T02:58:25.362Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=3, accelerator=H100, ttft=16.98ms, itl=8.16ms, cost=300.00, maxBatch=256, arrivalRate=1181.65, avgInputTokens=235.31, avgOutputTokens=415.04"}
{"level":"DEBUG","ts":"2025-12-06T02:58:25.362Z","msg":"Experimental model tuner is enabled globally (EXPERIMENTAL_MODEL_TUNER_ENABLED=true) tuning model performance parameters for active VAs"}
{"level":"DEBUG","ts":"2025-12-06T02:58:25.362Z","msg":"Using state vals from VA status to tune variant ms-inference-scheduling-llm-d-modelservice-decode: alpha= 7.276710, beta= 0.042049, gamma= 16.988239, delta= 0.000321"}
{"level":"DEBUG","ts":"2025-12-06T02:58:25.362Z","msg":"[Tuner Config] variant=ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler | Initial state: alpha=7.276710, beta=0.042049, gamma=16.988239, delta=0.000321 | SLO targets: TTFT=1000.00ms, ITL=10.00ms | Has covariance=true"}
{"level":"INFO","ts":"2025-12-06T02:58:25.363Z","msg":"Tuner validation succeeded for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler, server ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler - New NIS: 0.045933"}
{"level":"DEBUG","ts":"2025-12-06T02:58:25.363Z","msg":"Model tuner results: model=unsloth/Meta-Llama-3.1-8B, accelerator=H100, alpha=7.202464, beta=0.042032, gamma=16.936785, delta=0.000321, NIS=0.05"}
{"level":"DEBUG","ts":"2025-12-06T02:58:25.363Z","msg":"Updated tuner status for variant ms-inference-scheduling-llm-d-modelservice-decode: alpha=7.202464, beta=0.042032, gamma=16.936785, delta=0.000321, NIS=0.045933"}
{"level":"INFO","ts":"2025-12-06T02:58:25.363Z","msg":"Tuned performance parameters result: variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler - alpha: 7.202464, beta: 0.042032, gamma: 16.936785, delta: 0.000321"}
{"level":"DEBUG","ts":"2025-12-06T02:58:25.372Z","msg":"Optimization solution - system: Solution: \ns=ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler; c=Premium; m=unsloth/Meta-Llama-3.1-8B; rate=1181.65; inTk=235; outTk=415; sol=1, sat=false, alloc={acc=H100; numRep=2; maxBatch=512; cost=200, val=-100, itl=8.752403, ttft=19.718418, rho=0.070068404, maxRPM=945.0864}; slo-itl=10, slo-ttft=1000, slo-tps=0 \nAllocationByType: \nname=NVIDIA-H100-80GB-HBM3, count=2, limit=0, cost=200 \ntotalCost=200 \n"}
{"level":"DEBUG","ts":"2025-12-06T02:58:25.372Z","msg":"Setting accelerator name: Name=H100, allocationData={H100 2 512 200 8.752403 19.718418 {1181.65 235 415}}"}
{"level":"INFO","ts":"2025-12-06T02:58:25.372Z","msg":"Model-based optimization completed for model: unsloth/Meta-Llama-3.1-8B - model-based targets: map[ms-inference-scheduling-llm-d-modelservice-decode:2]"}
{"level":"WARN","ts":"2025-12-06T02:58:25.372Z","msg":"saturation analysis unavailable, using model-based targets only: modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-06T02:58:25.372Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-12-06T02:58:25.372Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=scale-down, current=3→target=2"}
{"level":"DEBUG","ts":"2025-12-06T02:58:25.372Z","msg":"Found VA in map: variant=ms-inference-scheduling-llm-d-modelservice-decode, hasCurrentAlloc=true, accelerator=H100"}
{"level":"INFO","ts":"2025-12-06T02:58:25.372Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 3, desired-replicas: 2, accelerator: H100"}
{"level":"INFO","ts":"2025-12-06T02:58:25.372Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=2, accelerator=H100, SaturationOnly=false"}
{"level":"INFO","ts":"2025-12-06T02:58:25.378Z","msg":"Applied Saturation decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=scale-down, current=3, target=2, reason=model-based only (Saturation unavailable)"}
{"level":"INFO","ts":"2025-12-06T02:58:25.378Z","msg":"Reconciliation completed successfully: mode=model-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-12-06T02:58:25.427Z","msg":"saturation scaling config cache updated: entries=1, has_default=true"}
{"level":"INFO","ts":"2025-12-06T02:58:25.427Z","msg":"saturation scaling configuration loaded successfully"}
{"level":"INFO","ts":"2025-12-06T02:58:25.427Z","msg":"Operating in MODEL-ONLY mode: model-based optimization only"}
{"level":"INFO","ts":"2025-12-06T02:58:25.427Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-12-06T02:58:25.427Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-12-06T02:58:25.429Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dgzznw, usage=0.054 (5.4%)"}
{"level":"INFO","ts":"2025-12-06T02:58:25.429Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984d5fntr, usage=0.015 (1.5%)"}
{"level":"INFO","ts":"2025-12-06T02:58:25.429Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dczzts, usage=0.044 (4.4%)"}
{"level":"INFO","ts":"2025-12-06T02:58:25.429Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984ds7q94, usage=0.024 (2.4%)"}
{"level":"DEBUG","ts":"2025-12-06T02:58:25.429Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=4"}
{"level":"INFO","ts":"2025-12-06T02:58:25.429Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dgzznw, queueLength=0"}
{"level":"INFO","ts":"2025-12-06T02:58:25.429Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984d5fntr, queueLength=0"}
{"level":"INFO","ts":"2025-12-06T02:58:25.429Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dczzts, queueLength=0"}
{"level":"INFO","ts":"2025-12-06T02:58:25.429Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984ds7q94, queueLength=0"}
{"level":"DEBUG","ts":"2025-12-06T02:58:25.429Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=4"}
{"level":"DEBUG","ts":"2025-12-06T02:58:25.431Z","msg":"Pod-to-variant matching successful: totalPods=4, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:4]"}
{"level":"DEBUG","ts":"2025-12-06T02:58:25.431Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=4"}
{"level":"DEBUG","ts":"2025-12-06T02:58:25.431Z","msg":"Collected capacity metrics for observability in model-only mode: modelID=unsloth/Meta-Llama-3.1-8B, replicaCount=4"}
{"level":"INFO","ts":"2025-12-06T02:58:25.431Z","msg":"Found SLO for model: model=unsloth/Meta-Llama-3.1-8B, class=Premium, slo-tpot=10, slo-ttft=1000"}
{"level":"DEBUG","ts":"2025-12-06T02:58:25.442Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=3, accelerator=H100, ttft=16.98ms, itl=8.16ms, cost=300.00, maxBatch=256, arrivalRate=1181.65, avgInputTokens=235.31, avgOutputTokens=415.04"}
{"level":"DEBUG","ts":"2025-12-06T02:58:25.442Z","msg":"Experimental model tuner is enabled globally (EXPERIMENTAL_MODEL_TUNER_ENABLED=true) tuning model performance parameters for active VAs"}
{"level":"DEBUG","ts":"2025-12-06T02:58:25.442Z","msg":"Using state vals from VA status to tune variant ms-inference-scheduling-llm-d-modelservice-decode: alpha= 7.202464, beta= 0.042032, gamma= 16.936785, delta= 0.000321"}
{"level":"DEBUG","ts":"2025-12-06T02:58:25.442Z","msg":"[Tuner Config] variant=ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler | Initial state: alpha=7.202464, beta=0.042032, gamma=16.936785, delta=0.000321 | SLO targets: TTFT=1000.00ms, ITL=10.00ms | Has covariance=true"}
{"level":"INFO","ts":"2025-12-06T02:58:25.442Z","msg":"Tuner validation succeeded for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler, server ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler - New NIS: 0.006529"}
{"level":"DEBUG","ts":"2025-12-06T02:58:25.442Z","msg":"Model tuner results: model=unsloth/Meta-Llama-3.1-8B, accelerator=H100, alpha=7.185843, beta=0.042024, gamma=16.886667, delta=0.000321, NIS=0.01"}
{"level":"DEBUG","ts":"2025-12-06T02:58:25.442Z","msg":"Updated tuner status for variant ms-inference-scheduling-llm-d-modelservice-decode: alpha=7.185843, beta=0.042024, gamma=16.886667, delta=0.000321, NIS=0.006529"}
{"level":"INFO","ts":"2025-12-06T02:58:25.442Z","msg":"Tuned performance parameters result: variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler - alpha: 7.185843, beta: 0.042024, gamma: 16.886667, delta: 0.000321"}
{"level":"DEBUG","ts":"2025-12-06T02:58:25.452Z","msg":"Optimization solution - system: Solution: \ns=ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler; c=Premium; m=unsloth/Meta-Llama-3.1-8B; rate=1181.65; inTk=235; outTk=415; sol=1, sat=false, alloc={acc=H100; numRep=2; maxBatch=512; cost=200, val=-100, itl=8.731946, ttft=19.66197, rho=0.06990444, maxRPM=950.98254}; slo-itl=10, slo-ttft=1000, slo-tps=0 \nAllocationByType: \nname=NVIDIA-H100-80GB-HBM3, count=2, limit=0, cost=200 \ntotalCost=200 \n"}
{"level":"DEBUG","ts":"2025-12-06T02:58:25.452Z","msg":"Setting accelerator name: Name=H100, allocationData={H100 2 512 200 8.731946 19.66197 {1181.65 235 415}}"}
{"level":"INFO","ts":"2025-12-06T02:58:25.452Z","msg":"Model-based optimization completed for model: unsloth/Meta-Llama-3.1-8B - model-based targets: map[ms-inference-scheduling-llm-d-modelservice-decode:2]"}
{"level":"WARN","ts":"2025-12-06T02:58:25.452Z","msg":"saturation analysis unavailable, using model-based targets only: modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-06T02:58:25.452Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-12-06T02:58:25.452Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=scale-down, current=3→target=2"}
{"level":"DEBUG","ts":"2025-12-06T02:58:25.452Z","msg":"Found VA in map: variant=ms-inference-scheduling-llm-d-modelservice-decode, hasCurrentAlloc=true, accelerator=H100"}
{"level":"INFO","ts":"2025-12-06T02:58:25.452Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 3, desired-replicas: 2, accelerator: H100"}
{"level":"INFO","ts":"2025-12-06T02:58:25.452Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=2, accelerator=H100, SaturationOnly=false"}
{"level":"INFO","ts":"2025-12-06T02:58:25.457Z","msg":"Applied Saturation decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=scale-down, current=3, target=2, reason=model-based only (Saturation unavailable)"}
{"level":"INFO","ts":"2025-12-06T02:58:25.457Z","msg":"Reconciliation completed successfully: mode=model-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-12-06T02:59:25.379Z","msg":"saturation scaling config cache updated: entries=1, has_default=true"}
{"level":"INFO","ts":"2025-12-06T02:59:25.379Z","msg":"saturation scaling configuration loaded successfully"}
{"level":"INFO","ts":"2025-12-06T02:59:25.379Z","msg":"Operating in MODEL-ONLY mode: model-based optimization only"}
{"level":"INFO","ts":"2025-12-06T02:59:25.379Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-12-06T02:59:25.379Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-12-06T02:59:25.383Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dczzts, queueLength=0"}
{"level":"INFO","ts":"2025-12-06T02:59:25.383Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984ds7q94, queueLength=0"}
{"level":"DEBUG","ts":"2025-12-06T02:59:25.383Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=2"}
{"level":"INFO","ts":"2025-12-06T02:59:25.383Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dczzts, usage=0.057 (5.7%)"}
{"level":"INFO","ts":"2025-12-06T02:59:25.383Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984ds7q94, usage=0.060 (6.0%)"}
{"level":"DEBUG","ts":"2025-12-06T02:59:25.383Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=2"}
{"level":"DEBUG","ts":"2025-12-06T02:59:25.386Z","msg":"Pod-to-variant matching successful: totalPods=2, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:2]"}
{"level":"DEBUG","ts":"2025-12-06T02:59:25.386Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=2"}
{"level":"DEBUG","ts":"2025-12-06T02:59:25.386Z","msg":"Collected capacity metrics for observability in model-only mode: modelID=unsloth/Meta-Llama-3.1-8B, replicaCount=2"}
{"level":"INFO","ts":"2025-12-06T02:59:25.386Z","msg":"Found SLO for model: model=unsloth/Meta-Llama-3.1-8B, class=Premium, slo-tpot=10, slo-ttft=1000"}
{"level":"DEBUG","ts":"2025-12-06T02:59:25.395Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=2, accelerator=H100, ttft=18.23ms, itl=8.69ms, cost=200.00, maxBatch=256, arrivalRate=1064.68, avgInputTokens=246.34, avgOutputTokens=420.16"}
{"level":"DEBUG","ts":"2025-12-06T02:59:25.395Z","msg":"Experimental model tuner is enabled globally (EXPERIMENTAL_MODEL_TUNER_ENABLED=true) tuning model performance parameters for active VAs"}
{"level":"DEBUG","ts":"2025-12-06T02:59:25.395Z","msg":"Using state vals from VA status to tune variant ms-inference-scheduling-llm-d-modelservice-decode: alpha= 7.185843, beta= 0.042024, gamma= 16.886667, delta= 0.000321"}
{"level":"DEBUG","ts":"2025-12-06T02:59:25.395Z","msg":"[Tuner Config] variant=ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler | Initial state: alpha=7.185843, beta=0.042024, gamma=16.886667, delta=0.000321 | SLO targets: TTFT=1000.00ms, ITL=10.00ms | Has covariance=true"}
{"level":"INFO","ts":"2025-12-06T02:59:25.396Z","msg":"Tuner validation succeeded for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler, server ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler - New NIS: 0.044180"}
{"level":"DEBUG","ts":"2025-12-06T02:59:25.396Z","msg":"Model tuner results: model=unsloth/Meta-Llama-3.1-8B, accelerator=H100, alpha=7.250196, beta=0.042419, gamma=16.849110, delta=0.000321, NIS=0.04"}
{"level":"DEBUG","ts":"2025-12-06T02:59:25.396Z","msg":"Updated tuner status for variant ms-inference-scheduling-llm-d-modelservice-decode: alpha=7.250196, beta=0.042419, gamma=16.849110, delta=0.000321, NIS=0.044180"}
{"level":"INFO","ts":"2025-12-06T02:59:25.396Z","msg":"Tuned performance parameters result: variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler - alpha: 7.250196, beta: 0.042419, gamma: 16.849110, delta: 0.000321"}
{"level":"DEBUG","ts":"2025-12-06T02:59:25.400Z","msg":"Optimization solution - system: Solution: \ns=ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler; c=Premium; m=unsloth/Meta-Llama-3.1-8B; rate=1064.68; inTk=246; outTk=420; sol=1, sat=false, alloc={acc=H100; numRep=2; maxBatch=512; cost=200, val=0, itl=8.666599, ttft=19.485838, rho=0.063263744, maxRPM=909.1981}; slo-itl=10, slo-ttft=1000, slo-tps=0 \nAllocationByType: \nname=NVIDIA-H100-80GB-HBM3, count=2, limit=0, cost=200 \ntotalCost=200 \n"}
{"level":"DEBUG","ts":"2025-12-06T02:59:25.400Z","msg":"Setting accelerator name: Name=H100, allocationData={H100 2 512 200 8.666599 19.485838 {1064.68 246 420}}"}
{"level":"INFO","ts":"2025-12-06T02:59:25.400Z","msg":"Model-based optimization completed for model: unsloth/Meta-Llama-3.1-8B - model-based targets: map[ms-inference-scheduling-llm-d-modelservice-decode:2]"}
{"level":"WARN","ts":"2025-12-06T02:59:25.400Z","msg":"saturation analysis unavailable, using model-based targets only: modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-06T02:59:25.400Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-12-06T02:59:25.400Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=2→target=2"}
{"level":"DEBUG","ts":"2025-12-06T02:59:25.400Z","msg":"Found VA in map: variant=ms-inference-scheduling-llm-d-modelservice-decode, hasCurrentAlloc=true, accelerator=H100"}
{"level":"INFO","ts":"2025-12-06T02:59:25.400Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 2, desired-replicas: 2, accelerator: H100"}
{"level":"INFO","ts":"2025-12-06T02:59:25.400Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=2, accelerator=H100, SaturationOnly=false"}
{"level":"INFO","ts":"2025-12-06T02:59:25.406Z","msg":"Applied Saturation decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=2, target=2, reason=model-based only (Saturation unavailable)"}
{"level":"INFO","ts":"2025-12-06T02:59:25.406Z","msg":"Reconciliation completed successfully: mode=model-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-12-06T02:59:25.457Z","msg":"saturation scaling config cache updated: entries=1, has_default=true"}
{"level":"INFO","ts":"2025-12-06T02:59:25.457Z","msg":"saturation scaling configuration loaded successfully"}
{"level":"INFO","ts":"2025-12-06T02:59:25.457Z","msg":"Operating in MODEL-ONLY mode: model-based optimization only"}
{"level":"INFO","ts":"2025-12-06T02:59:25.457Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-12-06T02:59:25.457Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-12-06T02:59:25.459Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dczzts, queueLength=0"}
{"level":"INFO","ts":"2025-12-06T02:59:25.459Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984ds7q94, queueLength=0"}
{"level":"DEBUG","ts":"2025-12-06T02:59:25.459Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=2"}
{"level":"INFO","ts":"2025-12-06T02:59:25.459Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dczzts, usage=0.057 (5.7%)"}
{"level":"INFO","ts":"2025-12-06T02:59:25.459Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984ds7q94, usage=0.060 (6.0%)"}
{"level":"DEBUG","ts":"2025-12-06T02:59:25.459Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=2"}
{"level":"DEBUG","ts":"2025-12-06T02:59:25.462Z","msg":"Pod-to-variant matching successful: totalPods=2, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:2]"}
{"level":"DEBUG","ts":"2025-12-06T02:59:25.462Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=2"}
{"level":"DEBUG","ts":"2025-12-06T02:59:25.462Z","msg":"Collected capacity metrics for observability in model-only mode: modelID=unsloth/Meta-Llama-3.1-8B, replicaCount=2"}
{"level":"INFO","ts":"2025-12-06T02:59:25.462Z","msg":"Found SLO for model: model=unsloth/Meta-Llama-3.1-8B, class=Premium, slo-tpot=10, slo-ttft=1000"}
{"level":"DEBUG","ts":"2025-12-06T02:59:25.471Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=2, accelerator=H100, ttft=18.23ms, itl=8.69ms, cost=200.00, maxBatch=256, arrivalRate=1064.68, avgInputTokens=246.34, avgOutputTokens=420.16"}
{"level":"DEBUG","ts":"2025-12-06T02:59:25.471Z","msg":"Experimental model tuner is enabled globally (EXPERIMENTAL_MODEL_TUNER_ENABLED=true) tuning model performance parameters for active VAs"}
{"level":"DEBUG","ts":"2025-12-06T02:59:25.471Z","msg":"Using state vals from VA status to tune variant ms-inference-scheduling-llm-d-modelservice-decode: alpha= 7.250196, beta= 0.042419, gamma= 16.849110, delta= 0.000321"}
{"level":"DEBUG","ts":"2025-12-06T02:59:25.471Z","msg":"[Tuner Config] variant=ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler | Initial state: alpha=7.250196, beta=0.042419, gamma=16.849110, delta=0.000321 | SLO targets: TTFT=1000.00ms, ITL=10.00ms | Has covariance=true"}
{"level":"INFO","ts":"2025-12-06T02:59:25.471Z","msg":"Tuner validation succeeded for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler, server ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler - New NIS: 0.004136"}
{"level":"DEBUG","ts":"2025-12-06T02:59:25.471Z","msg":"Model tuner results: model=unsloth/Meta-Llama-3.1-8B, accelerator=H100, alpha=7.264621, beta=0.042453, gamma=16.811779, delta=0.000321, NIS=0.00"}
{"level":"DEBUG","ts":"2025-12-06T02:59:25.471Z","msg":"Updated tuner status for variant ms-inference-scheduling-llm-d-modelservice-decode: alpha=7.264621, beta=0.042453, gamma=16.811779, delta=0.000321, NIS=0.004136"}
{"level":"INFO","ts":"2025-12-06T02:59:25.471Z","msg":"Tuned performance parameters result: variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler - alpha: 7.264621, beta: 0.042453, gamma: 16.811779, delta: 0.000321"}
{"level":"DEBUG","ts":"2025-12-06T02:59:25.480Z","msg":"Optimization solution - system: Solution: \ns=ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler; c=Premium; m=unsloth/Meta-Llama-3.1-8B; rate=1064.68; inTk=246; outTk=420; sol=1, sat=false, alloc={acc=H100; numRep=2; maxBatch=512; cost=200, val=0, itl=8.685066, ttft=19.453901, rho=0.06339727, maxRPM=903.6314}; slo-itl=10, slo-ttft=1000, slo-tps=0 \nAllocationByType: \nname=NVIDIA-H100-80GB-HBM3, count=2, limit=0, cost=200 \ntotalCost=200 \n"}
{"level":"DEBUG","ts":"2025-12-06T02:59:25.480Z","msg":"Setting accelerator name: Name=H100, allocationData={H100 2 512 200 8.685066 19.453901 {1064.68 246 420}}"}
{"level":"INFO","ts":"2025-12-06T02:59:25.480Z","msg":"Model-based optimization completed for model: unsloth/Meta-Llama-3.1-8B - model-based targets: map[ms-inference-scheduling-llm-d-modelservice-decode:2]"}
{"level":"WARN","ts":"2025-12-06T02:59:25.480Z","msg":"saturation analysis unavailable, using model-based targets only: modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-06T02:59:25.480Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-12-06T02:59:25.480Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=2→target=2"}
{"level":"DEBUG","ts":"2025-12-06T02:59:25.480Z","msg":"Found VA in map: variant=ms-inference-scheduling-llm-d-modelservice-decode, hasCurrentAlloc=true, accelerator=H100"}
{"level":"INFO","ts":"2025-12-06T02:59:25.481Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 2, desired-replicas: 2, accelerator: H100"}
{"level":"INFO","ts":"2025-12-06T02:59:25.481Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=2, accelerator=H100, SaturationOnly=false"}
{"level":"INFO","ts":"2025-12-06T02:59:25.486Z","msg":"Applied Saturation decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=2, target=2, reason=model-based only (Saturation unavailable)"}
{"level":"INFO","ts":"2025-12-06T02:59:25.486Z","msg":"Reconciliation completed successfully: mode=model-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-12-06T03:00:25.407Z","msg":"saturation scaling config cache updated: entries=1, has_default=true"}
{"level":"INFO","ts":"2025-12-06T03:00:25.407Z","msg":"saturation scaling configuration loaded successfully"}
{"level":"INFO","ts":"2025-12-06T03:00:25.407Z","msg":"Operating in MODEL-ONLY mode: model-based optimization only"}
{"level":"INFO","ts":"2025-12-06T03:00:25.407Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-12-06T03:00:25.407Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-12-06T03:00:25.414Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dczzts, usage=0.068 (6.8%)"}
{"level":"INFO","ts":"2025-12-06T03:00:25.414Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984ds7q94, usage=0.060 (6.0%)"}
{"level":"DEBUG","ts":"2025-12-06T03:00:25.414Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=2"}
{"level":"INFO","ts":"2025-12-06T03:00:25.414Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dczzts, queueLength=0"}
{"level":"INFO","ts":"2025-12-06T03:00:25.414Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984ds7q94, queueLength=0"}
{"level":"DEBUG","ts":"2025-12-06T03:00:25.414Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=2"}
{"level":"DEBUG","ts":"2025-12-06T03:00:25.417Z","msg":"Pod-to-variant matching successful: totalPods=2, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:2]"}
{"level":"DEBUG","ts":"2025-12-06T03:00:25.417Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=2"}
{"level":"DEBUG","ts":"2025-12-06T03:00:25.417Z","msg":"Collected capacity metrics for observability in model-only mode: modelID=unsloth/Meta-Llama-3.1-8B, replicaCount=2"}
{"level":"INFO","ts":"2025-12-06T03:00:25.417Z","msg":"Found SLO for model: model=unsloth/Meta-Llama-3.1-8B, class=Premium, slo-tpot=10, slo-ttft=1000"}
{"level":"DEBUG","ts":"2025-12-06T03:00:25.426Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=2, accelerator=H100, ttft=18.77ms, itl=9.16ms, cost=200.00, maxBatch=256, arrivalRate=1175.90, avgInputTokens=231.80, avgOutputTokens=458.24"}
{"level":"DEBUG","ts":"2025-12-06T03:00:25.426Z","msg":"Experimental model tuner is enabled globally (EXPERIMENTAL_MODEL_TUNER_ENABLED=true) tuning model performance parameters for active VAs"}
{"level":"DEBUG","ts":"2025-12-06T03:00:25.426Z","msg":"Using state vals from VA status to tune variant ms-inference-scheduling-llm-d-modelservice-decode: alpha= 7.264621, beta= 0.042453, gamma= 16.811779, delta= 0.000321"}
{"level":"DEBUG","ts":"2025-12-06T03:00:25.426Z","msg":"[Tuner Config] variant=ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler | Initial state: alpha=7.264621, beta=0.042453, gamma=16.811779, delta=0.000321 | SLO targets: TTFT=1000.00ms, ITL=10.00ms | Has covariance=true"}
{"level":"INFO","ts":"2025-12-06T03:00:25.426Z","msg":"Tuner validation succeeded for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler, server ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler - New NIS: 0.049099"}
{"level":"DEBUG","ts":"2025-12-06T03:00:25.426Z","msg":"Model tuner results: model=unsloth/Meta-Llama-3.1-8B, accelerator=H100, alpha=7.329998, beta=0.042893, gamma=16.777142, delta=0.000321, NIS=0.05"}
{"level":"DEBUG","ts":"2025-12-06T03:00:25.426Z","msg":"Updated tuner status for variant ms-inference-scheduling-llm-d-modelservice-decode: alpha=7.329998, beta=0.042893, gamma=16.777142, delta=0.000321, NIS=0.049099"}
{"level":"INFO","ts":"2025-12-06T03:00:25.426Z","msg":"Tuned performance parameters result: variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler - alpha: 7.329998, beta: 0.042893, gamma: 16.777142, delta: 0.000321"}
{"level":"DEBUG","ts":"2025-12-06T03:00:25.436Z","msg":"Optimization solution - system: Solution: \ns=ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler; c=Premium; m=unsloth/Meta-Llama-3.1-8B; rate=1175.9; inTk=231; outTk=458; sol=1, sat=false, alloc={acc=H100; numRep=2; maxBatch=512; cost=200, val=0, itl=9.136177, ttft=19.899513, rho=0.08029063, maxRPM=800.38043}; slo-itl=10, slo-ttft=1000, slo-tps=0 \nAllocationByType: \nname=NVIDIA-H100-80GB-HBM3, count=2, limit=0, cost=200 \ntotalCost=200 \n"}
{"level":"DEBUG","ts":"2025-12-06T03:00:25.436Z","msg":"Setting accelerator name: Name=H100, allocationData={H100 2 512 200 9.136177 19.899513 {1175.9 231 458}}"}
{"level":"INFO","ts":"2025-12-06T03:00:25.436Z","msg":"Model-based optimization completed for model: unsloth/Meta-Llama-3.1-8B - model-based targets: map[ms-inference-scheduling-llm-d-modelservice-decode:2]"}
{"level":"WARN","ts":"2025-12-06T03:00:25.436Z","msg":"saturation analysis unavailable, using model-based targets only: modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-06T03:00:25.436Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-12-06T03:00:25.436Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=2→target=2"}
{"level":"DEBUG","ts":"2025-12-06T03:00:25.436Z","msg":"Found VA in map: variant=ms-inference-scheduling-llm-d-modelservice-decode, hasCurrentAlloc=true, accelerator=H100"}
{"level":"INFO","ts":"2025-12-06T03:00:25.436Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 2, desired-replicas: 2, accelerator: H100"}
{"level":"INFO","ts":"2025-12-06T03:00:25.436Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=2, accelerator=H100, SaturationOnly=false"}
{"level":"INFO","ts":"2025-12-06T03:00:25.442Z","msg":"Applied Saturation decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=2, target=2, reason=model-based only (Saturation unavailable)"}
{"level":"INFO","ts":"2025-12-06T03:00:25.442Z","msg":"Reconciliation completed successfully: mode=model-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-12-06T03:00:25.487Z","msg":"saturation scaling config cache updated: entries=1, has_default=true"}
{"level":"INFO","ts":"2025-12-06T03:00:25.487Z","msg":"saturation scaling configuration loaded successfully"}
{"level":"INFO","ts":"2025-12-06T03:00:25.487Z","msg":"Operating in MODEL-ONLY mode: model-based optimization only"}
{"level":"INFO","ts":"2025-12-06T03:00:25.487Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-12-06T03:00:25.487Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-12-06T03:00:25.489Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dczzts, usage=0.068 (6.8%)"}
{"level":"INFO","ts":"2025-12-06T03:00:25.489Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984ds7q94, usage=0.060 (6.0%)"}
{"level":"DEBUG","ts":"2025-12-06T03:00:25.489Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=2"}
{"level":"INFO","ts":"2025-12-06T03:00:25.489Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dczzts, queueLength=0"}
{"level":"INFO","ts":"2025-12-06T03:00:25.489Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984ds7q94, queueLength=0"}
{"level":"DEBUG","ts":"2025-12-06T03:00:25.489Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=2"}
{"level":"DEBUG","ts":"2025-12-06T03:00:25.491Z","msg":"Pod-to-variant matching successful: totalPods=2, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:2]"}
{"level":"DEBUG","ts":"2025-12-06T03:00:25.491Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=2"}
{"level":"DEBUG","ts":"2025-12-06T03:00:25.491Z","msg":"Collected capacity metrics for observability in model-only mode: modelID=unsloth/Meta-Llama-3.1-8B, replicaCount=2"}
{"level":"INFO","ts":"2025-12-06T03:00:25.491Z","msg":"Found SLO for model: model=unsloth/Meta-Llama-3.1-8B, class=Premium, slo-tpot=10, slo-ttft=1000"}
{"level":"DEBUG","ts":"2025-12-06T03:00:25.501Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=2, accelerator=H100, ttft=18.77ms, itl=9.16ms, cost=200.00, maxBatch=256, arrivalRate=1175.90, avgInputTokens=231.80, avgOutputTokens=458.24"}
{"level":"DEBUG","ts":"2025-12-06T03:00:25.501Z","msg":"Experimental model tuner is enabled globally (EXPERIMENTAL_MODEL_TUNER_ENABLED=true) tuning model performance parameters for active VAs"}
{"level":"DEBUG","ts":"2025-12-06T03:00:25.501Z","msg":"Using state vals from VA status to tune variant ms-inference-scheduling-llm-d-modelservice-decode: alpha= 7.329998, beta= 0.042893, gamma= 16.777142, delta= 0.000321"}
{"level":"DEBUG","ts":"2025-12-06T03:00:25.501Z","msg":"[Tuner Config] variant=ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler | Initial state: alpha=7.329998, beta=0.042893, gamma=16.777142, delta=0.000321 | SLO targets: TTFT=1000.00ms, ITL=10.00ms | Has covariance=true"}
{"level":"INFO","ts":"2025-12-06T03:00:25.501Z","msg":"Tuner validation succeeded for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler, server ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler - New NIS: 0.003615"}
{"level":"DEBUG","ts":"2025-12-06T03:00:25.501Z","msg":"Model tuner results: model=unsloth/Meta-Llama-3.1-8B, accelerator=H100, alpha=7.344028, beta=0.042928, gamma=16.743126, delta=0.000321, NIS=0.00"}
{"level":"DEBUG","ts":"2025-12-06T03:00:25.501Z","msg":"Updated tuner status for variant ms-inference-scheduling-llm-d-modelservice-decode: alpha=7.344028, beta=0.042928, gamma=16.743126, delta=0.000321, NIS=0.003615"}
{"level":"INFO","ts":"2025-12-06T03:00:25.501Z","msg":"Tuned performance parameters result: variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler - alpha: 7.344028, beta: 0.042928, gamma: 16.743126, delta: 0.000321"}
{"level":"DEBUG","ts":"2025-12-06T03:00:25.510Z","msg":"Optimization solution - system: Solution: \ns=ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler; c=Premium; m=unsloth/Meta-Llama-3.1-8B; rate=1175.9; inTk=231; outTk=458; sol=1, sat=false, alloc={acc=H100; numRep=2; maxBatch=512; cost=200, val=0, itl=9.155355, ttft=19.871843, rho=0.08045784, maxRPM=795.4601}; slo-itl=10, slo-ttft=1000, slo-tps=0 \nAllocationByType: \nname=NVIDIA-H100-80GB-HBM3, count=2, limit=0, cost=200 \ntotalCost=200 \n"}
{"level":"DEBUG","ts":"2025-12-06T03:00:25.510Z","msg":"Setting accelerator name: Name=H100, allocationData={H100 2 512 200 9.155355 19.871843 {1175.9 231 458}}"}
{"level":"INFO","ts":"2025-12-06T03:00:25.510Z","msg":"Model-based optimization completed for model: unsloth/Meta-Llama-3.1-8B - model-based targets: map[ms-inference-scheduling-llm-d-modelservice-decode:2]"}
{"level":"WARN","ts":"2025-12-06T03:00:25.510Z","msg":"saturation analysis unavailable, using model-based targets only: modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-06T03:00:25.510Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-12-06T03:00:25.510Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=2→target=2"}
{"level":"DEBUG","ts":"2025-12-06T03:00:25.510Z","msg":"Found VA in map: variant=ms-inference-scheduling-llm-d-modelservice-decode, hasCurrentAlloc=true, accelerator=H100"}
{"level":"INFO","ts":"2025-12-06T03:00:25.510Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 2, desired-replicas: 2, accelerator: H100"}
{"level":"INFO","ts":"2025-12-06T03:00:25.510Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=2, accelerator=H100, SaturationOnly=false"}
{"level":"INFO","ts":"2025-12-06T03:00:25.515Z","msg":"Applied Saturation decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=2, target=2, reason=model-based only (Saturation unavailable)"}
{"level":"INFO","ts":"2025-12-06T03:00:25.515Z","msg":"Reconciliation completed successfully: mode=model-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-12-06T03:01:25.443Z","msg":"saturation scaling config cache updated: entries=1, has_default=true"}
{"level":"INFO","ts":"2025-12-06T03:01:25.443Z","msg":"saturation scaling configuration loaded successfully"}
{"level":"INFO","ts":"2025-12-06T03:01:25.443Z","msg":"Operating in MODEL-ONLY mode: model-based optimization only"}
{"level":"INFO","ts":"2025-12-06T03:01:25.443Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-12-06T03:01:25.443Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-12-06T03:01:25.449Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dczzts, usage=0.028 (2.8%)"}
{"level":"INFO","ts":"2025-12-06T03:01:25.449Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984ds7q94, usage=0.029 (2.9%)"}
{"level":"DEBUG","ts":"2025-12-06T03:01:25.449Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=2"}
{"level":"INFO","ts":"2025-12-06T03:01:25.449Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dczzts, queueLength=0"}
{"level":"INFO","ts":"2025-12-06T03:01:25.449Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984ds7q94, queueLength=0"}
{"level":"DEBUG","ts":"2025-12-06T03:01:25.449Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=2"}
{"level":"DEBUG","ts":"2025-12-06T03:01:25.451Z","msg":"Pod-to-variant matching successful: totalPods=2, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:2]"}
{"level":"DEBUG","ts":"2025-12-06T03:01:25.451Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=2"}
{"level":"DEBUG","ts":"2025-12-06T03:01:25.451Z","msg":"Collected capacity metrics for observability in model-only mode: modelID=unsloth/Meta-Llama-3.1-8B, replicaCount=2"}
{"level":"INFO","ts":"2025-12-06T03:01:25.451Z","msg":"Found SLO for model: model=unsloth/Meta-Llama-3.1-8B, class=Premium, slo-tpot=10, slo-ttft=1000"}
{"level":"DEBUG","ts":"2025-12-06T03:01:25.460Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=2, accelerator=H100, ttft=16.30ms, itl=7.65ms, cost=200.00, maxBatch=256, arrivalRate=600.68, avgInputTokens=231.89, avgOutputTokens=447.92"}
{"level":"DEBUG","ts":"2025-12-06T03:01:25.460Z","msg":"Experimental model tuner is enabled globally (EXPERIMENTAL_MODEL_TUNER_ENABLED=true) tuning model performance parameters for active VAs"}
{"level":"DEBUG","ts":"2025-12-06T03:01:25.460Z","msg":"Using state vals from VA status to tune variant ms-inference-scheduling-llm-d-modelservice-decode: alpha= 7.344028, beta= 0.042928, gamma= 16.743126, delta= 0.000321"}
{"level":"DEBUG","ts":"2025-12-06T03:01:25.460Z","msg":"[Tuner Config] variant=ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler | Initial state: alpha=7.344028, beta=0.042928, gamma=16.743126, delta=0.000321 | SLO targets: TTFT=1000.00ms, ITL=10.00ms | Has covariance=true"}
{"level":"INFO","ts":"2025-12-06T03:01:25.461Z","msg":"Tuner validation succeeded for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler, server ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler - New NIS: 0.849331"}
{"level":"DEBUG","ts":"2025-12-06T03:01:25.461Z","msg":"Model tuner results: model=unsloth/Meta-Llama-3.1-8B, accelerator=H100, alpha=6.901657, beta=0.046192, gamma=16.693216, delta=0.000321, NIS=0.85"}
{"level":"DEBUG","ts":"2025-12-06T03:01:25.461Z","msg":"Updated tuner status for variant ms-inference-scheduling-llm-d-modelservice-decode: alpha=6.901657, beta=0.046192, gamma=16.693216, delta=0.000321, NIS=0.849331"}
{"level":"INFO","ts":"2025-12-06T03:01:25.461Z","msg":"Tuned performance parameters result: variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler - alpha: 6.901657, beta: 0.046192, gamma: 16.693216, delta: 0.000321"}
{"level":"DEBUG","ts":"2025-12-06T03:01:25.470Z","msg":"Optimization solution - system: Solution: \ns=ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler; c=Premium; m=unsloth/Meta-Llama-3.1-8B; rate=600.68; inTk=231; outTk=447; sol=1, sat=false, alloc={acc=H100; numRep=1; maxBatch=512; cost=100, val=-100, itl=8.764671, ttft=19.683826, rho=0.07681991, maxRPM=884.606}; slo-itl=10, slo-ttft=1000, slo-tps=0 \nAllocationByType: \nname=NVIDIA-H100-80GB-HBM3, count=1, limit=0, cost=100 \ntotalCost=100 \n"}
{"level":"DEBUG","ts":"2025-12-06T03:01:25.470Z","msg":"Setting accelerator name: Name=H100, allocationData={H100 1 512 100 8.764671 19.683826 {600.68 231 447}}"}
{"level":"INFO","ts":"2025-12-06T03:01:25.470Z","msg":"Model-based optimization completed for model: unsloth/Meta-Llama-3.1-8B - model-based targets: map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"WARN","ts":"2025-12-06T03:01:25.470Z","msg":"saturation analysis unavailable, using model-based targets only: modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-06T03:01:25.470Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-12-06T03:01:25.470Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=scale-down, current=2→target=1"}
{"level":"DEBUG","ts":"2025-12-06T03:01:25.470Z","msg":"Found VA in map: variant=ms-inference-scheduling-llm-d-modelservice-decode, hasCurrentAlloc=true, accelerator=H100"}
{"level":"INFO","ts":"2025-12-06T03:01:25.470Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 2, desired-replicas: 1, accelerator: H100"}
{"level":"INFO","ts":"2025-12-06T03:01:25.470Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=1, accelerator=H100, SaturationOnly=false"}
{"level":"INFO","ts":"2025-12-06T03:01:25.477Z","msg":"Applied Saturation decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=scale-down, current=2, target=1, reason=model-based only (Saturation unavailable)"}
{"level":"INFO","ts":"2025-12-06T03:01:25.477Z","msg":"Reconciliation completed successfully: mode=model-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-12-06T03:01:25.516Z","msg":"saturation scaling config cache updated: entries=1, has_default=true"}
{"level":"INFO","ts":"2025-12-06T03:01:25.516Z","msg":"saturation scaling configuration loaded successfully"}
{"level":"INFO","ts":"2025-12-06T03:01:25.516Z","msg":"Operating in MODEL-ONLY mode: model-based optimization only"}
{"level":"INFO","ts":"2025-12-06T03:01:25.516Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-12-06T03:01:25.516Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-12-06T03:01:25.518Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dczzts, usage=0.028 (2.8%)"}
{"level":"INFO","ts":"2025-12-06T03:01:25.518Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984ds7q94, usage=0.029 (2.9%)"}
{"level":"DEBUG","ts":"2025-12-06T03:01:25.518Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=2"}
{"level":"INFO","ts":"2025-12-06T03:01:25.518Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dczzts, queueLength=0"}
{"level":"INFO","ts":"2025-12-06T03:01:25.518Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984ds7q94, queueLength=0"}
{"level":"DEBUG","ts":"2025-12-06T03:01:25.518Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=2"}
{"level":"DEBUG","ts":"2025-12-06T03:01:25.520Z","msg":"Pod-to-variant matching successful: totalPods=2, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:2]"}
{"level":"DEBUG","ts":"2025-12-06T03:01:25.520Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=2"}
{"level":"DEBUG","ts":"2025-12-06T03:01:25.520Z","msg":"Collected capacity metrics for observability in model-only mode: modelID=unsloth/Meta-Llama-3.1-8B, replicaCount=2"}
{"level":"INFO","ts":"2025-12-06T03:01:25.520Z","msg":"Found SLO for model: model=unsloth/Meta-Llama-3.1-8B, class=Premium, slo-tpot=10, slo-ttft=1000"}
{"level":"DEBUG","ts":"2025-12-06T03:01:25.528Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=2, accelerator=H100, ttft=16.30ms, itl=7.65ms, cost=200.00, maxBatch=256, arrivalRate=600.68, avgInputTokens=231.89, avgOutputTokens=447.92"}
{"level":"DEBUG","ts":"2025-12-06T03:01:25.528Z","msg":"Experimental model tuner is enabled globally (EXPERIMENTAL_MODEL_TUNER_ENABLED=true) tuning model performance parameters for active VAs"}
{"level":"DEBUG","ts":"2025-12-06T03:01:25.528Z","msg":"Using state vals from VA status to tune variant ms-inference-scheduling-llm-d-modelservice-decode: alpha= 6.901657, beta= 0.046192, gamma= 16.693216, delta= 0.000321"}
{"level":"DEBUG","ts":"2025-12-06T03:01:25.528Z","msg":"[Tuner Config] variant=ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler | Initial state: alpha=6.901657, beta=0.046192, gamma=16.693216, delta=0.000321 | SLO targets: TTFT=1000.00ms, ITL=10.00ms | Has covariance=true"}
{"level":"INFO","ts":"2025-12-06T03:01:25.528Z","msg":"Tuner validation succeeded for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler, server ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler - New NIS: 0.042711"}
{"level":"DEBUG","ts":"2025-12-06T03:01:25.528Z","msg":"Model tuner results: model=unsloth/Meta-Llama-3.1-8B, accelerator=H100, alpha=6.830044, beta=0.046336, gamma=16.640581, delta=0.000321, NIS=0.04"}
{"level":"DEBUG","ts":"2025-12-06T03:01:25.528Z","msg":"Updated tuner status for variant ms-inference-scheduling-llm-d-modelservice-decode: alpha=6.830044, beta=0.046336, gamma=16.640581, delta=0.000321, NIS=0.042711"}
{"level":"INFO","ts":"2025-12-06T03:01:25.528Z","msg":"Tuned performance parameters result: variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler - alpha: 6.830044, beta: 0.046336, gamma: 16.640581, delta: 0.000321"}
{"level":"DEBUG","ts":"2025-12-06T03:01:25.538Z","msg":"Optimization solution - system: Solution: \ns=ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler; c=Premium; m=unsloth/Meta-Llama-3.1-8B; rate=600.68; inTk=231; outTk=447; sol=1, sat=false, alloc={acc=H100; numRep=1; maxBatch=512; cost=100, val=-100, itl=8.681663, ttft=19.603653, rho=0.07609444, maxRPM=902.4933}; slo-itl=10, slo-ttft=1000, slo-tps=0 \nAllocationByType: \nname=NVIDIA-H100-80GB-HBM3, count=1, limit=0, cost=100 \ntotalCost=100 \n"}
{"level":"DEBUG","ts":"2025-12-06T03:01:25.538Z","msg":"Setting accelerator name: Name=H100, allocationData={H100 1 512 100 8.681663 19.603653 {600.68 231 447}}"}
{"level":"INFO","ts":"2025-12-06T03:01:25.538Z","msg":"Model-based optimization completed for model: unsloth/Meta-Llama-3.1-8B - model-based targets: map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"WARN","ts":"2025-12-06T03:01:25.538Z","msg":"saturation analysis unavailable, using model-based targets only: modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-06T03:01:25.538Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-12-06T03:01:25.538Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=scale-down, current=2→target=1"}
{"level":"DEBUG","ts":"2025-12-06T03:01:25.538Z","msg":"Found VA in map: variant=ms-inference-scheduling-llm-d-modelservice-decode, hasCurrentAlloc=true, accelerator=H100"}
{"level":"INFO","ts":"2025-12-06T03:01:25.538Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 2, desired-replicas: 1, accelerator: H100"}
{"level":"INFO","ts":"2025-12-06T03:01:25.538Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=1, accelerator=H100, SaturationOnly=false"}
{"level":"INFO","ts":"2025-12-06T03:01:25.544Z","msg":"Applied Saturation decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=scale-down, current=2, target=1, reason=model-based only (Saturation unavailable)"}
{"level":"INFO","ts":"2025-12-06T03:01:25.544Z","msg":"Reconciliation completed successfully: mode=model-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-12-06T03:02:25.478Z","msg":"saturation scaling config cache updated: entries=1, has_default=true"}
{"level":"INFO","ts":"2025-12-06T03:02:25.478Z","msg":"saturation scaling configuration loaded successfully"}
{"level":"INFO","ts":"2025-12-06T03:02:25.478Z","msg":"Operating in MODEL-ONLY mode: model-based optimization only"}
{"level":"INFO","ts":"2025-12-06T03:02:25.478Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-12-06T03:02:25.478Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-12-06T03:02:25.488Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dczzts, usage=0.051 (5.1%)"}
{"level":"INFO","ts":"2025-12-06T03:02:25.488Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984ds7q94, usage=0.043 (4.3%)"}
{"level":"INFO","ts":"2025-12-06T03:02:25.488Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dczzts, queueLength=0"}
{"level":"INFO","ts":"2025-12-06T03:02:25.488Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984ds7q94, queueLength=0"}
{"level":"DEBUG","ts":"2025-12-06T03:02:25.488Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=2"}
{"level":"DEBUG","ts":"2025-12-06T03:02:25.488Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=2"}
{"level":"DEBUG","ts":"2025-12-06T03:02:25.491Z","msg":"Pod-to-variant matching successful: totalPods=2, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:2]"}
{"level":"DEBUG","ts":"2025-12-06T03:02:25.491Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=2"}
{"level":"DEBUG","ts":"2025-12-06T03:02:25.491Z","msg":"Collected capacity metrics for observability in model-only mode: modelID=unsloth/Meta-Llama-3.1-8B, replicaCount=2"}
{"level":"INFO","ts":"2025-12-06T03:02:25.491Z","msg":"Found SLO for model: model=unsloth/Meta-Llama-3.1-8B, class=Premium, slo-tpot=10, slo-ttft=1000"}
{"level":"DEBUG","ts":"2025-12-06T03:02:25.499Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=1, accelerator=H100, ttft=18.63ms, itl=8.83ms, cost=100.00, maxBatch=256, arrivalRate=576.68, avgInputTokens=241.11, avgOutputTokens=443.98"}
{"level":"DEBUG","ts":"2025-12-06T03:02:25.499Z","msg":"Experimental model tuner is enabled globally (EXPERIMENTAL_MODEL_TUNER_ENABLED=true) tuning model performance parameters for active VAs"}
{"level":"DEBUG","ts":"2025-12-06T03:02:25.499Z","msg":"Using state vals from VA status to tune variant ms-inference-scheduling-llm-d-modelservice-decode: alpha= 6.830044, beta= 0.046336, gamma= 16.640581, delta= 0.000321"}
{"level":"DEBUG","ts":"2025-12-06T03:02:25.499Z","msg":"[Tuner Config] variant=ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler | Initial state: alpha=6.830044, beta=0.046336, gamma=16.640581, delta=0.000321 | SLO targets: TTFT=1000.00ms, ITL=10.00ms | Has covariance=true"}
{"level":"INFO","ts":"2025-12-06T03:02:25.500Z","msg":"Tuner validation succeeded for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler, server ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler - New NIS: 0.183919"}
{"level":"DEBUG","ts":"2025-12-06T03:02:25.500Z","msg":"Model tuner results: model=unsloth/Meta-Llama-3.1-8B, accelerator=H100, alpha=6.941481, beta=0.047885, gamma=16.612795, delta=0.000321, NIS=0.18"}
{"level":"DEBUG","ts":"2025-12-06T03:02:25.500Z","msg":"Updated tuner status for variant ms-inference-scheduling-llm-d-modelservice-decode: alpha=6.941481, beta=0.047885, gamma=16.612795, delta=0.000321, NIS=0.183919"}
{"level":"INFO","ts":"2025-12-06T03:02:25.500Z","msg":"Tuned performance parameters result: variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler - alpha: 6.941481, beta: 0.047885, gamma: 16.612795, delta: 0.000321"}
{"level":"DEBUG","ts":"2025-12-06T03:02:25.509Z","msg":"Optimization solution - system: Solution: \ns=ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler; c=Premium; m=unsloth/Meta-Llama-3.1-8B; rate=576.68; inTk=241; outTk=443; sol=1, sat=false, alloc={acc=H100; numRep=1; maxBatch=512; cost=100, val=0, itl=8.785601, ttft=19.592049, rho=0.073264405, maxRPM=849.32654}; slo-itl=10, slo-ttft=1000, slo-tps=0 \nAllocationByType: \nname=NVIDIA-H100-80GB-HBM3, count=1, limit=0, cost=100 \ntotalCost=100 \n"}
{"level":"DEBUG","ts":"2025-12-06T03:02:25.509Z","msg":"Setting accelerator name: Name=H100, allocationData={H100 1 512 100 8.785601 19.592049 {576.68 241 443}}"}
{"level":"INFO","ts":"2025-12-06T03:02:25.509Z","msg":"Model-based optimization completed for model: unsloth/Meta-Llama-3.1-8B - model-based targets: map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"WARN","ts":"2025-12-06T03:02:25.509Z","msg":"saturation analysis unavailable, using model-based targets only: modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-06T03:02:25.509Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-12-06T03:02:25.509Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1→target=1"}
{"level":"DEBUG","ts":"2025-12-06T03:02:25.509Z","msg":"Found VA in map: variant=ms-inference-scheduling-llm-d-modelservice-decode, hasCurrentAlloc=true, accelerator=H100"}
{"level":"INFO","ts":"2025-12-06T03:02:25.509Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 1, desired-replicas: 1, accelerator: H100"}
{"level":"INFO","ts":"2025-12-06T03:02:25.509Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=1, accelerator=H100, SaturationOnly=false"}
{"level":"INFO","ts":"2025-12-06T03:02:25.515Z","msg":"Applied Saturation decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1, target=1, reason=model-based only (Saturation unavailable)"}
{"level":"INFO","ts":"2025-12-06T03:02:25.515Z","msg":"Reconciliation completed successfully: mode=model-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-12-06T03:02:25.545Z","msg":"saturation scaling config cache updated: entries=1, has_default=true"}
{"level":"INFO","ts":"2025-12-06T03:02:25.545Z","msg":"saturation scaling configuration loaded successfully"}
{"level":"INFO","ts":"2025-12-06T03:02:25.545Z","msg":"Operating in MODEL-ONLY mode: model-based optimization only"}
{"level":"INFO","ts":"2025-12-06T03:02:25.545Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-12-06T03:02:25.545Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-12-06T03:02:25.547Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dczzts, queueLength=0"}
{"level":"INFO","ts":"2025-12-06T03:02:25.547Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dczzts, usage=0.051 (5.1%)"}
{"level":"INFO","ts":"2025-12-06T03:02:25.547Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984ds7q94, usage=0.043 (4.3%)"}
{"level":"DEBUG","ts":"2025-12-06T03:02:25.547Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=2"}
{"level":"INFO","ts":"2025-12-06T03:02:25.547Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984ds7q94, queueLength=0"}
{"level":"DEBUG","ts":"2025-12-06T03:02:25.547Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=2"}
{"level":"DEBUG","ts":"2025-12-06T03:02:25.551Z","msg":"Pod-to-variant matching successful: totalPods=2, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:2]"}
{"level":"DEBUG","ts":"2025-12-06T03:02:25.551Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=2"}
{"level":"DEBUG","ts":"2025-12-06T03:02:25.551Z","msg":"Collected capacity metrics for observability in model-only mode: modelID=unsloth/Meta-Llama-3.1-8B, replicaCount=2"}
{"level":"INFO","ts":"2025-12-06T03:02:25.551Z","msg":"Found SLO for model: model=unsloth/Meta-Llama-3.1-8B, class=Premium, slo-tpot=10, slo-ttft=1000"}
{"level":"DEBUG","ts":"2025-12-06T03:02:25.559Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=1, accelerator=H100, ttft=18.63ms, itl=8.83ms, cost=100.00, maxBatch=256, arrivalRate=576.68, avgInputTokens=241.11, avgOutputTokens=443.98"}
{"level":"DEBUG","ts":"2025-12-06T03:02:25.559Z","msg":"Experimental model tuner is enabled globally (EXPERIMENTAL_MODEL_TUNER_ENABLED=true) tuning model performance parameters for active VAs"}
{"level":"DEBUG","ts":"2025-12-06T03:02:25.559Z","msg":"Using state vals from VA status to tune variant ms-inference-scheduling-llm-d-modelservice-decode: alpha= 6.941481, beta= 0.047885, gamma= 16.612795, delta= 0.000321"}
{"level":"DEBUG","ts":"2025-12-06T03:02:25.559Z","msg":"[Tuner Config] variant=ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler | Initial state: alpha=6.941481, beta=0.047885, gamma=16.612795, delta=0.000321 | SLO targets: TTFT=1000.00ms, ITL=10.00ms | Has covariance=true"}
{"level":"INFO","ts":"2025-12-06T03:02:25.560Z","msg":"Tuner validation succeeded for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler, server ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler - New NIS: 0.007513"}
{"level":"DEBUG","ts":"2025-12-06T03:02:25.560Z","msg":"Model tuner results: model=unsloth/Meta-Llama-3.1-8B, accelerator=H100, alpha=6.965749, beta=0.047988, gamma=16.583254, delta=0.000321, NIS=0.01"}
{"level":"DEBUG","ts":"2025-12-06T03:02:25.560Z","msg":"Updated tuner status for variant ms-inference-scheduling-llm-d-modelservice-decode: alpha=6.965749, beta=0.047988, gamma=16.583254, delta=0.000321, NIS=0.007513"}
{"level":"INFO","ts":"2025-12-06T03:02:25.560Z","msg":"Tuned performance parameters result: variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler - alpha: 6.965749, beta: 0.047988, gamma: 16.583254, delta: 0.000321"}
{"level":"DEBUG","ts":"2025-12-06T03:02:25.569Z","msg":"Optimization solution - system: Solution: \ns=ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler; c=Premium; m=unsloth/Meta-Llama-3.1-8B; rate=576.68; inTk=241; outTk=443; sol=1, sat=false, alloc={acc=H100; numRep=1; maxBatch=512; cost=100, val=0, itl=8.821054, ttft=19.57414, rho=0.07355824, maxRPM=840.6587}; slo-itl=10, slo-ttft=1000, slo-tps=0 \nAllocationByType: \nname=NVIDIA-H100-80GB-HBM3, count=1, limit=0, cost=100 \ntotalCost=100 \n"}
{"level":"DEBUG","ts":"2025-12-06T03:02:25.569Z","msg":"Setting accelerator name: Name=H100, allocationData={H100 1 512 100 8.821054 19.57414 {576.68 241 443}}"}
{"level":"INFO","ts":"2025-12-06T03:02:25.569Z","msg":"Model-based optimization completed for model: unsloth/Meta-Llama-3.1-8B - model-based targets: map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"WARN","ts":"2025-12-06T03:02:25.569Z","msg":"saturation analysis unavailable, using model-based targets only: modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-06T03:02:25.569Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-12-06T03:02:25.569Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1→target=1"}
{"level":"DEBUG","ts":"2025-12-06T03:02:25.569Z","msg":"Found VA in map: variant=ms-inference-scheduling-llm-d-modelservice-decode, hasCurrentAlloc=true, accelerator=H100"}
{"level":"INFO","ts":"2025-12-06T03:02:25.569Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 1, desired-replicas: 1, accelerator: H100"}
{"level":"INFO","ts":"2025-12-06T03:02:25.569Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=1, accelerator=H100, SaturationOnly=false"}
{"level":"INFO","ts":"2025-12-06T03:02:25.574Z","msg":"Applied Saturation decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1, target=1, reason=model-based only (Saturation unavailable)"}
{"level":"INFO","ts":"2025-12-06T03:02:25.574Z","msg":"Reconciliation completed successfully: mode=model-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-12-06T03:03:25.515Z","msg":"saturation scaling config cache updated: entries=1, has_default=true"}
{"level":"INFO","ts":"2025-12-06T03:03:25.515Z","msg":"saturation scaling configuration loaded successfully"}
{"level":"INFO","ts":"2025-12-06T03:03:25.515Z","msg":"Operating in MODEL-ONLY mode: model-based optimization only"}
{"level":"INFO","ts":"2025-12-06T03:03:25.515Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-12-06T03:03:25.515Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-12-06T03:03:25.520Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dczzts, queueLength=0"}
{"level":"DEBUG","ts":"2025-12-06T03:03:25.520Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"INFO","ts":"2025-12-06T03:03:25.520Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dczzts, usage=0.073 (7.3%)"}
{"level":"DEBUG","ts":"2025-12-06T03:03:25.520Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"DEBUG","ts":"2025-12-06T03:03:25.522Z","msg":"Pod-to-variant matching successful: totalPods=1, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"DEBUG","ts":"2025-12-06T03:03:25.522Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=1"}
{"level":"DEBUG","ts":"2025-12-06T03:03:25.522Z","msg":"Collected capacity metrics for observability in model-only mode: modelID=unsloth/Meta-Llama-3.1-8B, replicaCount=1"}
{"level":"INFO","ts":"2025-12-06T03:03:25.522Z","msg":"Found SLO for model: model=unsloth/Meta-Llama-3.1-8B, class=Premium, slo-tpot=10, slo-ttft=1000"}
{"level":"DEBUG","ts":"2025-12-06T03:03:25.531Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=1, accelerator=H100, ttft=17.91ms, itl=8.91ms, cost=100.00, maxBatch=256, arrivalRate=598.69, avgInputTokens=219.46, avgOutputTokens=465.91"}
{"level":"DEBUG","ts":"2025-12-06T03:03:25.531Z","msg":"Experimental model tuner is enabled globally (EXPERIMENTAL_MODEL_TUNER_ENABLED=true) tuning model performance parameters for active VAs"}
{"level":"DEBUG","ts":"2025-12-06T03:03:25.531Z","msg":"Using state vals from VA status to tune variant ms-inference-scheduling-llm-d-modelservice-decode: alpha= 6.965749, beta= 0.047988, gamma= 16.583254, delta= 0.000321"}
{"level":"DEBUG","ts":"2025-12-06T03:03:25.531Z","msg":"[Tuner Config] variant=ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler | Initial state: alpha=6.965749, beta=0.047988, gamma=16.583254, delta=0.000321 | SLO targets: TTFT=1000.00ms, ITL=10.00ms | Has covariance=true"}
{"level":"INFO","ts":"2025-12-06T03:03:25.531Z","msg":"Tuner validation succeeded for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler, server ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler - New NIS: 0.045657"}
{"level":"DEBUG","ts":"2025-12-06T03:03:25.531Z","msg":"Model tuner results: model=unsloth/Meta-Llama-3.1-8B, accelerator=H100, alpha=6.904738, beta=0.047660, gamma=16.532450, delta=0.000321, NIS=0.05"}
{"level":"DEBUG","ts":"2025-12-06T03:03:25.531Z","msg":"Updated tuner status for variant ms-inference-scheduling-llm-d-modelservice-decode: alpha=6.904738, beta=0.047660, gamma=16.532450, delta=0.000321, NIS=0.045657"}
{"level":"INFO","ts":"2025-12-06T03:03:25.531Z","msg":"Tuned performance parameters result: variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler - alpha: 6.904738, beta: 0.047660, gamma: 16.532450, delta: 0.000321"}
{"level":"DEBUG","ts":"2025-12-06T03:03:25.539Z","msg":"Optimization solution - system: Solution: \ns=ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler; c=Premium; m=unsloth/Meta-Llama-3.1-8B; rate=598.69; inTk=219; outTk=465; sol=1, sat=false, alloc={acc=H100; numRep=1; maxBatch=512; cost=100, val=0, itl=8.932815, ttft=19.523792, rho=0.08115737, maxRPM=823.1193}; slo-itl=10, slo-ttft=1000, slo-tps=0 \nAllocationByType: \nname=NVIDIA-H100-80GB-HBM3, count=1, limit=0, cost=100 \ntotalCost=100 \n"}
{"level":"DEBUG","ts":"2025-12-06T03:03:25.539Z","msg":"Setting accelerator name: Name=H100, allocationData={H100 1 512 100 8.932815 19.523792 {598.69 219 465}}"}
{"level":"INFO","ts":"2025-12-06T03:03:25.539Z","msg":"Model-based optimization completed for model: unsloth/Meta-Llama-3.1-8B - model-based targets: map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"WARN","ts":"2025-12-06T03:03:25.539Z","msg":"saturation analysis unavailable, using model-based targets only: modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-06T03:03:25.539Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-12-06T03:03:25.539Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1→target=1"}
{"level":"DEBUG","ts":"2025-12-06T03:03:25.539Z","msg":"Found VA in map: variant=ms-inference-scheduling-llm-d-modelservice-decode, hasCurrentAlloc=true, accelerator=H100"}
{"level":"INFO","ts":"2025-12-06T03:03:25.540Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 1, desired-replicas: 1, accelerator: H100"}
{"level":"INFO","ts":"2025-12-06T03:03:25.540Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=1, accelerator=H100, SaturationOnly=false"}
{"level":"INFO","ts":"2025-12-06T03:03:25.545Z","msg":"Applied Saturation decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1, target=1, reason=model-based only (Saturation unavailable)"}
{"level":"INFO","ts":"2025-12-06T03:03:25.545Z","msg":"Reconciliation completed successfully: mode=model-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-12-06T03:03:25.575Z","msg":"saturation scaling config cache updated: entries=1, has_default=true"}
{"level":"INFO","ts":"2025-12-06T03:03:25.575Z","msg":"saturation scaling configuration loaded successfully"}
{"level":"INFO","ts":"2025-12-06T03:03:25.575Z","msg":"Operating in MODEL-ONLY mode: model-based optimization only"}
{"level":"INFO","ts":"2025-12-06T03:03:25.575Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-12-06T03:03:25.575Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-12-06T03:03:25.577Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dczzts, queueLength=0"}
{"level":"DEBUG","ts":"2025-12-06T03:03:25.577Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"INFO","ts":"2025-12-06T03:03:25.577Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dczzts, usage=0.073 (7.3%)"}
{"level":"DEBUG","ts":"2025-12-06T03:03:25.577Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"DEBUG","ts":"2025-12-06T03:03:25.579Z","msg":"Pod-to-variant matching successful: totalPods=1, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"DEBUG","ts":"2025-12-06T03:03:25.579Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=1"}
{"level":"DEBUG","ts":"2025-12-06T03:03:25.579Z","msg":"Collected capacity metrics for observability in model-only mode: modelID=unsloth/Meta-Llama-3.1-8B, replicaCount=1"}
{"level":"INFO","ts":"2025-12-06T03:03:25.579Z","msg":"Found SLO for model: model=unsloth/Meta-Llama-3.1-8B, class=Premium, slo-tpot=10, slo-ttft=1000"}
{"level":"DEBUG","ts":"2025-12-06T03:03:25.587Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=1, accelerator=H100, ttft=17.91ms, itl=8.91ms, cost=100.00, maxBatch=256, arrivalRate=598.69, avgInputTokens=219.46, avgOutputTokens=465.91"}
{"level":"DEBUG","ts":"2025-12-06T03:03:25.587Z","msg":"Experimental model tuner is enabled globally (EXPERIMENTAL_MODEL_TUNER_ENABLED=true) tuning model performance parameters for active VAs"}
{"level":"DEBUG","ts":"2025-12-06T03:03:25.587Z","msg":"Using state vals from VA status to tune variant ms-inference-scheduling-llm-d-modelservice-decode: alpha= 6.904738, beta= 0.047660, gamma= 16.532450, delta= 0.000321"}
{"level":"DEBUG","ts":"2025-12-06T03:03:25.587Z","msg":"[Tuner Config] variant=ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler | Initial state: alpha=6.904738, beta=0.047660, gamma=16.532450, delta=0.000321 | SLO targets: TTFT=1000.00ms, ITL=10.00ms | Has covariance=true"}
{"level":"INFO","ts":"2025-12-06T03:03:25.587Z","msg":"Tuner validation succeeded for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler, server ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler - New NIS: 0.005424"}
{"level":"DEBUG","ts":"2025-12-06T03:03:25.587Z","msg":"Model tuner results: model=unsloth/Meta-Llama-3.1-8B, accelerator=H100, alpha=6.891732, beta=0.047630, gamma=16.483240, delta=0.000321, NIS=0.01"}
{"level":"DEBUG","ts":"2025-12-06T03:03:25.587Z","msg":"Updated tuner status for variant ms-inference-scheduling-llm-d-modelservice-decode: alpha=6.891732, beta=0.047630, gamma=16.483240, delta=0.000321, NIS=0.005424"}
{"level":"INFO","ts":"2025-12-06T03:03:25.587Z","msg":"Tuned performance parameters result: variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler - alpha: 6.891732, beta: 0.047630, gamma: 16.483240, delta: 0.000321"}
{"level":"DEBUG","ts":"2025-12-06T03:03:25.595Z","msg":"Optimization solution - system: Solution: \ns=ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler; c=Premium; m=unsloth/Meta-Llama-3.1-8B; rate=598.69; inTk=219; outTk=465; sol=1, sat=false, alloc={acc=H100; numRep=1; maxBatch=512; cost=100, val=0, itl=8.914454, ttft=19.468576, rho=0.08099027, maxRPM=827.1678}; slo-itl=10, slo-ttft=1000, slo-tps=0 \nAllocationByType: \nname=NVIDIA-H100-80GB-HBM3, count=1, limit=0, cost=100 \ntotalCost=100 \n"}
{"level":"DEBUG","ts":"2025-12-06T03:03:25.595Z","msg":"Setting accelerator name: Name=H100, allocationData={H100 1 512 100 8.914454 19.468576 {598.69 219 465}}"}
{"level":"INFO","ts":"2025-12-06T03:03:25.595Z","msg":"Model-based optimization completed for model: unsloth/Meta-Llama-3.1-8B - model-based targets: map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"WARN","ts":"2025-12-06T03:03:25.595Z","msg":"saturation analysis unavailable, using model-based targets only: modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-06T03:03:25.595Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-12-06T03:03:25.595Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1→target=1"}
{"level":"DEBUG","ts":"2025-12-06T03:03:25.595Z","msg":"Found VA in map: variant=ms-inference-scheduling-llm-d-modelservice-decode, hasCurrentAlloc=true, accelerator=H100"}
{"level":"INFO","ts":"2025-12-06T03:03:25.595Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 1, desired-replicas: 1, accelerator: H100"}
{"level":"INFO","ts":"2025-12-06T03:03:25.595Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=1, accelerator=H100, SaturationOnly=false"}
{"level":"INFO","ts":"2025-12-06T03:03:25.602Z","msg":"Applied Saturation decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1, target=1, reason=model-based only (Saturation unavailable)"}
{"level":"INFO","ts":"2025-12-06T03:03:25.602Z","msg":"Reconciliation completed successfully: mode=model-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-12-06T03:04:25.546Z","msg":"saturation scaling config cache updated: entries=1, has_default=true"}
{"level":"INFO","ts":"2025-12-06T03:04:25.546Z","msg":"saturation scaling configuration loaded successfully"}
{"level":"INFO","ts":"2025-12-06T03:04:25.546Z","msg":"Operating in MODEL-ONLY mode: model-based optimization only"}
{"level":"INFO","ts":"2025-12-06T03:04:25.546Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-12-06T03:04:25.546Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-12-06T03:04:25.556Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dczzts, queueLength=0"}
{"level":"DEBUG","ts":"2025-12-06T03:04:25.556Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"INFO","ts":"2025-12-06T03:04:25.556Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dczzts, usage=0.000 (0.0%)"}
{"level":"DEBUG","ts":"2025-12-06T03:04:25.556Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"DEBUG","ts":"2025-12-06T03:04:25.558Z","msg":"Pod-to-variant matching successful: totalPods=1, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"DEBUG","ts":"2025-12-06T03:04:25.558Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=1"}
{"level":"DEBUG","ts":"2025-12-06T03:04:25.558Z","msg":"Collected capacity metrics for observability in model-only mode: modelID=unsloth/Meta-Llama-3.1-8B, replicaCount=1"}
{"level":"INFO","ts":"2025-12-06T03:04:25.558Z","msg":"Found SLO for model: model=unsloth/Meta-Llama-3.1-8B, class=Premium, slo-tpot=10, slo-ttft=1000"}
{"level":"DEBUG","ts":"2025-12-06T03:04:25.567Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=1, accelerator=H100, ttft=0.00ms, itl=0.00ms, cost=100.00, maxBatch=256, arrivalRate=0.00, avgInputTokens=0.00, avgOutputTokens=0.00"}
{"level":"DEBUG","ts":"2025-12-06T03:04:25.567Z","msg":"Experimental model tuner is enabled globally (EXPERIMENTAL_MODEL_TUNER_ENABLED=true) tuning model performance parameters for active VAs"}
{"level":"DEBUG","ts":"2025-12-06T03:04:25.567Z","msg":"Using state vals from VA status to tune variant ms-inference-scheduling-llm-d-modelservice-decode: alpha= 6.891732, beta= 0.047630, gamma= 16.483240, delta= 0.000321"}
{"level":"DEBUG","ts":"2025-12-06T03:04:25.567Z","msg":"[Tuner Config] variant=ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler | Initial state: alpha=6.891732, beta=0.047630, gamma=16.483240, delta=0.000321 | SLO targets: TTFT=1000.00ms, ITL=10.00ms | Has covariance=true"}
{"level":"WARN","ts":"2025-12-06T03:04:25.567Z","msg":"Tuner failed completely for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler: failed to get/create tuner: failed to create tuner: invalid environment: &{0 0 0 256 0 0 1}. Using fallback parameters."}
{"level":"INFO","ts":"2025-12-06T03:04:25.567Z","msg":"Using tuned parameters from status for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler"}
{"level":"DEBUG","ts":"2025-12-06T03:04:25.567Z","msg":"Updated SystemData for model=unsloth/Meta-Llama-3.1-8B, accelerator=H100: alpha=6.891732, beta=0.047630, gamma=16.483240, delta=0.000321"}
{"level":"DEBUG","ts":"2025-12-06T03:04:25.567Z","msg":"Optimization solution - system: Solution: \ns=ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler; c=Premium; m=unsloth/Meta-Llama-3.1-8B; rate=0; inTk=0; outTk=0; sol=1, sat=false, alloc={acc=H100; numRep=1; maxBatch=512; cost=100, val=0, itl=6.939362, ttft=16.48356, rho=0, maxRPM=643191.1}; slo-itl=10, slo-ttft=1000, slo-tps=0 \nAllocationByType: \nname=NVIDIA-H100-80GB-HBM3, count=1, limit=0, cost=100 \ntotalCost=100 \n"}
{"level":"DEBUG","ts":"2025-12-06T03:04:25.567Z","msg":"Setting accelerator name: Name=H100, allocationData={H100 1 512 100 6.939362 16.48356 {0 0 0}}"}
{"level":"INFO","ts":"2025-12-06T03:04:25.567Z","msg":"Model-based optimization completed for model: unsloth/Meta-Llama-3.1-8B - model-based targets: map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"WARN","ts":"2025-12-06T03:04:25.567Z","msg":"saturation analysis unavailable, using model-based targets only: modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-06T03:04:25.567Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-12-06T03:04:25.567Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1→target=1"}
{"level":"DEBUG","ts":"2025-12-06T03:04:25.567Z","msg":"Found VA in map: variant=ms-inference-scheduling-llm-d-modelservice-decode, hasCurrentAlloc=true, accelerator=H100"}
{"level":"INFO","ts":"2025-12-06T03:04:25.567Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 1, desired-replicas: 1, accelerator: H100"}
{"level":"INFO","ts":"2025-12-06T03:04:25.567Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=1, accelerator=H100, SaturationOnly=false"}
{"level":"INFO","ts":"2025-12-06T03:04:25.573Z","msg":"Applied Saturation decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1, target=1, reason=model-based only (Saturation unavailable)"}
{"level":"INFO","ts":"2025-12-06T03:04:25.573Z","msg":"Reconciliation completed successfully: mode=model-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-12-06T03:04:25.603Z","msg":"saturation scaling config cache updated: entries=1, has_default=true"}
{"level":"INFO","ts":"2025-12-06T03:04:25.603Z","msg":"saturation scaling configuration loaded successfully"}
{"level":"INFO","ts":"2025-12-06T03:04:25.603Z","msg":"Operating in MODEL-ONLY mode: model-based optimization only"}
{"level":"INFO","ts":"2025-12-06T03:04:25.603Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-12-06T03:04:25.603Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-12-06T03:04:25.605Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dczzts, queueLength=0"}
{"level":"DEBUG","ts":"2025-12-06T03:04:25.605Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"INFO","ts":"2025-12-06T03:04:25.605Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dczzts, usage=0.000 (0.0%)"}
{"level":"DEBUG","ts":"2025-12-06T03:04:25.605Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"DEBUG","ts":"2025-12-06T03:04:25.607Z","msg":"Pod-to-variant matching successful: totalPods=1, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"DEBUG","ts":"2025-12-06T03:04:25.607Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=1"}
{"level":"DEBUG","ts":"2025-12-06T03:04:25.607Z","msg":"Collected capacity metrics for observability in model-only mode: modelID=unsloth/Meta-Llama-3.1-8B, replicaCount=1"}
{"level":"INFO","ts":"2025-12-06T03:04:25.607Z","msg":"Found SLO for model: model=unsloth/Meta-Llama-3.1-8B, class=Premium, slo-tpot=10, slo-ttft=1000"}
{"level":"DEBUG","ts":"2025-12-06T03:04:25.615Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=1, accelerator=H100, ttft=0.00ms, itl=0.00ms, cost=100.00, maxBatch=256, arrivalRate=0.00, avgInputTokens=0.00, avgOutputTokens=0.00"}
{"level":"DEBUG","ts":"2025-12-06T03:04:25.615Z","msg":"Experimental model tuner is enabled globally (EXPERIMENTAL_MODEL_TUNER_ENABLED=true) tuning model performance parameters for active VAs"}
{"level":"DEBUG","ts":"2025-12-06T03:04:25.615Z","msg":"Using state vals from VA status to tune variant ms-inference-scheduling-llm-d-modelservice-decode: alpha= 6.891732, beta= 0.047630, gamma= 16.483240, delta= 0.000321"}
{"level":"DEBUG","ts":"2025-12-06T03:04:25.615Z","msg":"[Tuner Config] variant=ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler | Initial state: alpha=6.891732, beta=0.047630, gamma=16.483240, delta=0.000321 | SLO targets: TTFT=1000.00ms, ITL=10.00ms | Has covariance=true"}
{"level":"WARN","ts":"2025-12-06T03:04:25.615Z","msg":"Tuner failed completely for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler: failed to get/create tuner: failed to create tuner: invalid environment: &{0 0 0 256 0 0 1}. Using fallback parameters."}
{"level":"INFO","ts":"2025-12-06T03:04:25.615Z","msg":"Using tuned parameters from status for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler"}
{"level":"DEBUG","ts":"2025-12-06T03:04:25.615Z","msg":"Updated SystemData for model=unsloth/Meta-Llama-3.1-8B, accelerator=H100: alpha=6.891732, beta=0.047630, gamma=16.483240, delta=0.000321"}
{"level":"DEBUG","ts":"2025-12-06T03:04:25.615Z","msg":"Optimization solution - system: Solution: \ns=ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler; c=Premium; m=unsloth/Meta-Llama-3.1-8B; rate=0; inTk=0; outTk=0; sol=1, sat=false, alloc={acc=H100; numRep=1; maxBatch=512; cost=100, val=0, itl=6.939362, ttft=16.48356, rho=0, maxRPM=643191.1}; slo-itl=10, slo-ttft=1000, slo-tps=0 \nAllocationByType: \nname=NVIDIA-H100-80GB-HBM3, count=1, limit=0, cost=100 \ntotalCost=100 \n"}
{"level":"DEBUG","ts":"2025-12-06T03:04:25.615Z","msg":"Setting accelerator name: Name=H100, allocationData={H100 1 512 100 6.939362 16.48356 {0 0 0}}"}
{"level":"INFO","ts":"2025-12-06T03:04:25.615Z","msg":"Model-based optimization completed for model: unsloth/Meta-Llama-3.1-8B - model-based targets: map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"WARN","ts":"2025-12-06T03:04:25.615Z","msg":"saturation analysis unavailable, using model-based targets only: modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-06T03:04:25.615Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-12-06T03:04:25.615Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1→target=1"}
{"level":"DEBUG","ts":"2025-12-06T03:04:25.615Z","msg":"Found VA in map: variant=ms-inference-scheduling-llm-d-modelservice-decode, hasCurrentAlloc=true, accelerator=H100"}
{"level":"INFO","ts":"2025-12-06T03:04:25.615Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 1, desired-replicas: 1, accelerator: H100"}
{"level":"INFO","ts":"2025-12-06T03:04:25.615Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=1, accelerator=H100, SaturationOnly=false"}
{"level":"INFO","ts":"2025-12-06T03:04:25.620Z","msg":"Applied Saturation decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1, target=1, reason=model-based only (Saturation unavailable)"}
{"level":"INFO","ts":"2025-12-06T03:04:25.620Z","msg":"Reconciliation completed successfully: mode=model-only, modelsProcessed=1, decisionsApplied=1"}
