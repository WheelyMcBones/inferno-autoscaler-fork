{"level":"INFO","ts":"2025-12-10T04:58:56.125Z","msg":"Operating in MODEL-ONLY mode: model-based optimization only"}
{"level":"INFO","ts":"2025-12-10T04:58:56.125Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-12-10T04:58:56.125Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-12-10T04:58:56.130Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dgftqv, usage=0.000 (0.0%)"}
{"level":"DEBUG","ts":"2025-12-10T04:58:56.130Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"INFO","ts":"2025-12-10T04:58:56.130Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dgftqv, queueLength=0"}
{"level":"DEBUG","ts":"2025-12-10T04:58:56.130Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"DEBUG","ts":"2025-12-10T04:58:56.132Z","msg":"Pod-to-variant matching successful: totalPods=1, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"DEBUG","ts":"2025-12-10T04:58:56.132Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=1"}
{"level":"DEBUG","ts":"2025-12-10T04:58:56.132Z","msg":"Collected capacity metrics for observability in model-only mode: modelID=unsloth/Meta-Llama-3.1-8B, replicaCount=1"}
{"level":"DEBUG","ts":"2025-12-10T04:58:56.144Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=1, accelerator=H100, ttft=0.00ms, itl=0.00ms, cost=100.00, maxBatch=256, arrivalRate=0.00, avgInputTokens=0.00, avgOutputTokens=0.00"}
{"level":"DEBUG","ts":"2025-12-10T04:58:56.144Z","msg":"Experimental model tuner is enabled globally: tuning model performance parameters for active VAs"}
{"level":"DEBUG","ts":"2025-12-10T04:58:56.144Z","msg":"Using state vals from VA status to tune variant ms-inference-scheduling-llm-d-modelservice-decode: alpha= 7.293937, beta= 0.046236, gamma= 16.286325, delta= 0.000259"}
{"level":"DEBUG","ts":"2025-12-10T04:58:56.144Z","msg":"Skipping tuning for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler due to insufficient load. Using fallback parameters."}
{"level":"INFO","ts":"2025-12-10T04:58:56.144Z","msg":"Using tuned parameters from status for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler"}
{"level":"DEBUG","ts":"2025-12-10T04:58:56.144Z","msg":"Updated SystemData for model=unsloth/Meta-Llama-3.1-8B, accelerator=H100: alpha=7.293937, beta=0.046236, gamma=16.286325, delta=0.000259"}
{"level":"DEBUG","ts":"2025-12-10T04:58:56.144Z","msg":"Optimization solution - system: Solution: \ns=ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler; c=Premium; m=unsloth/Meta-Llama-3.1-8B; rate=0; inTk=0; outTk=0; sol=1, sat=false, alloc={acc=H100; numRep=1; maxBatch=512; cost=100, val=0, itl=7.3401732, ttft=16.286585, rho=0, maxRPM=650112.56}; slo-itl=10, slo-ttft=1000, slo-tps=0 \nAllocationByType: \nname=NVIDIA-H100-80GB-HBM3, count=1, limit=0, cost=100 \ntotalCost=100 \n"}
{"level":"DEBUG","ts":"2025-12-10T04:58:56.144Z","msg":"Setting accelerator name: Name=H100, allocationData={H100 1 512 100 7.3401732 16.286585 {0 0 0}}"}
{"level":"INFO","ts":"2025-12-10T04:58:56.144Z","msg":"Model-based optimization completed for model: unsloth/Meta-Llama-3.1-8B - model-based targets: map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"WARN","ts":"2025-12-10T04:58:56.144Z","msg":"saturation analysis unavailable, using model-based targets only: modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-10T04:58:56.144Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-12-10T04:58:56.144Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1→target=1"}
{"level":"INFO","ts":"2025-12-10T04:58:56.144Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 1, desired-replicas: 1, accelerator: H100"}
{"level":"INFO","ts":"2025-12-10T04:58:56.144Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=1, accelerator=H100, SaturationOnly=false"}
{"level":"INFO","ts":"2025-12-10T04:58:56.149Z","msg":"Applied Saturation decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1, target=1, reason=model-based only (Saturation unavailable)"}
{"level":"INFO","ts":"2025-12-10T04:58:56.149Z","msg":"Reconciliation completed successfully: mode=model-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-12-10T04:59:56.150Z","msg":"Reconciling VariantAutoscaling: name=ms-inference-scheduling-llm-d-modelservice-decode, namespace=llm-d-inference-scheduler, modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-10T04:59:56.150Z","msg":"saturation scaling config cache updated: entries=1, has_default=true"}
{"level":"INFO","ts":"2025-12-10T04:59:56.150Z","msg":"saturation scaling configuration loaded successfully"}
{"level":"INFO","ts":"2025-12-10T04:59:56.150Z","msg":"Operating in MODEL-ONLY mode: model-based optimization only"}
{"level":"INFO","ts":"2025-12-10T04:59:56.150Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-12-10T04:59:56.150Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-12-10T04:59:56.156Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dgftqv, queueLength=0"}
{"level":"DEBUG","ts":"2025-12-10T04:59:56.156Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"INFO","ts":"2025-12-10T04:59:56.167Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dgftqv, usage=0.000 (0.0%)"}
{"level":"DEBUG","ts":"2025-12-10T04:59:56.167Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"DEBUG","ts":"2025-12-10T04:59:56.169Z","msg":"Pod-to-variant matching successful: totalPods=1, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"DEBUG","ts":"2025-12-10T04:59:56.169Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=1"}
{"level":"DEBUG","ts":"2025-12-10T04:59:56.169Z","msg":"Collected capacity metrics for observability in model-only mode: modelID=unsloth/Meta-Llama-3.1-8B, replicaCount=1"}
{"level":"DEBUG","ts":"2025-12-10T04:59:56.180Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=1, accelerator=H100, ttft=0.00ms, itl=0.00ms, cost=100.00, maxBatch=256, arrivalRate=0.00, avgInputTokens=0.00, avgOutputTokens=0.00"}
{"level":"DEBUG","ts":"2025-12-10T04:59:56.180Z","msg":"Experimental model tuner is enabled globally: tuning model performance parameters for active VAs"}
{"level":"DEBUG","ts":"2025-12-10T04:59:56.180Z","msg":"Using state vals from VA status to tune variant ms-inference-scheduling-llm-d-modelservice-decode: alpha= 7.293937, beta= 0.046236, gamma= 16.286325, delta= 0.000259"}
{"level":"DEBUG","ts":"2025-12-10T04:59:56.180Z","msg":"Skipping tuning for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler due to insufficient load. Using fallback parameters."}
{"level":"INFO","ts":"2025-12-10T04:59:56.180Z","msg":"Using tuned parameters from status for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler"}
{"level":"DEBUG","ts":"2025-12-10T04:59:56.180Z","msg":"Updated SystemData for model=unsloth/Meta-Llama-3.1-8B, accelerator=H100: alpha=7.293937, beta=0.046236, gamma=16.286325, delta=0.000259"}
{"level":"DEBUG","ts":"2025-12-10T04:59:56.180Z","msg":"Optimization solution - system: Solution: \ns=ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler; c=Premium; m=unsloth/Meta-Llama-3.1-8B; rate=0; inTk=0; outTk=0; sol=1, sat=false, alloc={acc=H100; numRep=1; maxBatch=512; cost=100, val=0, itl=7.3401732, ttft=16.286585, rho=0, maxRPM=650112.56}; slo-itl=10, slo-ttft=1000, slo-tps=0 \nAllocationByType: \nname=NVIDIA-H100-80GB-HBM3, count=1, limit=0, cost=100 \ntotalCost=100 \n"}
{"level":"DEBUG","ts":"2025-12-10T04:59:56.180Z","msg":"Setting accelerator name: Name=H100, allocationData={H100 1 512 100 7.3401732 16.286585 {0 0 0}}"}
{"level":"INFO","ts":"2025-12-10T04:59:56.180Z","msg":"Model-based optimization completed for model: unsloth/Meta-Llama-3.1-8B - model-based targets: map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"WARN","ts":"2025-12-10T04:59:56.180Z","msg":"saturation analysis unavailable, using model-based targets only: modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-10T04:59:56.180Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-12-10T04:59:56.180Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1→target=1"}
{"level":"INFO","ts":"2025-12-10T04:59:56.180Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 1, desired-replicas: 1, accelerator: H100"}
{"level":"INFO","ts":"2025-12-10T04:59:56.180Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=1, accelerator=H100, SaturationOnly=false"}
{"level":"INFO","ts":"2025-12-10T04:59:56.185Z","msg":"Applied Saturation decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1, target=1, reason=model-based only (Saturation unavailable)"}
{"level":"INFO","ts":"2025-12-10T04:59:56.185Z","msg":"Reconciliation completed successfully: mode=model-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-12-10T05:00:56.186Z","msg":"Reconciling VariantAutoscaling: name=ms-inference-scheduling-llm-d-modelservice-decode, namespace=llm-d-inference-scheduler, modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-10T05:00:56.186Z","msg":"saturation scaling config cache updated: entries=1, has_default=true"}
{"level":"INFO","ts":"2025-12-10T05:00:56.186Z","msg":"saturation scaling configuration loaded successfully"}
{"level":"INFO","ts":"2025-12-10T05:00:56.186Z","msg":"Operating in MODEL-ONLY mode: model-based optimization only"}
{"level":"INFO","ts":"2025-12-10T05:00:56.186Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-12-10T05:00:56.186Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-12-10T05:00:56.195Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dgftqv, queueLength=0"}
{"level":"DEBUG","ts":"2025-12-10T05:00:56.195Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"INFO","ts":"2025-12-10T05:00:56.195Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dgftqv, usage=0.000 (0.0%)"}
{"level":"DEBUG","ts":"2025-12-10T05:00:56.195Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"DEBUG","ts":"2025-12-10T05:00:56.197Z","msg":"Pod-to-variant matching successful: totalPods=1, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"DEBUG","ts":"2025-12-10T05:00:56.197Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=1"}
{"level":"DEBUG","ts":"2025-12-10T05:00:56.197Z","msg":"Collected capacity metrics for observability in model-only mode: modelID=unsloth/Meta-Llama-3.1-8B, replicaCount=1"}
{"level":"DEBUG","ts":"2025-12-10T05:00:56.208Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=1, accelerator=H100, ttft=0.00ms, itl=0.00ms, cost=100.00, maxBatch=256, arrivalRate=0.00, avgInputTokens=0.00, avgOutputTokens=0.00"}
{"level":"DEBUG","ts":"2025-12-10T05:00:56.208Z","msg":"Experimental model tuner is enabled globally: tuning model performance parameters for active VAs"}
{"level":"DEBUG","ts":"2025-12-10T05:00:56.208Z","msg":"Using state vals from VA status to tune variant ms-inference-scheduling-llm-d-modelservice-decode: alpha= 7.293937, beta= 0.046236, gamma= 16.286325, delta= 0.000259"}
{"level":"DEBUG","ts":"2025-12-10T05:00:56.208Z","msg":"Skipping tuning for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler due to insufficient load. Using fallback parameters."}
{"level":"INFO","ts":"2025-12-10T05:00:56.208Z","msg":"Using tuned parameters from status for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler"}
{"level":"DEBUG","ts":"2025-12-10T05:00:56.208Z","msg":"Updated SystemData for model=unsloth/Meta-Llama-3.1-8B, accelerator=H100: alpha=7.293937, beta=0.046236, gamma=16.286325, delta=0.000259"}
{"level":"DEBUG","ts":"2025-12-10T05:00:56.208Z","msg":"Optimization solution - system: Solution: \ns=ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler; c=Premium; m=unsloth/Meta-Llama-3.1-8B; rate=0; inTk=0; outTk=0; sol=1, sat=false, alloc={acc=H100; numRep=1; maxBatch=512; cost=100, val=0, itl=7.3401732, ttft=16.286585, rho=0, maxRPM=650112.56}; slo-itl=10, slo-ttft=1000, slo-tps=0 \nAllocationByType: \nname=NVIDIA-H100-80GB-HBM3, count=1, limit=0, cost=100 \ntotalCost=100 \n"}
{"level":"DEBUG","ts":"2025-12-10T05:00:56.208Z","msg":"Setting accelerator name: Name=H100, allocationData={H100 1 512 100 7.3401732 16.286585 {0 0 0}}"}
{"level":"INFO","ts":"2025-12-10T05:00:56.208Z","msg":"Model-based optimization completed for model: unsloth/Meta-Llama-3.1-8B - model-based targets: map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"WARN","ts":"2025-12-10T05:00:56.208Z","msg":"saturation analysis unavailable, using model-based targets only: modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-10T05:00:56.208Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-12-10T05:00:56.208Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1→target=1"}
{"level":"INFO","ts":"2025-12-10T05:00:56.208Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 1, desired-replicas: 1, accelerator: H100"}
{"level":"INFO","ts":"2025-12-10T05:00:56.208Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=1, accelerator=H100, SaturationOnly=false"}
{"level":"INFO","ts":"2025-12-10T05:00:56.215Z","msg":"Applied Saturation decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1, target=1, reason=model-based only (Saturation unavailable)"}
{"level":"INFO","ts":"2025-12-10T05:00:56.215Z","msg":"Reconciliation completed successfully: mode=model-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-12-10T05:01:56.215Z","msg":"Reconciling VariantAutoscaling: name=ms-inference-scheduling-llm-d-modelservice-decode, namespace=llm-d-inference-scheduler, modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-10T05:01:56.215Z","msg":"saturation scaling config cache updated: entries=1, has_default=true"}
{"level":"INFO","ts":"2025-12-10T05:01:56.215Z","msg":"saturation scaling configuration loaded successfully"}
{"level":"INFO","ts":"2025-12-10T05:01:56.215Z","msg":"Operating in MODEL-ONLY mode: model-based optimization only"}
{"level":"INFO","ts":"2025-12-10T05:01:56.215Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-12-10T05:01:56.215Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-12-10T05:01:56.225Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dgftqv, queueLength=0"}
{"level":"DEBUG","ts":"2025-12-10T05:01:56.225Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"INFO","ts":"2025-12-10T05:01:56.225Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dgftqv, usage=0.000 (0.0%)"}
{"level":"DEBUG","ts":"2025-12-10T05:01:56.225Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"DEBUG","ts":"2025-12-10T05:01:56.227Z","msg":"Pod-to-variant matching successful: totalPods=1, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"DEBUG","ts":"2025-12-10T05:01:56.227Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=1"}
{"level":"DEBUG","ts":"2025-12-10T05:01:56.227Z","msg":"Collected capacity metrics for observability in model-only mode: modelID=unsloth/Meta-Llama-3.1-8B, replicaCount=1"}
{"level":"DEBUG","ts":"2025-12-10T05:01:56.240Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=1, accelerator=H100, ttft=0.00ms, itl=0.00ms, cost=100.00, maxBatch=256, arrivalRate=0.00, avgInputTokens=0.00, avgOutputTokens=0.00"}
{"level":"DEBUG","ts":"2025-12-10T05:01:56.240Z","msg":"Experimental model tuner is enabled globally: tuning model performance parameters for active VAs"}
{"level":"DEBUG","ts":"2025-12-10T05:01:56.240Z","msg":"Using state vals from VA status to tune variant ms-inference-scheduling-llm-d-modelservice-decode: alpha= 7.293937, beta= 0.046236, gamma= 16.286325, delta= 0.000259"}
{"level":"DEBUG","ts":"2025-12-10T05:01:56.240Z","msg":"Skipping tuning for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler due to insufficient load. Using fallback parameters."}
{"level":"INFO","ts":"2025-12-10T05:01:56.240Z","msg":"Using tuned parameters from status for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler"}
{"level":"DEBUG","ts":"2025-12-10T05:01:56.240Z","msg":"Updated SystemData for model=unsloth/Meta-Llama-3.1-8B, accelerator=H100: alpha=7.293937, beta=0.046236, gamma=16.286325, delta=0.000259"}
{"level":"DEBUG","ts":"2025-12-10T05:01:56.240Z","msg":"Optimization solution - system: Solution: \ns=ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler; c=Premium; m=unsloth/Meta-Llama-3.1-8B; rate=0; inTk=0; outTk=0; sol=1, sat=false, alloc={acc=H100; numRep=1; maxBatch=512; cost=100, val=0, itl=7.3401732, ttft=16.286585, rho=0, maxRPM=650112.56}; slo-itl=10, slo-ttft=1000, slo-tps=0 \nAllocationByType: \nname=NVIDIA-H100-80GB-HBM3, count=1, limit=0, cost=100 \ntotalCost=100 \n"}
{"level":"DEBUG","ts":"2025-12-10T05:01:56.240Z","msg":"Setting accelerator name: Name=H100, allocationData={H100 1 512 100 7.3401732 16.286585 {0 0 0}}"}
{"level":"INFO","ts":"2025-12-10T05:01:56.240Z","msg":"Model-based optimization completed for model: unsloth/Meta-Llama-3.1-8B - model-based targets: map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"WARN","ts":"2025-12-10T05:01:56.240Z","msg":"saturation analysis unavailable, using model-based targets only: modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-10T05:01:56.240Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-12-10T05:01:56.240Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1→target=1"}
{"level":"INFO","ts":"2025-12-10T05:01:56.240Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 1, desired-replicas: 1, accelerator: H100"}
{"level":"INFO","ts":"2025-12-10T05:01:56.240Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=1, accelerator=H100, SaturationOnly=false"}
{"level":"INFO","ts":"2025-12-10T05:01:56.248Z","msg":"Applied Saturation decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1, target=1, reason=model-based only (Saturation unavailable)"}
{"level":"INFO","ts":"2025-12-10T05:01:56.248Z","msg":"Reconciliation completed successfully: mode=model-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-12-10T05:02:56.248Z","msg":"Reconciling VariantAutoscaling: name=ms-inference-scheduling-llm-d-modelservice-decode, namespace=llm-d-inference-scheduler, modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-10T05:02:56.248Z","msg":"saturation scaling config cache updated: entries=1, has_default=true"}
{"level":"INFO","ts":"2025-12-10T05:02:56.248Z","msg":"saturation scaling configuration loaded successfully"}
{"level":"INFO","ts":"2025-12-10T05:02:56.248Z","msg":"Operating in MODEL-ONLY mode: model-based optimization only"}
{"level":"INFO","ts":"2025-12-10T05:02:56.248Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-12-10T05:02:56.248Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-12-10T05:02:56.254Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dgftqv, queueLength=0"}
{"level":"DEBUG","ts":"2025-12-10T05:02:56.254Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"INFO","ts":"2025-12-10T05:02:56.255Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dgftqv, usage=0.000 (0.0%)"}
{"level":"DEBUG","ts":"2025-12-10T05:02:56.255Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"DEBUG","ts":"2025-12-10T05:02:56.257Z","msg":"Pod-to-variant matching successful: totalPods=1, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"DEBUG","ts":"2025-12-10T05:02:56.257Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=1"}
{"level":"DEBUG","ts":"2025-12-10T05:02:56.257Z","msg":"Collected capacity metrics for observability in model-only mode: modelID=unsloth/Meta-Llama-3.1-8B, replicaCount=1"}
{"level":"DEBUG","ts":"2025-12-10T05:02:56.269Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=1, accelerator=H100, ttft=0.00ms, itl=0.00ms, cost=100.00, maxBatch=256, arrivalRate=0.00, avgInputTokens=0.00, avgOutputTokens=0.00"}
{"level":"DEBUG","ts":"2025-12-10T05:02:56.269Z","msg":"Experimental model tuner is enabled globally: tuning model performance parameters for active VAs"}
{"level":"DEBUG","ts":"2025-12-10T05:02:56.269Z","msg":"Using state vals from VA status to tune variant ms-inference-scheduling-llm-d-modelservice-decode: alpha= 7.293937, beta= 0.046236, gamma= 16.286325, delta= 0.000259"}
{"level":"DEBUG","ts":"2025-12-10T05:02:56.269Z","msg":"Skipping tuning for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler due to insufficient load. Using fallback parameters."}
{"level":"INFO","ts":"2025-12-10T05:02:56.269Z","msg":"Using tuned parameters from status for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler"}
{"level":"DEBUG","ts":"2025-12-10T05:02:56.269Z","msg":"Updated SystemData for model=unsloth/Meta-Llama-3.1-8B, accelerator=H100: alpha=7.293937, beta=0.046236, gamma=16.286325, delta=0.000259"}
{"level":"DEBUG","ts":"2025-12-10T05:02:56.269Z","msg":"Optimization solution - system: Solution: \ns=ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler; c=Premium; m=unsloth/Meta-Llama-3.1-8B; rate=0; inTk=0; outTk=0; sol=1, sat=false, alloc={acc=H100; numRep=1; maxBatch=512; cost=100, val=0, itl=7.3401732, ttft=16.286585, rho=0, maxRPM=650112.56}; slo-itl=10, slo-ttft=1000, slo-tps=0 \nAllocationByType: \nname=NVIDIA-H100-80GB-HBM3, count=1, limit=0, cost=100 \ntotalCost=100 \n"}
{"level":"DEBUG","ts":"2025-12-10T05:02:56.269Z","msg":"Setting accelerator name: Name=H100, allocationData={H100 1 512 100 7.3401732 16.286585 {0 0 0}}"}
{"level":"INFO","ts":"2025-12-10T05:02:56.269Z","msg":"Model-based optimization completed for model: unsloth/Meta-Llama-3.1-8B - model-based targets: map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"WARN","ts":"2025-12-10T05:02:56.269Z","msg":"saturation analysis unavailable, using model-based targets only: modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-10T05:02:56.269Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-12-10T05:02:56.269Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1→target=1"}
{"level":"INFO","ts":"2025-12-10T05:02:56.269Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 1, desired-replicas: 1, accelerator: H100"}
{"level":"INFO","ts":"2025-12-10T05:02:56.269Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=1, accelerator=H100, SaturationOnly=false"}
{"level":"INFO","ts":"2025-12-10T05:02:56.276Z","msg":"Applied Saturation decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1, target=1, reason=model-based only (Saturation unavailable)"}
{"level":"INFO","ts":"2025-12-10T05:02:56.276Z","msg":"Reconciliation completed successfully: mode=model-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-12-10T05:03:56.277Z","msg":"Reconciling VariantAutoscaling: name=ms-inference-scheduling-llm-d-modelservice-decode, namespace=llm-d-inference-scheduler, modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-10T05:03:56.277Z","msg":"saturation scaling config cache updated: entries=1, has_default=true"}
{"level":"INFO","ts":"2025-12-10T05:03:56.277Z","msg":"saturation scaling configuration loaded successfully"}
{"level":"INFO","ts":"2025-12-10T05:03:56.277Z","msg":"Operating in MODEL-ONLY mode: model-based optimization only"}
{"level":"INFO","ts":"2025-12-10T05:03:56.277Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-12-10T05:03:56.277Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-12-10T05:03:56.284Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dgftqv, queueLength=0"}
{"level":"DEBUG","ts":"2025-12-10T05:03:56.284Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"INFO","ts":"2025-12-10T05:03:56.284Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dgftqv, usage=0.186 (18.6%)"}
{"level":"DEBUG","ts":"2025-12-10T05:03:56.284Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"DEBUG","ts":"2025-12-10T05:03:56.286Z","msg":"Pod-to-variant matching successful: totalPods=1, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"DEBUG","ts":"2025-12-10T05:03:56.286Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=1"}
{"level":"DEBUG","ts":"2025-12-10T05:03:56.286Z","msg":"Collected capacity metrics for observability in model-only mode: modelID=unsloth/Meta-Llama-3.1-8B, replicaCount=1"}
{"level":"DEBUG","ts":"2025-12-10T05:03:56.298Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=1, accelerator=H100, ttft=28.16ms, itl=14.20ms, cost=100.00, maxBatch=256, arrivalRate=1093.56, avgInputTokens=252.34, avgOutputTokens=410.20"}
{"level":"DEBUG","ts":"2025-12-10T05:03:56.298Z","msg":"Experimental model tuner is enabled globally: tuning model performance parameters for active VAs"}
{"level":"DEBUG","ts":"2025-12-10T05:03:56.298Z","msg":"Using state vals from VA status to tune variant ms-inference-scheduling-llm-d-modelservice-decode: alpha= 7.293937, beta= 0.046236, gamma= 16.286325, delta= 0.000259"}
{"level":"DEBUG","ts":"2025-12-10T05:03:56.298Z","msg":"[Tuner Config] variant=ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler | Initial state: alpha=7.293937, beta=0.046236, gamma=16.286325, delta=0.000259 | Expected obs (for R): TTFT=28.16ms, ITL=14.20ms | Has covariance=true"}
{"level":"WARN","ts":"2025-12-10T05:03:56.300Z","msg":"Tuner validation failed (NIS=22.70), validation error: normalized innovation squared (NIS=22.70) exceeds threshold (7.38), rejecting update as outlier - returning previous state: alpha=7.293937, beta=0.046236, gamma=16.286325, delta=0.000259"}
{"level":"WARN","ts":"2025-12-10T05:03:56.300Z","msg":"Tuner NIS validation failed for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler, server ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler (NIS=22.70 exceeds threshold 7.38) - Keeping previous state: alpha=7.293937, beta=0.046236, gamma=16.286325, delta=0.000259"}
{"level":"INFO","ts":"2025-12-10T05:03:56.300Z","msg":"Keeping previous tuned parameters for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler due to NIS validation failure (NIS=22.700261)"}
{"level":"DEBUG","ts":"2025-12-10T05:03:56.300Z","msg":"Model tuner results: model=unsloth/Meta-Llama-3.1-8B, accelerator=H100, alpha=7.293937, beta=0.046236, gamma=16.286325, delta=0.000259, NIS=22.70"}
{"level":"DEBUG","ts":"2025-12-10T05:03:56.300Z","msg":"Updated tuner status for variant ms-inference-scheduling-llm-d-modelservice-decode: alpha=7.293937, beta=0.046236, gamma=16.286325, delta=0.000259, NIS=22.700261"}
{"level":"DEBUG","ts":"2025-12-10T05:03:56.310Z","msg":"Optimization solution - system: Solution: \ns=ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler; c=Premium; m=unsloth/Meta-Llama-3.1-8B; rate=1093.56; inTk=252; outTk=410; sol=1, sat=false, alloc={acc=H100; numRep=2; maxBatch=512; cost=200, val=100, itl=8.877925, ttft=18.522326, rho=0.06495849, maxRPM=839.79065}; slo-itl=10, slo-ttft=1000, slo-tps=0 \nAllocationByType: \nname=NVIDIA-H100-80GB-HBM3, count=2, limit=0, cost=200 \ntotalCost=200 \n"}
{"level":"DEBUG","ts":"2025-12-10T05:03:56.310Z","msg":"Setting accelerator name: Name=H100, allocationData={H100 2 512 200 8.877925 18.522326 {1093.56 252 410}}"}
{"level":"INFO","ts":"2025-12-10T05:03:56.310Z","msg":"Model-based optimization completed for model: unsloth/Meta-Llama-3.1-8B - model-based targets: map[ms-inference-scheduling-llm-d-modelservice-decode:2]"}
{"level":"WARN","ts":"2025-12-10T05:03:56.310Z","msg":"saturation analysis unavailable, using model-based targets only: modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-10T05:03:56.310Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-12-10T05:03:56.310Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=scale-up, current=1→target=2"}
{"level":"INFO","ts":"2025-12-10T05:03:56.310Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 1, desired-replicas: 2, accelerator: H100"}
{"level":"INFO","ts":"2025-12-10T05:03:56.310Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=2, accelerator=H100, SaturationOnly=false"}
{"level":"INFO","ts":"2025-12-10T05:03:56.318Z","msg":"Applied Saturation decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=scale-up, current=1, target=2, reason=model-based only (Saturation unavailable)"}
{"level":"INFO","ts":"2025-12-10T05:03:56.318Z","msg":"Reconciliation completed successfully: mode=model-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-12-10T05:04:56.319Z","msg":"Reconciling VariantAutoscaling: name=ms-inference-scheduling-llm-d-modelservice-decode, namespace=llm-d-inference-scheduler, modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-10T05:04:56.319Z","msg":"saturation scaling config cache updated: entries=1, has_default=true"}
{"level":"INFO","ts":"2025-12-10T05:04:56.319Z","msg":"saturation scaling configuration loaded successfully"}
{"level":"INFO","ts":"2025-12-10T05:04:56.319Z","msg":"Operating in MODEL-ONLY mode: model-based optimization only"}
{"level":"INFO","ts":"2025-12-10T05:04:56.319Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-12-10T05:04:56.319Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-12-10T05:04:56.325Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dgftqv, queueLength=0"}
{"level":"DEBUG","ts":"2025-12-10T05:04:56.325Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"INFO","ts":"2025-12-10T05:04:56.325Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dgftqv, usage=0.207 (20.7%)"}
{"level":"DEBUG","ts":"2025-12-10T05:04:56.325Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"DEBUG","ts":"2025-12-10T05:04:56.329Z","msg":"Pod-to-variant matching successful: totalPods=1, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"DEBUG","ts":"2025-12-10T05:04:56.329Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=1"}
{"level":"DEBUG","ts":"2025-12-10T05:04:56.329Z","msg":"Collected capacity metrics for observability in model-only mode: modelID=unsloth/Meta-Llama-3.1-8B, replicaCount=1"}
{"level":"DEBUG","ts":"2025-12-10T05:04:56.345Z","msg":"Replica count mismatch for llm-d-inference-scheduler/unsloth/Meta-Llama-3.1-8B: spec=2, reporting_metrics=1"}
{"level":"DEBUG","ts":"2025-12-10T05:04:56.345Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=1, accelerator=H100, ttft=30.50ms, itl=15.62ms, cost=100.00, maxBatch=256, arrivalRate=1244.48, avgInputTokens=228.43, avgOutputTokens=461.18"}
{"level":"DEBUG","ts":"2025-12-10T05:04:56.345Z","msg":"Experimental model tuner is enabled globally: tuning model performance parameters for active VAs"}
{"level":"DEBUG","ts":"2025-12-10T05:04:56.345Z","msg":"Using state vals from VA status to tune variant ms-inference-scheduling-llm-d-modelservice-decode: alpha= 7.293937, beta= 0.046236, gamma= 16.286325, delta= 0.000259"}
{"level":"DEBUG","ts":"2025-12-10T05:04:56.345Z","msg":"[Tuner Config] variant=ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler | Initial state: alpha=7.293937, beta=0.046236, gamma=16.286325, delta=0.000259 | Expected obs (for R): TTFT=30.50ms, ITL=15.62ms | Has covariance=true"}
{"level":"WARN","ts":"2025-12-10T05:04:56.346Z","msg":"Tuner validation failed (NIS=19.63), validation error: normalized innovation squared (NIS=19.63) exceeds threshold (7.38), rejecting update as outlier - returning previous state: alpha=7.293937, beta=0.046236, gamma=16.286325, delta=0.000259"}
{"level":"WARN","ts":"2025-12-10T05:04:56.346Z","msg":"Tuner NIS validation failed for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler, server ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler (NIS=19.63 exceeds threshold 7.38) - Keeping previous state: alpha=7.293937, beta=0.046236, gamma=16.286325, delta=0.000259"}
{"level":"INFO","ts":"2025-12-10T05:04:56.346Z","msg":"Keeping previous tuned parameters for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler due to NIS validation failure (NIS=19.633069)"}
{"level":"DEBUG","ts":"2025-12-10T05:04:56.346Z","msg":"Model tuner results: model=unsloth/Meta-Llama-3.1-8B, accelerator=H100, alpha=7.293937, beta=0.046236, gamma=16.286325, delta=0.000259, NIS=19.63"}
{"level":"DEBUG","ts":"2025-12-10T05:04:56.346Z","msg":"Updated tuner status for variant ms-inference-scheduling-llm-d-modelservice-decode: alpha=7.293937, beta=0.046236, gamma=16.286325, delta=0.000259, NIS=19.633069"}
{"level":"DEBUG","ts":"2025-12-10T05:04:56.356Z","msg":"Optimization solution - system: Solution: \ns=ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler; c=Premium; m=unsloth/Meta-Llama-3.1-8B; rate=1244.48; inTk=228; outTk=461; sol=1, sat=false, alloc={acc=H100; numRep=2; maxBatch=512; cost=200, val=100, itl=9.429047, ttft=19.01326, rho=0.088239275, maxRPM=747.14886}; slo-itl=10, slo-ttft=1000, slo-tps=0 \nAllocationByType: \nname=NVIDIA-H100-80GB-HBM3, count=2, limit=0, cost=200 \ntotalCost=200 \n"}
{"level":"DEBUG","ts":"2025-12-10T05:04:56.356Z","msg":"Setting accelerator name: Name=H100, allocationData={H100 2 512 200 9.429047 19.01326 {1244.48 228 461}}"}
{"level":"INFO","ts":"2025-12-10T05:04:56.356Z","msg":"Model-based optimization completed for model: unsloth/Meta-Llama-3.1-8B - model-based targets: map[ms-inference-scheduling-llm-d-modelservice-decode:2]"}
{"level":"WARN","ts":"2025-12-10T05:04:56.356Z","msg":"saturation analysis unavailable, using model-based targets only: modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-10T05:04:56.356Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-12-10T05:04:56.356Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=scale-up, current=1→target=2"}
{"level":"INFO","ts":"2025-12-10T05:04:56.356Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 1, desired-replicas: 2, accelerator: H100"}
{"level":"INFO","ts":"2025-12-10T05:04:56.356Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=2, accelerator=H100, SaturationOnly=false"}
{"level":"INFO","ts":"2025-12-10T05:04:56.362Z","msg":"Applied Saturation decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=scale-up, current=1, target=2, reason=model-based only (Saturation unavailable)"}
{"level":"INFO","ts":"2025-12-10T05:04:56.362Z","msg":"Reconciliation completed successfully: mode=model-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-12-10T05:05:56.362Z","msg":"Reconciling VariantAutoscaling: name=ms-inference-scheduling-llm-d-modelservice-decode, namespace=llm-d-inference-scheduler, modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-10T05:05:56.362Z","msg":"saturation scaling config cache updated: entries=1, has_default=true"}
{"level":"INFO","ts":"2025-12-10T05:05:56.362Z","msg":"saturation scaling configuration loaded successfully"}
{"level":"INFO","ts":"2025-12-10T05:05:56.362Z","msg":"Operating in MODEL-ONLY mode: model-based optimization only"}
{"level":"INFO","ts":"2025-12-10T05:05:56.362Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-12-10T05:05:56.362Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-12-10T05:05:56.372Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dgftqv, usage=0.707 (70.7%)"}
{"level":"DEBUG","ts":"2025-12-10T05:05:56.372Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"INFO","ts":"2025-12-10T05:05:56.372Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dgftqv, queueLength=0"}
{"level":"DEBUG","ts":"2025-12-10T05:05:56.372Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"DEBUG","ts":"2025-12-10T05:05:56.375Z","msg":"Pod-to-variant matching successful: totalPods=1, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"DEBUG","ts":"2025-12-10T05:05:56.375Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=1"}
{"level":"DEBUG","ts":"2025-12-10T05:05:56.375Z","msg":"Collected capacity metrics for observability in model-only mode: modelID=unsloth/Meta-Llama-3.1-8B, replicaCount=1"}
{"level":"DEBUG","ts":"2025-12-10T05:05:56.387Z","msg":"Replica count mismatch for llm-d-inference-scheduler/unsloth/Meta-Llama-3.1-8B: spec=2, reporting_metrics=1"}
{"level":"DEBUG","ts":"2025-12-10T05:05:56.387Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=1, accelerator=H100, ttft=57.31ms, itl=30.29ms, cost=100.00, maxBatch=256, arrivalRate=1632.39, avgInputTokens=277.66, avgOutputTokens=252.39"}
{"level":"DEBUG","ts":"2025-12-10T05:05:56.387Z","msg":"Experimental model tuner is enabled globally: tuning model performance parameters for active VAs"}
{"level":"DEBUG","ts":"2025-12-10T05:05:56.387Z","msg":"Using state vals from VA status to tune variant ms-inference-scheduling-llm-d-modelservice-decode: alpha= 7.293937, beta= 0.046236, gamma= 16.286325, delta= 0.000259"}
{"level":"DEBUG","ts":"2025-12-10T05:05:56.387Z","msg":"[Tuner Config] variant=ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler | Initial state: alpha=7.293937, beta=0.046236, gamma=16.286325, delta=0.000259 | Expected obs (for R): TTFT=57.31ms, ITL=30.29ms | Has covariance=true"}
{"level":"WARN","ts":"2025-12-10T05:05:56.389Z","msg":"Tuner validation failed (NIS=489.37), validation error: normalized innovation squared (NIS=489.37) exceeds threshold (7.38), rejecting update as outlier - returning previous state: alpha=7.293937, beta=0.046236, gamma=16.286325, delta=0.000259"}
{"level":"WARN","ts":"2025-12-10T05:05:56.389Z","msg":"Tuner NIS validation failed for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler, server ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler (NIS=489.37 exceeds threshold 7.38) - Keeping previous state: alpha=7.293937, beta=0.046236, gamma=16.286325, delta=0.000259"}
{"level":"INFO","ts":"2025-12-10T05:05:56.389Z","msg":"Keeping previous tuned parameters for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler due to NIS validation failure (NIS=489.374056)"}
{"level":"DEBUG","ts":"2025-12-10T05:05:56.389Z","msg":"Model tuner results: model=unsloth/Meta-Llama-3.1-8B, accelerator=H100, alpha=7.293937, beta=0.046236, gamma=16.286325, delta=0.000259, NIS=489.37"}
{"level":"DEBUG","ts":"2025-12-10T05:05:56.389Z","msg":"Updated tuner status for variant ms-inference-scheduling-llm-d-modelservice-decode: alpha=7.293937, beta=0.046236, gamma=16.286325, delta=0.000259, NIS=489.374056"}
{"level":"DEBUG","ts":"2025-12-10T05:05:56.398Z","msg":"Optimization solution - system: Solution: \ns=ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler; c=Premium; m=unsloth/Meta-Llama-3.1-8B; rate=1632.39; inTk=277; outTk=252; sol=1, sat=false, alloc={acc=H100; numRep=2; maxBatch=512; cost=200, val=100, itl=8.730017, ttft=18.514647, rho=0.058710482, maxRPM=1364.0225}; slo-itl=10, slo-ttft=1000, slo-tps=0 \nAllocationByType: \nname=NVIDIA-H100-80GB-HBM3, count=2, limit=0, cost=200 \ntotalCost=200 \n"}
{"level":"DEBUG","ts":"2025-12-10T05:05:56.398Z","msg":"Setting accelerator name: Name=H100, allocationData={H100 2 512 200 8.730017 18.514647 {1632.39 277 252}}"}
{"level":"INFO","ts":"2025-12-10T05:05:56.398Z","msg":"Model-based optimization completed for model: unsloth/Meta-Llama-3.1-8B - model-based targets: map[ms-inference-scheduling-llm-d-modelservice-decode:2]"}
{"level":"WARN","ts":"2025-12-10T05:05:56.398Z","msg":"saturation analysis unavailable, using model-based targets only: modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-10T05:05:56.398Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-12-10T05:05:56.398Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=scale-up, current=1→target=2"}
{"level":"INFO","ts":"2025-12-10T05:05:56.398Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 1, desired-replicas: 2, accelerator: H100"}
{"level":"INFO","ts":"2025-12-10T05:05:56.398Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=2, accelerator=H100, SaturationOnly=false"}
{"level":"INFO","ts":"2025-12-10T05:05:56.405Z","msg":"Applied Saturation decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=scale-up, current=1, target=2, reason=model-based only (Saturation unavailable)"}
{"level":"INFO","ts":"2025-12-10T05:05:56.405Z","msg":"Reconciliation completed successfully: mode=model-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-12-10T05:06:56.405Z","msg":"Reconciling VariantAutoscaling: name=ms-inference-scheduling-llm-d-modelservice-decode, namespace=llm-d-inference-scheduler, modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-10T05:06:56.406Z","msg":"saturation scaling config cache updated: entries=1, has_default=true"}
{"level":"INFO","ts":"2025-12-10T05:06:56.406Z","msg":"saturation scaling configuration loaded successfully"}
{"level":"INFO","ts":"2025-12-10T05:06:56.406Z","msg":"Operating in MODEL-ONLY mode: model-based optimization only"}
{"level":"INFO","ts":"2025-12-10T05:06:56.406Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-12-10T05:06:56.406Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-12-10T05:06:56.409Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dgftqv, queueLength=452"}
{"level":"INFO","ts":"2025-12-10T05:06:56.409Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dkj52n, queueLength=0"}
{"level":"DEBUG","ts":"2025-12-10T05:06:56.409Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=2"}
{"level":"INFO","ts":"2025-12-10T05:06:56.409Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dgftqv, usage=0.994 (99.4%)"}
{"level":"INFO","ts":"2025-12-10T05:06:56.409Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dkj52n, usage=0.464 (46.4%)"}
{"level":"DEBUG","ts":"2025-12-10T05:06:56.409Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=2"}
{"level":"DEBUG","ts":"2025-12-10T05:06:56.412Z","msg":"Pod-to-variant matching successful: totalPods=2, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:2]"}
{"level":"DEBUG","ts":"2025-12-10T05:06:56.412Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=2"}
{"level":"DEBUG","ts":"2025-12-10T05:06:56.412Z","msg":"Collected capacity metrics for observability in model-only mode: modelID=unsloth/Meta-Llama-3.1-8B, replicaCount=2"}
{"level":"DEBUG","ts":"2025-12-10T05:06:56.425Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=2, accelerator=H100, ttft=7724.16ms, itl=58.46ms, cost=200.00, maxBatch=256, arrivalRate=1441.42, avgInputTokens=218.05, avgOutputTokens=550.77"}
{"level":"DEBUG","ts":"2025-12-10T05:06:56.425Z","msg":"Experimental model tuner is enabled globally: tuning model performance parameters for active VAs"}
{"level":"DEBUG","ts":"2025-12-10T05:06:56.425Z","msg":"Using state vals from VA status to tune variant ms-inference-scheduling-llm-d-modelservice-decode: alpha= 7.293937, beta= 0.046236, gamma= 16.286325, delta= 0.000259"}
{"level":"DEBUG","ts":"2025-12-10T05:06:56.425Z","msg":"[Tuner Config] variant=ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler | Initial state: alpha=7.293937, beta=0.046236, gamma=16.286325, delta=0.000259 | Expected obs (for R): TTFT=7724.16ms, ITL=58.46ms | Has covariance=true"}
{"level":"WARN","ts":"2025-12-10T05:06:56.426Z","msg":"Tuner validation failed (NIS=2226.44), validation error: normalized innovation squared (NIS=2226.44) exceeds threshold (7.38), rejecting update as outlier - returning previous state: alpha=7.293937, beta=0.046236, gamma=16.286325, delta=0.000259"}
{"level":"WARN","ts":"2025-12-10T05:06:56.426Z","msg":"Tuner NIS validation failed for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler, server ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler (NIS=2226.44 exceeds threshold 7.38) - Keeping previous state: alpha=7.293937, beta=0.046236, gamma=16.286325, delta=0.000259"}
{"level":"INFO","ts":"2025-12-10T05:06:56.426Z","msg":"Keeping previous tuned parameters for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler due to NIS validation failure (NIS=2226.440533)"}
{"level":"DEBUG","ts":"2025-12-10T05:06:56.426Z","msg":"Model tuner results: model=unsloth/Meta-Llama-3.1-8B, accelerator=H100, alpha=7.293937, beta=0.046236, gamma=16.286325, delta=0.000259, NIS=2226.44"}
{"level":"DEBUG","ts":"2025-12-10T05:06:56.426Z","msg":"Updated tuner status for variant ms-inference-scheduling-llm-d-modelservice-decode: alpha=7.293937, beta=0.046236, gamma=16.286325, delta=0.000259, NIS=2226.440533"}
{"level":"DEBUG","ts":"2025-12-10T05:06:56.429Z","msg":"Optimization solution - system: Solution: \ns=ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler; c=Premium; m=unsloth/Meta-Llama-3.1-8B; rate=1441.42; inTk=218; outTk=550; sol=1, sat=false, alloc={acc=H100; numRep=3; maxBatch=512; cost=300, val=100, itl=9.221523, ttft=18.640234, rho=0.07947293, maxRPM=626.4777}; slo-itl=10, slo-ttft=1000, slo-tps=0 \nAllocationByType: \nname=NVIDIA-H100-80GB-HBM3, count=3, limit=0, cost=300 \ntotalCost=300 \n"}
{"level":"DEBUG","ts":"2025-12-10T05:06:56.429Z","msg":"Setting accelerator name: Name=H100, allocationData={H100 3 512 300 9.221523 18.640234 {1441.42 218 550}}"}
{"level":"INFO","ts":"2025-12-10T05:06:56.429Z","msg":"Model-based optimization completed for model: unsloth/Meta-Llama-3.1-8B - model-based targets: map[ms-inference-scheduling-llm-d-modelservice-decode:3]"}
{"level":"WARN","ts":"2025-12-10T05:06:56.429Z","msg":"saturation analysis unavailable, using model-based targets only: modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-10T05:06:56.429Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-12-10T05:06:56.429Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=scale-up, current=2→target=3"}
{"level":"INFO","ts":"2025-12-10T05:06:56.429Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 2, desired-replicas: 3, accelerator: H100"}
{"level":"INFO","ts":"2025-12-10T05:06:56.429Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=3, accelerator=H100, SaturationOnly=false"}
{"level":"INFO","ts":"2025-12-10T05:06:56.436Z","msg":"Applied Saturation decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=scale-up, current=2, target=3, reason=model-based only (Saturation unavailable)"}
{"level":"INFO","ts":"2025-12-10T05:06:56.436Z","msg":"Reconciliation completed successfully: mode=model-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-12-10T05:07:56.437Z","msg":"Reconciling VariantAutoscaling: name=ms-inference-scheduling-llm-d-modelservice-decode, namespace=llm-d-inference-scheduler, modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-10T05:07:56.437Z","msg":"saturation scaling config cache updated: entries=1, has_default=true"}
{"level":"INFO","ts":"2025-12-10T05:07:56.437Z","msg":"saturation scaling configuration loaded successfully"}
{"level":"INFO","ts":"2025-12-10T05:07:56.437Z","msg":"Operating in MODEL-ONLY mode: model-based optimization only"}
{"level":"INFO","ts":"2025-12-10T05:07:56.437Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-12-10T05:07:56.437Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-12-10T05:07:56.462Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dgftqv, usage=0.953 (95.3%)"}
{"level":"INFO","ts":"2025-12-10T05:07:56.462Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dkj52n, usage=0.469 (46.9%)"}
{"level":"DEBUG","ts":"2025-12-10T05:07:56.462Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=2"}
{"level":"INFO","ts":"2025-12-10T05:07:56.472Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dgftqv, queueLength=0"}
{"level":"INFO","ts":"2025-12-10T05:07:56.472Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dkj52n, queueLength=0"}
{"level":"DEBUG","ts":"2025-12-10T05:07:56.472Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=2"}
{"level":"DEBUG","ts":"2025-12-10T05:07:56.475Z","msg":"Pod-to-variant matching successful: totalPods=2, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:2]"}
{"level":"DEBUG","ts":"2025-12-10T05:07:56.475Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=2"}
{"level":"DEBUG","ts":"2025-12-10T05:07:56.475Z","msg":"Collected capacity metrics for observability in model-only mode: modelID=unsloth/Meta-Llama-3.1-8B, replicaCount=2"}
{"level":"DEBUG","ts":"2025-12-10T05:07:56.488Z","msg":"Replica count mismatch for llm-d-inference-scheduler/unsloth/Meta-Llama-3.1-8B: spec=3, reporting_metrics=2"}
{"level":"DEBUG","ts":"2025-12-10T05:07:56.488Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=2, accelerator=H100, ttft=65.67ms, itl=39.86ms, cost=200.00, maxBatch=256, arrivalRate=3046.20, avgInputTokens=228.29, avgOutputTokens=480.96"}
{"level":"DEBUG","ts":"2025-12-10T05:07:56.488Z","msg":"Experimental model tuner is enabled globally: tuning model performance parameters for active VAs"}
{"level":"DEBUG","ts":"2025-12-10T05:07:56.488Z","msg":"Using state vals from VA status to tune variant ms-inference-scheduling-llm-d-modelservice-decode: alpha= 7.293937, beta= 0.046236, gamma= 16.286325, delta= 0.000259"}
{"level":"DEBUG","ts":"2025-12-10T05:07:56.488Z","msg":"[Tuner Config] variant=ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler | Initial state: alpha=7.293937, beta=0.046236, gamma=16.286325, delta=0.000259 | Expected obs (for R): TTFT=65.67ms, ITL=39.86ms | Has covariance=true"}
{"level":"WARN","ts":"2025-12-10T05:07:56.489Z","msg":"Tuner validation failed (NIS=404.20), validation error: normalized innovation squared (NIS=404.20) exceeds threshold (7.38), rejecting update as outlier - returning previous state: alpha=7.293937, beta=0.046236, gamma=16.286325, delta=0.000259"}
{"level":"WARN","ts":"2025-12-10T05:07:56.489Z","msg":"Tuner NIS validation failed for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler, server ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler (NIS=404.20 exceeds threshold 7.38) - Keeping previous state: alpha=7.293937, beta=0.046236, gamma=16.286325, delta=0.000259"}
{"level":"INFO","ts":"2025-12-10T05:07:56.489Z","msg":"Keeping previous tuned parameters for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler due to NIS validation failure (NIS=404.202323)"}
{"level":"DEBUG","ts":"2025-12-10T05:07:56.489Z","msg":"Model tuner results: model=unsloth/Meta-Llama-3.1-8B, accelerator=H100, alpha=7.293937, beta=0.046236, gamma=16.286325, delta=0.000259, NIS=404.20"}
{"level":"DEBUG","ts":"2025-12-10T05:07:56.489Z","msg":"Updated tuner status for variant ms-inference-scheduling-llm-d-modelservice-decode: alpha=7.293937, beta=0.046236, gamma=16.286325, delta=0.000259, NIS=404.202323"}
{"level":"DEBUG","ts":"2025-12-10T05:07:56.496Z","msg":"Optimization solution - system: Solution: \ns=ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler; c=Premium; m=unsloth/Meta-Llama-3.1-8B; rate=3046.2; inTk=228; outTk=480; sol=1, sat=false, alloc={acc=H100; numRep=5; maxBatch=512; cost=500, val=300, itl=9.481295, ttft=19.079988, rho=0.09044637, maxRPM=717.63275}; slo-itl=10, slo-ttft=1000, slo-tps=0 \nAllocationByType: \nname=NVIDIA-H100-80GB-HBM3, count=5, limit=0, cost=500 \ntotalCost=500 \n"}
{"level":"DEBUG","ts":"2025-12-10T05:07:56.496Z","msg":"Setting accelerator name: Name=H100, allocationData={H100 5 512 500 9.481295 19.079988 {3046.2 228 480}}"}
{"level":"INFO","ts":"2025-12-10T05:07:56.496Z","msg":"Model-based optimization completed for model: unsloth/Meta-Llama-3.1-8B - model-based targets: map[ms-inference-scheduling-llm-d-modelservice-decode:5]"}
{"level":"WARN","ts":"2025-12-10T05:07:56.496Z","msg":"saturation analysis unavailable, using model-based targets only: modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-10T05:07:56.496Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-12-10T05:07:56.496Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=scale-up, current=2→target=5"}
{"level":"INFO","ts":"2025-12-10T05:07:56.496Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 2, desired-replicas: 5, accelerator: H100"}
{"level":"INFO","ts":"2025-12-10T05:07:56.496Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=5, accelerator=H100, SaturationOnly=false"}
{"level":"INFO","ts":"2025-12-10T05:07:56.502Z","msg":"Applied Saturation decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=scale-up, current=2, target=5, reason=model-based only (Saturation unavailable)"}
{"level":"INFO","ts":"2025-12-10T05:07:56.502Z","msg":"Reconciliation completed successfully: mode=model-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-12-10T05:08:56.502Z","msg":"Reconciling VariantAutoscaling: name=ms-inference-scheduling-llm-d-modelservice-decode, namespace=llm-d-inference-scheduler, modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-10T05:08:56.502Z","msg":"saturation scaling config cache updated: entries=1, has_default=true"}
{"level":"INFO","ts":"2025-12-10T05:08:56.502Z","msg":"saturation scaling configuration loaded successfully"}
{"level":"INFO","ts":"2025-12-10T05:08:56.502Z","msg":"Operating in MODEL-ONLY mode: model-based optimization only"}
{"level":"INFO","ts":"2025-12-10T05:08:56.502Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-12-10T05:08:56.502Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-12-10T05:08:56.508Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dgftqv, queueLength=0"}
{"level":"INFO","ts":"2025-12-10T05:08:56.508Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dgftqv, usage=0.936 (93.6%)"}
{"level":"INFO","ts":"2025-12-10T05:08:56.508Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dkj52n, usage=0.857 (85.7%)"}
{"level":"DEBUG","ts":"2025-12-10T05:08:56.508Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=2"}
{"level":"INFO","ts":"2025-12-10T05:08:56.508Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dkj52n, queueLength=0"}
{"level":"DEBUG","ts":"2025-12-10T05:08:56.508Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=2"}
{"level":"DEBUG","ts":"2025-12-10T05:08:56.511Z","msg":"Pod-to-variant matching successful: totalPods=2, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:2]"}
{"level":"DEBUG","ts":"2025-12-10T05:08:56.511Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=2"}
{"level":"DEBUG","ts":"2025-12-10T05:08:56.511Z","msg":"Collected capacity metrics for observability in model-only mode: modelID=unsloth/Meta-Llama-3.1-8B, replicaCount=2"}
{"level":"DEBUG","ts":"2025-12-10T05:08:56.525Z","msg":"Replica count mismatch for llm-d-inference-scheduler/unsloth/Meta-Llama-3.1-8B: spec=5, reporting_metrics=2"}
{"level":"DEBUG","ts":"2025-12-10T05:08:56.525Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=2, accelerator=H100, ttft=85.14ms, itl=52.00ms, cost=200.00, maxBatch=256, arrivalRate=3117.83, avgInputTokens=255.99, avgOutputTokens=388.23"}
{"level":"DEBUG","ts":"2025-12-10T05:08:56.525Z","msg":"Experimental model tuner is enabled globally: tuning model performance parameters for active VAs"}
{"level":"DEBUG","ts":"2025-12-10T05:08:56.525Z","msg":"Using state vals from VA status to tune variant ms-inference-scheduling-llm-d-modelservice-decode: alpha= 7.293937, beta= 0.046236, gamma= 16.286325, delta= 0.000259"}
{"level":"DEBUG","ts":"2025-12-10T05:08:56.525Z","msg":"[Tuner Config] variant=ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler | Initial state: alpha=7.293937, beta=0.046236, gamma=16.286325, delta=0.000259 | Expected obs (for R): TTFT=85.14ms, ITL=52.00ms | Has covariance=true"}
{"level":"WARN","ts":"2025-12-10T05:08:56.525Z","msg":"Tuner validation failed (NIS=422.27), validation error: normalized innovation squared (NIS=422.27) exceeds threshold (7.38), rejecting update as outlier - returning previous state: alpha=7.293937, beta=0.046236, gamma=16.286325, delta=0.000259"}
{"level":"WARN","ts":"2025-12-10T05:08:56.525Z","msg":"Tuner NIS validation failed for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler, server ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler (NIS=422.27 exceeds threshold 7.38) - Keeping previous state: alpha=7.293937, beta=0.046236, gamma=16.286325, delta=0.000259"}
{"level":"INFO","ts":"2025-12-10T05:08:56.525Z","msg":"Keeping previous tuned parameters for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler due to NIS validation failure (NIS=422.266701)"}
{"level":"DEBUG","ts":"2025-12-10T05:08:56.525Z","msg":"Model tuner results: model=unsloth/Meta-Llama-3.1-8B, accelerator=H100, alpha=7.293937, beta=0.046236, gamma=16.286325, delta=0.000259, NIS=422.27"}
{"level":"DEBUG","ts":"2025-12-10T05:08:56.525Z","msg":"Updated tuner status for variant ms-inference-scheduling-llm-d-modelservice-decode: alpha=7.293937, beta=0.046236, gamma=16.286325, delta=0.000259, NIS=422.266701"}
{"level":"DEBUG","ts":"2025-12-10T05:08:56.535Z","msg":"Optimization solution - system: Solution: \ns=ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler; c=Premium; m=unsloth/Meta-Llama-3.1-8B; rate=3117.83; inTk=255; outTk=388; sol=1, sat=false, alloc={acc=H100; numRep=4; maxBatch=512; cost=400, val=200, itl=9.57844, ttft=19.549582, rho=0.094549954, maxRPM=887.2747}; slo-itl=10, slo-ttft=1000, slo-tps=0 \nAllocationByType: \nname=NVIDIA-H100-80GB-HBM3, count=4, limit=0, cost=400 \ntotalCost=400 \n"}
{"level":"DEBUG","ts":"2025-12-10T05:08:56.535Z","msg":"Setting accelerator name: Name=H100, allocationData={H100 4 512 400 9.57844 19.549582 {3117.83 255 388}}"}
{"level":"INFO","ts":"2025-12-10T05:08:56.535Z","msg":"Model-based optimization completed for model: unsloth/Meta-Llama-3.1-8B - model-based targets: map[ms-inference-scheduling-llm-d-modelservice-decode:4]"}
{"level":"WARN","ts":"2025-12-10T05:08:56.535Z","msg":"saturation analysis unavailable, using model-based targets only: modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-10T05:08:56.535Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-12-10T05:08:56.535Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=scale-up, current=2→target=4"}
{"level":"INFO","ts":"2025-12-10T05:08:56.535Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 2, desired-replicas: 4, accelerator: H100"}
{"level":"INFO","ts":"2025-12-10T05:08:56.535Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=4, accelerator=H100, SaturationOnly=false"}
{"level":"INFO","ts":"2025-12-10T05:08:56.542Z","msg":"Applied Saturation decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=scale-up, current=2, target=4, reason=model-based only (Saturation unavailable)"}
{"level":"INFO","ts":"2025-12-10T05:08:56.542Z","msg":"Reconciliation completed successfully: mode=model-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-12-10T05:09:56.543Z","msg":"Reconciling VariantAutoscaling: name=ms-inference-scheduling-llm-d-modelservice-decode, namespace=llm-d-inference-scheduler, modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-10T05:09:56.543Z","msg":"saturation scaling config cache updated: entries=1, has_default=true"}
{"level":"INFO","ts":"2025-12-10T05:09:56.543Z","msg":"saturation scaling configuration loaded successfully"}
{"level":"INFO","ts":"2025-12-10T05:09:56.543Z","msg":"Operating in MODEL-ONLY mode: model-based optimization only"}
{"level":"INFO","ts":"2025-12-10T05:09:56.543Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-12-10T05:09:56.543Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-12-10T05:09:56.553Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dgftqv, queueLength=0"}
{"level":"INFO","ts":"2025-12-10T05:09:56.553Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dkj52n, queueLength=0"}
{"level":"INFO","ts":"2025-12-10T05:09:56.553Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dczcvb, queueLength=0"}
{"level":"DEBUG","ts":"2025-12-10T05:09:56.553Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=3"}
{"level":"INFO","ts":"2025-12-10T05:09:56.553Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dgftqv, usage=0.926 (92.6%)"}
{"level":"INFO","ts":"2025-12-10T05:09:56.553Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dkj52n, usage=0.945 (94.5%)"}
{"level":"INFO","ts":"2025-12-10T05:09:56.553Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dczcvb, usage=0.108 (10.8%)"}
{"level":"DEBUG","ts":"2025-12-10T05:09:56.553Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=3"}
{"level":"DEBUG","ts":"2025-12-10T05:09:56.556Z","msg":"Pod-to-variant matching successful: totalPods=3, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:3]"}
{"level":"DEBUG","ts":"2025-12-10T05:09:56.556Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=3"}
{"level":"DEBUG","ts":"2025-12-10T05:09:56.556Z","msg":"Collected capacity metrics for observability in model-only mode: modelID=unsloth/Meta-Llama-3.1-8B, replicaCount=3"}
{"level":"DEBUG","ts":"2025-12-10T05:09:56.569Z","msg":"Replica count mismatch for llm-d-inference-scheduler/unsloth/Meta-Llama-3.1-8B: spec=4, reporting_metrics=3"}
{"level":"DEBUG","ts":"2025-12-10T05:09:56.569Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=3, accelerator=H100, ttft=90.90ms, itl=56.48ms, cost=300.00, maxBatch=256, arrivalRate=3315.12, avgInputTokens=237.06, avgOutputTokens=424.59"}
{"level":"DEBUG","ts":"2025-12-10T05:09:56.569Z","msg":"Experimental model tuner is enabled globally: tuning model performance parameters for active VAs"}
{"level":"DEBUG","ts":"2025-12-10T05:09:56.569Z","msg":"Using state vals from VA status to tune variant ms-inference-scheduling-llm-d-modelservice-decode: alpha= 7.293937, beta= 0.046236, gamma= 16.286325, delta= 0.000259"}
{"level":"DEBUG","ts":"2025-12-10T05:09:56.569Z","msg":"[Tuner Config] variant=ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler | Initial state: alpha=7.293937, beta=0.046236, gamma=16.286325, delta=0.000259 | Expected obs (for R): TTFT=90.90ms, ITL=56.48ms | Has covariance=true"}
{"level":"WARN","ts":"2025-12-10T05:09:56.571Z","msg":"Tuner validation failed (NIS=1033.43), validation error: normalized innovation squared (NIS=1033.43) exceeds threshold (7.38), rejecting update as outlier - returning previous state: alpha=7.293937, beta=0.046236, gamma=16.286325, delta=0.000259"}
{"level":"WARN","ts":"2025-12-10T05:09:56.571Z","msg":"Tuner NIS validation failed for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler, server ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler (NIS=1033.43 exceeds threshold 7.38) - Keeping previous state: alpha=7.293937, beta=0.046236, gamma=16.286325, delta=0.000259"}
{"level":"INFO","ts":"2025-12-10T05:09:56.571Z","msg":"Keeping previous tuned parameters for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler due to NIS validation failure (NIS=1033.425681)"}
{"level":"DEBUG","ts":"2025-12-10T05:09:56.571Z","msg":"Model tuner results: model=unsloth/Meta-Llama-3.1-8B, accelerator=H100, alpha=7.293937, beta=0.046236, gamma=16.286325, delta=0.000259, NIS=1033.43"}
{"level":"DEBUG","ts":"2025-12-10T05:09:56.571Z","msg":"Updated tuner status for variant ms-inference-scheduling-llm-d-modelservice-decode: alpha=7.293937, beta=0.046236, gamma=16.286325, delta=0.000259, NIS=1033.425681"}
{"level":"DEBUG","ts":"2025-12-10T05:09:56.581Z","msg":"Optimization solution - system: Solution: \ns=ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler; c=Premium; m=unsloth/Meta-Llama-3.1-8B; rate=3315.12; inTk=237; outTk=424; sol=1, sat=false, alloc={acc=H100; numRep=5; maxBatch=512; cost=500, val=200, itl=9.376339, ttft=19.050924, rho=0.086012736, maxRPM=812.1706}; slo-itl=10, slo-ttft=1000, slo-tps=0 \nAllocationByType: \nname=NVIDIA-H100-80GB-HBM3, count=5, limit=0, cost=500 \ntotalCost=500 \n"}
{"level":"DEBUG","ts":"2025-12-10T05:09:56.581Z","msg":"Setting accelerator name: Name=H100, allocationData={H100 5 512 500 9.376339 19.050924 {3315.12 237 424}}"}
{"level":"INFO","ts":"2025-12-10T05:09:56.581Z","msg":"Model-based optimization completed for model: unsloth/Meta-Llama-3.1-8B - model-based targets: map[ms-inference-scheduling-llm-d-modelservice-decode:5]"}
{"level":"WARN","ts":"2025-12-10T05:09:56.581Z","msg":"saturation analysis unavailable, using model-based targets only: modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-10T05:09:56.581Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-12-10T05:09:56.581Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=scale-up, current=3→target=5"}
{"level":"INFO","ts":"2025-12-10T05:09:56.581Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 3, desired-replicas: 5, accelerator: H100"}
{"level":"INFO","ts":"2025-12-10T05:09:56.581Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=5, accelerator=H100, SaturationOnly=false"}
{"level":"INFO","ts":"2025-12-10T05:09:56.589Z","msg":"Applied Saturation decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=scale-up, current=3, target=5, reason=model-based only (Saturation unavailable)"}
{"level":"INFO","ts":"2025-12-10T05:09:56.589Z","msg":"Reconciliation completed successfully: mode=model-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-12-10T05:10:56.589Z","msg":"Reconciling VariantAutoscaling: name=ms-inference-scheduling-llm-d-modelservice-decode, namespace=llm-d-inference-scheduler, modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-10T05:10:56.590Z","msg":"saturation scaling config cache updated: entries=1, has_default=true"}
{"level":"INFO","ts":"2025-12-10T05:10:56.590Z","msg":"saturation scaling configuration loaded successfully"}
{"level":"INFO","ts":"2025-12-10T05:10:56.590Z","msg":"Operating in MODEL-ONLY mode: model-based optimization only"}
{"level":"INFO","ts":"2025-12-10T05:10:56.590Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-12-10T05:10:56.590Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-12-10T05:10:56.609Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dgftqv, queueLength=0"}
{"level":"INFO","ts":"2025-12-10T05:10:56.609Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dkj52n, queueLength=0"}
{"level":"INFO","ts":"2025-12-10T05:10:56.609Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dczcvb, queueLength=0"}
{"level":"DEBUG","ts":"2025-12-10T05:10:56.609Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=3"}
{"level":"INFO","ts":"2025-12-10T05:10:56.609Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dgftqv, usage=0.983 (98.3%)"}
{"level":"INFO","ts":"2025-12-10T05:10:56.609Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dkj52n, usage=0.871 (87.1%)"}
{"level":"INFO","ts":"2025-12-10T05:10:56.609Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dczcvb, usage=0.277 (27.7%)"}
{"level":"DEBUG","ts":"2025-12-10T05:10:56.609Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=3"}
{"level":"DEBUG","ts":"2025-12-10T05:10:56.619Z","msg":"Pod-to-variant matching successful: totalPods=3, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:3]"}
{"level":"DEBUG","ts":"2025-12-10T05:10:56.619Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=3"}
{"level":"DEBUG","ts":"2025-12-10T05:10:56.619Z","msg":"Collected capacity metrics for observability in model-only mode: modelID=unsloth/Meta-Llama-3.1-8B, replicaCount=3"}
{"level":"DEBUG","ts":"2025-12-10T05:10:56.663Z","msg":"Replica count mismatch for llm-d-inference-scheduler/unsloth/Meta-Llama-3.1-8B: spec=5, reporting_metrics=3"}
{"level":"DEBUG","ts":"2025-12-10T05:10:56.663Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=3, accelerator=H100, ttft=64.11ms, itl=38.95ms, cost=300.00, maxBatch=256, arrivalRate=4437.03, avgInputTokens=222.88, avgOutputTokens=501.05"}
{"level":"DEBUG","ts":"2025-12-10T05:10:56.663Z","msg":"Experimental model tuner is enabled globally: tuning model performance parameters for active VAs"}
{"level":"DEBUG","ts":"2025-12-10T05:10:56.664Z","msg":"Using state vals from VA status to tune variant ms-inference-scheduling-llm-d-modelservice-decode: alpha= 7.293937, beta= 0.046236, gamma= 16.286325, delta= 0.000259"}
{"level":"DEBUG","ts":"2025-12-10T05:10:56.664Z","msg":"[Tuner Config] variant=ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler | Initial state: alpha=7.293937, beta=0.046236, gamma=16.286325, delta=0.000259 | Expected obs (for R): TTFT=64.11ms, ITL=38.95ms | Has covariance=true"}
{"level":"WARN","ts":"2025-12-10T05:10:56.664Z","msg":"Tuner validation failed (NIS=449.24), validation error: normalized innovation squared (NIS=449.24) exceeds threshold (7.38), rejecting update as outlier - returning previous state: alpha=7.293937, beta=0.046236, gamma=16.286325, delta=0.000259"}
{"level":"WARN","ts":"2025-12-10T05:10:56.664Z","msg":"Tuner NIS validation failed for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler, server ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler (NIS=449.24 exceeds threshold 7.38) - Keeping previous state: alpha=7.293937, beta=0.046236, gamma=16.286325, delta=0.000259"}
{"level":"INFO","ts":"2025-12-10T05:10:56.664Z","msg":"Keeping previous tuned parameters for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler due to NIS validation failure (NIS=449.236911)"}
{"level":"DEBUG","ts":"2025-12-10T05:10:56.664Z","msg":"Model tuner results: model=unsloth/Meta-Llama-3.1-8B, accelerator=H100, alpha=7.293937, beta=0.046236, gamma=16.286325, delta=0.000259, NIS=449.24"}
{"level":"DEBUG","ts":"2025-12-10T05:10:56.664Z","msg":"Updated tuner status for variant ms-inference-scheduling-llm-d-modelservice-decode: alpha=7.293937, beta=0.046236, gamma=16.286325, delta=0.000259, NIS=449.236911"}
{"level":"DEBUG","ts":"2025-12-10T05:10:56.673Z","msg":"Optimization solution - system: Solution: \ns=ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler; c=Premium; m=unsloth/Meta-Llama-3.1-8B; rate=4437.03; inTk=222; outTk=501; sol=1, sat=false, alloc={acc=H100; numRep=7; maxBatch=512; cost=700, val=400, itl=9.724616, ttft=19.309061, rho=0.10072489, maxRPM=687.62506}; slo-itl=10, slo-ttft=1000, slo-tps=0 \nAllocationByType: \nname=NVIDIA-H100-80GB-HBM3, count=7, limit=0, cost=700 \ntotalCost=700 \n"}
{"level":"DEBUG","ts":"2025-12-10T05:10:56.673Z","msg":"Setting accelerator name: Name=H100, allocationData={H100 7 512 700 9.724616 19.309061 {4437.03 222 501}}"}
{"level":"INFO","ts":"2025-12-10T05:10:56.673Z","msg":"Model-based optimization completed for model: unsloth/Meta-Llama-3.1-8B - model-based targets: map[ms-inference-scheduling-llm-d-modelservice-decode:7]"}
{"level":"WARN","ts":"2025-12-10T05:10:56.673Z","msg":"saturation analysis unavailable, using model-based targets only: modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-10T05:10:56.673Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-12-10T05:10:56.673Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=scale-up, current=3→target=7"}
{"level":"INFO","ts":"2025-12-10T05:10:56.674Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 3, desired-replicas: 7, accelerator: H100"}
{"level":"INFO","ts":"2025-12-10T05:10:56.674Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=7, accelerator=H100, SaturationOnly=false"}
{"level":"INFO","ts":"2025-12-10T05:10:56.680Z","msg":"Applied Saturation decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=scale-up, current=3, target=7, reason=model-based only (Saturation unavailable)"}
{"level":"INFO","ts":"2025-12-10T05:10:56.680Z","msg":"Reconciliation completed successfully: mode=model-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-12-10T05:11:56.680Z","msg":"Reconciling VariantAutoscaling: name=ms-inference-scheduling-llm-d-modelservice-decode, namespace=llm-d-inference-scheduler, modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-10T05:11:56.680Z","msg":"saturation scaling config cache updated: entries=1, has_default=true"}
{"level":"INFO","ts":"2025-12-10T05:11:56.680Z","msg":"saturation scaling configuration loaded successfully"}
{"level":"INFO","ts":"2025-12-10T05:11:56.680Z","msg":"Operating in MODEL-ONLY mode: model-based optimization only"}
{"level":"INFO","ts":"2025-12-10T05:11:56.680Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-12-10T05:11:56.680Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-12-10T05:11:56.692Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dgftqv, usage=0.744 (74.4%)"}
{"level":"INFO","ts":"2025-12-10T05:11:56.692Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984d72f8b, usage=0.149 (14.9%)"}
{"level":"INFO","ts":"2025-12-10T05:11:56.692Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dkj52n, usage=0.439 (43.9%)"}
{"level":"INFO","ts":"2025-12-10T05:11:56.692Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dczcvb, usage=0.315 (31.5%)"}
{"level":"DEBUG","ts":"2025-12-10T05:11:56.692Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=4"}
{"level":"INFO","ts":"2025-12-10T05:11:56.692Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dgftqv, queueLength=0"}
{"level":"INFO","ts":"2025-12-10T05:11:56.692Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984d72f8b, queueLength=0"}
{"level":"INFO","ts":"2025-12-10T05:11:56.692Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dkj52n, queueLength=0"}
{"level":"INFO","ts":"2025-12-10T05:11:56.692Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dczcvb, queueLength=0"}
{"level":"DEBUG","ts":"2025-12-10T05:11:56.692Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=4"}
{"level":"DEBUG","ts":"2025-12-10T05:11:56.696Z","msg":"Pod-to-variant matching successful: totalPods=4, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:4]"}
{"level":"DEBUG","ts":"2025-12-10T05:11:56.696Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=4"}
{"level":"DEBUG","ts":"2025-12-10T05:11:56.696Z","msg":"Collected capacity metrics for observability in model-only mode: modelID=unsloth/Meta-Llama-3.1-8B, replicaCount=4"}
{"level":"DEBUG","ts":"2025-12-10T05:11:56.710Z","msg":"Replica count mismatch for llm-d-inference-scheduler/unsloth/Meta-Llama-3.1-8B: spec=7, reporting_metrics=4"}
{"level":"DEBUG","ts":"2025-12-10T05:11:56.710Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=4, accelerator=H100, ttft=40.99ms, itl=24.37ms, cost=400.00, maxBatch=256, arrivalRate=5163.69, avgInputTokens=232.09, avgOutputTokens=465.84"}
{"level":"DEBUG","ts":"2025-12-10T05:11:56.710Z","msg":"Experimental model tuner is enabled globally: tuning model performance parameters for active VAs"}
{"level":"DEBUG","ts":"2025-12-10T05:11:56.710Z","msg":"Using state vals from VA status to tune variant ms-inference-scheduling-llm-d-modelservice-decode: alpha= 7.293937, beta= 0.046236, gamma= 16.286325, delta= 0.000259"}
{"level":"DEBUG","ts":"2025-12-10T05:11:56.710Z","msg":"[Tuner Config] variant=ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler | Initial state: alpha=7.293937, beta=0.046236, gamma=16.286325, delta=0.000259 | Expected obs (for R): TTFT=40.99ms, ITL=24.37ms | Has covariance=true"}
{"level":"WARN","ts":"2025-12-10T05:11:56.711Z","msg":"Tuner validation failed (NIS=66.77), validation error: normalized innovation squared (NIS=66.77) exceeds threshold (7.38), rejecting update as outlier - returning previous state: alpha=7.293937, beta=0.046236, gamma=16.286325, delta=0.000259"}
{"level":"WARN","ts":"2025-12-10T05:11:56.711Z","msg":"Tuner NIS validation failed for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler, server ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler (NIS=66.77 exceeds threshold 7.38) - Keeping previous state: alpha=7.293937, beta=0.046236, gamma=16.286325, delta=0.000259"}
{"level":"INFO","ts":"2025-12-10T05:11:56.711Z","msg":"Keeping previous tuned parameters for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler due to NIS validation failure (NIS=66.765055)"}
{"level":"DEBUG","ts":"2025-12-10T05:11:56.711Z","msg":"Model tuner results: model=unsloth/Meta-Llama-3.1-8B, accelerator=H100, alpha=7.293937, beta=0.046236, gamma=16.286325, delta=0.000259, NIS=66.77"}
{"level":"DEBUG","ts":"2025-12-10T05:11:56.711Z","msg":"Updated tuner status for variant ms-inference-scheduling-llm-d-modelservice-decode: alpha=7.293937, beta=0.046236, gamma=16.286325, delta=0.000259, NIS=66.765055"}
{"level":"DEBUG","ts":"2025-12-10T05:11:56.720Z","msg":"Optimization solution - system: Solution: \ns=ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler; c=Premium; m=unsloth/Meta-Llama-3.1-8B; rate=5163.69; inTk=232; outTk=465; sol=1, sat=false, alloc={acc=H100; numRep=7; maxBatch=512; cost=700, val=300, itl=9.985087, ttft=19.783726, rho=0.11172781, maxRPM=740.7258}; slo-itl=10, slo-ttft=1000, slo-tps=0 \nAllocationByType: \nname=NVIDIA-H100-80GB-HBM3, count=7, limit=0, cost=700 \ntotalCost=700 \n"}
{"level":"DEBUG","ts":"2025-12-10T05:11:56.720Z","msg":"Setting accelerator name: Name=H100, allocationData={H100 7 512 700 9.985087 19.783726 {5163.69 232 465}}"}
{"level":"INFO","ts":"2025-12-10T05:11:56.720Z","msg":"Model-based optimization completed for model: unsloth/Meta-Llama-3.1-8B - model-based targets: map[ms-inference-scheduling-llm-d-modelservice-decode:7]"}
{"level":"WARN","ts":"2025-12-10T05:11:56.720Z","msg":"saturation analysis unavailable, using model-based targets only: modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-10T05:11:56.720Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-12-10T05:11:56.720Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=scale-up, current=4→target=7"}
{"level":"INFO","ts":"2025-12-10T05:11:56.720Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 4, desired-replicas: 7, accelerator: H100"}
{"level":"INFO","ts":"2025-12-10T05:11:56.720Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=7, accelerator=H100, SaturationOnly=false"}
{"level":"INFO","ts":"2025-12-10T05:11:56.732Z","msg":"Applied Saturation decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=scale-up, current=4, target=7, reason=model-based only (Saturation unavailable)"}
{"level":"INFO","ts":"2025-12-10T05:11:56.732Z","msg":"Reconciliation completed successfully: mode=model-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-12-10T05:12:56.732Z","msg":"Reconciling VariantAutoscaling: name=ms-inference-scheduling-llm-d-modelservice-decode, namespace=llm-d-inference-scheduler, modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-10T05:12:56.732Z","msg":"saturation scaling config cache updated: entries=1, has_default=true"}
{"level":"INFO","ts":"2025-12-10T05:12:56.732Z","msg":"saturation scaling configuration loaded successfully"}
{"level":"INFO","ts":"2025-12-10T05:12:56.732Z","msg":"Operating in MODEL-ONLY mode: model-based optimization only"}
{"level":"INFO","ts":"2025-12-10T05:12:56.732Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-12-10T05:12:56.732Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-12-10T05:12:56.737Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dgftqv, queueLength=0"}
{"level":"INFO","ts":"2025-12-10T05:12:56.737Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984d72f8b, queueLength=0"}
{"level":"INFO","ts":"2025-12-10T05:12:56.737Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dd7xc5, queueLength=0"}
{"level":"INFO","ts":"2025-12-10T05:12:56.737Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dkj52n, queueLength=0"}
{"level":"INFO","ts":"2025-12-10T05:12:56.737Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dczcvb, queueLength=0"}
{"level":"DEBUG","ts":"2025-12-10T05:12:56.737Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=5"}
{"level":"INFO","ts":"2025-12-10T05:12:56.737Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dgftqv, usage=0.346 (34.6%)"}
{"level":"INFO","ts":"2025-12-10T05:12:56.737Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984d72f8b, usage=0.225 (22.5%)"}
{"level":"INFO","ts":"2025-12-10T05:12:56.737Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dd7xc5, usage=0.074 (7.4%)"}
{"level":"INFO","ts":"2025-12-10T05:12:56.737Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dkj52n, usage=0.281 (28.1%)"}
{"level":"INFO","ts":"2025-12-10T05:12:56.737Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dczcvb, usage=0.303 (30.3%)"}
{"level":"DEBUG","ts":"2025-12-10T05:12:56.737Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=5"}
{"level":"DEBUG","ts":"2025-12-10T05:12:56.740Z","msg":"Pod-to-variant matching successful: totalPods=5, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:5]"}
{"level":"DEBUG","ts":"2025-12-10T05:12:56.740Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=5"}
{"level":"DEBUG","ts":"2025-12-10T05:12:56.740Z","msg":"Collected capacity metrics for observability in model-only mode: modelID=unsloth/Meta-Llama-3.1-8B, replicaCount=5"}
{"level":"DEBUG","ts":"2025-12-10T05:12:56.755Z","msg":"Replica count mismatch for llm-d-inference-scheduler/unsloth/Meta-Llama-3.1-8B: spec=7, reporting_metrics=5"}
{"level":"DEBUG","ts":"2025-12-10T05:12:56.755Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=5, accelerator=H100, ttft=33.25ms, itl=17.76ms, cost=500.00, maxBatch=256, arrivalRate=5211.94, avgInputTokens=212.69, avgOutputTokens=506.47"}
{"level":"DEBUG","ts":"2025-12-10T05:12:56.755Z","msg":"Experimental model tuner is enabled globally: tuning model performance parameters for active VAs"}
{"level":"DEBUG","ts":"2025-12-10T05:12:56.755Z","msg":"Using state vals from VA status to tune variant ms-inference-scheduling-llm-d-modelservice-decode: alpha= 7.293937, beta= 0.046236, gamma= 16.286325, delta= 0.000259"}
{"level":"DEBUG","ts":"2025-12-10T05:12:56.755Z","msg":"[Tuner Config] variant=ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler | Initial state: alpha=7.293937, beta=0.046236, gamma=16.286325, delta=0.000259 | Expected obs (for R): TTFT=33.25ms, ITL=17.76ms | Has covariance=true"}
{"level":"WARN","ts":"2025-12-10T05:12:56.756Z","msg":"Tuner validation failed (NIS=52.46), validation error: normalized innovation squared (NIS=52.46) exceeds threshold (7.38), rejecting update as outlier - returning previous state: alpha=7.293937, beta=0.046236, gamma=16.286325, delta=0.000259"}
{"level":"WARN","ts":"2025-12-10T05:12:56.756Z","msg":"Tuner NIS validation failed for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler, server ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler (NIS=52.46 exceeds threshold 7.38) - Keeping previous state: alpha=7.293937, beta=0.046236, gamma=16.286325, delta=0.000259"}
{"level":"INFO","ts":"2025-12-10T05:12:56.756Z","msg":"Keeping previous tuned parameters for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler due to NIS validation failure (NIS=52.460600)"}
{"level":"DEBUG","ts":"2025-12-10T05:12:56.756Z","msg":"Model tuner results: model=unsloth/Meta-Llama-3.1-8B, accelerator=H100, alpha=7.293937, beta=0.046236, gamma=16.286325, delta=0.000259, NIS=52.46"}
{"level":"DEBUG","ts":"2025-12-10T05:12:56.756Z","msg":"Updated tuner status for variant ms-inference-scheduling-llm-d-modelservice-decode: alpha=7.293937, beta=0.046236, gamma=16.286325, delta=0.000259, NIS=52.460600"}
{"level":"DEBUG","ts":"2025-12-10T05:12:56.766Z","msg":"Optimization solution - system: Solution: \ns=ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler; c=Premium; m=unsloth/Meta-Llama-3.1-8B; rate=5211.94; inTk=212; outTk=506; sol=1, sat=false, alloc={acc=H100; numRep=8; maxBatch=512; cost=800, val=300, itl=9.846175, ttft=19.31726, rho=0.10585981, maxRPM=680.8607}; slo-itl=10, slo-ttft=1000, slo-tps=0 \nAllocationByType: \nname=NVIDIA-H100-80GB-HBM3, count=8, limit=0, cost=800 \ntotalCost=800 \n"}
{"level":"DEBUG","ts":"2025-12-10T05:12:56.766Z","msg":"Setting accelerator name: Name=H100, allocationData={H100 8 512 800 9.846175 19.31726 {5211.94 212 506}}"}
{"level":"INFO","ts":"2025-12-10T05:12:56.766Z","msg":"Model-based optimization completed for model: unsloth/Meta-Llama-3.1-8B - model-based targets: map[ms-inference-scheduling-llm-d-modelservice-decode:8]"}
{"level":"WARN","ts":"2025-12-10T05:12:56.766Z","msg":"saturation analysis unavailable, using model-based targets only: modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-10T05:12:56.766Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-12-10T05:12:56.766Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=scale-up, current=5→target=8"}
{"level":"INFO","ts":"2025-12-10T05:12:56.766Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 5, desired-replicas: 8, accelerator: H100"}
{"level":"INFO","ts":"2025-12-10T05:12:56.766Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=8, accelerator=H100, SaturationOnly=false"}
{"level":"INFO","ts":"2025-12-10T05:12:56.772Z","msg":"Applied Saturation decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=scale-up, current=5, target=8, reason=model-based only (Saturation unavailable)"}
{"level":"INFO","ts":"2025-12-10T05:12:56.772Z","msg":"Reconciliation completed successfully: mode=model-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-12-10T05:13:56.772Z","msg":"Reconciling VariantAutoscaling: name=ms-inference-scheduling-llm-d-modelservice-decode, namespace=llm-d-inference-scheduler, modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-10T05:13:56.773Z","msg":"saturation scaling config cache updated: entries=1, has_default=true"}
{"level":"INFO","ts":"2025-12-10T05:13:56.773Z","msg":"saturation scaling configuration loaded successfully"}
{"level":"INFO","ts":"2025-12-10T05:13:56.773Z","msg":"Operating in MODEL-ONLY mode: model-based optimization only"}
{"level":"INFO","ts":"2025-12-10T05:13:56.773Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-12-10T05:13:56.773Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-12-10T05:13:56.791Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dgftqv, usage=0.090 (9.0%)"}
{"level":"INFO","ts":"2025-12-10T05:13:56.791Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984d72f8b, usage=0.081 (8.1%)"}
{"level":"INFO","ts":"2025-12-10T05:13:56.791Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dd7xc5, usage=0.086 (8.6%)"}
{"level":"INFO","ts":"2025-12-10T05:13:56.791Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984ddkg9x, usage=0.005 (0.5%)"}
{"level":"INFO","ts":"2025-12-10T05:13:56.791Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984d4rkq7, usage=0.011 (1.1%)"}
{"level":"INFO","ts":"2025-12-10T05:13:56.791Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dkj52n, usage=0.111 (11.1%)"}
{"level":"INFO","ts":"2025-12-10T05:13:56.791Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dczcvb, usage=0.132 (13.2%)"}
{"level":"DEBUG","ts":"2025-12-10T05:13:56.791Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=7"}
{"level":"INFO","ts":"2025-12-10T05:13:56.791Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dgftqv, queueLength=0"}
{"level":"INFO","ts":"2025-12-10T05:13:56.791Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984d72f8b, queueLength=0"}
{"level":"INFO","ts":"2025-12-10T05:13:56.791Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dd7xc5, queueLength=0"}
{"level":"INFO","ts":"2025-12-10T05:13:56.791Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984ddkg9x, queueLength=0"}
{"level":"INFO","ts":"2025-12-10T05:13:56.791Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984d4rkq7, queueLength=0"}
{"level":"INFO","ts":"2025-12-10T05:13:56.791Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dkj52n, queueLength=0"}
{"level":"INFO","ts":"2025-12-10T05:13:56.791Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dczcvb, queueLength=0"}
{"level":"DEBUG","ts":"2025-12-10T05:13:56.791Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=7"}
{"level":"DEBUG","ts":"2025-12-10T05:13:56.796Z","msg":"Pod-to-variant matching successful: totalPods=7, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:7]"}
{"level":"DEBUG","ts":"2025-12-10T05:13:56.796Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=7"}
{"level":"DEBUG","ts":"2025-12-10T05:13:56.796Z","msg":"Collected capacity metrics for observability in model-only mode: modelID=unsloth/Meta-Llama-3.1-8B, replicaCount=7"}
{"level":"DEBUG","ts":"2025-12-10T05:13:56.812Z","msg":"Replica count mismatch for llm-d-inference-scheduler/unsloth/Meta-Llama-3.1-8B: spec=8, reporting_metrics=7"}
{"level":"DEBUG","ts":"2025-12-10T05:13:56.813Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=7, accelerator=H100, ttft=21.06ms, itl=10.37ms, cost=700.00, maxBatch=256, arrivalRate=3890.78, avgInputTokens=237.72, avgOutputTokens=448.29"}
{"level":"DEBUG","ts":"2025-12-10T05:13:56.813Z","msg":"Experimental model tuner is enabled globally: tuning model performance parameters for active VAs"}
{"level":"DEBUG","ts":"2025-12-10T05:13:56.813Z","msg":"Using state vals from VA status to tune variant ms-inference-scheduling-llm-d-modelservice-decode: alpha= 7.293937, beta= 0.046236, gamma= 16.286325, delta= 0.000259"}
{"level":"DEBUG","ts":"2025-12-10T05:13:56.813Z","msg":"[Tuner Config] variant=ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler | Initial state: alpha=7.293937, beta=0.046236, gamma=16.286325, delta=0.000259 | Expected obs (for R): TTFT=21.06ms, ITL=10.37ms | Has covariance=true"}
{"level":"WARN","ts":"2025-12-10T05:13:56.813Z","msg":"Tuner validation failed (NIS=8.05), validation error: normalized innovation squared (NIS=8.05) exceeds threshold (7.38), rejecting update as outlier - returning previous state: alpha=7.293937, beta=0.046236, gamma=16.286325, delta=0.000259"}
{"level":"WARN","ts":"2025-12-10T05:13:56.813Z","msg":"Tuner NIS validation failed for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler, server ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler (NIS=8.05 exceeds threshold 7.38) - Keeping previous state: alpha=7.293937, beta=0.046236, gamma=16.286325, delta=0.000259"}
{"level":"INFO","ts":"2025-12-10T05:13:56.813Z","msg":"Keeping previous tuned parameters for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler due to NIS validation failure (NIS=8.053136)"}
{"level":"DEBUG","ts":"2025-12-10T05:13:56.813Z","msg":"Model tuner results: model=unsloth/Meta-Llama-3.1-8B, accelerator=H100, alpha=7.293937, beta=0.046236, gamma=16.286325, delta=0.000259, NIS=8.05"}
{"level":"DEBUG","ts":"2025-12-10T05:13:56.813Z","msg":"Updated tuner status for variant ms-inference-scheduling-llm-d-modelservice-decode: alpha=7.293937, beta=0.046236, gamma=16.286325, delta=0.000259, NIS=8.053136"}
{"level":"DEBUG","ts":"2025-12-10T05:13:56.822Z","msg":"Optimization solution - system: Solution: \ns=ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler; c=Premium; m=unsloth/Meta-Llama-3.1-8B; rate=3890.78; inTk=237; outTk=448; sol=1, sat=false, alloc={acc=H100; numRep=6; maxBatch=512; cost=600, val=-100, itl=9.463629, ttft=19.166811, rho=0.08970006, maxRPM=768.756}; slo-itl=10, slo-ttft=1000, slo-tps=0 \nAllocationByType: \nname=NVIDIA-H100-80GB-HBM3, count=6, limit=0, cost=600 \ntotalCost=600 \n"}
{"level":"DEBUG","ts":"2025-12-10T05:13:56.822Z","msg":"Setting accelerator name: Name=H100, allocationData={H100 6 512 600 9.463629 19.166811 {3890.78 237 448}}"}
{"level":"INFO","ts":"2025-12-10T05:13:56.822Z","msg":"Model-based optimization completed for model: unsloth/Meta-Llama-3.1-8B - model-based targets: map[ms-inference-scheduling-llm-d-modelservice-decode:6]"}
{"level":"WARN","ts":"2025-12-10T05:13:56.822Z","msg":"saturation analysis unavailable, using model-based targets only: modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-10T05:13:56.822Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-12-10T05:13:56.822Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=scale-down, current=7→target=6"}
{"level":"INFO","ts":"2025-12-10T05:13:56.822Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 7, desired-replicas: 6, accelerator: H100"}
{"level":"INFO","ts":"2025-12-10T05:13:56.822Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=6, accelerator=H100, SaturationOnly=false"}
{"level":"INFO","ts":"2025-12-10T05:13:56.828Z","msg":"Applied Saturation decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=scale-down, current=7, target=6, reason=model-based only (Saturation unavailable)"}
{"level":"INFO","ts":"2025-12-10T05:13:56.828Z","msg":"Reconciliation completed successfully: mode=model-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-12-10T05:14:56.829Z","msg":"Reconciling VariantAutoscaling: name=ms-inference-scheduling-llm-d-modelservice-decode, namespace=llm-d-inference-scheduler, modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-10T05:14:56.829Z","msg":"saturation scaling config cache updated: entries=1, has_default=true"}
{"level":"INFO","ts":"2025-12-10T05:14:56.829Z","msg":"saturation scaling configuration loaded successfully"}
{"level":"INFO","ts":"2025-12-10T05:14:56.829Z","msg":"Operating in MODEL-ONLY mode: model-based optimization only"}
{"level":"INFO","ts":"2025-12-10T05:14:56.829Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-12-10T05:14:56.829Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-12-10T05:14:56.847Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dgftqv, queueLength=0"}
{"level":"INFO","ts":"2025-12-10T05:14:56.847Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984d72f8b, queueLength=0"}
{"level":"INFO","ts":"2025-12-10T05:14:56.847Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dd7xc5, queueLength=0"}
{"level":"INFO","ts":"2025-12-10T05:14:56.847Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984ddkg9x, queueLength=0"}
{"level":"INFO","ts":"2025-12-10T05:14:56.847Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984d4rkq7, queueLength=0"}
{"level":"INFO","ts":"2025-12-10T05:14:56.847Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dkj52n, queueLength=0"}
{"level":"INFO","ts":"2025-12-10T05:14:56.847Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dczcvb, queueLength=0"}
{"level":"DEBUG","ts":"2025-12-10T05:14:56.847Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=7"}
{"level":"INFO","ts":"2025-12-10T05:14:56.848Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dgftqv, usage=0.067 (6.7%)"}
{"level":"INFO","ts":"2025-12-10T05:14:56.848Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984d72f8b, usage=0.044 (4.4%)"}
{"level":"INFO","ts":"2025-12-10T05:14:56.848Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dd7xc5, usage=0.059 (5.9%)"}
{"level":"INFO","ts":"2025-12-10T05:14:56.848Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984ddkg9x, usage=0.051 (5.1%)"}
{"level":"INFO","ts":"2025-12-10T05:14:56.848Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984d4rkq7, usage=0.050 (5.0%)"}
{"level":"INFO","ts":"2025-12-10T05:14:56.848Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dkj52n, usage=0.079 (7.9%)"}
{"level":"INFO","ts":"2025-12-10T05:14:56.848Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dczcvb, usage=0.087 (8.7%)"}
{"level":"DEBUG","ts":"2025-12-10T05:14:56.848Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=7"}
{"level":"DEBUG","ts":"2025-12-10T05:14:56.852Z","msg":"Pod-to-variant matching successful: totalPods=7, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:7]"}
{"level":"DEBUG","ts":"2025-12-10T05:14:56.852Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=7"}
{"level":"DEBUG","ts":"2025-12-10T05:14:56.852Z","msg":"Collected capacity metrics for observability in model-only mode: modelID=unsloth/Meta-Llama-3.1-8B, replicaCount=7"}
{"level":"DEBUG","ts":"2025-12-10T05:14:56.867Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=6, accelerator=H100, ttft=18.89ms, itl=9.22ms, cost=600.00, maxBatch=256, arrivalRate=3569.24, avgInputTokens=240.24, avgOutputTokens=444.06"}
{"level":"DEBUG","ts":"2025-12-10T05:14:56.867Z","msg":"Experimental model tuner is enabled globally: tuning model performance parameters for active VAs"}
{"level":"DEBUG","ts":"2025-12-10T05:14:56.867Z","msg":"Using state vals from VA status to tune variant ms-inference-scheduling-llm-d-modelservice-decode: alpha= 7.293937, beta= 0.046236, gamma= 16.286325, delta= 0.000259"}
{"level":"DEBUG","ts":"2025-12-10T05:14:56.867Z","msg":"[Tuner Config] variant=ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler | Initial state: alpha=7.293937, beta=0.046236, gamma=16.286325, delta=0.000259 | Expected obs (for R): TTFT=18.89ms, ITL=9.22ms | Has covariance=true"}
{"level":"INFO","ts":"2025-12-10T05:14:56.868Z","msg":"Tuner validation succeeded for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler, server ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler - New NIS: 0.000156"}
{"level":"DEBUG","ts":"2025-12-10T05:14:56.868Z","msg":"Model tuner results: model=unsloth/Meta-Llama-3.1-8B, accelerator=H100, alpha=7.293468, beta=0.046225, gamma=16.296036, delta=0.000259, NIS=0.00"}
{"level":"DEBUG","ts":"2025-12-10T05:14:56.868Z","msg":"Updated tuner status for variant ms-inference-scheduling-llm-d-modelservice-decode: alpha=7.293468, beta=0.046225, gamma=16.296036, delta=0.000259, NIS=0.000156"}
{"level":"INFO","ts":"2025-12-10T05:14:56.868Z","msg":"Tuned performance parameters result: variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler - alpha: 7.293468, beta: 0.046225, gamma: 16.296036, delta: 0.000259"}
{"level":"DEBUG","ts":"2025-12-10T05:14:56.878Z","msg":"Optimization solution - system: Solution: \ns=ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler; c=Premium; m=unsloth/Meta-Llama-3.1-8B; rate=3569.24; inTk=240; outTk=444; sol=1, sat=false, alloc={acc=H100; numRep=5; maxBatch=512; cost=500, val=-100, itl=9.718116, ttft=19.556602, rho=0.10049368, maxRPM=775.9767}; slo-itl=10, slo-ttft=1000, slo-tps=0 \nAllocationByType: \nname=NVIDIA-H100-80GB-HBM3, count=5, limit=0, cost=500 \ntotalCost=500 \n"}
{"level":"DEBUG","ts":"2025-12-10T05:14:56.878Z","msg":"Setting accelerator name: Name=H100, allocationData={H100 5 512 500 9.718116 19.556602 {3569.24 240 444}}"}
{"level":"INFO","ts":"2025-12-10T05:14:56.878Z","msg":"Model-based optimization completed for model: unsloth/Meta-Llama-3.1-8B - model-based targets: map[ms-inference-scheduling-llm-d-modelservice-decode:5]"}
{"level":"WARN","ts":"2025-12-10T05:14:56.878Z","msg":"saturation analysis unavailable, using model-based targets only: modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-10T05:14:56.878Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-12-10T05:14:56.878Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=scale-down, current=6→target=5"}
{"level":"INFO","ts":"2025-12-10T05:14:56.878Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 6, desired-replicas: 5, accelerator: H100"}
{"level":"INFO","ts":"2025-12-10T05:14:56.878Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=5, accelerator=H100, SaturationOnly=false"}
{"level":"INFO","ts":"2025-12-10T05:14:56.884Z","msg":"Applied Saturation decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=scale-down, current=6, target=5, reason=model-based only (Saturation unavailable)"}
{"level":"INFO","ts":"2025-12-10T05:14:56.884Z","msg":"Reconciliation completed successfully: mode=model-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-12-10T05:15:56.885Z","msg":"Reconciling VariantAutoscaling: name=ms-inference-scheduling-llm-d-modelservice-decode, namespace=llm-d-inference-scheduler, modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-10T05:15:56.885Z","msg":"saturation scaling config cache updated: entries=1, has_default=true"}
{"level":"INFO","ts":"2025-12-10T05:15:56.885Z","msg":"saturation scaling configuration loaded successfully"}
{"level":"INFO","ts":"2025-12-10T05:15:56.885Z","msg":"Operating in MODEL-ONLY mode: model-based optimization only"}
{"level":"INFO","ts":"2025-12-10T05:15:56.885Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-12-10T05:15:56.885Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-12-10T05:15:56.899Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dgftqv, usage=0.071 (7.1%)"}
{"level":"INFO","ts":"2025-12-10T05:15:56.899Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984d72f8b, usage=0.076 (7.6%)"}
{"level":"INFO","ts":"2025-12-10T05:15:56.899Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dd7xc5, usage=0.066 (6.6%)"}
{"level":"INFO","ts":"2025-12-10T05:15:56.899Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984ddkg9x, usage=0.062 (6.2%)"}
{"level":"INFO","ts":"2025-12-10T05:15:56.899Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dkj52n, usage=0.077 (7.7%)"}
{"level":"INFO","ts":"2025-12-10T05:15:56.899Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dczcvb, usage=0.081 (8.1%)"}
{"level":"DEBUG","ts":"2025-12-10T05:15:56.899Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=6"}
{"level":"INFO","ts":"2025-12-10T05:15:56.900Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dgftqv, queueLength=0"}
{"level":"INFO","ts":"2025-12-10T05:15:56.900Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984d72f8b, queueLength=0"}
{"level":"INFO","ts":"2025-12-10T05:15:56.900Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dd7xc5, queueLength=0"}
{"level":"INFO","ts":"2025-12-10T05:15:56.900Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984ddkg9x, queueLength=0"}
{"level":"INFO","ts":"2025-12-10T05:15:56.900Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dkj52n, queueLength=0"}
{"level":"INFO","ts":"2025-12-10T05:15:56.900Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dczcvb, queueLength=0"}
{"level":"DEBUG","ts":"2025-12-10T05:15:56.900Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=6"}
{"level":"DEBUG","ts":"2025-12-10T05:15:56.905Z","msg":"Pod-to-variant matching successful: totalPods=6, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:6]"}
{"level":"DEBUG","ts":"2025-12-10T05:15:56.905Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=6"}
{"level":"DEBUG","ts":"2025-12-10T05:15:56.905Z","msg":"Collected capacity metrics for observability in model-only mode: modelID=unsloth/Meta-Llama-3.1-8B, replicaCount=6"}
{"level":"DEBUG","ts":"2025-12-10T05:15:56.920Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=5, accelerator=H100, ttft=19.73ms, itl=9.65ms, cost=500.00, maxBatch=256, arrivalRate=3401.10, avgInputTokens=241.79, avgOutputTokens=435.09"}
{"level":"DEBUG","ts":"2025-12-10T05:15:56.920Z","msg":"Experimental model tuner is enabled globally: tuning model performance parameters for active VAs"}
{"level":"DEBUG","ts":"2025-12-10T05:15:56.920Z","msg":"Using state vals from VA status to tune variant ms-inference-scheduling-llm-d-modelservice-decode: alpha= 7.293468, beta= 0.046225, gamma= 16.296036, delta= 0.000259"}
{"level":"DEBUG","ts":"2025-12-10T05:15:56.920Z","msg":"[Tuner Config] variant=ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler | Initial state: alpha=7.293468, beta=0.046225, gamma=16.296036, delta=0.000259 | Expected obs (for R): TTFT=19.73ms, ITL=9.65ms | Has covariance=true"}
{"level":"INFO","ts":"2025-12-10T05:15:56.921Z","msg":"Tuner validation succeeded for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler, server ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler - New NIS: 0.200517"}
{"level":"DEBUG","ts":"2025-12-10T05:15:56.921Z","msg":"Model tuner results: model=unsloth/Meta-Llama-3.1-8B, accelerator=H100, alpha=7.358946, beta=0.046773, gamma=16.601824, delta=0.000259, NIS=0.20"}
{"level":"DEBUG","ts":"2025-12-10T05:15:56.921Z","msg":"Updated tuner status for variant ms-inference-scheduling-llm-d-modelservice-decode: alpha=7.358946, beta=0.046773, gamma=16.601824, delta=0.000259, NIS=0.200517"}
{"level":"INFO","ts":"2025-12-10T05:15:56.921Z","msg":"Tuned performance parameters result: variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler - alpha: 7.358946, beta: 0.046773, gamma: 16.601824, delta: 0.000259"}
{"level":"DEBUG","ts":"2025-12-10T05:15:56.931Z","msg":"Optimization solution - system: Solution: \ns=ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler; c=Premium; m=unsloth/Meta-Llama-3.1-8B; rate=3401.1; inTk=241; outTk=435; sol=1, sat=false, alloc={acc=H100; numRep=5; maxBatch=512; cost=500, val=0, itl=9.633053, ttft=19.636364, rho=0.09300727, maxRPM=763.25854}; slo-itl=10, slo-ttft=1000, slo-tps=0 \nAllocationByType: \nname=NVIDIA-H100-80GB-HBM3, count=5, limit=0, cost=500 \ntotalCost=500 \n"}
{"level":"DEBUG","ts":"2025-12-10T05:15:56.931Z","msg":"Setting accelerator name: Name=H100, allocationData={H100 5 512 500 9.633053 19.636364 {3401.1 241 435}}"}
{"level":"INFO","ts":"2025-12-10T05:15:56.931Z","msg":"Model-based optimization completed for model: unsloth/Meta-Llama-3.1-8B - model-based targets: map[ms-inference-scheduling-llm-d-modelservice-decode:5]"}
{"level":"WARN","ts":"2025-12-10T05:15:56.931Z","msg":"saturation analysis unavailable, using model-based targets only: modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-10T05:15:56.931Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-12-10T05:15:56.931Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=5→target=5"}
{"level":"INFO","ts":"2025-12-10T05:15:56.931Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 5, desired-replicas: 5, accelerator: H100"}
{"level":"INFO","ts":"2025-12-10T05:15:56.931Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=5, accelerator=H100, SaturationOnly=false"}
{"level":"INFO","ts":"2025-12-10T05:15:56.936Z","msg":"Applied Saturation decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=5, target=5, reason=model-based only (Saturation unavailable)"}
{"level":"INFO","ts":"2025-12-10T05:15:56.936Z","msg":"Reconciliation completed successfully: mode=model-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-12-10T05:16:56.937Z","msg":"Reconciling VariantAutoscaling: name=ms-inference-scheduling-llm-d-modelservice-decode, namespace=llm-d-inference-scheduler, modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-10T05:16:56.938Z","msg":"saturation scaling config cache updated: entries=1, has_default=true"}
{"level":"INFO","ts":"2025-12-10T05:16:56.938Z","msg":"saturation scaling configuration loaded successfully"}
{"level":"INFO","ts":"2025-12-10T05:16:56.938Z","msg":"Operating in MODEL-ONLY mode: model-based optimization only"}
{"level":"INFO","ts":"2025-12-10T05:16:56.938Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-12-10T05:16:56.938Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-12-10T05:16:56.945Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dgftqv, queueLength=0"}
{"level":"INFO","ts":"2025-12-10T05:16:56.945Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984d72f8b, queueLength=0"}
{"level":"INFO","ts":"2025-12-10T05:16:56.945Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dd7xc5, queueLength=0"}
{"level":"INFO","ts":"2025-12-10T05:16:56.945Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dkj52n, queueLength=0"}
{"level":"INFO","ts":"2025-12-10T05:16:56.945Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dczcvb, queueLength=0"}
{"level":"DEBUG","ts":"2025-12-10T05:16:56.945Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=5"}
{"level":"INFO","ts":"2025-12-10T05:16:56.945Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dgftqv, usage=0.086 (8.6%)"}
{"level":"INFO","ts":"2025-12-10T05:16:56.945Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984d72f8b, usage=0.092 (9.2%)"}
{"level":"INFO","ts":"2025-12-10T05:16:56.945Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dd7xc5, usage=0.085 (8.5%)"}
{"level":"INFO","ts":"2025-12-10T05:16:56.945Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dkj52n, usage=0.085 (8.5%)"}
{"level":"INFO","ts":"2025-12-10T05:16:56.945Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dczcvb, usage=0.091 (9.1%)"}
{"level":"DEBUG","ts":"2025-12-10T05:16:56.945Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=5"}
{"level":"DEBUG","ts":"2025-12-10T05:16:56.948Z","msg":"Pod-to-variant matching successful: totalPods=5, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:5]"}
{"level":"DEBUG","ts":"2025-12-10T05:16:56.948Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=5"}
{"level":"DEBUG","ts":"2025-12-10T05:16:56.948Z","msg":"Collected capacity metrics for observability in model-only mode: modelID=unsloth/Meta-Llama-3.1-8B, replicaCount=5"}
{"level":"DEBUG","ts":"2025-12-10T05:16:56.962Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=5, accelerator=H100, ttft=20.73ms, itl=10.26ms, cost=500.00, maxBatch=256, arrivalRate=3800.40, avgInputTokens=231.13, avgOutputTokens=460.23"}
{"level":"DEBUG","ts":"2025-12-10T05:16:56.962Z","msg":"Experimental model tuner is enabled globally: tuning model performance parameters for active VAs"}
{"level":"DEBUG","ts":"2025-12-10T05:16:56.962Z","msg":"Using state vals from VA status to tune variant ms-inference-scheduling-llm-d-modelservice-decode: alpha= 7.358946, beta= 0.046773, gamma= 16.601824, delta= 0.000259"}
{"level":"DEBUG","ts":"2025-12-10T05:16:56.962Z","msg":"[Tuner Config] variant=ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler | Initial state: alpha=7.358946, beta=0.046773, gamma=16.601824, delta=0.000259 | Expected obs (for R): TTFT=20.73ms, ITL=10.26ms | Has covariance=true"}
{"level":"INFO","ts":"2025-12-10T05:16:56.963Z","msg":"Tuner validation succeeded for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler, server ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler - New NIS: 0.215757"}
{"level":"DEBUG","ts":"2025-12-10T05:16:56.963Z","msg":"Model tuner results: model=unsloth/Meta-Llama-3.1-8B, accelerator=H100, alpha=7.385785, beta=0.047145, gamma=16.967550, delta=0.000259, NIS=0.22"}
{"level":"DEBUG","ts":"2025-12-10T05:16:56.963Z","msg":"Updated tuner status for variant ms-inference-scheduling-llm-d-modelservice-decode: alpha=7.385785, beta=0.047145, gamma=16.967550, delta=0.000259, NIS=0.215757"}
{"level":"INFO","ts":"2025-12-10T05:16:56.963Z","msg":"Tuned performance parameters result: variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler - alpha: 7.385785, beta: 0.047145, gamma: 16.967550, delta: 0.000259"}
{"level":"DEBUG","ts":"2025-12-10T05:16:56.972Z","msg":"Optimization solution - system: Solution: \ns=ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler; c=Premium; m=unsloth/Meta-Llama-3.1-8B; rate=3800.4; inTk=231; outTk=460; sol=1, sat=false, alloc={acc=H100; numRep=6; maxBatch=512; cost=600, val=100, itl=9.64648, ttft=19.838034, rho=0.09170222, maxRPM=708.6313}; slo-itl=10, slo-ttft=1000, slo-tps=0 \nAllocationByType: \nname=NVIDIA-H100-80GB-HBM3, count=6, limit=0, cost=600 \ntotalCost=600 \n"}
{"level":"DEBUG","ts":"2025-12-10T05:16:56.972Z","msg":"Setting accelerator name: Name=H100, allocationData={H100 6 512 600 9.64648 19.838034 {3800.4 231 460}}"}
{"level":"INFO","ts":"2025-12-10T05:16:56.972Z","msg":"Model-based optimization completed for model: unsloth/Meta-Llama-3.1-8B - model-based targets: map[ms-inference-scheduling-llm-d-modelservice-decode:6]"}
{"level":"WARN","ts":"2025-12-10T05:16:56.972Z","msg":"saturation analysis unavailable, using model-based targets only: modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-10T05:16:56.972Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-12-10T05:16:56.972Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=scale-up, current=5→target=6"}
{"level":"INFO","ts":"2025-12-10T05:16:56.972Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 5, desired-replicas: 6, accelerator: H100"}
{"level":"INFO","ts":"2025-12-10T05:16:56.972Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=6, accelerator=H100, SaturationOnly=false"}
{"level":"INFO","ts":"2025-12-10T05:16:56.978Z","msg":"Applied Saturation decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=scale-up, current=5, target=6, reason=model-based only (Saturation unavailable)"}
{"level":"INFO","ts":"2025-12-10T05:16:56.978Z","msg":"Reconciliation completed successfully: mode=model-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-12-10T05:17:56.978Z","msg":"Reconciling VariantAutoscaling: name=ms-inference-scheduling-llm-d-modelservice-decode, namespace=llm-d-inference-scheduler, modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-10T05:17:56.978Z","msg":"saturation scaling config cache updated: entries=1, has_default=true"}
{"level":"INFO","ts":"2025-12-10T05:17:56.978Z","msg":"saturation scaling configuration loaded successfully"}
{"level":"INFO","ts":"2025-12-10T05:17:56.978Z","msg":"Operating in MODEL-ONLY mode: model-based optimization only"}
{"level":"INFO","ts":"2025-12-10T05:17:56.978Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-12-10T05:17:56.978Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-12-10T05:17:56.987Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dgftqv, queueLength=0"}
{"level":"INFO","ts":"2025-12-10T05:17:56.987Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984d72f8b, queueLength=0"}
{"level":"INFO","ts":"2025-12-10T05:17:56.987Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dd7xc5, queueLength=0"}
{"level":"INFO","ts":"2025-12-10T05:17:56.987Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dkj52n, queueLength=0"}
{"level":"INFO","ts":"2025-12-10T05:17:56.987Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dczcvb, queueLength=0"}
{"level":"DEBUG","ts":"2025-12-10T05:17:56.987Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=5"}
{"level":"INFO","ts":"2025-12-10T05:17:56.987Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dgftqv, usage=0.093 (9.3%)"}
{"level":"INFO","ts":"2025-12-10T05:17:56.987Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984d72f8b, usage=0.099 (9.9%)"}
{"level":"INFO","ts":"2025-12-10T05:17:56.987Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dd7xc5, usage=0.089 (8.9%)"}
{"level":"INFO","ts":"2025-12-10T05:17:56.987Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dkj52n, usage=0.096 (9.6%)"}
{"level":"INFO","ts":"2025-12-10T05:17:56.987Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dczcvb, usage=0.108 (10.8%)"}
{"level":"DEBUG","ts":"2025-12-10T05:17:56.987Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=5"}
{"level":"DEBUG","ts":"2025-12-10T05:17:56.990Z","msg":"Pod-to-variant matching successful: totalPods=5, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:5]"}
{"level":"DEBUG","ts":"2025-12-10T05:17:56.990Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=5"}
{"level":"DEBUG","ts":"2025-12-10T05:17:56.990Z","msg":"Collected capacity metrics for observability in model-only mode: modelID=unsloth/Meta-Llama-3.1-8B, replicaCount=5"}
{"level":"DEBUG","ts":"2025-12-10T05:17:57.005Z","msg":"Replica count mismatch for llm-d-inference-scheduler/unsloth/Meta-Llama-3.1-8B: spec=6, reporting_metrics=5"}
{"level":"DEBUG","ts":"2025-12-10T05:17:57.005Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=5, accelerator=H100, ttft=20.55ms, itl=10.23ms, cost=500.00, maxBatch=256, arrivalRate=3868.15, avgInputTokens=231.51, avgOutputTokens=451.96"}
{"level":"DEBUG","ts":"2025-12-10T05:17:57.005Z","msg":"Experimental model tuner is enabled globally: tuning model performance parameters for active VAs"}
{"level":"DEBUG","ts":"2025-12-10T05:17:57.005Z","msg":"Using state vals from VA status to tune variant ms-inference-scheduling-llm-d-modelservice-decode: alpha= 7.385785, beta= 0.047145, gamma= 16.967550, delta= 0.000259"}
{"level":"DEBUG","ts":"2025-12-10T05:17:57.005Z","msg":"[Tuner Config] variant=ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler | Initial state: alpha=7.385785, beta=0.047145, gamma=16.967550, delta=0.000259 | Expected obs (for R): TTFT=20.55ms, ITL=10.23ms | Has covariance=true"}
{"level":"INFO","ts":"2025-12-10T05:17:57.005Z","msg":"Tuner validation succeeded for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler, server ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler - New NIS: 0.002600"}
{"level":"DEBUG","ts":"2025-12-10T05:17:57.005Z","msg":"Model tuner results: model=unsloth/Meta-Llama-3.1-8B, accelerator=H100, alpha=7.375920, beta=0.047109, gamma=16.932793, delta=0.000259, NIS=0.00"}
{"level":"DEBUG","ts":"2025-12-10T05:17:57.005Z","msg":"Updated tuner status for variant ms-inference-scheduling-llm-d-modelservice-decode: alpha=7.375920, beta=0.047109, gamma=16.932793, delta=0.000259, NIS=0.002600"}
{"level":"INFO","ts":"2025-12-10T05:17:57.005Z","msg":"Tuned performance parameters result: variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler - alpha: 7.375920, beta: 0.047109, gamma: 16.932793, delta: 0.000259"}
{"level":"DEBUG","ts":"2025-12-10T05:17:57.014Z","msg":"Optimization solution - system: Solution: \ns=ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler; c=Premium; m=unsloth/Meta-Llama-3.1-8B; rate=3868.15; inTk=231; outTk=451; sol=1, sat=false, alloc={acc=H100; numRep=6; maxBatch=512; cost=600, val=100, itl=9.625551, ttft=19.789719, rho=0.091316365, maxRPM=726.09784}; slo-itl=10, slo-ttft=1000, slo-tps=0 \nAllocationByType: \nname=NVIDIA-H100-80GB-HBM3, count=6, limit=0, cost=600 \ntotalCost=600 \n"}
{"level":"DEBUG","ts":"2025-12-10T05:17:57.014Z","msg":"Setting accelerator name: Name=H100, allocationData={H100 6 512 600 9.625551 19.789719 {3868.15 231 451}}"}
{"level":"INFO","ts":"2025-12-10T05:17:57.014Z","msg":"Model-based optimization completed for model: unsloth/Meta-Llama-3.1-8B - model-based targets: map[ms-inference-scheduling-llm-d-modelservice-decode:6]"}
{"level":"WARN","ts":"2025-12-10T05:17:57.014Z","msg":"saturation analysis unavailable, using model-based targets only: modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-10T05:17:57.014Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-12-10T05:17:57.014Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=scale-up, current=5→target=6"}
{"level":"INFO","ts":"2025-12-10T05:17:57.014Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 5, desired-replicas: 6, accelerator: H100"}
{"level":"INFO","ts":"2025-12-10T05:17:57.014Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=6, accelerator=H100, SaturationOnly=false"}
{"level":"INFO","ts":"2025-12-10T05:17:57.020Z","msg":"Applied Saturation decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=scale-up, current=5, target=6, reason=model-based only (Saturation unavailable)"}
{"level":"INFO","ts":"2025-12-10T05:17:57.020Z","msg":"Reconciliation completed successfully: mode=model-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-12-10T05:18:57.020Z","msg":"Reconciling VariantAutoscaling: name=ms-inference-scheduling-llm-d-modelservice-decode, namespace=llm-d-inference-scheduler, modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-10T05:18:57.020Z","msg":"saturation scaling config cache updated: entries=1, has_default=true"}
{"level":"INFO","ts":"2025-12-10T05:18:57.020Z","msg":"saturation scaling configuration loaded successfully"}
{"level":"INFO","ts":"2025-12-10T05:18:57.020Z","msg":"Operating in MODEL-ONLY mode: model-based optimization only"}
{"level":"INFO","ts":"2025-12-10T05:18:57.020Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-12-10T05:18:57.020Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-12-10T05:18:57.025Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dgftqv, usage=0.063 (6.3%)"}
{"level":"INFO","ts":"2025-12-10T05:18:57.025Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984d72f8b, usage=0.067 (6.7%)"}
{"level":"INFO","ts":"2025-12-10T05:18:57.025Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dd7xc5, usage=0.059 (5.9%)"}
{"level":"INFO","ts":"2025-12-10T05:18:57.025Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dkj52n, usage=0.059 (5.9%)"}
{"level":"INFO","ts":"2025-12-10T05:18:57.025Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dczcvb, usage=0.104 (10.4%)"}
{"level":"DEBUG","ts":"2025-12-10T05:18:57.025Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=5"}
{"level":"INFO","ts":"2025-12-10T05:18:57.025Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dgftqv, queueLength=0"}
{"level":"INFO","ts":"2025-12-10T05:18:57.025Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984d72f8b, queueLength=0"}
{"level":"INFO","ts":"2025-12-10T05:18:57.025Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dd7xc5, queueLength=0"}
{"level":"INFO","ts":"2025-12-10T05:18:57.025Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dkj52n, queueLength=0"}
{"level":"INFO","ts":"2025-12-10T05:18:57.025Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dczcvb, queueLength=0"}
{"level":"DEBUG","ts":"2025-12-10T05:18:57.025Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=5"}
{"level":"DEBUG","ts":"2025-12-10T05:18:57.029Z","msg":"Pod-to-variant matching successful: totalPods=5, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:5]"}
{"level":"DEBUG","ts":"2025-12-10T05:18:57.029Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=5"}
{"level":"DEBUG","ts":"2025-12-10T05:18:57.029Z","msg":"Collected capacity metrics for observability in model-only mode: modelID=unsloth/Meta-Llama-3.1-8B, replicaCount=5"}
{"level":"DEBUG","ts":"2025-12-10T05:18:57.043Z","msg":"Replica count mismatch for llm-d-inference-scheduler/unsloth/Meta-Llama-3.1-8B: spec=6, reporting_metrics=5"}
{"level":"DEBUG","ts":"2025-12-10T05:18:57.043Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=5, accelerator=H100, ttft=18.78ms, itl=9.13ms, cost=500.00, maxBatch=256, arrivalRate=2823.43, avgInputTokens=232.60, avgOutputTokens=479.25"}
{"level":"DEBUG","ts":"2025-12-10T05:18:57.043Z","msg":"Experimental model tuner is enabled globally: tuning model performance parameters for active VAs"}
{"level":"DEBUG","ts":"2025-12-10T05:18:57.043Z","msg":"Using state vals from VA status to tune variant ms-inference-scheduling-llm-d-modelservice-decode: alpha= 7.375920, beta= 0.047109, gamma= 16.932793, delta= 0.000259"}
{"level":"DEBUG","ts":"2025-12-10T05:18:57.043Z","msg":"[Tuner Config] variant=ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler | Initial state: alpha=7.375920, beta=0.047109, gamma=16.932793, delta=0.000259 | Expected obs (for R): TTFT=18.78ms, ITL=9.13ms | Has covariance=true"}
{"level":"INFO","ts":"2025-12-10T05:18:57.044Z","msg":"Tuner validation succeeded for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler, server ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler - New NIS: 0.677998"}
{"level":"DEBUG","ts":"2025-12-10T05:18:57.044Z","msg":"Model tuner results: model=unsloth/Meta-Llama-3.1-8B, accelerator=H100, alpha=7.119803, beta=0.048240, gamma=16.368946, delta=0.000259, NIS=0.68"}
{"level":"DEBUG","ts":"2025-12-10T05:18:57.044Z","msg":"Updated tuner status for variant ms-inference-scheduling-llm-d-modelservice-decode: alpha=7.119803, beta=0.048240, gamma=16.368946, delta=0.000259, NIS=0.677998"}
{"level":"INFO","ts":"2025-12-10T05:18:57.044Z","msg":"Tuned performance parameters result: variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler - alpha: 7.119803, beta: 0.048240, gamma: 16.368946, delta: 0.000259"}
{"level":"DEBUG","ts":"2025-12-10T05:18:57.049Z","msg":"Optimization solution - system: Solution: \ns=ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler; c=Premium; m=unsloth/Meta-Llama-3.1-8B; rate=2823.43; inTk=232; outTk=479; sol=1, sat=false, alloc={acc=H100; numRep=4; maxBatch=512; cost=400, val=-100, itl=9.851767, ttft=19.774302, rho=0.108656995, maxRPM=733.8222}; slo-itl=10, slo-ttft=1000, slo-tps=0 \nAllocationByType: \nname=NVIDIA-H100-80GB-HBM3, count=4, limit=0, cost=400 \ntotalCost=400 \n"}
{"level":"DEBUG","ts":"2025-12-10T05:18:57.049Z","msg":"Setting accelerator name: Name=H100, allocationData={H100 4 512 400 9.851767 19.774302 {2823.43 232 479}}"}
{"level":"INFO","ts":"2025-12-10T05:18:57.049Z","msg":"Model-based optimization completed for model: unsloth/Meta-Llama-3.1-8B - model-based targets: map[ms-inference-scheduling-llm-d-modelservice-decode:4]"}
{"level":"WARN","ts":"2025-12-10T05:18:57.049Z","msg":"saturation analysis unavailable, using model-based targets only: modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-10T05:18:57.049Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-12-10T05:18:57.049Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=scale-down, current=5→target=4"}
{"level":"INFO","ts":"2025-12-10T05:18:57.049Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 5, desired-replicas: 4, accelerator: H100"}
{"level":"INFO","ts":"2025-12-10T05:18:57.049Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=4, accelerator=H100, SaturationOnly=false"}
{"level":"INFO","ts":"2025-12-10T05:18:57.056Z","msg":"Applied Saturation decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=scale-down, current=5, target=4, reason=model-based only (Saturation unavailable)"}
{"level":"INFO","ts":"2025-12-10T05:18:57.056Z","msg":"Reconciliation completed successfully: mode=model-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-12-10T05:19:57.057Z","msg":"Reconciling VariantAutoscaling: name=ms-inference-scheduling-llm-d-modelservice-decode, namespace=llm-d-inference-scheduler, modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-10T05:19:57.057Z","msg":"saturation scaling config cache updated: entries=1, has_default=true"}
{"level":"INFO","ts":"2025-12-10T05:19:57.057Z","msg":"saturation scaling configuration loaded successfully"}
{"level":"INFO","ts":"2025-12-10T05:19:57.057Z","msg":"Operating in MODEL-ONLY mode: model-based optimization only"}
{"level":"INFO","ts":"2025-12-10T05:19:57.057Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-12-10T05:19:57.057Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-12-10T05:19:57.065Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dgftqv, queueLength=0"}
{"level":"INFO","ts":"2025-12-10T05:19:57.065Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984d72f8b, queueLength=0"}
{"level":"INFO","ts":"2025-12-10T05:19:57.065Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dkj52n, queueLength=0"}
{"level":"INFO","ts":"2025-12-10T05:19:57.065Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dczcvb, queueLength=0"}
{"level":"DEBUG","ts":"2025-12-10T05:19:57.065Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=4"}
{"level":"INFO","ts":"2025-12-10T05:19:57.065Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dgftqv, usage=0.064 (6.4%)"}
{"level":"INFO","ts":"2025-12-10T05:19:57.065Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984d72f8b, usage=0.053 (5.3%)"}
{"level":"INFO","ts":"2025-12-10T05:19:57.065Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dkj52n, usage=0.069 (6.9%)"}
{"level":"INFO","ts":"2025-12-10T05:19:57.065Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dczcvb, usage=0.084 (8.4%)"}
{"level":"DEBUG","ts":"2025-12-10T05:19:57.065Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=4"}
{"level":"DEBUG","ts":"2025-12-10T05:19:57.068Z","msg":"Pod-to-variant matching successful: totalPods=4, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:4]"}
{"level":"DEBUG","ts":"2025-12-10T05:19:57.068Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=4"}
{"level":"DEBUG","ts":"2025-12-10T05:19:57.068Z","msg":"Collected capacity metrics for observability in model-only mode: modelID=unsloth/Meta-Llama-3.1-8B, replicaCount=4"}
{"level":"DEBUG","ts":"2025-12-10T05:19:57.082Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=4, accelerator=H100, ttft=19.52ms, itl=9.63ms, cost=400.00, maxBatch=256, arrivalRate=2623.94, avgInputTokens=231.86, avgOutputTokens=444.82"}
{"level":"DEBUG","ts":"2025-12-10T05:19:57.082Z","msg":"Experimental model tuner is enabled globally: tuning model performance parameters for active VAs"}
{"level":"DEBUG","ts":"2025-12-10T05:19:57.082Z","msg":"Using state vals from VA status to tune variant ms-inference-scheduling-llm-d-modelservice-decode: alpha= 7.119803, beta= 0.048240, gamma= 16.368946, delta= 0.000259"}
{"level":"DEBUG","ts":"2025-12-10T05:19:57.082Z","msg":"[Tuner Config] variant=ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler | Initial state: alpha=7.119803, beta=0.048240, gamma=16.368946, delta=0.000259 | Expected obs (for R): TTFT=19.52ms, ITL=9.63ms | Has covariance=true"}
{"level":"INFO","ts":"2025-12-10T05:19:57.082Z","msg":"Tuner validation succeeded for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler, server ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler - New NIS: 0.287088"}
{"level":"DEBUG","ts":"2025-12-10T05:19:57.082Z","msg":"Model tuner results: model=unsloth/Meta-Llama-3.1-8B, accelerator=H100, alpha=7.271648, beta=0.048614, gamma=16.600313, delta=0.000259, NIS=0.29"}
{"level":"DEBUG","ts":"2025-12-10T05:19:57.082Z","msg":"Updated tuner status for variant ms-inference-scheduling-llm-d-modelservice-decode: alpha=7.271648, beta=0.048614, gamma=16.600313, delta=0.000259, NIS=0.287088"}
{"level":"INFO","ts":"2025-12-10T05:19:57.082Z","msg":"Tuned performance parameters result: variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler - alpha: 7.271648, beta: 0.048614, gamma: 16.600313, delta: 0.000259"}
{"level":"DEBUG","ts":"2025-12-10T05:19:57.087Z","msg":"Optimization solution - system: Solution: \ns=ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler; c=Premium; m=unsloth/Meta-Llama-3.1-8B; rate=2623.94; inTk=231; outTk=444; sol=1, sat=false, alloc={acc=H100; numRep=4; maxBatch=512; cost=400, val=0, itl=9.588155, ttft=19.45065, rho=0.09111622, maxRPM=743.2443}; slo-itl=10, slo-ttft=1000, slo-tps=0 \nAllocationByType: \nname=NVIDIA-H100-80GB-HBM3, count=4, limit=0, cost=400 \ntotalCost=400 \n"}
{"level":"DEBUG","ts":"2025-12-10T05:19:57.087Z","msg":"Setting accelerator name: Name=H100, allocationData={H100 4 512 400 9.588155 19.45065 {2623.94 231 444}}"}
{"level":"INFO","ts":"2025-12-10T05:19:57.087Z","msg":"Model-based optimization completed for model: unsloth/Meta-Llama-3.1-8B - model-based targets: map[ms-inference-scheduling-llm-d-modelservice-decode:4]"}
{"level":"WARN","ts":"2025-12-10T05:19:57.087Z","msg":"saturation analysis unavailable, using model-based targets only: modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-10T05:19:57.087Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-12-10T05:19:57.087Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=4→target=4"}
{"level":"INFO","ts":"2025-12-10T05:19:57.087Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 4, desired-replicas: 4, accelerator: H100"}
{"level":"INFO","ts":"2025-12-10T05:19:57.087Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=4, accelerator=H100, SaturationOnly=false"}
{"level":"INFO","ts":"2025-12-10T05:19:57.096Z","msg":"Applied Saturation decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=4, target=4, reason=model-based only (Saturation unavailable)"}
{"level":"INFO","ts":"2025-12-10T05:19:57.096Z","msg":"Reconciliation completed successfully: mode=model-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-12-10T05:20:57.096Z","msg":"Reconciling VariantAutoscaling: name=ms-inference-scheduling-llm-d-modelservice-decode, namespace=llm-d-inference-scheduler, modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-10T05:20:57.097Z","msg":"saturation scaling config cache updated: entries=1, has_default=true"}
{"level":"INFO","ts":"2025-12-10T05:20:57.097Z","msg":"saturation scaling configuration loaded successfully"}
{"level":"INFO","ts":"2025-12-10T05:20:57.097Z","msg":"Operating in MODEL-ONLY mode: model-based optimization only"}
{"level":"INFO","ts":"2025-12-10T05:20:57.097Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-12-10T05:20:57.097Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-12-10T05:20:57.105Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dgftqv, usage=0.080 (8.0%)"}
{"level":"INFO","ts":"2025-12-10T05:20:57.105Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984d72f8b, usage=0.080 (8.0%)"}
{"level":"INFO","ts":"2025-12-10T05:20:57.105Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dkj52n, usage=0.059 (5.9%)"}
{"level":"INFO","ts":"2025-12-10T05:20:57.105Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dczcvb, usage=0.089 (8.9%)"}
{"level":"DEBUG","ts":"2025-12-10T05:20:57.105Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=4"}
{"level":"INFO","ts":"2025-12-10T05:20:57.105Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dgftqv, queueLength=0"}
{"level":"INFO","ts":"2025-12-10T05:20:57.105Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984d72f8b, queueLength=0"}
{"level":"INFO","ts":"2025-12-10T05:20:57.105Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dkj52n, queueLength=0"}
{"level":"INFO","ts":"2025-12-10T05:20:57.105Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dczcvb, queueLength=0"}
{"level":"DEBUG","ts":"2025-12-10T05:20:57.105Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=4"}
{"level":"DEBUG","ts":"2025-12-10T05:20:57.108Z","msg":"Pod-to-variant matching successful: totalPods=4, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:4]"}
{"level":"DEBUG","ts":"2025-12-10T05:20:57.108Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=4"}
{"level":"DEBUG","ts":"2025-12-10T05:20:57.108Z","msg":"Collected capacity metrics for observability in model-only mode: modelID=unsloth/Meta-Llama-3.1-8B, replicaCount=4"}
{"level":"DEBUG","ts":"2025-12-10T05:20:57.122Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=4, accelerator=H100, ttft=19.09ms, itl=9.12ms, cost=400.00, maxBatch=256, arrivalRate=1924.61, avgInputTokens=218.61, avgOutputTokens=516.36"}
{"level":"DEBUG","ts":"2025-12-10T05:20:57.122Z","msg":"Experimental model tuner is enabled globally: tuning model performance parameters for active VAs"}
{"level":"DEBUG","ts":"2025-12-10T05:20:57.122Z","msg":"Using state vals from VA status to tune variant ms-inference-scheduling-llm-d-modelservice-decode: alpha= 7.271648, beta= 0.048614, gamma= 16.600313, delta= 0.000259"}
{"level":"DEBUG","ts":"2025-12-10T05:20:57.122Z","msg":"[Tuner Config] variant=ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler | Initial state: alpha=7.271648, beta=0.048614, gamma=16.600313, delta=0.000259 | Expected obs (for R): TTFT=19.09ms, ITL=9.12ms | Has covariance=true"}
{"level":"INFO","ts":"2025-12-10T05:20:57.122Z","msg":"Tuner validation succeeded for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler, server ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler - New NIS: 0.084583"}
{"level":"DEBUG","ts":"2025-12-10T05:20:57.122Z","msg":"Model tuner results: model=unsloth/Meta-Llama-3.1-8B, accelerator=H100, alpha=7.239023, beta=0.048669, gamma=16.834854, delta=0.000259, NIS=0.08"}
{"level":"DEBUG","ts":"2025-12-10T05:20:57.122Z","msg":"Updated tuner status for variant ms-inference-scheduling-llm-d-modelservice-decode: alpha=7.239023, beta=0.048669, gamma=16.834854, delta=0.000259, NIS=0.084583"}
{"level":"INFO","ts":"2025-12-10T05:20:57.122Z","msg":"Tuned performance parameters result: variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler - alpha: 7.239023, beta: 0.048669, gamma: 16.834854, delta: 0.000259"}
{"level":"DEBUG","ts":"2025-12-10T05:20:57.131Z","msg":"Optimization solution - system: Solution: \ns=ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler; c=Premium; m=unsloth/Meta-Llama-3.1-8B; rate=1924.61; inTk=218; outTk=516; sol=1, sat=false, alloc={acc=H100; numRep=3; maxBatch=512; cost=300, val=-100, itl=9.970063, ttft=20.001331, rho=0.107645, maxRPM=646.758}; slo-itl=10, slo-ttft=1000, slo-tps=0 \nAllocationByType: \nname=NVIDIA-H100-80GB-HBM3, count=3, limit=0, cost=300 \ntotalCost=300 \n"}
{"level":"DEBUG","ts":"2025-12-10T05:20:57.131Z","msg":"Setting accelerator name: Name=H100, allocationData={H100 3 512 300 9.970063 20.001331 {1924.61 218 516}}"}
{"level":"INFO","ts":"2025-12-10T05:20:57.131Z","msg":"Model-based optimization completed for model: unsloth/Meta-Llama-3.1-8B - model-based targets: map[ms-inference-scheduling-llm-d-modelservice-decode:3]"}
{"level":"WARN","ts":"2025-12-10T05:20:57.131Z","msg":"saturation analysis unavailable, using model-based targets only: modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-10T05:20:57.131Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-12-10T05:20:57.131Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=scale-down, current=4→target=3"}
{"level":"INFO","ts":"2025-12-10T05:20:57.131Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 4, desired-replicas: 3, accelerator: H100"}
{"level":"INFO","ts":"2025-12-10T05:20:57.131Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=3, accelerator=H100, SaturationOnly=false"}
{"level":"INFO","ts":"2025-12-10T05:20:57.143Z","msg":"Applied Saturation decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=scale-down, current=4, target=3, reason=model-based only (Saturation unavailable)"}
{"level":"INFO","ts":"2025-12-10T05:20:57.143Z","msg":"Reconciliation completed successfully: mode=model-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-12-10T05:21:57.144Z","msg":"Reconciling VariantAutoscaling: name=ms-inference-scheduling-llm-d-modelservice-decode, namespace=llm-d-inference-scheduler, modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-10T05:21:57.144Z","msg":"saturation scaling config cache updated: entries=1, has_default=true"}
{"level":"INFO","ts":"2025-12-10T05:21:57.144Z","msg":"saturation scaling configuration loaded successfully"}
{"level":"INFO","ts":"2025-12-10T05:21:57.144Z","msg":"Operating in MODEL-ONLY mode: model-based optimization only"}
{"level":"INFO","ts":"2025-12-10T05:21:57.144Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-12-10T05:21:57.144Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-12-10T05:21:57.153Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dgftqv, queueLength=0"}
{"level":"INFO","ts":"2025-12-10T05:21:57.153Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984d72f8b, queueLength=0"}
{"level":"INFO","ts":"2025-12-10T05:21:57.153Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dczcvb, queueLength=0"}
{"level":"DEBUG","ts":"2025-12-10T05:21:57.153Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=3"}
{"level":"INFO","ts":"2025-12-10T05:21:57.153Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dgftqv, usage=0.027 (2.7%)"}
{"level":"INFO","ts":"2025-12-10T05:21:57.153Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984d72f8b, usage=0.020 (2.0%)"}
{"level":"INFO","ts":"2025-12-10T05:21:57.153Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dczcvb, usage=0.031 (3.1%)"}
{"level":"DEBUG","ts":"2025-12-10T05:21:57.153Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=3"}
{"level":"DEBUG","ts":"2025-12-10T05:21:57.156Z","msg":"Pod-to-variant matching successful: totalPods=3, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:3]"}
{"level":"DEBUG","ts":"2025-12-10T05:21:57.157Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=3"}
{"level":"DEBUG","ts":"2025-12-10T05:21:57.157Z","msg":"Collected capacity metrics for observability in model-only mode: modelID=unsloth/Meta-Llama-3.1-8B, replicaCount=3"}
{"level":"DEBUG","ts":"2025-12-10T05:21:57.170Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=3, accelerator=H100, ttft=16.31ms, itl=7.74ms, cost=300.00, maxBatch=256, arrivalRate=835.72, avgInputTokens=248.97, avgOutputTokens=426.54"}
{"level":"DEBUG","ts":"2025-12-10T05:21:57.170Z","msg":"Experimental model tuner is enabled globally: tuning model performance parameters for active VAs"}
{"level":"DEBUG","ts":"2025-12-10T05:21:57.170Z","msg":"Using state vals from VA status to tune variant ms-inference-scheduling-llm-d-modelservice-decode: alpha= 7.239023, beta= 0.048669, gamma= 16.834854, delta= 0.000259"}
{"level":"DEBUG","ts":"2025-12-10T05:21:57.170Z","msg":"[Tuner Config] variant=ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler | Initial state: alpha=7.239023, beta=0.048669, gamma=16.834854, delta=0.000259 | Expected obs (for R): TTFT=16.31ms, ITL=7.74ms | Has covariance=true"}
{"level":"INFO","ts":"2025-12-10T05:21:57.171Z","msg":"Tuner validation succeeded for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler, server ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler - New NIS: 2.722414"}
{"level":"DEBUG","ts":"2025-12-10T05:21:57.171Z","msg":"Model tuner results: model=unsloth/Meta-Llama-3.1-8B, accelerator=H100, alpha=6.936156, beta=0.051197, gamma=15.503791, delta=0.000261, NIS=2.72"}
{"level":"DEBUG","ts":"2025-12-10T05:21:57.171Z","msg":"Updated tuner status for variant ms-inference-scheduling-llm-d-modelservice-decode: alpha=6.936156, beta=0.051197, gamma=15.503791, delta=0.000261, NIS=2.722414"}
{"level":"INFO","ts":"2025-12-10T05:21:57.171Z","msg":"Tuned performance parameters result: variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler - alpha: 6.936156, beta: 0.051197, gamma: 15.503791, delta: 0.000261"}
{"level":"DEBUG","ts":"2025-12-10T05:21:57.175Z","msg":"Optimization solution - system: Solution: \ns=ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler; c=Premium; m=unsloth/Meta-Llama-3.1-8B; rate=835.72; inTk=248; outTk=426; sol=1, sat=false, alloc={acc=H100; numRep=2; maxBatch=512; cost=200, val=-100, itl=8.242496, ttft=17.15488, rho=0.04788272, maxRPM=826.9721}; slo-itl=10, slo-ttft=1000, slo-tps=0 \nAllocationByType: \nname=NVIDIA-H100-80GB-HBM3, count=2, limit=0, cost=200 \ntotalCost=200 \n"}
{"level":"DEBUG","ts":"2025-12-10T05:21:57.175Z","msg":"Setting accelerator name: Name=H100, allocationData={H100 2 512 200 8.242496 17.15488 {835.72 248 426}}"}
{"level":"INFO","ts":"2025-12-10T05:21:57.175Z","msg":"Model-based optimization completed for model: unsloth/Meta-Llama-3.1-8B - model-based targets: map[ms-inference-scheduling-llm-d-modelservice-decode:2]"}
{"level":"WARN","ts":"2025-12-10T05:21:57.175Z","msg":"saturation analysis unavailable, using model-based targets only: modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-10T05:21:57.175Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-12-10T05:21:57.175Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=scale-down, current=3→target=2"}
{"level":"INFO","ts":"2025-12-10T05:21:57.175Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 3, desired-replicas: 2, accelerator: H100"}
{"level":"INFO","ts":"2025-12-10T05:21:57.175Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=2, accelerator=H100, SaturationOnly=false"}
{"level":"INFO","ts":"2025-12-10T05:21:57.182Z","msg":"Applied Saturation decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=scale-down, current=3, target=2, reason=model-based only (Saturation unavailable)"}
{"level":"INFO","ts":"2025-12-10T05:21:57.182Z","msg":"Reconciliation completed successfully: mode=model-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-12-10T05:22:57.182Z","msg":"Reconciling VariantAutoscaling: name=ms-inference-scheduling-llm-d-modelservice-decode, namespace=llm-d-inference-scheduler, modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-10T05:22:57.183Z","msg":"saturation scaling config cache updated: entries=1, has_default=true"}
{"level":"INFO","ts":"2025-12-10T05:22:57.183Z","msg":"saturation scaling configuration loaded successfully"}
{"level":"INFO","ts":"2025-12-10T05:22:57.183Z","msg":"Operating in MODEL-ONLY mode: model-based optimization only"}
{"level":"INFO","ts":"2025-12-10T05:22:57.183Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-12-10T05:22:57.183Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-12-10T05:22:57.188Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dgftqv, queueLength=0"}
{"level":"INFO","ts":"2025-12-10T05:22:57.188Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984d72f8b, queueLength=0"}
{"level":"INFO","ts":"2025-12-10T05:22:57.188Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dczcvb, queueLength=0"}
{"level":"DEBUG","ts":"2025-12-10T05:22:57.188Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=3"}
{"level":"INFO","ts":"2025-12-10T05:22:57.188Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dgftqv, usage=0.043 (4.3%)"}
{"level":"INFO","ts":"2025-12-10T05:22:57.188Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984d72f8b, usage=0.019 (1.9%)"}
{"level":"INFO","ts":"2025-12-10T05:22:57.188Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dczcvb, usage=0.027 (2.7%)"}
{"level":"DEBUG","ts":"2025-12-10T05:22:57.188Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=3"}
{"level":"DEBUG","ts":"2025-12-10T05:22:57.191Z","msg":"Filtering pod from stale vLLM metrics: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984d72f8b, namespace=llm-d-inference-scheduler, model=unsloth/Meta-Llama-3.1-8B"}
{"level":"DEBUG","ts":"2025-12-10T05:22:57.191Z","msg":"Pod-to-variant matching successful: totalPods=2, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:2]"}
{"level":"DEBUG","ts":"2025-12-10T05:22:57.191Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=2"}
{"level":"DEBUG","ts":"2025-12-10T05:22:57.191Z","msg":"Collected capacity metrics for observability in model-only mode: modelID=unsloth/Meta-Llama-3.1-8B, replicaCount=2"}
{"level":"DEBUG","ts":"2025-12-10T05:22:57.205Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=2, accelerator=H100, ttft=17.88ms, itl=8.30ms, cost=200.00, maxBatch=256, arrivalRate=555.54, avgInputTokens=234.13, avgOutputTokens=509.78"}
{"level":"DEBUG","ts":"2025-12-10T05:22:57.205Z","msg":"Experimental model tuner is enabled globally: tuning model performance parameters for active VAs"}
{"level":"DEBUG","ts":"2025-12-10T05:22:57.205Z","msg":"Using state vals from VA status to tune variant ms-inference-scheduling-llm-d-modelservice-decode: alpha= 6.936156, beta= 0.051197, gamma= 15.503791, delta= 0.000261"}
{"level":"DEBUG","ts":"2025-12-10T05:22:57.205Z","msg":"[Tuner Config] variant=ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler | Initial state: alpha=6.936156, beta=0.051197, gamma=15.503791, delta=0.000261 | Expected obs (for R): TTFT=17.88ms, ITL=8.30ms | Has covariance=true"}
{"level":"INFO","ts":"2025-12-10T05:22:57.205Z","msg":"Tuner validation succeeded for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler, server ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler - New NIS: 1.864486"}
{"level":"DEBUG","ts":"2025-12-10T05:22:57.205Z","msg":"Model tuner results: model=unsloth/Meta-Llama-3.1-8B, accelerator=H100, alpha=7.188125, beta=0.051383, gamma=16.384689, delta=0.000261, NIS=1.86"}
{"level":"DEBUG","ts":"2025-12-10T05:22:57.205Z","msg":"Updated tuner status for variant ms-inference-scheduling-llm-d-modelservice-decode: alpha=7.188125, beta=0.051383, gamma=16.384689, delta=0.000261, NIS=1.864486"}
{"level":"INFO","ts":"2025-12-10T05:22:57.205Z","msg":"Tuned performance parameters result: variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler - alpha: 7.188125, beta: 0.051383, gamma: 16.384689, delta: 0.000261"}
{"level":"DEBUG","ts":"2025-12-10T05:22:57.209Z","msg":"Optimization solution - system: Solution: \ns=ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler; c=Premium; m=unsloth/Meta-Llama-3.1-8B; rate=555.54; inTk=234; outTk=509; sol=1, sat=false, alloc={acc=H100; numRep=1; maxBatch=512; cost=100, val=-100, itl=9.558842, ttft=19.19983, rho=0.08816107, maxRPM=632.0836}; slo-itl=10, slo-ttft=1000, slo-tps=0 \nAllocationByType: \nname=NVIDIA-H100-80GB-HBM3, count=1, limit=0, cost=100 \ntotalCost=100 \n"}
{"level":"DEBUG","ts":"2025-12-10T05:22:57.209Z","msg":"Setting accelerator name: Name=H100, allocationData={H100 1 512 100 9.558842 19.19983 {555.54 234 509}}"}
{"level":"INFO","ts":"2025-12-10T05:22:57.209Z","msg":"Model-based optimization completed for model: unsloth/Meta-Llama-3.1-8B - model-based targets: map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"WARN","ts":"2025-12-10T05:22:57.209Z","msg":"saturation analysis unavailable, using model-based targets only: modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-10T05:22:57.209Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-12-10T05:22:57.209Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=scale-down, current=2→target=1"}
{"level":"INFO","ts":"2025-12-10T05:22:57.209Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 2, desired-replicas: 1, accelerator: H100"}
{"level":"INFO","ts":"2025-12-10T05:22:57.209Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=1, accelerator=H100, SaturationOnly=false"}
{"level":"INFO","ts":"2025-12-10T05:22:57.220Z","msg":"Applied Saturation decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=scale-down, current=2, target=1, reason=model-based only (Saturation unavailable)"}
{"level":"INFO","ts":"2025-12-10T05:22:57.220Z","msg":"Reconciliation completed successfully: mode=model-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-12-10T05:23:57.220Z","msg":"Reconciling VariantAutoscaling: name=ms-inference-scheduling-llm-d-modelservice-decode, namespace=llm-d-inference-scheduler, modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-10T05:23:57.220Z","msg":"saturation scaling config cache updated: entries=1, has_default=true"}
{"level":"INFO","ts":"2025-12-10T05:23:57.220Z","msg":"saturation scaling configuration loaded successfully"}
{"level":"INFO","ts":"2025-12-10T05:23:57.220Z","msg":"Operating in MODEL-ONLY mode: model-based optimization only"}
{"level":"INFO","ts":"2025-12-10T05:23:57.220Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-12-10T05:23:57.220Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-12-10T05:23:57.230Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dgftqv, queueLength=0"}
{"level":"INFO","ts":"2025-12-10T05:23:57.230Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dczcvb, queueLength=0"}
{"level":"DEBUG","ts":"2025-12-10T05:23:57.230Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=2"}
{"level":"INFO","ts":"2025-12-10T05:23:57.231Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dgftqv, usage=0.000 (0.0%)"}
{"level":"INFO","ts":"2025-12-10T05:23:57.231Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dczcvb, usage=0.000 (0.0%)"}
{"level":"DEBUG","ts":"2025-12-10T05:23:57.231Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=2"}
{"level":"DEBUG","ts":"2025-12-10T05:23:57.233Z","msg":"Filtering pod from stale vLLM metrics: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dczcvb, namespace=llm-d-inference-scheduler, model=unsloth/Meta-Llama-3.1-8B"}
{"level":"DEBUG","ts":"2025-12-10T05:23:57.233Z","msg":"Pod-to-variant matching successful: totalPods=1, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"DEBUG","ts":"2025-12-10T05:23:57.233Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=1"}
{"level":"DEBUG","ts":"2025-12-10T05:23:57.233Z","msg":"Collected capacity metrics for observability in model-only mode: modelID=unsloth/Meta-Llama-3.1-8B, replicaCount=1"}
{"level":"DEBUG","ts":"2025-12-10T05:23:57.246Z","msg":"Filtered 1 stale pod(s) with metrics for llm-d-inference-scheduler/unsloth/Meta-Llama-3.1-8B"}
{"level":"DEBUG","ts":"2025-12-10T05:23:57.246Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=1, accelerator=H100, ttft=0.00ms, itl=0.00ms, cost=100.00, maxBatch=256, arrivalRate=0.00, avgInputTokens=0.00, avgOutputTokens=0.00"}
{"level":"DEBUG","ts":"2025-12-10T05:23:57.246Z","msg":"Experimental model tuner is enabled globally: tuning model performance parameters for active VAs"}
{"level":"DEBUG","ts":"2025-12-10T05:23:57.246Z","msg":"Using state vals from VA status to tune variant ms-inference-scheduling-llm-d-modelservice-decode: alpha= 7.188125, beta= 0.051383, gamma= 16.384689, delta= 0.000261"}
{"level":"DEBUG","ts":"2025-12-10T05:23:57.246Z","msg":"Skipping tuning for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler due to insufficient load. Using fallback parameters."}
{"level":"INFO","ts":"2025-12-10T05:23:57.246Z","msg":"Using tuned parameters from status for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler"}
{"level":"DEBUG","ts":"2025-12-10T05:23:57.246Z","msg":"Updated SystemData for model=unsloth/Meta-Llama-3.1-8B, accelerator=H100: alpha=7.188125, beta=0.051383, gamma=16.384689, delta=0.000261"}
{"level":"DEBUG","ts":"2025-12-10T05:23:57.246Z","msg":"Optimization solution - system: Solution: \ns=ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler; c=Premium; m=unsloth/Meta-Llama-3.1-8B; rate=0; inTk=0; outTk=0; sol=1, sat=false, alloc={acc=H100; numRep=1; maxBatch=512; cost=100, val=0, itl=7.239508, ttft=16.38495, rho=0, maxRPM=615863.6}; slo-itl=10, slo-ttft=1000, slo-tps=0 \nAllocationByType: \nname=NVIDIA-H100-80GB-HBM3, count=1, limit=0, cost=100 \ntotalCost=100 \n"}
{"level":"DEBUG","ts":"2025-12-10T05:23:57.246Z","msg":"Setting accelerator name: Name=H100, allocationData={H100 1 512 100 7.239508 16.38495 {0 0 0}}"}
{"level":"INFO","ts":"2025-12-10T05:23:57.246Z","msg":"Model-based optimization completed for model: unsloth/Meta-Llama-3.1-8B - model-based targets: map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"WARN","ts":"2025-12-10T05:23:57.246Z","msg":"saturation analysis unavailable, using model-based targets only: modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-10T05:23:57.246Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-12-10T05:23:57.246Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1→target=1"}
{"level":"INFO","ts":"2025-12-10T05:23:57.246Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 1, desired-replicas: 1, accelerator: H100"}
{"level":"INFO","ts":"2025-12-10T05:23:57.246Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=1, accelerator=H100, SaturationOnly=false"}
{"level":"INFO","ts":"2025-12-10T05:23:57.254Z","msg":"Applied Saturation decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1, target=1, reason=model-based only (Saturation unavailable)"}
{"level":"INFO","ts":"2025-12-10T05:23:57.254Z","msg":"Reconciliation completed successfully: mode=model-only, modelsProcessed=1, decisionsApplied=1"}
