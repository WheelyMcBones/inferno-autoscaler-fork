{"level":"INFO","ts":"2025-11-28T20:51:51.682Z","msg":"Operating in CAPACITY-ONLY mode: reactive capacity-based scaling only"}
{"level":"INFO","ts":"2025-11-28T20:51:51.683Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-11-28T20:51:51.683Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-11-28T20:51:51.695Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=1, accelerator=H100, ttft=0.00ms, itl=0.00ms, cost=10.00, arrivalRate=0.00"}
{"level":"INFO","ts":"2025-11-28T20:51:51.697Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9l76md, queueLength=0"}
{"level":"DEBUG","ts":"2025-11-28T20:51:51.697Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"INFO","ts":"2025-11-28T20:51:51.697Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9l76md, usage=0.000 (0.0%)"}
{"level":"DEBUG","ts":"2025-11-28T20:51:51.697Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"DEBUG","ts":"2025-11-28T20:51:51.697Z","msg":"Pod-to-variant matching successful: totalPods=1, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"DEBUG","ts":"2025-11-28T20:51:51.697Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=1"}
{"level":"DEBUG","ts":"2025-11-28T20:51:51.697Z","msg":"Collected capacity metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, metricsCount=1"}
{"level":"DEBUG","ts":"2025-11-28T20:51:51.697Z","msg":"Variant analysis initialized: variant=ms-inference-scheduling-llm-d-modelservice-decode, accelerator=H100, cost=10.00, replicaCount=1"}
{"level":"DEBUG","ts":"2025-11-28T20:51:51.697Z","msg":"Scale-down unsafe: insufficient non-saturated replicas: nonSaturated=1, required=2"}
{"level":"DEBUG","ts":"2025-11-28T20:51:51.697Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, totalReplicas=1, nonSaturated=1, avgSpareKv=0.500, avgSpareQueue=5.0, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"INFO","ts":"2025-11-28T20:51:51.697Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, totalReplicas=1, nonSaturated=1, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"DEBUG","ts":"2025-11-28T20:51:51.697Z","msg":"Capacity targets: no scaling needed (avgSpareKv=0.500, avgSpareQueue=5.0, all variants stable)"}
{"level":"DEBUG","ts":"2025-11-28T20:51:51.697Z","msg":"Capacity targets calculated: modelID=unsloth/Meta-Llama-3.1-8B, targets=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"INFO","ts":"2025-11-28T20:51:51.697Z","msg":"Capacity-only decisions made for model: unsloth/Meta-Llama-3.1-8B - decision count: 1"}
{"level":"INFO","ts":"2025-11-28T20:51:51.697Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-11-28T20:51:51.697Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1→target=1"}
{"level":"DEBUG","ts":"2025-11-28T20:51:51.697Z","msg":"Found VA in map: variant=ms-inference-scheduling-llm-d-modelservice-decode, hasCurrentAlloc=true, accelerator=H100"}
{"level":"INFO","ts":"2025-11-28T20:51:51.697Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 1, desired-replicas: 1, accelerator: H100"}
{"level":"INFO","ts":"2025-11-28T20:51:51.697Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=1, accelerator=H100, capacityOnly=true"}
{"level":"INFO","ts":"2025-11-28T20:51:51.711Z","msg":"Applied capacity decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1, target=1, reason=capacity-only mode: no-change"}
{"level":"INFO","ts":"2025-11-28T20:51:51.711Z","msg":"Reconciliation completed successfully: mode=capacity-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-11-28T20:51:51.791Z","msg":"Operating in CAPACITY-ONLY mode: reactive capacity-based scaling only"}
{"level":"INFO","ts":"2025-11-28T20:51:51.791Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-11-28T20:51:51.791Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-11-28T20:51:51.801Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=1, accelerator=H100, ttft=0.00ms, itl=0.00ms, cost=10.00, arrivalRate=0.00"}
{"level":"INFO","ts":"2025-11-28T20:51:51.803Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9l76md, usage=0.000 (0.0%)"}
{"level":"DEBUG","ts":"2025-11-28T20:51:51.803Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"INFO","ts":"2025-11-28T20:51:51.803Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9l76md, queueLength=0"}
{"level":"DEBUG","ts":"2025-11-28T20:51:51.803Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"DEBUG","ts":"2025-11-28T20:51:51.803Z","msg":"Pod-to-variant matching successful: totalPods=1, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"DEBUG","ts":"2025-11-28T20:51:51.803Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=1"}
{"level":"DEBUG","ts":"2025-11-28T20:51:51.803Z","msg":"Collected capacity metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, metricsCount=1"}
{"level":"DEBUG","ts":"2025-11-28T20:51:51.803Z","msg":"Variant analysis initialized: variant=ms-inference-scheduling-llm-d-modelservice-decode, accelerator=H100, cost=10.00, replicaCount=1"}
{"level":"DEBUG","ts":"2025-11-28T20:51:51.803Z","msg":"Scale-down unsafe: insufficient non-saturated replicas: nonSaturated=1, required=2"}
{"level":"DEBUG","ts":"2025-11-28T20:51:51.803Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, totalReplicas=1, nonSaturated=1, avgSpareKv=0.500, avgSpareQueue=5.0, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"INFO","ts":"2025-11-28T20:51:51.803Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, totalReplicas=1, nonSaturated=1, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"DEBUG","ts":"2025-11-28T20:51:51.803Z","msg":"Capacity targets: no scaling needed (avgSpareKv=0.500, avgSpareQueue=5.0, all variants stable)"}
{"level":"DEBUG","ts":"2025-11-28T20:51:51.803Z","msg":"Capacity targets calculated: modelID=unsloth/Meta-Llama-3.1-8B, targets=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"INFO","ts":"2025-11-28T20:51:51.803Z","msg":"Capacity-only decisions made for model: unsloth/Meta-Llama-3.1-8B - decision count: 1"}
{"level":"INFO","ts":"2025-11-28T20:51:51.803Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-11-28T20:51:51.803Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1→target=1"}
{"level":"DEBUG","ts":"2025-11-28T20:51:51.803Z","msg":"Found VA in map: variant=ms-inference-scheduling-llm-d-modelservice-decode, hasCurrentAlloc=true, accelerator=H100"}
{"level":"INFO","ts":"2025-11-28T20:51:51.803Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 1, desired-replicas: 1, accelerator: H100"}
{"level":"INFO","ts":"2025-11-28T20:51:51.803Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=1, accelerator=H100, capacityOnly=true"}
{"level":"INFO","ts":"2025-11-28T20:51:51.808Z","msg":"Applied capacity decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1, target=1, reason=capacity-only mode: no-change"}
{"level":"INFO","ts":"2025-11-28T20:51:51.808Z","msg":"Reconciliation completed successfully: mode=capacity-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-11-28T20:52:21.712Z","msg":"Operating in CAPACITY-ONLY mode: reactive capacity-based scaling only"}
{"level":"INFO","ts":"2025-11-28T20:52:21.712Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-11-28T20:52:21.712Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-11-28T20:52:21.724Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=1, accelerator=H100, ttft=0.00ms, itl=0.00ms, cost=10.00, arrivalRate=0.00"}
{"level":"INFO","ts":"2025-11-28T20:52:21.725Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9l76md, queueLength=0"}
{"level":"DEBUG","ts":"2025-11-28T20:52:21.725Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"INFO","ts":"2025-11-28T20:52:21.726Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9l76md, usage=0.000 (0.0%)"}
{"level":"DEBUG","ts":"2025-11-28T20:52:21.726Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"DEBUG","ts":"2025-11-28T20:52:21.726Z","msg":"Pod-to-variant matching successful: totalPods=1, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"DEBUG","ts":"2025-11-28T20:52:21.726Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=1"}
{"level":"DEBUG","ts":"2025-11-28T20:52:21.726Z","msg":"Collected capacity metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, metricsCount=1"}
{"level":"DEBUG","ts":"2025-11-28T20:52:21.726Z","msg":"Variant analysis initialized: variant=ms-inference-scheduling-llm-d-modelservice-decode, accelerator=H100, cost=10.00, replicaCount=1"}
{"level":"DEBUG","ts":"2025-11-28T20:52:21.726Z","msg":"Scale-down unsafe: insufficient non-saturated replicas: nonSaturated=1, required=2"}
{"level":"DEBUG","ts":"2025-11-28T20:52:21.726Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, totalReplicas=1, nonSaturated=1, avgSpareKv=0.500, avgSpareQueue=5.0, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"INFO","ts":"2025-11-28T20:52:21.726Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, totalReplicas=1, nonSaturated=1, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"DEBUG","ts":"2025-11-28T20:52:21.726Z","msg":"Capacity targets: no scaling needed (avgSpareKv=0.500, avgSpareQueue=5.0, all variants stable)"}
{"level":"DEBUG","ts":"2025-11-28T20:52:21.726Z","msg":"Capacity targets calculated: modelID=unsloth/Meta-Llama-3.1-8B, targets=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"INFO","ts":"2025-11-28T20:52:21.726Z","msg":"Capacity-only decisions made for model: unsloth/Meta-Llama-3.1-8B - decision count: 1"}
{"level":"INFO","ts":"2025-11-28T20:52:21.726Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-11-28T20:52:21.726Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1→target=1"}
{"level":"DEBUG","ts":"2025-11-28T20:52:21.726Z","msg":"Found VA in map: variant=ms-inference-scheduling-llm-d-modelservice-decode, hasCurrentAlloc=true, accelerator=H100"}
{"level":"INFO","ts":"2025-11-28T20:52:21.726Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 1, desired-replicas: 1, accelerator: H100"}
{"level":"INFO","ts":"2025-11-28T20:52:21.726Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=1, accelerator=H100, capacityOnly=true"}
{"level":"INFO","ts":"2025-11-28T20:52:21.731Z","msg":"Applied capacity decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1, target=1, reason=capacity-only mode: no-change"}
{"level":"INFO","ts":"2025-11-28T20:52:21.731Z","msg":"Reconciliation completed successfully: mode=capacity-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-11-28T20:52:21.809Z","msg":"Operating in CAPACITY-ONLY mode: reactive capacity-based scaling only"}
{"level":"INFO","ts":"2025-11-28T20:52:21.809Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-11-28T20:52:21.809Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-11-28T20:52:21.818Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=1, accelerator=H100, ttft=0.00ms, itl=0.00ms, cost=10.00, arrivalRate=0.00"}
{"level":"INFO","ts":"2025-11-28T20:52:21.819Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9l76md, queueLength=0"}
{"level":"DEBUG","ts":"2025-11-28T20:52:21.819Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"INFO","ts":"2025-11-28T20:52:21.819Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9l76md, usage=0.000 (0.0%)"}
{"level":"DEBUG","ts":"2025-11-28T20:52:21.819Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"DEBUG","ts":"2025-11-28T20:52:21.820Z","msg":"Pod-to-variant matching successful: totalPods=1, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"DEBUG","ts":"2025-11-28T20:52:21.820Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=1"}
{"level":"DEBUG","ts":"2025-11-28T20:52:21.820Z","msg":"Collected capacity metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, metricsCount=1"}
{"level":"DEBUG","ts":"2025-11-28T20:52:21.820Z","msg":"Variant analysis initialized: variant=ms-inference-scheduling-llm-d-modelservice-decode, accelerator=H100, cost=10.00, replicaCount=1"}
{"level":"DEBUG","ts":"2025-11-28T20:52:21.820Z","msg":"Scale-down unsafe: insufficient non-saturated replicas: nonSaturated=1, required=2"}
{"level":"DEBUG","ts":"2025-11-28T20:52:21.820Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, totalReplicas=1, nonSaturated=1, avgSpareKv=0.500, avgSpareQueue=5.0, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"INFO","ts":"2025-11-28T20:52:21.820Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, totalReplicas=1, nonSaturated=1, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"DEBUG","ts":"2025-11-28T20:52:21.820Z","msg":"Capacity targets: no scaling needed (avgSpareKv=0.500, avgSpareQueue=5.0, all variants stable)"}
{"level":"DEBUG","ts":"2025-11-28T20:52:21.820Z","msg":"Capacity targets calculated: modelID=unsloth/Meta-Llama-3.1-8B, targets=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"INFO","ts":"2025-11-28T20:52:21.820Z","msg":"Capacity-only decisions made for model: unsloth/Meta-Llama-3.1-8B - decision count: 1"}
{"level":"INFO","ts":"2025-11-28T20:52:21.820Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-11-28T20:52:21.820Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1→target=1"}
{"level":"DEBUG","ts":"2025-11-28T20:52:21.820Z","msg":"Found VA in map: variant=ms-inference-scheduling-llm-d-modelservice-decode, hasCurrentAlloc=true, accelerator=H100"}
{"level":"INFO","ts":"2025-11-28T20:52:21.820Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 1, desired-replicas: 1, accelerator: H100"}
{"level":"INFO","ts":"2025-11-28T20:52:21.820Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=1, accelerator=H100, capacityOnly=true"}
{"level":"INFO","ts":"2025-11-28T20:52:22.035Z","msg":"Applied capacity decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1, target=1, reason=capacity-only mode: no-change"}
{"level":"INFO","ts":"2025-11-28T20:52:22.035Z","msg":"Reconciliation completed successfully: mode=capacity-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-11-28T20:52:51.731Z","msg":"Operating in CAPACITY-ONLY mode: reactive capacity-based scaling only"}
{"level":"INFO","ts":"2025-11-28T20:52:51.731Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-11-28T20:52:51.732Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-11-28T20:52:51.747Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=1, accelerator=H100, ttft=0.00ms, itl=0.00ms, cost=10.00, arrivalRate=0.00"}
{"level":"INFO","ts":"2025-11-28T20:52:51.748Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9l76md, usage=0.000 (0.0%)"}
{"level":"DEBUG","ts":"2025-11-28T20:52:51.748Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"INFO","ts":"2025-11-28T20:52:51.748Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9l76md, queueLength=0"}
{"level":"DEBUG","ts":"2025-11-28T20:52:51.748Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"DEBUG","ts":"2025-11-28T20:52:51.748Z","msg":"Pod-to-variant matching successful: totalPods=1, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"DEBUG","ts":"2025-11-28T20:52:51.748Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=1"}
{"level":"DEBUG","ts":"2025-11-28T20:52:51.748Z","msg":"Collected capacity metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, metricsCount=1"}
{"level":"DEBUG","ts":"2025-11-28T20:52:51.748Z","msg":"Variant analysis initialized: variant=ms-inference-scheduling-llm-d-modelservice-decode, accelerator=H100, cost=10.00, replicaCount=1"}
{"level":"DEBUG","ts":"2025-11-28T20:52:51.748Z","msg":"Scale-down unsafe: insufficient non-saturated replicas: nonSaturated=1, required=2"}
{"level":"DEBUG","ts":"2025-11-28T20:52:51.748Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, totalReplicas=1, nonSaturated=1, avgSpareKv=0.500, avgSpareQueue=5.0, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"INFO","ts":"2025-11-28T20:52:51.748Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, totalReplicas=1, nonSaturated=1, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"DEBUG","ts":"2025-11-28T20:52:51.748Z","msg":"Capacity targets: no scaling needed (avgSpareKv=0.500, avgSpareQueue=5.0, all variants stable)"}
{"level":"DEBUG","ts":"2025-11-28T20:52:51.748Z","msg":"Capacity targets calculated: modelID=unsloth/Meta-Llama-3.1-8B, targets=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"INFO","ts":"2025-11-28T20:52:51.748Z","msg":"Capacity-only decisions made for model: unsloth/Meta-Llama-3.1-8B - decision count: 1"}
{"level":"INFO","ts":"2025-11-28T20:52:51.748Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-11-28T20:52:51.748Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1→target=1"}
{"level":"DEBUG","ts":"2025-11-28T20:52:51.748Z","msg":"Found VA in map: variant=ms-inference-scheduling-llm-d-modelservice-decode, hasCurrentAlloc=true, accelerator=H100"}
{"level":"INFO","ts":"2025-11-28T20:52:51.748Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 1, desired-replicas: 1, accelerator: H100"}
{"level":"INFO","ts":"2025-11-28T20:52:51.748Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=1, accelerator=H100, capacityOnly=true"}
{"level":"INFO","ts":"2025-11-28T20:52:51.752Z","msg":"Applied capacity decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1, target=1, reason=capacity-only mode: no-change"}
{"level":"INFO","ts":"2025-11-28T20:52:51.752Z","msg":"Reconciliation completed successfully: mode=capacity-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-11-28T20:52:52.036Z","msg":"Operating in CAPACITY-ONLY mode: reactive capacity-based scaling only"}
{"level":"INFO","ts":"2025-11-28T20:52:52.036Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-11-28T20:52:52.036Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-11-28T20:52:52.046Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=1, accelerator=H100, ttft=0.00ms, itl=0.00ms, cost=10.00, arrivalRate=0.00"}
{"level":"INFO","ts":"2025-11-28T20:52:52.047Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9l76md, queueLength=0"}
{"level":"DEBUG","ts":"2025-11-28T20:52:52.047Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"INFO","ts":"2025-11-28T20:52:52.047Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9l76md, usage=0.000 (0.0%)"}
{"level":"DEBUG","ts":"2025-11-28T20:52:52.047Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"DEBUG","ts":"2025-11-28T20:52:52.047Z","msg":"Pod-to-variant matching successful: totalPods=1, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"DEBUG","ts":"2025-11-28T20:52:52.047Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=1"}
{"level":"DEBUG","ts":"2025-11-28T20:52:52.047Z","msg":"Collected capacity metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, metricsCount=1"}
{"level":"DEBUG","ts":"2025-11-28T20:52:52.047Z","msg":"Variant analysis initialized: variant=ms-inference-scheduling-llm-d-modelservice-decode, accelerator=H100, cost=10.00, replicaCount=1"}
{"level":"DEBUG","ts":"2025-11-28T20:52:52.047Z","msg":"Scale-down unsafe: insufficient non-saturated replicas: nonSaturated=1, required=2"}
{"level":"DEBUG","ts":"2025-11-28T20:52:52.047Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, totalReplicas=1, nonSaturated=1, avgSpareKv=0.500, avgSpareQueue=5.0, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"INFO","ts":"2025-11-28T20:52:52.047Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, totalReplicas=1, nonSaturated=1, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"DEBUG","ts":"2025-11-28T20:52:52.047Z","msg":"Capacity targets: no scaling needed (avgSpareKv=0.500, avgSpareQueue=5.0, all variants stable)"}
{"level":"DEBUG","ts":"2025-11-28T20:52:52.047Z","msg":"Capacity targets calculated: modelID=unsloth/Meta-Llama-3.1-8B, targets=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"INFO","ts":"2025-11-28T20:52:52.047Z","msg":"Capacity-only decisions made for model: unsloth/Meta-Llama-3.1-8B - decision count: 1"}
{"level":"INFO","ts":"2025-11-28T20:52:52.047Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-11-28T20:52:52.047Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1→target=1"}
{"level":"DEBUG","ts":"2025-11-28T20:52:52.047Z","msg":"Found VA in map: variant=ms-inference-scheduling-llm-d-modelservice-decode, hasCurrentAlloc=true, accelerator=H100"}
{"level":"INFO","ts":"2025-11-28T20:52:52.048Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 1, desired-replicas: 1, accelerator: H100"}
{"level":"INFO","ts":"2025-11-28T20:52:52.048Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=1, accelerator=H100, capacityOnly=true"}
{"level":"INFO","ts":"2025-11-28T20:52:52.060Z","msg":"Applied capacity decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1, target=1, reason=capacity-only mode: no-change"}
{"level":"INFO","ts":"2025-11-28T20:52:52.060Z","msg":"Reconciliation completed successfully: mode=capacity-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-11-28T20:53:21.753Z","msg":"Operating in CAPACITY-ONLY mode: reactive capacity-based scaling only"}
{"level":"INFO","ts":"2025-11-28T20:53:21.753Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-11-28T20:53:21.753Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-11-28T20:53:21.770Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=1, accelerator=H100, ttft=0.00ms, itl=0.00ms, cost=10.00, arrivalRate=0.00"}
{"level":"INFO","ts":"2025-11-28T20:53:21.772Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9l76md, queueLength=0"}
{"level":"DEBUG","ts":"2025-11-28T20:53:21.772Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"INFO","ts":"2025-11-28T20:53:21.772Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9l76md, usage=0.000 (0.0%)"}
{"level":"DEBUG","ts":"2025-11-28T20:53:21.772Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"DEBUG","ts":"2025-11-28T20:53:21.772Z","msg":"Pod-to-variant matching successful: totalPods=1, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"DEBUG","ts":"2025-11-28T20:53:21.772Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=1"}
{"level":"DEBUG","ts":"2025-11-28T20:53:21.772Z","msg":"Collected capacity metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, metricsCount=1"}
{"level":"DEBUG","ts":"2025-11-28T20:53:21.772Z","msg":"Variant analysis initialized: variant=ms-inference-scheduling-llm-d-modelservice-decode, accelerator=H100, cost=10.00, replicaCount=1"}
{"level":"DEBUG","ts":"2025-11-28T20:53:21.772Z","msg":"Scale-down unsafe: insufficient non-saturated replicas: nonSaturated=1, required=2"}
{"level":"DEBUG","ts":"2025-11-28T20:53:21.772Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, totalReplicas=1, nonSaturated=1, avgSpareKv=0.500, avgSpareQueue=5.0, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"INFO","ts":"2025-11-28T20:53:21.772Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, totalReplicas=1, nonSaturated=1, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"DEBUG","ts":"2025-11-28T20:53:21.772Z","msg":"Capacity targets: no scaling needed (avgSpareKv=0.500, avgSpareQueue=5.0, all variants stable)"}
{"level":"DEBUG","ts":"2025-11-28T20:53:21.772Z","msg":"Capacity targets calculated: modelID=unsloth/Meta-Llama-3.1-8B, targets=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"INFO","ts":"2025-11-28T20:53:21.772Z","msg":"Capacity-only decisions made for model: unsloth/Meta-Llama-3.1-8B - decision count: 1"}
{"level":"INFO","ts":"2025-11-28T20:53:21.772Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-11-28T20:53:21.772Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1→target=1"}
{"level":"DEBUG","ts":"2025-11-28T20:53:21.772Z","msg":"Found VA in map: variant=ms-inference-scheduling-llm-d-modelservice-decode, hasCurrentAlloc=true, accelerator=H100"}
{"level":"INFO","ts":"2025-11-28T20:53:21.772Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 1, desired-replicas: 1, accelerator: H100"}
{"level":"INFO","ts":"2025-11-28T20:53:21.772Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=1, accelerator=H100, capacityOnly=true"}
{"level":"INFO","ts":"2025-11-28T20:53:21.779Z","msg":"Applied capacity decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1, target=1, reason=capacity-only mode: no-change"}
{"level":"INFO","ts":"2025-11-28T20:53:21.779Z","msg":"Reconciliation completed successfully: mode=capacity-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-11-28T20:53:22.060Z","msg":"Operating in CAPACITY-ONLY mode: reactive capacity-based scaling only"}
{"level":"INFO","ts":"2025-11-28T20:53:22.060Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-11-28T20:53:22.060Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-11-28T20:53:22.071Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=1, accelerator=H100, ttft=0.00ms, itl=0.00ms, cost=10.00, arrivalRate=0.00"}
{"level":"INFO","ts":"2025-11-28T20:53:22.073Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9l76md, queueLength=0"}
{"level":"DEBUG","ts":"2025-11-28T20:53:22.073Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"INFO","ts":"2025-11-28T20:53:22.073Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9l76md, usage=0.000 (0.0%)"}
{"level":"DEBUG","ts":"2025-11-28T20:53:22.073Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"DEBUG","ts":"2025-11-28T20:53:22.073Z","msg":"Pod-to-variant matching successful: totalPods=1, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"DEBUG","ts":"2025-11-28T20:53:22.073Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=1"}
{"level":"DEBUG","ts":"2025-11-28T20:53:22.073Z","msg":"Collected capacity metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, metricsCount=1"}
{"level":"DEBUG","ts":"2025-11-28T20:53:22.073Z","msg":"Variant analysis initialized: variant=ms-inference-scheduling-llm-d-modelservice-decode, accelerator=H100, cost=10.00, replicaCount=1"}
{"level":"DEBUG","ts":"2025-11-28T20:53:22.073Z","msg":"Scale-down unsafe: insufficient non-saturated replicas: nonSaturated=1, required=2"}
{"level":"DEBUG","ts":"2025-11-28T20:53:22.073Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, totalReplicas=1, nonSaturated=1, avgSpareKv=0.500, avgSpareQueue=5.0, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"INFO","ts":"2025-11-28T20:53:22.073Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, totalReplicas=1, nonSaturated=1, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"DEBUG","ts":"2025-11-28T20:53:22.073Z","msg":"Capacity targets: no scaling needed (avgSpareKv=0.500, avgSpareQueue=5.0, all variants stable)"}
{"level":"DEBUG","ts":"2025-11-28T20:53:22.073Z","msg":"Capacity targets calculated: modelID=unsloth/Meta-Llama-3.1-8B, targets=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"INFO","ts":"2025-11-28T20:53:22.073Z","msg":"Capacity-only decisions made for model: unsloth/Meta-Llama-3.1-8B - decision count: 1"}
{"level":"INFO","ts":"2025-11-28T20:53:22.073Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-11-28T20:53:22.073Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1→target=1"}
{"level":"DEBUG","ts":"2025-11-28T20:53:22.073Z","msg":"Found VA in map: variant=ms-inference-scheduling-llm-d-modelservice-decode, hasCurrentAlloc=true, accelerator=H100"}
{"level":"INFO","ts":"2025-11-28T20:53:22.073Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 1, desired-replicas: 1, accelerator: H100"}
{"level":"INFO","ts":"2025-11-28T20:53:22.073Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=1, accelerator=H100, capacityOnly=true"}
{"level":"INFO","ts":"2025-11-28T20:53:22.079Z","msg":"Applied capacity decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1, target=1, reason=capacity-only mode: no-change"}
{"level":"INFO","ts":"2025-11-28T20:53:22.079Z","msg":"Reconciliation completed successfully: mode=capacity-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-11-28T20:53:51.780Z","msg":"Operating in CAPACITY-ONLY mode: reactive capacity-based scaling only"}
{"level":"INFO","ts":"2025-11-28T20:53:51.780Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-11-28T20:53:51.780Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-11-28T20:53:51.799Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=1, accelerator=H100, ttft=0.00ms, itl=0.00ms, cost=10.00, arrivalRate=0.00"}
{"level":"INFO","ts":"2025-11-28T20:53:51.801Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9l76md, queueLength=0"}
{"level":"DEBUG","ts":"2025-11-28T20:53:51.801Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"INFO","ts":"2025-11-28T20:53:51.801Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9l76md, usage=0.000 (0.0%)"}
{"level":"DEBUG","ts":"2025-11-28T20:53:51.801Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"DEBUG","ts":"2025-11-28T20:53:51.801Z","msg":"Pod-to-variant matching successful: totalPods=1, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"DEBUG","ts":"2025-11-28T20:53:51.801Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=1"}
{"level":"DEBUG","ts":"2025-11-28T20:53:51.801Z","msg":"Collected capacity metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, metricsCount=1"}
{"level":"DEBUG","ts":"2025-11-28T20:53:51.801Z","msg":"Variant analysis initialized: variant=ms-inference-scheduling-llm-d-modelservice-decode, accelerator=H100, cost=10.00, replicaCount=1"}
{"level":"DEBUG","ts":"2025-11-28T20:53:51.801Z","msg":"Scale-down unsafe: insufficient non-saturated replicas: nonSaturated=1, required=2"}
{"level":"DEBUG","ts":"2025-11-28T20:53:51.801Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, totalReplicas=1, nonSaturated=1, avgSpareKv=0.500, avgSpareQueue=5.0, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"INFO","ts":"2025-11-28T20:53:51.801Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, totalReplicas=1, nonSaturated=1, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"DEBUG","ts":"2025-11-28T20:53:51.801Z","msg":"Capacity targets: no scaling needed (avgSpareKv=0.500, avgSpareQueue=5.0, all variants stable)"}
{"level":"DEBUG","ts":"2025-11-28T20:53:51.801Z","msg":"Capacity targets calculated: modelID=unsloth/Meta-Llama-3.1-8B, targets=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"INFO","ts":"2025-11-28T20:53:51.801Z","msg":"Capacity-only decisions made for model: unsloth/Meta-Llama-3.1-8B - decision count: 1"}
{"level":"INFO","ts":"2025-11-28T20:53:51.801Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-11-28T20:53:51.801Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1→target=1"}
{"level":"DEBUG","ts":"2025-11-28T20:53:51.801Z","msg":"Found VA in map: variant=ms-inference-scheduling-llm-d-modelservice-decode, hasCurrentAlloc=true, accelerator=H100"}
{"level":"INFO","ts":"2025-11-28T20:53:51.801Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 1, desired-replicas: 1, accelerator: H100"}
{"level":"INFO","ts":"2025-11-28T20:53:51.801Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=1, accelerator=H100, capacityOnly=true"}
{"level":"INFO","ts":"2025-11-28T20:53:51.805Z","msg":"Applied capacity decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1, target=1, reason=capacity-only mode: no-change"}
{"level":"INFO","ts":"2025-11-28T20:53:51.805Z","msg":"Reconciliation completed successfully: mode=capacity-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-11-28T20:53:52.080Z","msg":"Operating in CAPACITY-ONLY mode: reactive capacity-based scaling only"}
{"level":"INFO","ts":"2025-11-28T20:53:52.080Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-11-28T20:53:52.080Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-11-28T20:53:52.091Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=1, accelerator=H100, ttft=0.00ms, itl=0.00ms, cost=10.00, arrivalRate=0.00"}
{"level":"INFO","ts":"2025-11-28T20:53:52.092Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9l76md, queueLength=0"}
{"level":"DEBUG","ts":"2025-11-28T20:53:52.092Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"INFO","ts":"2025-11-28T20:53:52.092Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9l76md, usage=0.000 (0.0%)"}
{"level":"DEBUG","ts":"2025-11-28T20:53:52.092Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"DEBUG","ts":"2025-11-28T20:53:52.093Z","msg":"Pod-to-variant matching successful: totalPods=1, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"DEBUG","ts":"2025-11-28T20:53:52.093Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=1"}
{"level":"DEBUG","ts":"2025-11-28T20:53:52.093Z","msg":"Collected capacity metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, metricsCount=1"}
{"level":"DEBUG","ts":"2025-11-28T20:53:52.093Z","msg":"Variant analysis initialized: variant=ms-inference-scheduling-llm-d-modelservice-decode, accelerator=H100, cost=10.00, replicaCount=1"}
{"level":"DEBUG","ts":"2025-11-28T20:53:52.093Z","msg":"Scale-down unsafe: insufficient non-saturated replicas: nonSaturated=1, required=2"}
{"level":"DEBUG","ts":"2025-11-28T20:53:52.093Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, totalReplicas=1, nonSaturated=1, avgSpareKv=0.500, avgSpareQueue=5.0, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"INFO","ts":"2025-11-28T20:53:52.093Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, totalReplicas=1, nonSaturated=1, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"DEBUG","ts":"2025-11-28T20:53:52.093Z","msg":"Capacity targets: no scaling needed (avgSpareKv=0.500, avgSpareQueue=5.0, all variants stable)"}
{"level":"DEBUG","ts":"2025-11-28T20:53:52.093Z","msg":"Capacity targets calculated: modelID=unsloth/Meta-Llama-3.1-8B, targets=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"INFO","ts":"2025-11-28T20:53:52.093Z","msg":"Capacity-only decisions made for model: unsloth/Meta-Llama-3.1-8B - decision count: 1"}
{"level":"INFO","ts":"2025-11-28T20:53:52.093Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-11-28T20:53:52.093Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1→target=1"}
{"level":"DEBUG","ts":"2025-11-28T20:53:52.093Z","msg":"Found VA in map: variant=ms-inference-scheduling-llm-d-modelservice-decode, hasCurrentAlloc=true, accelerator=H100"}
{"level":"INFO","ts":"2025-11-28T20:53:52.093Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 1, desired-replicas: 1, accelerator: H100"}
{"level":"INFO","ts":"2025-11-28T20:53:52.093Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=1, accelerator=H100, capacityOnly=true"}
{"level":"INFO","ts":"2025-11-28T20:53:52.098Z","msg":"Applied capacity decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1, target=1, reason=capacity-only mode: no-change"}
{"level":"INFO","ts":"2025-11-28T20:53:52.098Z","msg":"Reconciliation completed successfully: mode=capacity-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-11-28T20:54:21.806Z","msg":"Operating in CAPACITY-ONLY mode: reactive capacity-based scaling only"}
{"level":"INFO","ts":"2025-11-28T20:54:21.806Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-11-28T20:54:21.806Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-11-28T20:54:21.819Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=1, accelerator=H100, ttft=0.00ms, itl=0.00ms, cost=10.00, arrivalRate=0.00"}
{"level":"INFO","ts":"2025-11-28T20:54:21.821Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9l76md, queueLength=0"}
{"level":"DEBUG","ts":"2025-11-28T20:54:21.821Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"INFO","ts":"2025-11-28T20:54:21.821Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9l76md, usage=0.000 (0.0%)"}
{"level":"DEBUG","ts":"2025-11-28T20:54:21.821Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"DEBUG","ts":"2025-11-28T20:54:21.821Z","msg":"Pod-to-variant matching successful: totalPods=1, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"DEBUG","ts":"2025-11-28T20:54:21.821Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=1"}
{"level":"DEBUG","ts":"2025-11-28T20:54:21.821Z","msg":"Collected capacity metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, metricsCount=1"}
{"level":"DEBUG","ts":"2025-11-28T20:54:21.821Z","msg":"Variant analysis initialized: variant=ms-inference-scheduling-llm-d-modelservice-decode, accelerator=H100, cost=10.00, replicaCount=1"}
{"level":"DEBUG","ts":"2025-11-28T20:54:21.821Z","msg":"Scale-down unsafe: insufficient non-saturated replicas: nonSaturated=1, required=2"}
{"level":"DEBUG","ts":"2025-11-28T20:54:21.821Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, totalReplicas=1, nonSaturated=1, avgSpareKv=0.500, avgSpareQueue=5.0, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"INFO","ts":"2025-11-28T20:54:21.821Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, totalReplicas=1, nonSaturated=1, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"DEBUG","ts":"2025-11-28T20:54:21.821Z","msg":"Capacity targets: no scaling needed (avgSpareKv=0.500, avgSpareQueue=5.0, all variants stable)"}
{"level":"DEBUG","ts":"2025-11-28T20:54:21.821Z","msg":"Capacity targets calculated: modelID=unsloth/Meta-Llama-3.1-8B, targets=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"INFO","ts":"2025-11-28T20:54:21.821Z","msg":"Capacity-only decisions made for model: unsloth/Meta-Llama-3.1-8B - decision count: 1"}
{"level":"INFO","ts":"2025-11-28T20:54:21.821Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-11-28T20:54:21.821Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1→target=1"}
{"level":"DEBUG","ts":"2025-11-28T20:54:21.821Z","msg":"Found VA in map: variant=ms-inference-scheduling-llm-d-modelservice-decode, hasCurrentAlloc=true, accelerator=H100"}
{"level":"INFO","ts":"2025-11-28T20:54:21.821Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 1, desired-replicas: 1, accelerator: H100"}
{"level":"INFO","ts":"2025-11-28T20:54:21.821Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=1, accelerator=H100, capacityOnly=true"}
{"level":"INFO","ts":"2025-11-28T20:54:21.827Z","msg":"Applied capacity decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1, target=1, reason=capacity-only mode: no-change"}
{"level":"INFO","ts":"2025-11-28T20:54:21.827Z","msg":"Reconciliation completed successfully: mode=capacity-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-11-28T20:54:22.098Z","msg":"Operating in CAPACITY-ONLY mode: reactive capacity-based scaling only"}
{"level":"INFO","ts":"2025-11-28T20:54:22.098Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-11-28T20:54:22.098Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-11-28T20:54:22.109Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=1, accelerator=H100, ttft=0.00ms, itl=0.00ms, cost=10.00, arrivalRate=0.00"}
{"level":"INFO","ts":"2025-11-28T20:54:22.110Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9l76md, queueLength=0"}
{"level":"DEBUG","ts":"2025-11-28T20:54:22.110Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"INFO","ts":"2025-11-28T20:54:22.111Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9l76md, usage=0.000 (0.0%)"}
{"level":"DEBUG","ts":"2025-11-28T20:54:22.111Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"DEBUG","ts":"2025-11-28T20:54:22.111Z","msg":"Pod-to-variant matching successful: totalPods=1, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"DEBUG","ts":"2025-11-28T20:54:22.111Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=1"}
{"level":"DEBUG","ts":"2025-11-28T20:54:22.111Z","msg":"Collected capacity metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, metricsCount=1"}
{"level":"DEBUG","ts":"2025-11-28T20:54:22.111Z","msg":"Variant analysis initialized: variant=ms-inference-scheduling-llm-d-modelservice-decode, accelerator=H100, cost=10.00, replicaCount=1"}
{"level":"DEBUG","ts":"2025-11-28T20:54:22.111Z","msg":"Scale-down unsafe: insufficient non-saturated replicas: nonSaturated=1, required=2"}
{"level":"DEBUG","ts":"2025-11-28T20:54:22.111Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, totalReplicas=1, nonSaturated=1, avgSpareKv=0.500, avgSpareQueue=5.0, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"INFO","ts":"2025-11-28T20:54:22.111Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, totalReplicas=1, nonSaturated=1, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"DEBUG","ts":"2025-11-28T20:54:22.111Z","msg":"Capacity targets: no scaling needed (avgSpareKv=0.500, avgSpareQueue=5.0, all variants stable)"}
{"level":"DEBUG","ts":"2025-11-28T20:54:22.111Z","msg":"Capacity targets calculated: modelID=unsloth/Meta-Llama-3.1-8B, targets=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"INFO","ts":"2025-11-28T20:54:22.111Z","msg":"Capacity-only decisions made for model: unsloth/Meta-Llama-3.1-8B - decision count: 1"}
{"level":"INFO","ts":"2025-11-28T20:54:22.111Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-11-28T20:54:22.111Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1→target=1"}
{"level":"DEBUG","ts":"2025-11-28T20:54:22.111Z","msg":"Found VA in map: variant=ms-inference-scheduling-llm-d-modelservice-decode, hasCurrentAlloc=true, accelerator=H100"}
{"level":"INFO","ts":"2025-11-28T20:54:22.111Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 1, desired-replicas: 1, accelerator: H100"}
{"level":"INFO","ts":"2025-11-28T20:54:22.111Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=1, accelerator=H100, capacityOnly=true"}
{"level":"INFO","ts":"2025-11-28T20:54:22.115Z","msg":"Applied capacity decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1, target=1, reason=capacity-only mode: no-change"}
{"level":"INFO","ts":"2025-11-28T20:54:22.115Z","msg":"Reconciliation completed successfully: mode=capacity-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-11-28T20:54:51.829Z","msg":"Operating in CAPACITY-ONLY mode: reactive capacity-based scaling only"}
{"level":"INFO","ts":"2025-11-28T20:54:51.829Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-11-28T20:54:51.829Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-11-28T20:54:51.850Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=1, accelerator=H100, ttft=0.00ms, itl=0.00ms, cost=10.00, arrivalRate=0.00"}
{"level":"INFO","ts":"2025-11-28T20:54:51.852Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9l76md, queueLength=0"}
{"level":"DEBUG","ts":"2025-11-28T20:54:51.852Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"INFO","ts":"2025-11-28T20:54:51.852Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9l76md, usage=0.000 (0.0%)"}
{"level":"DEBUG","ts":"2025-11-28T20:54:51.852Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"DEBUG","ts":"2025-11-28T20:54:51.852Z","msg":"Pod-to-variant matching successful: totalPods=1, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"DEBUG","ts":"2025-11-28T20:54:51.852Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=1"}
{"level":"DEBUG","ts":"2025-11-28T20:54:51.852Z","msg":"Collected capacity metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, metricsCount=1"}
{"level":"DEBUG","ts":"2025-11-28T20:54:51.852Z","msg":"Variant analysis initialized: variant=ms-inference-scheduling-llm-d-modelservice-decode, accelerator=H100, cost=10.00, replicaCount=1"}
{"level":"DEBUG","ts":"2025-11-28T20:54:51.852Z","msg":"Scale-down unsafe: insufficient non-saturated replicas: nonSaturated=1, required=2"}
{"level":"DEBUG","ts":"2025-11-28T20:54:51.852Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, totalReplicas=1, nonSaturated=1, avgSpareKv=0.500, avgSpareQueue=5.0, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"INFO","ts":"2025-11-28T20:54:51.852Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, totalReplicas=1, nonSaturated=1, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"DEBUG","ts":"2025-11-28T20:54:51.852Z","msg":"Capacity targets: no scaling needed (avgSpareKv=0.500, avgSpareQueue=5.0, all variants stable)"}
{"level":"DEBUG","ts":"2025-11-28T20:54:51.852Z","msg":"Capacity targets calculated: modelID=unsloth/Meta-Llama-3.1-8B, targets=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"INFO","ts":"2025-11-28T20:54:51.852Z","msg":"Capacity-only decisions made for model: unsloth/Meta-Llama-3.1-8B - decision count: 1"}
{"level":"INFO","ts":"2025-11-28T20:54:51.852Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-11-28T20:54:51.852Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1→target=1"}
{"level":"DEBUG","ts":"2025-11-28T20:54:51.852Z","msg":"Found VA in map: variant=ms-inference-scheduling-llm-d-modelservice-decode, hasCurrentAlloc=true, accelerator=H100"}
{"level":"INFO","ts":"2025-11-28T20:54:51.852Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 1, desired-replicas: 1, accelerator: H100"}
{"level":"INFO","ts":"2025-11-28T20:54:51.852Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=1, accelerator=H100, capacityOnly=true"}
{"level":"INFO","ts":"2025-11-28T20:54:51.858Z","msg":"Applied capacity decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1, target=1, reason=capacity-only mode: no-change"}
{"level":"INFO","ts":"2025-11-28T20:54:51.858Z","msg":"Reconciliation completed successfully: mode=capacity-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-11-28T20:54:52.117Z","msg":"Operating in CAPACITY-ONLY mode: reactive capacity-based scaling only"}
{"level":"INFO","ts":"2025-11-28T20:54:52.117Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-11-28T20:54:52.117Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-11-28T20:54:52.128Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=1, accelerator=H100, ttft=0.00ms, itl=0.00ms, cost=10.00, arrivalRate=0.00"}
{"level":"INFO","ts":"2025-11-28T20:54:52.129Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9l76md, usage=0.000 (0.0%)"}
{"level":"DEBUG","ts":"2025-11-28T20:54:52.129Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"INFO","ts":"2025-11-28T20:54:52.129Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9l76md, queueLength=0"}
{"level":"DEBUG","ts":"2025-11-28T20:54:52.129Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"DEBUG","ts":"2025-11-28T20:54:52.129Z","msg":"Pod-to-variant matching successful: totalPods=1, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"DEBUG","ts":"2025-11-28T20:54:52.129Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=1"}
{"level":"DEBUG","ts":"2025-11-28T20:54:52.129Z","msg":"Collected capacity metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, metricsCount=1"}
{"level":"DEBUG","ts":"2025-11-28T20:54:52.129Z","msg":"Variant analysis initialized: variant=ms-inference-scheduling-llm-d-modelservice-decode, accelerator=H100, cost=10.00, replicaCount=1"}
{"level":"DEBUG","ts":"2025-11-28T20:54:52.129Z","msg":"Scale-down unsafe: insufficient non-saturated replicas: nonSaturated=1, required=2"}
{"level":"DEBUG","ts":"2025-11-28T20:54:52.129Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, totalReplicas=1, nonSaturated=1, avgSpareKv=0.500, avgSpareQueue=5.0, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"INFO","ts":"2025-11-28T20:54:52.129Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, totalReplicas=1, nonSaturated=1, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"DEBUG","ts":"2025-11-28T20:54:52.130Z","msg":"Capacity targets: no scaling needed (avgSpareKv=0.500, avgSpareQueue=5.0, all variants stable)"}
{"level":"DEBUG","ts":"2025-11-28T20:54:52.130Z","msg":"Capacity targets calculated: modelID=unsloth/Meta-Llama-3.1-8B, targets=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"INFO","ts":"2025-11-28T20:54:52.130Z","msg":"Capacity-only decisions made for model: unsloth/Meta-Llama-3.1-8B - decision count: 1"}
{"level":"INFO","ts":"2025-11-28T20:54:52.130Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-11-28T20:54:52.130Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1→target=1"}
{"level":"DEBUG","ts":"2025-11-28T20:54:52.130Z","msg":"Found VA in map: variant=ms-inference-scheduling-llm-d-modelservice-decode, hasCurrentAlloc=true, accelerator=H100"}
{"level":"INFO","ts":"2025-11-28T20:54:52.130Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 1, desired-replicas: 1, accelerator: H100"}
{"level":"INFO","ts":"2025-11-28T20:54:52.130Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=1, accelerator=H100, capacityOnly=true"}
{"level":"INFO","ts":"2025-11-28T20:54:52.135Z","msg":"Applied capacity decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1, target=1, reason=capacity-only mode: no-change"}
{"level":"INFO","ts":"2025-11-28T20:54:52.135Z","msg":"Reconciliation completed successfully: mode=capacity-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-11-28T20:55:21.859Z","msg":"Operating in CAPACITY-ONLY mode: reactive capacity-based scaling only"}
{"level":"INFO","ts":"2025-11-28T20:55:21.859Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-11-28T20:55:21.859Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-11-28T20:55:21.875Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=1, accelerator=H100, ttft=0.00ms, itl=0.00ms, cost=10.00, arrivalRate=0.00"}
{"level":"INFO","ts":"2025-11-28T20:55:21.876Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9l76md, usage=0.000 (0.0%)"}
{"level":"DEBUG","ts":"2025-11-28T20:55:21.876Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"INFO","ts":"2025-11-28T20:55:21.876Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9l76md, queueLength=0"}
{"level":"DEBUG","ts":"2025-11-28T20:55:21.876Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"DEBUG","ts":"2025-11-28T20:55:21.876Z","msg":"Pod-to-variant matching successful: totalPods=1, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"DEBUG","ts":"2025-11-28T20:55:21.876Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=1"}
{"level":"DEBUG","ts":"2025-11-28T20:55:21.876Z","msg":"Collected capacity metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, metricsCount=1"}
{"level":"DEBUG","ts":"2025-11-28T20:55:21.876Z","msg":"Variant analysis initialized: variant=ms-inference-scheduling-llm-d-modelservice-decode, accelerator=H100, cost=10.00, replicaCount=1"}
{"level":"DEBUG","ts":"2025-11-28T20:55:21.876Z","msg":"Scale-down unsafe: insufficient non-saturated replicas: nonSaturated=1, required=2"}
{"level":"DEBUG","ts":"2025-11-28T20:55:21.876Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, totalReplicas=1, nonSaturated=1, avgSpareKv=0.500, avgSpareQueue=5.0, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"INFO","ts":"2025-11-28T20:55:21.876Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, totalReplicas=1, nonSaturated=1, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"DEBUG","ts":"2025-11-28T20:55:21.876Z","msg":"Capacity targets: no scaling needed (avgSpareKv=0.500, avgSpareQueue=5.0, all variants stable)"}
{"level":"DEBUG","ts":"2025-11-28T20:55:21.876Z","msg":"Capacity targets calculated: modelID=unsloth/Meta-Llama-3.1-8B, targets=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"INFO","ts":"2025-11-28T20:55:21.876Z","msg":"Capacity-only decisions made for model: unsloth/Meta-Llama-3.1-8B - decision count: 1"}
{"level":"INFO","ts":"2025-11-28T20:55:21.876Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-11-28T20:55:21.876Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1→target=1"}
{"level":"DEBUG","ts":"2025-11-28T20:55:21.876Z","msg":"Found VA in map: variant=ms-inference-scheduling-llm-d-modelservice-decode, hasCurrentAlloc=true, accelerator=H100"}
{"level":"INFO","ts":"2025-11-28T20:55:21.876Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 1, desired-replicas: 1, accelerator: H100"}
{"level":"INFO","ts":"2025-11-28T20:55:21.876Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=1, accelerator=H100, capacityOnly=true"}
{"level":"INFO","ts":"2025-11-28T20:55:21.884Z","msg":"Applied capacity decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1, target=1, reason=capacity-only mode: no-change"}
{"level":"INFO","ts":"2025-11-28T20:55:21.884Z","msg":"Reconciliation completed successfully: mode=capacity-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-11-28T20:55:22.138Z","msg":"Operating in CAPACITY-ONLY mode: reactive capacity-based scaling only"}
{"level":"INFO","ts":"2025-11-28T20:55:22.138Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-11-28T20:55:22.138Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-11-28T20:55:22.149Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=1, accelerator=H100, ttft=0.00ms, itl=0.00ms, cost=10.00, arrivalRate=0.00"}
{"level":"INFO","ts":"2025-11-28T20:55:22.175Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9l76md, queueLength=0"}
{"level":"DEBUG","ts":"2025-11-28T20:55:22.175Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"INFO","ts":"2025-11-28T20:55:22.175Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9l76md, usage=0.000 (0.0%)"}
{"level":"DEBUG","ts":"2025-11-28T20:55:22.175Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"DEBUG","ts":"2025-11-28T20:55:22.175Z","msg":"Pod-to-variant matching successful: totalPods=1, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"DEBUG","ts":"2025-11-28T20:55:22.175Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=1"}
{"level":"DEBUG","ts":"2025-11-28T20:55:22.175Z","msg":"Collected capacity metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, metricsCount=1"}
{"level":"DEBUG","ts":"2025-11-28T20:55:22.175Z","msg":"Variant analysis initialized: variant=ms-inference-scheduling-llm-d-modelservice-decode, accelerator=H100, cost=10.00, replicaCount=1"}
{"level":"DEBUG","ts":"2025-11-28T20:55:22.175Z","msg":"Scale-down unsafe: insufficient non-saturated replicas: nonSaturated=1, required=2"}
{"level":"DEBUG","ts":"2025-11-28T20:55:22.175Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, totalReplicas=1, nonSaturated=1, avgSpareKv=0.500, avgSpareQueue=5.0, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"INFO","ts":"2025-11-28T20:55:22.175Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, totalReplicas=1, nonSaturated=1, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"DEBUG","ts":"2025-11-28T20:55:22.175Z","msg":"Capacity targets: no scaling needed (avgSpareKv=0.500, avgSpareQueue=5.0, all variants stable)"}
{"level":"DEBUG","ts":"2025-11-28T20:55:22.175Z","msg":"Capacity targets calculated: modelID=unsloth/Meta-Llama-3.1-8B, targets=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"INFO","ts":"2025-11-28T20:55:22.175Z","msg":"Capacity-only decisions made for model: unsloth/Meta-Llama-3.1-8B - decision count: 1"}
{"level":"INFO","ts":"2025-11-28T20:55:22.175Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-11-28T20:55:22.175Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1→target=1"}
{"level":"DEBUG","ts":"2025-11-28T20:55:22.175Z","msg":"Found VA in map: variant=ms-inference-scheduling-llm-d-modelservice-decode, hasCurrentAlloc=true, accelerator=H100"}
{"level":"INFO","ts":"2025-11-28T20:55:22.176Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 1, desired-replicas: 1, accelerator: H100"}
{"level":"INFO","ts":"2025-11-28T20:55:22.176Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=1, accelerator=H100, capacityOnly=true"}
{"level":"INFO","ts":"2025-11-28T20:55:22.182Z","msg":"Applied capacity decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1, target=1, reason=capacity-only mode: no-change"}
{"level":"INFO","ts":"2025-11-28T20:55:22.182Z","msg":"Reconciliation completed successfully: mode=capacity-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-11-28T20:55:51.885Z","msg":"Operating in CAPACITY-ONLY mode: reactive capacity-based scaling only"}
{"level":"INFO","ts":"2025-11-28T20:55:51.885Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-11-28T20:55:51.885Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-11-28T20:55:51.899Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=1, accelerator=H100, ttft=0.00ms, itl=0.00ms, cost=10.00, arrivalRate=0.00"}
{"level":"INFO","ts":"2025-11-28T20:55:51.901Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9l76md, queueLength=0"}
{"level":"DEBUG","ts":"2025-11-28T20:55:51.901Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"INFO","ts":"2025-11-28T20:55:51.901Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9l76md, usage=0.000 (0.0%)"}
{"level":"DEBUG","ts":"2025-11-28T20:55:51.901Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"DEBUG","ts":"2025-11-28T20:55:51.901Z","msg":"Pod-to-variant matching successful: totalPods=1, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"DEBUG","ts":"2025-11-28T20:55:51.901Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=1"}
{"level":"DEBUG","ts":"2025-11-28T20:55:51.901Z","msg":"Collected capacity metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, metricsCount=1"}
{"level":"DEBUG","ts":"2025-11-28T20:55:51.901Z","msg":"Variant analysis initialized: variant=ms-inference-scheduling-llm-d-modelservice-decode, accelerator=H100, cost=10.00, replicaCount=1"}
{"level":"DEBUG","ts":"2025-11-28T20:55:51.901Z","msg":"Scale-down unsafe: insufficient non-saturated replicas: nonSaturated=1, required=2"}
{"level":"DEBUG","ts":"2025-11-28T20:55:51.901Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, totalReplicas=1, nonSaturated=1, avgSpareKv=0.500, avgSpareQueue=5.0, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"INFO","ts":"2025-11-28T20:55:51.901Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, totalReplicas=1, nonSaturated=1, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"DEBUG","ts":"2025-11-28T20:55:51.901Z","msg":"Capacity targets: no scaling needed (avgSpareKv=0.500, avgSpareQueue=5.0, all variants stable)"}
{"level":"DEBUG","ts":"2025-11-28T20:55:51.901Z","msg":"Capacity targets calculated: modelID=unsloth/Meta-Llama-3.1-8B, targets=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"INFO","ts":"2025-11-28T20:55:51.901Z","msg":"Capacity-only decisions made for model: unsloth/Meta-Llama-3.1-8B - decision count: 1"}
{"level":"INFO","ts":"2025-11-28T20:55:51.901Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-11-28T20:55:51.901Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1→target=1"}
{"level":"DEBUG","ts":"2025-11-28T20:55:51.901Z","msg":"Found VA in map: variant=ms-inference-scheduling-llm-d-modelservice-decode, hasCurrentAlloc=true, accelerator=H100"}
{"level":"INFO","ts":"2025-11-28T20:55:51.901Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 1, desired-replicas: 1, accelerator: H100"}
{"level":"INFO","ts":"2025-11-28T20:55:51.901Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=1, accelerator=H100, capacityOnly=true"}
{"level":"INFO","ts":"2025-11-28T20:55:51.906Z","msg":"Applied capacity decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1, target=1, reason=capacity-only mode: no-change"}
{"level":"INFO","ts":"2025-11-28T20:55:51.906Z","msg":"Reconciliation completed successfully: mode=capacity-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-11-28T20:55:52.182Z","msg":"Operating in CAPACITY-ONLY mode: reactive capacity-based scaling only"}
{"level":"INFO","ts":"2025-11-28T20:55:52.182Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-11-28T20:55:52.182Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-11-28T20:55:52.193Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=1, accelerator=H100, ttft=0.00ms, itl=0.00ms, cost=10.00, arrivalRate=0.00"}
{"level":"INFO","ts":"2025-11-28T20:55:52.194Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9l76md, queueLength=0"}
{"level":"DEBUG","ts":"2025-11-28T20:55:52.194Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"INFO","ts":"2025-11-28T20:55:52.195Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9l76md, usage=0.000 (0.0%)"}
{"level":"DEBUG","ts":"2025-11-28T20:55:52.195Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"DEBUG","ts":"2025-11-28T20:55:52.196Z","msg":"Pod-to-variant matching successful: totalPods=1, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"DEBUG","ts":"2025-11-28T20:55:52.196Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=1"}
{"level":"DEBUG","ts":"2025-11-28T20:55:52.196Z","msg":"Collected capacity metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, metricsCount=1"}
{"level":"DEBUG","ts":"2025-11-28T20:55:52.196Z","msg":"Variant analysis initialized: variant=ms-inference-scheduling-llm-d-modelservice-decode, accelerator=H100, cost=10.00, replicaCount=1"}
{"level":"DEBUG","ts":"2025-11-28T20:55:52.196Z","msg":"Scale-down unsafe: insufficient non-saturated replicas: nonSaturated=1, required=2"}
{"level":"DEBUG","ts":"2025-11-28T20:55:52.196Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, totalReplicas=1, nonSaturated=1, avgSpareKv=0.500, avgSpareQueue=5.0, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"INFO","ts":"2025-11-28T20:55:52.196Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, totalReplicas=1, nonSaturated=1, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"DEBUG","ts":"2025-11-28T20:55:52.196Z","msg":"Capacity targets: no scaling needed (avgSpareKv=0.500, avgSpareQueue=5.0, all variants stable)"}
{"level":"DEBUG","ts":"2025-11-28T20:55:52.196Z","msg":"Capacity targets calculated: modelID=unsloth/Meta-Llama-3.1-8B, targets=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"INFO","ts":"2025-11-28T20:55:52.196Z","msg":"Capacity-only decisions made for model: unsloth/Meta-Llama-3.1-8B - decision count: 1"}
{"level":"INFO","ts":"2025-11-28T20:55:52.196Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-11-28T20:55:52.196Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1→target=1"}
{"level":"DEBUG","ts":"2025-11-28T20:55:52.196Z","msg":"Found VA in map: variant=ms-inference-scheduling-llm-d-modelservice-decode, hasCurrentAlloc=true, accelerator=H100"}
{"level":"INFO","ts":"2025-11-28T20:55:52.196Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 1, desired-replicas: 1, accelerator: H100"}
{"level":"INFO","ts":"2025-11-28T20:55:52.196Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=1, accelerator=H100, capacityOnly=true"}
{"level":"INFO","ts":"2025-11-28T20:55:52.201Z","msg":"Applied capacity decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1, target=1, reason=capacity-only mode: no-change"}
{"level":"INFO","ts":"2025-11-28T20:55:52.201Z","msg":"Reconciliation completed successfully: mode=capacity-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-11-28T20:56:21.907Z","msg":"Operating in CAPACITY-ONLY mode: reactive capacity-based scaling only"}
{"level":"INFO","ts":"2025-11-28T20:56:21.908Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-11-28T20:56:21.908Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-11-28T20:56:21.920Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=1, accelerator=H100, ttft=0.00ms, itl=0.00ms, cost=10.00, arrivalRate=0.00"}
{"level":"INFO","ts":"2025-11-28T20:56:21.922Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9l76md, usage=0.000 (0.0%)"}
{"level":"DEBUG","ts":"2025-11-28T20:56:21.922Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"INFO","ts":"2025-11-28T20:56:21.922Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9l76md, queueLength=0"}
{"level":"DEBUG","ts":"2025-11-28T20:56:21.922Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"DEBUG","ts":"2025-11-28T20:56:21.922Z","msg":"Pod-to-variant matching successful: totalPods=1, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"DEBUG","ts":"2025-11-28T20:56:21.922Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=1"}
{"level":"DEBUG","ts":"2025-11-28T20:56:21.922Z","msg":"Collected capacity metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, metricsCount=1"}
{"level":"DEBUG","ts":"2025-11-28T20:56:21.922Z","msg":"Variant analysis initialized: variant=ms-inference-scheduling-llm-d-modelservice-decode, accelerator=H100, cost=10.00, replicaCount=1"}
{"level":"DEBUG","ts":"2025-11-28T20:56:21.922Z","msg":"Scale-down unsafe: insufficient non-saturated replicas: nonSaturated=1, required=2"}
{"level":"DEBUG","ts":"2025-11-28T20:56:21.922Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, totalReplicas=1, nonSaturated=1, avgSpareKv=0.500, avgSpareQueue=5.0, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"INFO","ts":"2025-11-28T20:56:21.922Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, totalReplicas=1, nonSaturated=1, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"DEBUG","ts":"2025-11-28T20:56:21.922Z","msg":"Capacity targets: no scaling needed (avgSpareKv=0.500, avgSpareQueue=5.0, all variants stable)"}
{"level":"DEBUG","ts":"2025-11-28T20:56:21.922Z","msg":"Capacity targets calculated: modelID=unsloth/Meta-Llama-3.1-8B, targets=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"INFO","ts":"2025-11-28T20:56:21.922Z","msg":"Capacity-only decisions made for model: unsloth/Meta-Llama-3.1-8B - decision count: 1"}
{"level":"INFO","ts":"2025-11-28T20:56:21.922Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-11-28T20:56:21.922Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1→target=1"}
{"level":"DEBUG","ts":"2025-11-28T20:56:21.922Z","msg":"Found VA in map: variant=ms-inference-scheduling-llm-d-modelservice-decode, hasCurrentAlloc=true, accelerator=H100"}
{"level":"INFO","ts":"2025-11-28T20:56:21.922Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 1, desired-replicas: 1, accelerator: H100"}
{"level":"INFO","ts":"2025-11-28T20:56:21.922Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=1, accelerator=H100, capacityOnly=true"}
{"level":"INFO","ts":"2025-11-28T20:56:21.927Z","msg":"Applied capacity decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1, target=1, reason=capacity-only mode: no-change"}
{"level":"INFO","ts":"2025-11-28T20:56:21.927Z","msg":"Reconciliation completed successfully: mode=capacity-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-11-28T20:56:22.201Z","msg":"Operating in CAPACITY-ONLY mode: reactive capacity-based scaling only"}
{"level":"INFO","ts":"2025-11-28T20:56:22.201Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-11-28T20:56:22.201Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-11-28T20:56:22.212Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=1, accelerator=H100, ttft=0.00ms, itl=0.00ms, cost=10.00, arrivalRate=0.00"}
{"level":"INFO","ts":"2025-11-28T20:56:22.213Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9l76md, usage=0.000 (0.0%)"}
{"level":"DEBUG","ts":"2025-11-28T20:56:22.213Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"INFO","ts":"2025-11-28T20:56:22.213Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9l76md, queueLength=0"}
{"level":"DEBUG","ts":"2025-11-28T20:56:22.213Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"DEBUG","ts":"2025-11-28T20:56:22.214Z","msg":"Pod-to-variant matching successful: totalPods=1, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"DEBUG","ts":"2025-11-28T20:56:22.214Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=1"}
{"level":"DEBUG","ts":"2025-11-28T20:56:22.214Z","msg":"Collected capacity metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, metricsCount=1"}
{"level":"DEBUG","ts":"2025-11-28T20:56:22.214Z","msg":"Variant analysis initialized: variant=ms-inference-scheduling-llm-d-modelservice-decode, accelerator=H100, cost=10.00, replicaCount=1"}
{"level":"DEBUG","ts":"2025-11-28T20:56:22.214Z","msg":"Scale-down unsafe: insufficient non-saturated replicas: nonSaturated=1, required=2"}
{"level":"DEBUG","ts":"2025-11-28T20:56:22.214Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, totalReplicas=1, nonSaturated=1, avgSpareKv=0.500, avgSpareQueue=5.0, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"INFO","ts":"2025-11-28T20:56:22.214Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, totalReplicas=1, nonSaturated=1, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"DEBUG","ts":"2025-11-28T20:56:22.214Z","msg":"Capacity targets: no scaling needed (avgSpareKv=0.500, avgSpareQueue=5.0, all variants stable)"}
{"level":"DEBUG","ts":"2025-11-28T20:56:22.214Z","msg":"Capacity targets calculated: modelID=unsloth/Meta-Llama-3.1-8B, targets=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"INFO","ts":"2025-11-28T20:56:22.214Z","msg":"Capacity-only decisions made for model: unsloth/Meta-Llama-3.1-8B - decision count: 1"}
{"level":"INFO","ts":"2025-11-28T20:56:22.214Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-11-28T20:56:22.214Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1→target=1"}
{"level":"DEBUG","ts":"2025-11-28T20:56:22.214Z","msg":"Found VA in map: variant=ms-inference-scheduling-llm-d-modelservice-decode, hasCurrentAlloc=true, accelerator=H100"}
{"level":"INFO","ts":"2025-11-28T20:56:22.214Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 1, desired-replicas: 1, accelerator: H100"}
{"level":"INFO","ts":"2025-11-28T20:56:22.214Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=1, accelerator=H100, capacityOnly=true"}
{"level":"INFO","ts":"2025-11-28T20:56:22.219Z","msg":"Applied capacity decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1, target=1, reason=capacity-only mode: no-change"}
{"level":"INFO","ts":"2025-11-28T20:56:22.219Z","msg":"Reconciliation completed successfully: mode=capacity-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-11-28T20:56:51.927Z","msg":"Operating in CAPACITY-ONLY mode: reactive capacity-based scaling only"}
{"level":"INFO","ts":"2025-11-28T20:56:51.927Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-11-28T20:56:51.927Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-11-28T20:56:51.948Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=1, accelerator=H100, ttft=0.00ms, itl=0.00ms, cost=10.00, arrivalRate=0.00"}
{"level":"INFO","ts":"2025-11-28T20:56:51.949Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9l76md, queueLength=0"}
{"level":"DEBUG","ts":"2025-11-28T20:56:51.949Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"INFO","ts":"2025-11-28T20:56:51.950Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9l76md, usage=0.000 (0.0%)"}
{"level":"DEBUG","ts":"2025-11-28T20:56:51.950Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"DEBUG","ts":"2025-11-28T20:56:51.950Z","msg":"Pod-to-variant matching successful: totalPods=1, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"DEBUG","ts":"2025-11-28T20:56:51.950Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=1"}
{"level":"DEBUG","ts":"2025-11-28T20:56:51.950Z","msg":"Collected capacity metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, metricsCount=1"}
{"level":"DEBUG","ts":"2025-11-28T20:56:51.950Z","msg":"Variant analysis initialized: variant=ms-inference-scheduling-llm-d-modelservice-decode, accelerator=H100, cost=10.00, replicaCount=1"}
{"level":"DEBUG","ts":"2025-11-28T20:56:51.950Z","msg":"Scale-down unsafe: insufficient non-saturated replicas: nonSaturated=1, required=2"}
{"level":"DEBUG","ts":"2025-11-28T20:56:51.950Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, totalReplicas=1, nonSaturated=1, avgSpareKv=0.500, avgSpareQueue=5.0, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"INFO","ts":"2025-11-28T20:56:51.950Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, totalReplicas=1, nonSaturated=1, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"DEBUG","ts":"2025-11-28T20:56:51.950Z","msg":"Capacity targets: no scaling needed (avgSpareKv=0.500, avgSpareQueue=5.0, all variants stable)"}
{"level":"DEBUG","ts":"2025-11-28T20:56:51.950Z","msg":"Capacity targets calculated: modelID=unsloth/Meta-Llama-3.1-8B, targets=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"INFO","ts":"2025-11-28T20:56:51.950Z","msg":"Capacity-only decisions made for model: unsloth/Meta-Llama-3.1-8B - decision count: 1"}
{"level":"INFO","ts":"2025-11-28T20:56:51.950Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-11-28T20:56:51.950Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1→target=1"}
{"level":"DEBUG","ts":"2025-11-28T20:56:51.950Z","msg":"Found VA in map: variant=ms-inference-scheduling-llm-d-modelservice-decode, hasCurrentAlloc=true, accelerator=H100"}
{"level":"INFO","ts":"2025-11-28T20:56:51.950Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 1, desired-replicas: 1, accelerator: H100"}
{"level":"INFO","ts":"2025-11-28T20:56:51.950Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=1, accelerator=H100, capacityOnly=true"}
{"level":"INFO","ts":"2025-11-28T20:56:51.954Z","msg":"Applied capacity decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1, target=1, reason=capacity-only mode: no-change"}
{"level":"INFO","ts":"2025-11-28T20:56:51.954Z","msg":"Reconciliation completed successfully: mode=capacity-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-11-28T20:56:52.220Z","msg":"Operating in CAPACITY-ONLY mode: reactive capacity-based scaling only"}
{"level":"INFO","ts":"2025-11-28T20:56:52.220Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-11-28T20:56:52.220Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-11-28T20:56:52.231Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=1, accelerator=H100, ttft=0.00ms, itl=0.00ms, cost=10.00, arrivalRate=0.00"}
{"level":"INFO","ts":"2025-11-28T20:56:52.233Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9l76md, queueLength=0"}
{"level":"DEBUG","ts":"2025-11-28T20:56:52.233Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"INFO","ts":"2025-11-28T20:56:52.233Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9l76md, usage=0.000 (0.0%)"}
{"level":"DEBUG","ts":"2025-11-28T20:56:52.233Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"DEBUG","ts":"2025-11-28T20:56:52.233Z","msg":"Pod-to-variant matching successful: totalPods=1, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"DEBUG","ts":"2025-11-28T20:56:52.233Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=1"}
{"level":"DEBUG","ts":"2025-11-28T20:56:52.233Z","msg":"Collected capacity metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, metricsCount=1"}
{"level":"DEBUG","ts":"2025-11-28T20:56:52.233Z","msg":"Variant analysis initialized: variant=ms-inference-scheduling-llm-d-modelservice-decode, accelerator=H100, cost=10.00, replicaCount=1"}
{"level":"DEBUG","ts":"2025-11-28T20:56:52.233Z","msg":"Scale-down unsafe: insufficient non-saturated replicas: nonSaturated=1, required=2"}
{"level":"DEBUG","ts":"2025-11-28T20:56:52.233Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, totalReplicas=1, nonSaturated=1, avgSpareKv=0.500, avgSpareQueue=5.0, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"INFO","ts":"2025-11-28T20:56:52.233Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, totalReplicas=1, nonSaturated=1, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"DEBUG","ts":"2025-11-28T20:56:52.233Z","msg":"Capacity targets: no scaling needed (avgSpareKv=0.500, avgSpareQueue=5.0, all variants stable)"}
{"level":"DEBUG","ts":"2025-11-28T20:56:52.233Z","msg":"Capacity targets calculated: modelID=unsloth/Meta-Llama-3.1-8B, targets=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"INFO","ts":"2025-11-28T20:56:52.233Z","msg":"Capacity-only decisions made for model: unsloth/Meta-Llama-3.1-8B - decision count: 1"}
{"level":"INFO","ts":"2025-11-28T20:56:52.233Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-11-28T20:56:52.233Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1→target=1"}
{"level":"DEBUG","ts":"2025-11-28T20:56:52.233Z","msg":"Found VA in map: variant=ms-inference-scheduling-llm-d-modelservice-decode, hasCurrentAlloc=true, accelerator=H100"}
{"level":"INFO","ts":"2025-11-28T20:56:52.233Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 1, desired-replicas: 1, accelerator: H100"}
{"level":"INFO","ts":"2025-11-28T20:56:52.233Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=1, accelerator=H100, capacityOnly=true"}
{"level":"INFO","ts":"2025-11-28T20:56:52.239Z","msg":"Applied capacity decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1, target=1, reason=capacity-only mode: no-change"}
{"level":"INFO","ts":"2025-11-28T20:56:52.239Z","msg":"Reconciliation completed successfully: mode=capacity-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-11-28T20:57:21.955Z","msg":"Operating in CAPACITY-ONLY mode: reactive capacity-based scaling only"}
{"level":"INFO","ts":"2025-11-28T20:57:21.955Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-11-28T20:57:21.955Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-11-28T20:57:21.980Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=1, accelerator=H100, ttft=0.00ms, itl=0.00ms, cost=10.00, arrivalRate=0.00"}
{"level":"INFO","ts":"2025-11-28T20:57:21.982Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9l76md, queueLength=0"}
{"level":"DEBUG","ts":"2025-11-28T20:57:21.982Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"INFO","ts":"2025-11-28T20:57:21.982Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9l76md, usage=0.000 (0.0%)"}
{"level":"DEBUG","ts":"2025-11-28T20:57:21.982Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"DEBUG","ts":"2025-11-28T20:57:21.982Z","msg":"Pod-to-variant matching successful: totalPods=1, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"DEBUG","ts":"2025-11-28T20:57:21.982Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=1"}
{"level":"DEBUG","ts":"2025-11-28T20:57:21.982Z","msg":"Collected capacity metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, metricsCount=1"}
{"level":"DEBUG","ts":"2025-11-28T20:57:21.982Z","msg":"Variant analysis initialized: variant=ms-inference-scheduling-llm-d-modelservice-decode, accelerator=H100, cost=10.00, replicaCount=1"}
{"level":"DEBUG","ts":"2025-11-28T20:57:21.982Z","msg":"Scale-down unsafe: insufficient non-saturated replicas: nonSaturated=1, required=2"}
{"level":"DEBUG","ts":"2025-11-28T20:57:21.982Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, totalReplicas=1, nonSaturated=1, avgSpareKv=0.500, avgSpareQueue=5.0, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"INFO","ts":"2025-11-28T20:57:21.982Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, totalReplicas=1, nonSaturated=1, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"DEBUG","ts":"2025-11-28T20:57:21.982Z","msg":"Capacity targets: no scaling needed (avgSpareKv=0.500, avgSpareQueue=5.0, all variants stable)"}
{"level":"DEBUG","ts":"2025-11-28T20:57:21.982Z","msg":"Capacity targets calculated: modelID=unsloth/Meta-Llama-3.1-8B, targets=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"INFO","ts":"2025-11-28T20:57:21.982Z","msg":"Capacity-only decisions made for model: unsloth/Meta-Llama-3.1-8B - decision count: 1"}
{"level":"INFO","ts":"2025-11-28T20:57:21.982Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-11-28T20:57:21.982Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1→target=1"}
{"level":"DEBUG","ts":"2025-11-28T20:57:21.982Z","msg":"Found VA in map: variant=ms-inference-scheduling-llm-d-modelservice-decode, hasCurrentAlloc=true, accelerator=H100"}
{"level":"INFO","ts":"2025-11-28T20:57:21.982Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 1, desired-replicas: 1, accelerator: H100"}
{"level":"INFO","ts":"2025-11-28T20:57:21.982Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=1, accelerator=H100, capacityOnly=true"}
{"level":"INFO","ts":"2025-11-28T20:57:21.987Z","msg":"Applied capacity decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1, target=1, reason=capacity-only mode: no-change"}
{"level":"INFO","ts":"2025-11-28T20:57:21.987Z","msg":"Reconciliation completed successfully: mode=capacity-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-11-28T20:57:22.243Z","msg":"Operating in CAPACITY-ONLY mode: reactive capacity-based scaling only"}
{"level":"INFO","ts":"2025-11-28T20:57:22.243Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-11-28T20:57:22.243Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-11-28T20:57:22.280Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=1, accelerator=H100, ttft=0.00ms, itl=0.00ms, cost=10.00, arrivalRate=0.00"}
{"level":"INFO","ts":"2025-11-28T20:57:22.282Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9l76md, queueLength=0"}
{"level":"DEBUG","ts":"2025-11-28T20:57:22.282Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"INFO","ts":"2025-11-28T20:57:22.282Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9l76md, usage=0.000 (0.0%)"}
{"level":"DEBUG","ts":"2025-11-28T20:57:22.282Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"DEBUG","ts":"2025-11-28T20:57:22.282Z","msg":"Pod-to-variant matching successful: totalPods=1, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"DEBUG","ts":"2025-11-28T20:57:22.282Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=1"}
{"level":"DEBUG","ts":"2025-11-28T20:57:22.282Z","msg":"Collected capacity metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, metricsCount=1"}
{"level":"DEBUG","ts":"2025-11-28T20:57:22.282Z","msg":"Variant analysis initialized: variant=ms-inference-scheduling-llm-d-modelservice-decode, accelerator=H100, cost=10.00, replicaCount=1"}
{"level":"DEBUG","ts":"2025-11-28T20:57:22.282Z","msg":"Scale-down unsafe: insufficient non-saturated replicas: nonSaturated=1, required=2"}
{"level":"DEBUG","ts":"2025-11-28T20:57:22.282Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, totalReplicas=1, nonSaturated=1, avgSpareKv=0.500, avgSpareQueue=5.0, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"INFO","ts":"2025-11-28T20:57:22.282Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, totalReplicas=1, nonSaturated=1, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"DEBUG","ts":"2025-11-28T20:57:22.282Z","msg":"Capacity targets: no scaling needed (avgSpareKv=0.500, avgSpareQueue=5.0, all variants stable)"}
{"level":"DEBUG","ts":"2025-11-28T20:57:22.282Z","msg":"Capacity targets calculated: modelID=unsloth/Meta-Llama-3.1-8B, targets=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"INFO","ts":"2025-11-28T20:57:22.282Z","msg":"Capacity-only decisions made for model: unsloth/Meta-Llama-3.1-8B - decision count: 1"}
{"level":"INFO","ts":"2025-11-28T20:57:22.282Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-11-28T20:57:22.282Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1→target=1"}
{"level":"DEBUG","ts":"2025-11-28T20:57:22.282Z","msg":"Found VA in map: variant=ms-inference-scheduling-llm-d-modelservice-decode, hasCurrentAlloc=true, accelerator=H100"}
{"level":"INFO","ts":"2025-11-28T20:57:22.282Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 1, desired-replicas: 1, accelerator: H100"}
{"level":"INFO","ts":"2025-11-28T20:57:22.282Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=1, accelerator=H100, capacityOnly=true"}
{"level":"INFO","ts":"2025-11-28T20:57:22.287Z","msg":"Applied capacity decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1, target=1, reason=capacity-only mode: no-change"}
{"level":"INFO","ts":"2025-11-28T20:57:22.287Z","msg":"Reconciliation completed successfully: mode=capacity-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-11-28T20:57:51.988Z","msg":"Operating in CAPACITY-ONLY mode: reactive capacity-based scaling only"}
{"level":"INFO","ts":"2025-11-28T20:57:51.988Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-11-28T20:57:51.988Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-11-28T20:57:52.001Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=1, accelerator=H100, ttft=0.00ms, itl=0.00ms, cost=10.00, arrivalRate=0.00"}
{"level":"INFO","ts":"2025-11-28T20:57:52.003Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9l76md, usage=0.000 (0.0%)"}
{"level":"DEBUG","ts":"2025-11-28T20:57:52.003Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"INFO","ts":"2025-11-28T20:57:52.003Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9l76md, queueLength=0"}
{"level":"DEBUG","ts":"2025-11-28T20:57:52.003Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"DEBUG","ts":"2025-11-28T20:57:52.003Z","msg":"Pod-to-variant matching successful: totalPods=1, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"DEBUG","ts":"2025-11-28T20:57:52.003Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=1"}
{"level":"DEBUG","ts":"2025-11-28T20:57:52.003Z","msg":"Collected capacity metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, metricsCount=1"}
{"level":"DEBUG","ts":"2025-11-28T20:57:52.003Z","msg":"Variant analysis initialized: variant=ms-inference-scheduling-llm-d-modelservice-decode, accelerator=H100, cost=10.00, replicaCount=1"}
{"level":"DEBUG","ts":"2025-11-28T20:57:52.003Z","msg":"Scale-down unsafe: insufficient non-saturated replicas: nonSaturated=1, required=2"}
{"level":"DEBUG","ts":"2025-11-28T20:57:52.003Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, totalReplicas=1, nonSaturated=1, avgSpareKv=0.500, avgSpareQueue=5.0, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"INFO","ts":"2025-11-28T20:57:52.003Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, totalReplicas=1, nonSaturated=1, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"DEBUG","ts":"2025-11-28T20:57:52.003Z","msg":"Capacity targets: no scaling needed (avgSpareKv=0.500, avgSpareQueue=5.0, all variants stable)"}
{"level":"DEBUG","ts":"2025-11-28T20:57:52.003Z","msg":"Capacity targets calculated: modelID=unsloth/Meta-Llama-3.1-8B, targets=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"INFO","ts":"2025-11-28T20:57:52.003Z","msg":"Capacity-only decisions made for model: unsloth/Meta-Llama-3.1-8B - decision count: 1"}
{"level":"INFO","ts":"2025-11-28T20:57:52.003Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-11-28T20:57:52.003Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1→target=1"}
{"level":"DEBUG","ts":"2025-11-28T20:57:52.003Z","msg":"Found VA in map: variant=ms-inference-scheduling-llm-d-modelservice-decode, hasCurrentAlloc=true, accelerator=H100"}
{"level":"INFO","ts":"2025-11-28T20:57:52.003Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 1, desired-replicas: 1, accelerator: H100"}
{"level":"INFO","ts":"2025-11-28T20:57:52.003Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=1, accelerator=H100, capacityOnly=true"}
{"level":"INFO","ts":"2025-11-28T20:57:52.009Z","msg":"Applied capacity decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1, target=1, reason=capacity-only mode: no-change"}
{"level":"INFO","ts":"2025-11-28T20:57:52.009Z","msg":"Reconciliation completed successfully: mode=capacity-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-11-28T20:57:52.288Z","msg":"Operating in CAPACITY-ONLY mode: reactive capacity-based scaling only"}
{"level":"INFO","ts":"2025-11-28T20:57:52.288Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-11-28T20:57:52.288Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-11-28T20:57:52.299Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=1, accelerator=H100, ttft=0.00ms, itl=0.00ms, cost=10.00, arrivalRate=0.00"}
{"level":"INFO","ts":"2025-11-28T20:57:52.301Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9l76md, queueLength=0"}
{"level":"DEBUG","ts":"2025-11-28T20:57:52.301Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"INFO","ts":"2025-11-28T20:57:52.301Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9l76md, usage=0.000 (0.0%)"}
{"level":"DEBUG","ts":"2025-11-28T20:57:52.301Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"DEBUG","ts":"2025-11-28T20:57:52.301Z","msg":"Pod-to-variant matching successful: totalPods=1, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"DEBUG","ts":"2025-11-28T20:57:52.301Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=1"}
{"level":"DEBUG","ts":"2025-11-28T20:57:52.301Z","msg":"Collected capacity metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, metricsCount=1"}
{"level":"DEBUG","ts":"2025-11-28T20:57:52.301Z","msg":"Variant analysis initialized: variant=ms-inference-scheduling-llm-d-modelservice-decode, accelerator=H100, cost=10.00, replicaCount=1"}
{"level":"DEBUG","ts":"2025-11-28T20:57:52.301Z","msg":"Scale-down unsafe: insufficient non-saturated replicas: nonSaturated=1, required=2"}
{"level":"DEBUG","ts":"2025-11-28T20:57:52.301Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, totalReplicas=1, nonSaturated=1, avgSpareKv=0.500, avgSpareQueue=5.0, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"INFO","ts":"2025-11-28T20:57:52.301Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, totalReplicas=1, nonSaturated=1, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"DEBUG","ts":"2025-11-28T20:57:52.301Z","msg":"Capacity targets: no scaling needed (avgSpareKv=0.500, avgSpareQueue=5.0, all variants stable)"}
{"level":"DEBUG","ts":"2025-11-28T20:57:52.301Z","msg":"Capacity targets calculated: modelID=unsloth/Meta-Llama-3.1-8B, targets=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"INFO","ts":"2025-11-28T20:57:52.301Z","msg":"Capacity-only decisions made for model: unsloth/Meta-Llama-3.1-8B - decision count: 1"}
{"level":"INFO","ts":"2025-11-28T20:57:52.301Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-11-28T20:57:52.301Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1→target=1"}
{"level":"DEBUG","ts":"2025-11-28T20:57:52.301Z","msg":"Found VA in map: variant=ms-inference-scheduling-llm-d-modelservice-decode, hasCurrentAlloc=true, accelerator=H100"}
{"level":"INFO","ts":"2025-11-28T20:57:52.301Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 1, desired-replicas: 1, accelerator: H100"}
{"level":"INFO","ts":"2025-11-28T20:57:52.301Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=1, accelerator=H100, capacityOnly=true"}
{"level":"INFO","ts":"2025-11-28T20:57:52.306Z","msg":"Applied capacity decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1, target=1, reason=capacity-only mode: no-change"}
{"level":"INFO","ts":"2025-11-28T20:57:52.306Z","msg":"Reconciliation completed successfully: mode=capacity-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-11-28T20:58:22.009Z","msg":"Operating in CAPACITY-ONLY mode: reactive capacity-based scaling only"}
{"level":"INFO","ts":"2025-11-28T20:58:22.009Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-11-28T20:58:22.009Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-11-28T20:58:22.022Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=1, accelerator=H100, ttft=19.47ms, itl=9.09ms, cost=10.00, arrivalRate=76.00"}
{"level":"INFO","ts":"2025-11-28T20:58:22.024Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9l76md, queueLength=0"}
{"level":"DEBUG","ts":"2025-11-28T20:58:22.024Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"INFO","ts":"2025-11-28T20:58:22.024Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9l76md, usage=0.026 (2.6%)"}
{"level":"DEBUG","ts":"2025-11-28T20:58:22.024Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"DEBUG","ts":"2025-11-28T20:58:22.024Z","msg":"Pod-to-variant matching successful: totalPods=1, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"DEBUG","ts":"2025-11-28T20:58:22.024Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=1"}
{"level":"DEBUG","ts":"2025-11-28T20:58:22.024Z","msg":"Collected capacity metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, metricsCount=1"}
{"level":"DEBUG","ts":"2025-11-28T20:58:22.024Z","msg":"Variant analysis initialized: variant=ms-inference-scheduling-llm-d-modelservice-decode, accelerator=H100, cost=10.00, replicaCount=1"}
{"level":"DEBUG","ts":"2025-11-28T20:58:22.024Z","msg":"Scale-down unsafe: insufficient non-saturated replicas: nonSaturated=1, required=2"}
{"level":"DEBUG","ts":"2025-11-28T20:58:22.024Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, totalReplicas=1, nonSaturated=1, avgSpareKv=0.474, avgSpareQueue=5.0, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"INFO","ts":"2025-11-28T20:58:22.024Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, totalReplicas=1, nonSaturated=1, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"DEBUG","ts":"2025-11-28T20:58:22.024Z","msg":"Capacity targets: no scaling needed (avgSpareKv=0.474, avgSpareQueue=5.0, all variants stable)"}
{"level":"DEBUG","ts":"2025-11-28T20:58:22.024Z","msg":"Capacity targets calculated: modelID=unsloth/Meta-Llama-3.1-8B, targets=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"INFO","ts":"2025-11-28T20:58:22.024Z","msg":"Capacity-only decisions made for model: unsloth/Meta-Llama-3.1-8B - decision count: 1"}
{"level":"INFO","ts":"2025-11-28T20:58:22.024Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-11-28T20:58:22.024Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1→target=1"}
{"level":"DEBUG","ts":"2025-11-28T20:58:22.024Z","msg":"Found VA in map: variant=ms-inference-scheduling-llm-d-modelservice-decode, hasCurrentAlloc=true, accelerator=H100"}
{"level":"INFO","ts":"2025-11-28T20:58:22.024Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 1, desired-replicas: 1, accelerator: H100"}
{"level":"INFO","ts":"2025-11-28T20:58:22.024Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=1, accelerator=H100, capacityOnly=true"}
{"level":"INFO","ts":"2025-11-28T20:58:22.029Z","msg":"Applied capacity decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1, target=1, reason=capacity-only mode: no-change"}
{"level":"INFO","ts":"2025-11-28T20:58:22.029Z","msg":"Reconciliation completed successfully: mode=capacity-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-11-28T20:58:22.306Z","msg":"Operating in CAPACITY-ONLY mode: reactive capacity-based scaling only"}
{"level":"INFO","ts":"2025-11-28T20:58:22.307Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-11-28T20:58:22.307Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-11-28T20:58:22.317Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=1, accelerator=H100, ttft=19.47ms, itl=9.09ms, cost=10.00, arrivalRate=76.00"}
{"level":"INFO","ts":"2025-11-28T20:58:22.319Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9l76md, queueLength=0"}
{"level":"DEBUG","ts":"2025-11-28T20:58:22.319Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"INFO","ts":"2025-11-28T20:58:22.319Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9l76md, usage=0.026 (2.6%)"}
{"level":"DEBUG","ts":"2025-11-28T20:58:22.319Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"DEBUG","ts":"2025-11-28T20:58:22.319Z","msg":"Pod-to-variant matching successful: totalPods=1, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"DEBUG","ts":"2025-11-28T20:58:22.319Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=1"}
{"level":"DEBUG","ts":"2025-11-28T20:58:22.319Z","msg":"Collected capacity metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, metricsCount=1"}
{"level":"DEBUG","ts":"2025-11-28T20:58:22.319Z","msg":"Variant analysis initialized: variant=ms-inference-scheduling-llm-d-modelservice-decode, accelerator=H100, cost=10.00, replicaCount=1"}
{"level":"DEBUG","ts":"2025-11-28T20:58:22.319Z","msg":"Scale-down unsafe: insufficient non-saturated replicas: nonSaturated=1, required=2"}
{"level":"DEBUG","ts":"2025-11-28T20:58:22.319Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, totalReplicas=1, nonSaturated=1, avgSpareKv=0.474, avgSpareQueue=5.0, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"INFO","ts":"2025-11-28T20:58:22.319Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, totalReplicas=1, nonSaturated=1, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"DEBUG","ts":"2025-11-28T20:58:22.319Z","msg":"Capacity targets: no scaling needed (avgSpareKv=0.474, avgSpareQueue=5.0, all variants stable)"}
{"level":"DEBUG","ts":"2025-11-28T20:58:22.319Z","msg":"Capacity targets calculated: modelID=unsloth/Meta-Llama-3.1-8B, targets=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"INFO","ts":"2025-11-28T20:58:22.319Z","msg":"Capacity-only decisions made for model: unsloth/Meta-Llama-3.1-8B - decision count: 1"}
{"level":"INFO","ts":"2025-11-28T20:58:22.319Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-11-28T20:58:22.319Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1→target=1"}
{"level":"DEBUG","ts":"2025-11-28T20:58:22.319Z","msg":"Found VA in map: variant=ms-inference-scheduling-llm-d-modelservice-decode, hasCurrentAlloc=true, accelerator=H100"}
{"level":"INFO","ts":"2025-11-28T20:58:22.319Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 1, desired-replicas: 1, accelerator: H100"}
{"level":"INFO","ts":"2025-11-28T20:58:22.319Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=1, accelerator=H100, capacityOnly=true"}
{"level":"INFO","ts":"2025-11-28T20:58:22.323Z","msg":"Applied capacity decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1, target=1, reason=capacity-only mode: no-change"}
{"level":"INFO","ts":"2025-11-28T20:58:22.324Z","msg":"Reconciliation completed successfully: mode=capacity-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-11-28T20:58:52.030Z","msg":"Operating in CAPACITY-ONLY mode: reactive capacity-based scaling only"}
{"level":"INFO","ts":"2025-11-28T20:58:52.030Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-11-28T20:58:52.030Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-11-28T20:58:52.049Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=1, accelerator=H100, ttft=28.97ms, itl=15.56ms, cost=10.00, arrivalRate=976.00"}
{"level":"INFO","ts":"2025-11-28T20:58:52.051Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9l76md, queueLength=0"}
{"level":"DEBUG","ts":"2025-11-28T20:58:52.051Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"INFO","ts":"2025-11-28T20:58:52.051Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9l76md, usage=0.212 (21.2%)"}
{"level":"DEBUG","ts":"2025-11-28T20:58:52.051Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"DEBUG","ts":"2025-11-28T20:58:52.051Z","msg":"Pod-to-variant matching successful: totalPods=1, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"DEBUG","ts":"2025-11-28T20:58:52.051Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=1"}
{"level":"DEBUG","ts":"2025-11-28T20:58:52.051Z","msg":"Collected capacity metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, metricsCount=1"}
{"level":"DEBUG","ts":"2025-11-28T20:58:52.051Z","msg":"Variant analysis initialized: variant=ms-inference-scheduling-llm-d-modelservice-decode, accelerator=H100, cost=10.00, replicaCount=1"}
{"level":"DEBUG","ts":"2025-11-28T20:58:52.051Z","msg":"Scale-down unsafe: insufficient non-saturated replicas: nonSaturated=1, required=2"}
{"level":"DEBUG","ts":"2025-11-28T20:58:52.051Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, totalReplicas=1, nonSaturated=1, avgSpareKv=0.288, avgSpareQueue=5.0, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"INFO","ts":"2025-11-28T20:58:52.051Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, totalReplicas=1, nonSaturated=1, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"DEBUG","ts":"2025-11-28T20:58:52.051Z","msg":"Capacity targets: no scaling needed (avgSpareKv=0.288, avgSpareQueue=5.0, all variants stable)"}
{"level":"DEBUG","ts":"2025-11-28T20:58:52.051Z","msg":"Capacity targets calculated: modelID=unsloth/Meta-Llama-3.1-8B, targets=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"INFO","ts":"2025-11-28T20:58:52.051Z","msg":"Capacity-only decisions made for model: unsloth/Meta-Llama-3.1-8B - decision count: 1"}
{"level":"INFO","ts":"2025-11-28T20:58:52.051Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-11-28T20:58:52.051Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1→target=1"}
{"level":"DEBUG","ts":"2025-11-28T20:58:52.051Z","msg":"Found VA in map: variant=ms-inference-scheduling-llm-d-modelservice-decode, hasCurrentAlloc=true, accelerator=H100"}
{"level":"INFO","ts":"2025-11-28T20:58:52.051Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 1, desired-replicas: 1, accelerator: H100"}
{"level":"INFO","ts":"2025-11-28T20:58:52.051Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=1, accelerator=H100, capacityOnly=true"}
{"level":"INFO","ts":"2025-11-28T20:58:52.056Z","msg":"Applied capacity decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1, target=1, reason=capacity-only mode: no-change"}
{"level":"INFO","ts":"2025-11-28T20:58:52.056Z","msg":"Reconciliation completed successfully: mode=capacity-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-11-28T20:58:52.324Z","msg":"Operating in CAPACITY-ONLY mode: reactive capacity-based scaling only"}
{"level":"INFO","ts":"2025-11-28T20:58:52.324Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-11-28T20:58:52.324Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-11-28T20:58:52.341Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=1, accelerator=H100, ttft=28.97ms, itl=15.56ms, cost=10.00, arrivalRate=976.00"}
{"level":"INFO","ts":"2025-11-28T20:58:52.342Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9l76md, usage=0.212 (21.2%)"}
{"level":"DEBUG","ts":"2025-11-28T20:58:52.342Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"INFO","ts":"2025-11-28T20:58:52.343Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9l76md, queueLength=0"}
{"level":"DEBUG","ts":"2025-11-28T20:58:52.343Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"DEBUG","ts":"2025-11-28T20:58:52.343Z","msg":"Pod-to-variant matching successful: totalPods=1, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"DEBUG","ts":"2025-11-28T20:58:52.343Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=1"}
{"level":"DEBUG","ts":"2025-11-28T20:58:52.343Z","msg":"Collected capacity metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, metricsCount=1"}
{"level":"DEBUG","ts":"2025-11-28T20:58:52.343Z","msg":"Variant analysis initialized: variant=ms-inference-scheduling-llm-d-modelservice-decode, accelerator=H100, cost=10.00, replicaCount=1"}
{"level":"DEBUG","ts":"2025-11-28T20:58:52.343Z","msg":"Scale-down unsafe: insufficient non-saturated replicas: nonSaturated=1, required=2"}
{"level":"DEBUG","ts":"2025-11-28T20:58:52.343Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, totalReplicas=1, nonSaturated=1, avgSpareKv=0.288, avgSpareQueue=5.0, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"INFO","ts":"2025-11-28T20:58:52.343Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, totalReplicas=1, nonSaturated=1, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"DEBUG","ts":"2025-11-28T20:58:52.343Z","msg":"Capacity targets: no scaling needed (avgSpareKv=0.288, avgSpareQueue=5.0, all variants stable)"}
{"level":"DEBUG","ts":"2025-11-28T20:58:52.343Z","msg":"Capacity targets calculated: modelID=unsloth/Meta-Llama-3.1-8B, targets=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"INFO","ts":"2025-11-28T20:58:52.343Z","msg":"Capacity-only decisions made for model: unsloth/Meta-Llama-3.1-8B - decision count: 1"}
{"level":"INFO","ts":"2025-11-28T20:58:52.343Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-11-28T20:58:52.343Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1→target=1"}
{"level":"DEBUG","ts":"2025-11-28T20:58:52.343Z","msg":"Found VA in map: variant=ms-inference-scheduling-llm-d-modelservice-decode, hasCurrentAlloc=true, accelerator=H100"}
{"level":"INFO","ts":"2025-11-28T20:58:52.343Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 1, desired-replicas: 1, accelerator: H100"}
{"level":"INFO","ts":"2025-11-28T20:58:52.343Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=1, accelerator=H100, capacityOnly=true"}
{"level":"INFO","ts":"2025-11-28T20:58:52.347Z","msg":"Applied capacity decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1, target=1, reason=capacity-only mode: no-change"}
{"level":"INFO","ts":"2025-11-28T20:58:52.347Z","msg":"Reconciliation completed successfully: mode=capacity-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-11-28T20:59:22.057Z","msg":"Operating in CAPACITY-ONLY mode: reactive capacity-based scaling only"}
{"level":"INFO","ts":"2025-11-28T20:59:22.057Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-11-28T20:59:22.057Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-11-28T20:59:22.073Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=1, accelerator=H100, ttft=31.54ms, itl=16.53ms, cost=10.00, arrivalRate=1210.00"}
{"level":"INFO","ts":"2025-11-28T20:59:22.075Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9l76md, queueLength=0"}
{"level":"DEBUG","ts":"2025-11-28T20:59:22.075Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"INFO","ts":"2025-11-28T20:59:22.076Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9l76md, usage=0.212 (21.2%)"}
{"level":"DEBUG","ts":"2025-11-28T20:59:22.076Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"DEBUG","ts":"2025-11-28T20:59:22.076Z","msg":"Pod-to-variant matching successful: totalPods=1, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"DEBUG","ts":"2025-11-28T20:59:22.076Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=1"}
{"level":"DEBUG","ts":"2025-11-28T20:59:22.076Z","msg":"Collected capacity metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, metricsCount=1"}
{"level":"DEBUG","ts":"2025-11-28T20:59:22.076Z","msg":"Variant analysis initialized: variant=ms-inference-scheduling-llm-d-modelservice-decode, accelerator=H100, cost=10.00, replicaCount=1"}
{"level":"DEBUG","ts":"2025-11-28T20:59:22.076Z","msg":"Scale-down unsafe: insufficient non-saturated replicas: nonSaturated=1, required=2"}
{"level":"DEBUG","ts":"2025-11-28T20:59:22.076Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, totalReplicas=1, nonSaturated=1, avgSpareKv=0.288, avgSpareQueue=5.0, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"INFO","ts":"2025-11-28T20:59:22.076Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, totalReplicas=1, nonSaturated=1, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"DEBUG","ts":"2025-11-28T20:59:22.076Z","msg":"Capacity targets: no scaling needed (avgSpareKv=0.288, avgSpareQueue=5.0, all variants stable)"}
{"level":"DEBUG","ts":"2025-11-28T20:59:22.076Z","msg":"Capacity targets calculated: modelID=unsloth/Meta-Llama-3.1-8B, targets=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"INFO","ts":"2025-11-28T20:59:22.076Z","msg":"Capacity-only decisions made for model: unsloth/Meta-Llama-3.1-8B - decision count: 1"}
{"level":"INFO","ts":"2025-11-28T20:59:22.076Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-11-28T20:59:22.076Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1→target=1"}
{"level":"DEBUG","ts":"2025-11-28T20:59:22.076Z","msg":"Found VA in map: variant=ms-inference-scheduling-llm-d-modelservice-decode, hasCurrentAlloc=true, accelerator=H100"}
{"level":"INFO","ts":"2025-11-28T20:59:22.076Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 1, desired-replicas: 1, accelerator: H100"}
{"level":"INFO","ts":"2025-11-28T20:59:22.076Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=1, accelerator=H100, capacityOnly=true"}
{"level":"INFO","ts":"2025-11-28T20:59:22.081Z","msg":"Applied capacity decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1, target=1, reason=capacity-only mode: no-change"}
{"level":"INFO","ts":"2025-11-28T20:59:22.081Z","msg":"Reconciliation completed successfully: mode=capacity-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-11-28T20:59:22.349Z","msg":"Operating in CAPACITY-ONLY mode: reactive capacity-based scaling only"}
{"level":"INFO","ts":"2025-11-28T20:59:22.358Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-11-28T20:59:22.358Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-11-28T20:59:22.376Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=1, accelerator=H100, ttft=31.54ms, itl=16.53ms, cost=10.00, arrivalRate=1210.00"}
{"level":"INFO","ts":"2025-11-28T20:59:22.377Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9l76md, queueLength=0"}
{"level":"DEBUG","ts":"2025-11-28T20:59:22.377Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"INFO","ts":"2025-11-28T20:59:22.378Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9l76md, usage=0.212 (21.2%)"}
{"level":"DEBUG","ts":"2025-11-28T20:59:22.378Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"DEBUG","ts":"2025-11-28T20:59:22.378Z","msg":"Pod-to-variant matching successful: totalPods=1, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"DEBUG","ts":"2025-11-28T20:59:22.378Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=1"}
{"level":"DEBUG","ts":"2025-11-28T20:59:22.378Z","msg":"Collected capacity metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, metricsCount=1"}
{"level":"DEBUG","ts":"2025-11-28T20:59:22.378Z","msg":"Variant analysis initialized: variant=ms-inference-scheduling-llm-d-modelservice-decode, accelerator=H100, cost=10.00, replicaCount=1"}
{"level":"DEBUG","ts":"2025-11-28T20:59:22.378Z","msg":"Scale-down unsafe: insufficient non-saturated replicas: nonSaturated=1, required=2"}
{"level":"DEBUG","ts":"2025-11-28T20:59:22.378Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, totalReplicas=1, nonSaturated=1, avgSpareKv=0.288, avgSpareQueue=5.0, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"INFO","ts":"2025-11-28T20:59:22.378Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, totalReplicas=1, nonSaturated=1, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"DEBUG","ts":"2025-11-28T20:59:22.378Z","msg":"Capacity targets: no scaling needed (avgSpareKv=0.288, avgSpareQueue=5.0, all variants stable)"}
{"level":"DEBUG","ts":"2025-11-28T20:59:22.378Z","msg":"Capacity targets calculated: modelID=unsloth/Meta-Llama-3.1-8B, targets=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"INFO","ts":"2025-11-28T20:59:22.378Z","msg":"Capacity-only decisions made for model: unsloth/Meta-Llama-3.1-8B - decision count: 1"}
{"level":"INFO","ts":"2025-11-28T20:59:22.378Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-11-28T20:59:22.378Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1→target=1"}
{"level":"DEBUG","ts":"2025-11-28T20:59:22.378Z","msg":"Found VA in map: variant=ms-inference-scheduling-llm-d-modelservice-decode, hasCurrentAlloc=true, accelerator=H100"}
{"level":"INFO","ts":"2025-11-28T20:59:22.378Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 1, desired-replicas: 1, accelerator: H100"}
{"level":"INFO","ts":"2025-11-28T20:59:22.378Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=1, accelerator=H100, capacityOnly=true"}
{"level":"INFO","ts":"2025-11-28T20:59:22.384Z","msg":"Applied capacity decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1, target=1, reason=capacity-only mode: no-change"}
{"level":"INFO","ts":"2025-11-28T20:59:22.384Z","msg":"Reconciliation completed successfully: mode=capacity-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-11-28T20:59:52.082Z","msg":"Operating in CAPACITY-ONLY mode: reactive capacity-based scaling only"}
{"level":"INFO","ts":"2025-11-28T20:59:52.082Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-11-28T20:59:52.082Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-11-28T20:59:52.101Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=1, accelerator=H100, ttft=33.07ms, itl=17.72ms, cost=10.00, arrivalRate=1214.00"}
{"level":"INFO","ts":"2025-11-28T20:59:52.103Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9l76md, queueLength=0"}
{"level":"DEBUG","ts":"2025-11-28T20:59:52.103Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"INFO","ts":"2025-11-28T20:59:52.103Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9l76md, usage=0.242 (24.2%)"}
{"level":"DEBUG","ts":"2025-11-28T20:59:52.103Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"DEBUG","ts":"2025-11-28T20:59:52.103Z","msg":"Pod-to-variant matching successful: totalPods=1, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"DEBUG","ts":"2025-11-28T20:59:52.103Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=1"}
{"level":"DEBUG","ts":"2025-11-28T20:59:52.103Z","msg":"Collected capacity metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, metricsCount=1"}
{"level":"DEBUG","ts":"2025-11-28T20:59:52.103Z","msg":"Variant analysis initialized: variant=ms-inference-scheduling-llm-d-modelservice-decode, accelerator=H100, cost=10.00, replicaCount=1"}
{"level":"DEBUG","ts":"2025-11-28T20:59:52.103Z","msg":"Scale-down unsafe: insufficient non-saturated replicas: nonSaturated=1, required=2"}
{"level":"DEBUG","ts":"2025-11-28T20:59:52.103Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, totalReplicas=1, nonSaturated=1, avgSpareKv=0.258, avgSpareQueue=5.0, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"INFO","ts":"2025-11-28T20:59:52.103Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, totalReplicas=1, nonSaturated=1, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"DEBUG","ts":"2025-11-28T20:59:52.103Z","msg":"Capacity targets: no scaling needed (avgSpareKv=0.258, avgSpareQueue=5.0, all variants stable)"}
{"level":"DEBUG","ts":"2025-11-28T20:59:52.103Z","msg":"Capacity targets calculated: modelID=unsloth/Meta-Llama-3.1-8B, targets=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"INFO","ts":"2025-11-28T20:59:52.103Z","msg":"Capacity-only decisions made for model: unsloth/Meta-Llama-3.1-8B - decision count: 1"}
{"level":"INFO","ts":"2025-11-28T20:59:52.103Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-11-28T20:59:52.103Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1→target=1"}
{"level":"DEBUG","ts":"2025-11-28T20:59:52.103Z","msg":"Found VA in map: variant=ms-inference-scheduling-llm-d-modelservice-decode, hasCurrentAlloc=true, accelerator=H100"}
{"level":"INFO","ts":"2025-11-28T20:59:52.103Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 1, desired-replicas: 1, accelerator: H100"}
{"level":"INFO","ts":"2025-11-28T20:59:52.103Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=1, accelerator=H100, capacityOnly=true"}
{"level":"INFO","ts":"2025-11-28T20:59:52.109Z","msg":"Applied capacity decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1, target=1, reason=capacity-only mode: no-change"}
{"level":"INFO","ts":"2025-11-28T20:59:52.109Z","msg":"Reconciliation completed successfully: mode=capacity-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-11-28T20:59:52.384Z","msg":"Operating in CAPACITY-ONLY mode: reactive capacity-based scaling only"}
{"level":"INFO","ts":"2025-11-28T20:59:52.384Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-11-28T20:59:52.384Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-11-28T20:59:52.395Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=1, accelerator=H100, ttft=33.07ms, itl=17.72ms, cost=10.00, arrivalRate=1214.00"}
{"level":"INFO","ts":"2025-11-28T20:59:52.397Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9l76md, usage=0.242 (24.2%)"}
{"level":"DEBUG","ts":"2025-11-28T20:59:52.397Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"INFO","ts":"2025-11-28T20:59:52.397Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9l76md, queueLength=0"}
{"level":"DEBUG","ts":"2025-11-28T20:59:52.397Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"DEBUG","ts":"2025-11-28T20:59:52.397Z","msg":"Pod-to-variant matching successful: totalPods=1, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"DEBUG","ts":"2025-11-28T20:59:52.397Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=1"}
{"level":"DEBUG","ts":"2025-11-28T20:59:52.397Z","msg":"Collected capacity metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, metricsCount=1"}
{"level":"DEBUG","ts":"2025-11-28T20:59:52.397Z","msg":"Variant analysis initialized: variant=ms-inference-scheduling-llm-d-modelservice-decode, accelerator=H100, cost=10.00, replicaCount=1"}
{"level":"DEBUG","ts":"2025-11-28T20:59:52.397Z","msg":"Scale-down unsafe: insufficient non-saturated replicas: nonSaturated=1, required=2"}
{"level":"DEBUG","ts":"2025-11-28T20:59:52.397Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, totalReplicas=1, nonSaturated=1, avgSpareKv=0.258, avgSpareQueue=5.0, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"INFO","ts":"2025-11-28T20:59:52.397Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, totalReplicas=1, nonSaturated=1, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"DEBUG","ts":"2025-11-28T20:59:52.397Z","msg":"Capacity targets: no scaling needed (avgSpareKv=0.258, avgSpareQueue=5.0, all variants stable)"}
{"level":"DEBUG","ts":"2025-11-28T20:59:52.397Z","msg":"Capacity targets calculated: modelID=unsloth/Meta-Llama-3.1-8B, targets=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"INFO","ts":"2025-11-28T20:59:52.397Z","msg":"Capacity-only decisions made for model: unsloth/Meta-Llama-3.1-8B - decision count: 1"}
{"level":"INFO","ts":"2025-11-28T20:59:52.397Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-11-28T20:59:52.397Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1→target=1"}
{"level":"DEBUG","ts":"2025-11-28T20:59:52.397Z","msg":"Found VA in map: variant=ms-inference-scheduling-llm-d-modelservice-decode, hasCurrentAlloc=true, accelerator=H100"}
{"level":"INFO","ts":"2025-11-28T20:59:52.397Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 1, desired-replicas: 1, accelerator: H100"}
{"level":"INFO","ts":"2025-11-28T20:59:52.397Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=1, accelerator=H100, capacityOnly=true"}
{"level":"INFO","ts":"2025-11-28T20:59:52.402Z","msg":"Applied capacity decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1, target=1, reason=capacity-only mode: no-change"}
{"level":"INFO","ts":"2025-11-28T20:59:52.402Z","msg":"Reconciliation completed successfully: mode=capacity-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-11-28T21:00:22.110Z","msg":"Operating in CAPACITY-ONLY mode: reactive capacity-based scaling only"}
{"level":"INFO","ts":"2025-11-28T21:00:22.110Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-11-28T21:00:22.110Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-11-28T21:00:22.133Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=1, accelerator=H100, ttft=40.12ms, itl=20.34ms, cost=10.00, arrivalRate=950.28"}
{"level":"INFO","ts":"2025-11-28T21:00:22.134Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9l76md, queueLength=0"}
{"level":"DEBUG","ts":"2025-11-28T21:00:22.134Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"INFO","ts":"2025-11-28T21:00:22.134Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9l76md, usage=0.336 (33.6%)"}
{"level":"DEBUG","ts":"2025-11-28T21:00:22.134Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"DEBUG","ts":"2025-11-28T21:00:22.134Z","msg":"Pod-to-variant matching successful: totalPods=1, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"DEBUG","ts":"2025-11-28T21:00:22.134Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=1"}
{"level":"DEBUG","ts":"2025-11-28T21:00:22.134Z","msg":"Collected capacity metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, metricsCount=1"}
{"level":"DEBUG","ts":"2025-11-28T21:00:22.134Z","msg":"Variant analysis initialized: variant=ms-inference-scheduling-llm-d-modelservice-decode, accelerator=H100, cost=10.00, replicaCount=1"}
{"level":"DEBUG","ts":"2025-11-28T21:00:22.134Z","msg":"Scale-down unsafe: insufficient non-saturated replicas: nonSaturated=1, required=2"}
{"level":"DEBUG","ts":"2025-11-28T21:00:22.134Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, totalReplicas=1, nonSaturated=1, avgSpareKv=0.164, avgSpareQueue=5.0, shouldScaleUp=true, scaleDownSafe=false"}
{"level":"INFO","ts":"2025-11-28T21:00:22.134Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, totalReplicas=1, nonSaturated=1, shouldScaleUp=true, scaleDownSafe=false"}
{"level":"INFO","ts":"2025-11-28T21:00:22.134Z","msg":"Capacity target: scale-up cheapest variant: variant=ms-inference-scheduling-llm-d-modelservice-decode, cost=10.00, currentReplicas=1, readyReplicas=1, target=2, reason=KV spare capacity low (0.164 < 0.200)"}
{"level":"DEBUG","ts":"2025-11-28T21:00:22.134Z","msg":"Capacity targets calculated: modelID=unsloth/Meta-Llama-3.1-8B, targets=map[ms-inference-scheduling-llm-d-modelservice-decode:2]"}
{"level":"INFO","ts":"2025-11-28T21:00:22.134Z","msg":"Capacity-only decisions made for model: unsloth/Meta-Llama-3.1-8B - decision count: 1"}
{"level":"INFO","ts":"2025-11-28T21:00:22.134Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-11-28T21:00:22.134Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=scale-up, current=1→target=2"}
{"level":"DEBUG","ts":"2025-11-28T21:00:22.134Z","msg":"Found VA in map: variant=ms-inference-scheduling-llm-d-modelservice-decode, hasCurrentAlloc=true, accelerator=H100"}
{"level":"INFO","ts":"2025-11-28T21:00:22.134Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 1, desired-replicas: 2, accelerator: H100"}
{"level":"INFO","ts":"2025-11-28T21:00:22.134Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=2, accelerator=H100, capacityOnly=true"}
{"level":"INFO","ts":"2025-11-28T21:00:22.139Z","msg":"Applied capacity decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=scale-up, current=1, target=2, reason=capacity-only mode: scale-up"}
{"level":"INFO","ts":"2025-11-28T21:00:22.139Z","msg":"Reconciliation completed successfully: mode=capacity-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-11-28T21:00:22.403Z","msg":"Operating in CAPACITY-ONLY mode: reactive capacity-based scaling only"}
{"level":"INFO","ts":"2025-11-28T21:00:22.403Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-11-28T21:00:22.403Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-11-28T21:00:22.413Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=1, accelerator=H100, ttft=40.12ms, itl=20.34ms, cost=10.00, arrivalRate=957.05"}
{"level":"INFO","ts":"2025-11-28T21:00:22.415Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9l76md, usage=0.336 (33.6%)"}
{"level":"DEBUG","ts":"2025-11-28T21:00:22.415Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"INFO","ts":"2025-11-28T21:00:22.415Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9l76md, queueLength=0"}
{"level":"DEBUG","ts":"2025-11-28T21:00:22.415Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"DEBUG","ts":"2025-11-28T21:00:22.415Z","msg":"Pod-to-variant matching successful: totalPods=1, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"DEBUG","ts":"2025-11-28T21:00:22.415Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=1"}
{"level":"DEBUG","ts":"2025-11-28T21:00:22.415Z","msg":"Collected capacity metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, metricsCount=1"}
{"level":"DEBUG","ts":"2025-11-28T21:00:22.415Z","msg":"Variant analysis initialized: variant=ms-inference-scheduling-llm-d-modelservice-decode, accelerator=H100, cost=10.00, replicaCount=1"}
{"level":"DEBUG","ts":"2025-11-28T21:00:22.415Z","msg":"Scale-down unsafe: insufficient non-saturated replicas: nonSaturated=1, required=2"}
{"level":"DEBUG","ts":"2025-11-28T21:00:22.415Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, totalReplicas=1, nonSaturated=1, avgSpareKv=0.164, avgSpareQueue=5.0, shouldScaleUp=true, scaleDownSafe=false"}
{"level":"INFO","ts":"2025-11-28T21:00:22.415Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, totalReplicas=1, nonSaturated=1, shouldScaleUp=true, scaleDownSafe=false"}
{"level":"DEBUG","ts":"2025-11-28T21:00:22.415Z","msg":"Preserving desired replicas: variant=ms-inference-scheduling-llm-d-modelservice-decode, currentReplicas=1, readyReplicas=1, desired=2"}
{"level":"DEBUG","ts":"2025-11-28T21:00:22.415Z","msg":"Capacity targets calculated: modelID=unsloth/Meta-Llama-3.1-8B, targets=map[ms-inference-scheduling-llm-d-modelservice-decode:2]"}
{"level":"INFO","ts":"2025-11-28T21:00:22.415Z","msg":"Capacity-only decisions made for model: unsloth/Meta-Llama-3.1-8B - decision count: 1"}
{"level":"INFO","ts":"2025-11-28T21:00:22.415Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-11-28T21:00:22.415Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=scale-up, current=1→target=2"}
{"level":"DEBUG","ts":"2025-11-28T21:00:22.415Z","msg":"Found VA in map: variant=ms-inference-scheduling-llm-d-modelservice-decode, hasCurrentAlloc=true, accelerator=H100"}
{"level":"INFO","ts":"2025-11-28T21:00:22.415Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 1, desired-replicas: 2, accelerator: H100"}
{"level":"INFO","ts":"2025-11-28T21:00:22.415Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=2, accelerator=H100, capacityOnly=true"}
{"level":"INFO","ts":"2025-11-28T21:00:22.420Z","msg":"Applied capacity decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=scale-up, current=1, target=2, reason=capacity-only mode: scale-up"}
{"level":"INFO","ts":"2025-11-28T21:00:22.420Z","msg":"Reconciliation completed successfully: mode=capacity-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-11-28T21:00:52.140Z","msg":"Operating in CAPACITY-ONLY mode: reactive capacity-based scaling only"}
{"level":"INFO","ts":"2025-11-28T21:00:52.141Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-11-28T21:00:52.141Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-11-28T21:00:52.160Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=2, accelerator=H100, ttft=72.75ms, itl=46.04ms, cost=20.00, arrivalRate=1458.00"}
{"level":"INFO","ts":"2025-11-28T21:00:52.162Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9l76md, usage=0.938 (93.8%)"}
{"level":"DEBUG","ts":"2025-11-28T21:00:52.162Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"INFO","ts":"2025-11-28T21:00:52.162Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9l76md, queueLength=0"}
{"level":"DEBUG","ts":"2025-11-28T21:00:52.162Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"DEBUG","ts":"2025-11-28T21:00:52.162Z","msg":"Pod-to-variant matching successful: totalPods=1, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"DEBUG","ts":"2025-11-28T21:00:52.162Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=1"}
{"level":"DEBUG","ts":"2025-11-28T21:00:52.162Z","msg":"Collected capacity metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, metricsCount=1"}
{"level":"DEBUG","ts":"2025-11-28T21:00:52.162Z","msg":"Variant analysis initialized: variant=ms-inference-scheduling-llm-d-modelservice-decode, accelerator=H100, cost=10.00, replicaCount=1"}
{"level":"DEBUG","ts":"2025-11-28T21:00:52.162Z","msg":"Scale-down unsafe: insufficient non-saturated replicas: nonSaturated=0, required=2"}
{"level":"DEBUG","ts":"2025-11-28T21:00:52.162Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, totalReplicas=1, nonSaturated=0, avgSpareKv=0.000, avgSpareQueue=0.0, shouldScaleUp=true, scaleDownSafe=false"}
{"level":"INFO","ts":"2025-11-28T21:00:52.162Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, totalReplicas=1, nonSaturated=0, shouldScaleUp=true, scaleDownSafe=false"}
{"level":"INFO","ts":"2025-11-28T21:00:52.162Z","msg":"Capacity target: scale-up cheapest variant: variant=ms-inference-scheduling-llm-d-modelservice-decode, cost=10.00, currentReplicas=2, readyReplicas=1, target=2, reason=both KV spare (0.000 < 0.200) and queue spare (0.0 < 3.0)"}
{"level":"DEBUG","ts":"2025-11-28T21:00:52.162Z","msg":"Capacity targets calculated: modelID=unsloth/Meta-Llama-3.1-8B, targets=map[ms-inference-scheduling-llm-d-modelservice-decode:2]"}
{"level":"INFO","ts":"2025-11-28T21:00:52.162Z","msg":"Capacity-only decisions made for model: unsloth/Meta-Llama-3.1-8B - decision count: 1"}
{"level":"INFO","ts":"2025-11-28T21:00:52.162Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-11-28T21:00:52.162Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=2→target=2"}
{"level":"DEBUG","ts":"2025-11-28T21:00:52.162Z","msg":"Found VA in map: variant=ms-inference-scheduling-llm-d-modelservice-decode, hasCurrentAlloc=true, accelerator=H100"}
{"level":"INFO","ts":"2025-11-28T21:00:52.162Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 2, desired-replicas: 2, accelerator: H100"}
{"level":"INFO","ts":"2025-11-28T21:00:52.162Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=2, accelerator=H100, capacityOnly=true"}
{"level":"INFO","ts":"2025-11-28T21:00:52.167Z","msg":"Applied capacity decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=2, target=2, reason=capacity-only mode: no-change"}
{"level":"INFO","ts":"2025-11-28T21:00:52.167Z","msg":"Reconciliation completed successfully: mode=capacity-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-11-28T21:00:52.420Z","msg":"Operating in CAPACITY-ONLY mode: reactive capacity-based scaling only"}
{"level":"INFO","ts":"2025-11-28T21:00:52.420Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-11-28T21:00:52.420Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-11-28T21:00:52.431Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=2, accelerator=H100, ttft=72.75ms, itl=46.04ms, cost=20.00, arrivalRate=1458.00"}
{"level":"INFO","ts":"2025-11-28T21:00:52.433Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9l76md, queueLength=0"}
{"level":"DEBUG","ts":"2025-11-28T21:00:52.433Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"INFO","ts":"2025-11-28T21:00:52.433Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9l76md, usage=0.938 (93.8%)"}
{"level":"DEBUG","ts":"2025-11-28T21:00:52.433Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"DEBUG","ts":"2025-11-28T21:00:52.433Z","msg":"Pod-to-variant matching successful: totalPods=1, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"DEBUG","ts":"2025-11-28T21:00:52.433Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=1"}
{"level":"DEBUG","ts":"2025-11-28T21:00:52.433Z","msg":"Collected capacity metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, metricsCount=1"}
{"level":"DEBUG","ts":"2025-11-28T21:00:52.433Z","msg":"Variant analysis initialized: variant=ms-inference-scheduling-llm-d-modelservice-decode, accelerator=H100, cost=10.00, replicaCount=1"}
{"level":"DEBUG","ts":"2025-11-28T21:00:52.433Z","msg":"Scale-down unsafe: insufficient non-saturated replicas: nonSaturated=0, required=2"}
{"level":"DEBUG","ts":"2025-11-28T21:00:52.433Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, totalReplicas=1, nonSaturated=0, avgSpareKv=0.000, avgSpareQueue=0.0, shouldScaleUp=true, scaleDownSafe=false"}
{"level":"INFO","ts":"2025-11-28T21:00:52.433Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, totalReplicas=1, nonSaturated=0, shouldScaleUp=true, scaleDownSafe=false"}
{"level":"INFO","ts":"2025-11-28T21:00:52.433Z","msg":"Capacity target: scale-up cheapest variant: variant=ms-inference-scheduling-llm-d-modelservice-decode, cost=10.00, currentReplicas=2, readyReplicas=1, target=2, reason=both KV spare (0.000 < 0.200) and queue spare (0.0 < 3.0)"}
{"level":"DEBUG","ts":"2025-11-28T21:00:52.433Z","msg":"Capacity targets calculated: modelID=unsloth/Meta-Llama-3.1-8B, targets=map[ms-inference-scheduling-llm-d-modelservice-decode:2]"}
{"level":"INFO","ts":"2025-11-28T21:00:52.433Z","msg":"Capacity-only decisions made for model: unsloth/Meta-Llama-3.1-8B - decision count: 1"}
{"level":"INFO","ts":"2025-11-28T21:00:52.433Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-11-28T21:00:52.433Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=2→target=2"}
{"level":"DEBUG","ts":"2025-11-28T21:00:52.433Z","msg":"Found VA in map: variant=ms-inference-scheduling-llm-d-modelservice-decode, hasCurrentAlloc=true, accelerator=H100"}
{"level":"INFO","ts":"2025-11-28T21:00:52.433Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 2, desired-replicas: 2, accelerator: H100"}
{"level":"INFO","ts":"2025-11-28T21:00:52.433Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=2, accelerator=H100, capacityOnly=true"}
{"level":"INFO","ts":"2025-11-28T21:00:52.437Z","msg":"Applied capacity decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=2, target=2, reason=capacity-only mode: no-change"}
{"level":"INFO","ts":"2025-11-28T21:00:52.437Z","msg":"Reconciliation completed successfully: mode=capacity-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-11-28T21:01:22.168Z","msg":"Operating in CAPACITY-ONLY mode: reactive capacity-based scaling only"}
{"level":"INFO","ts":"2025-11-28T21:01:22.169Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-11-28T21:01:22.169Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-11-28T21:01:22.220Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=2, accelerator=H100, ttft=162.53ms, itl=55.03ms, cost=20.00, arrivalRate=1416.00"}
{"level":"INFO","ts":"2025-11-28T21:01:22.222Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9l76md, queueLength=0"}
{"level":"DEBUG","ts":"2025-11-28T21:01:22.222Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"INFO","ts":"2025-11-28T21:01:22.222Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9l76md, usage=0.938 (93.8%)"}
{"level":"DEBUG","ts":"2025-11-28T21:01:22.222Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"DEBUG","ts":"2025-11-28T21:01:22.222Z","msg":"Pod-to-variant matching successful: totalPods=1, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"DEBUG","ts":"2025-11-28T21:01:22.222Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=1"}
{"level":"DEBUG","ts":"2025-11-28T21:01:22.222Z","msg":"Collected capacity metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, metricsCount=1"}
{"level":"DEBUG","ts":"2025-11-28T21:01:22.222Z","msg":"Variant analysis initialized: variant=ms-inference-scheduling-llm-d-modelservice-decode, accelerator=H100, cost=10.00, replicaCount=1"}
{"level":"DEBUG","ts":"2025-11-28T21:01:22.222Z","msg":"Scale-down unsafe: insufficient non-saturated replicas: nonSaturated=0, required=2"}
{"level":"DEBUG","ts":"2025-11-28T21:01:22.222Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, totalReplicas=1, nonSaturated=0, avgSpareKv=0.000, avgSpareQueue=0.0, shouldScaleUp=true, scaleDownSafe=false"}
{"level":"INFO","ts":"2025-11-28T21:01:22.222Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, totalReplicas=1, nonSaturated=0, shouldScaleUp=true, scaleDownSafe=false"}
{"level":"INFO","ts":"2025-11-28T21:01:22.222Z","msg":"Capacity target: scale-up cheapest variant: variant=ms-inference-scheduling-llm-d-modelservice-decode, cost=10.00, currentReplicas=2, readyReplicas=1, target=2, reason=both KV spare (0.000 < 0.200) and queue spare (0.0 < 3.0)"}
{"level":"DEBUG","ts":"2025-11-28T21:01:22.223Z","msg":"Capacity targets calculated: modelID=unsloth/Meta-Llama-3.1-8B, targets=map[ms-inference-scheduling-llm-d-modelservice-decode:2]"}
{"level":"INFO","ts":"2025-11-28T21:01:22.223Z","msg":"Capacity-only decisions made for model: unsloth/Meta-Llama-3.1-8B - decision count: 1"}
{"level":"INFO","ts":"2025-11-28T21:01:22.223Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-11-28T21:01:22.223Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=2→target=2"}
{"level":"DEBUG","ts":"2025-11-28T21:01:22.223Z","msg":"Found VA in map: variant=ms-inference-scheduling-llm-d-modelservice-decode, hasCurrentAlloc=true, accelerator=H100"}
{"level":"INFO","ts":"2025-11-28T21:01:22.223Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 2, desired-replicas: 2, accelerator: H100"}
{"level":"INFO","ts":"2025-11-28T21:01:22.223Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=2, accelerator=H100, capacityOnly=true"}
{"level":"INFO","ts":"2025-11-28T21:01:22.230Z","msg":"Applied capacity decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=2, target=2, reason=capacity-only mode: no-change"}
{"level":"INFO","ts":"2025-11-28T21:01:22.230Z","msg":"Reconciliation completed successfully: mode=capacity-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-11-28T21:01:22.438Z","msg":"Operating in CAPACITY-ONLY mode: reactive capacity-based scaling only"}
{"level":"INFO","ts":"2025-11-28T21:01:22.438Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-11-28T21:01:22.438Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-11-28T21:01:22.449Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=2, accelerator=H100, ttft=162.53ms, itl=55.03ms, cost=20.00, arrivalRate=1416.00"}
{"level":"INFO","ts":"2025-11-28T21:01:22.451Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9l76md, usage=0.938 (93.8%)"}
{"level":"DEBUG","ts":"2025-11-28T21:01:22.451Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"INFO","ts":"2025-11-28T21:01:22.451Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9l76md, queueLength=0"}
{"level":"DEBUG","ts":"2025-11-28T21:01:22.451Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"DEBUG","ts":"2025-11-28T21:01:22.451Z","msg":"Pod-to-variant matching successful: totalPods=1, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"DEBUG","ts":"2025-11-28T21:01:22.451Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=1"}
{"level":"DEBUG","ts":"2025-11-28T21:01:22.451Z","msg":"Collected capacity metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, metricsCount=1"}
{"level":"DEBUG","ts":"2025-11-28T21:01:22.451Z","msg":"Variant analysis initialized: variant=ms-inference-scheduling-llm-d-modelservice-decode, accelerator=H100, cost=10.00, replicaCount=1"}
{"level":"DEBUG","ts":"2025-11-28T21:01:22.451Z","msg":"Scale-down unsafe: insufficient non-saturated replicas: nonSaturated=0, required=2"}
{"level":"DEBUG","ts":"2025-11-28T21:01:22.451Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, totalReplicas=1, nonSaturated=0, avgSpareKv=0.000, avgSpareQueue=0.0, shouldScaleUp=true, scaleDownSafe=false"}
{"level":"INFO","ts":"2025-11-28T21:01:22.451Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, totalReplicas=1, nonSaturated=0, shouldScaleUp=true, scaleDownSafe=false"}
{"level":"INFO","ts":"2025-11-28T21:01:22.451Z","msg":"Capacity target: scale-up cheapest variant: variant=ms-inference-scheduling-llm-d-modelservice-decode, cost=10.00, currentReplicas=2, readyReplicas=1, target=2, reason=both KV spare (0.000 < 0.200) and queue spare (0.0 < 3.0)"}
{"level":"DEBUG","ts":"2025-11-28T21:01:22.451Z","msg":"Capacity targets calculated: modelID=unsloth/Meta-Llama-3.1-8B, targets=map[ms-inference-scheduling-llm-d-modelservice-decode:2]"}
{"level":"INFO","ts":"2025-11-28T21:01:22.451Z","msg":"Capacity-only decisions made for model: unsloth/Meta-Llama-3.1-8B - decision count: 1"}
{"level":"INFO","ts":"2025-11-28T21:01:22.451Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-11-28T21:01:22.451Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=2→target=2"}
{"level":"DEBUG","ts":"2025-11-28T21:01:22.451Z","msg":"Found VA in map: variant=ms-inference-scheduling-llm-d-modelservice-decode, hasCurrentAlloc=true, accelerator=H100"}
{"level":"INFO","ts":"2025-11-28T21:01:22.451Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 2, desired-replicas: 2, accelerator: H100"}
{"level":"INFO","ts":"2025-11-28T21:01:22.451Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=2, accelerator=H100, capacityOnly=true"}
{"level":"INFO","ts":"2025-11-28T21:01:22.455Z","msg":"Applied capacity decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=2, target=2, reason=capacity-only mode: no-change"}
{"level":"INFO","ts":"2025-11-28T21:01:22.455Z","msg":"Reconciliation completed successfully: mode=capacity-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-11-28T21:01:52.231Z","msg":"Operating in CAPACITY-ONLY mode: reactive capacity-based scaling only"}
{"level":"INFO","ts":"2025-11-28T21:01:52.231Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-11-28T21:01:52.231Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-11-28T21:01:52.245Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=2, accelerator=H100, ttft=68.75ms, itl=41.62ms, cost=20.00, arrivalRate=1742.00"}
{"level":"INFO","ts":"2025-11-28T21:01:52.246Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9l76md, queueLength=0"}
{"level":"DEBUG","ts":"2025-11-28T21:01:52.246Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"INFO","ts":"2025-11-28T21:01:52.246Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9l76md, usage=0.872 (87.2%)"}
{"level":"DEBUG","ts":"2025-11-28T21:01:52.246Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"DEBUG","ts":"2025-11-28T21:01:52.246Z","msg":"Pod-to-variant matching successful: totalPods=1, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"DEBUG","ts":"2025-11-28T21:01:52.246Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=1"}
{"level":"DEBUG","ts":"2025-11-28T21:01:52.246Z","msg":"Collected capacity metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, metricsCount=1"}
{"level":"DEBUG","ts":"2025-11-28T21:01:52.246Z","msg":"Variant analysis initialized: variant=ms-inference-scheduling-llm-d-modelservice-decode, accelerator=H100, cost=10.00, replicaCount=1"}
{"level":"DEBUG","ts":"2025-11-28T21:01:52.246Z","msg":"Scale-down unsafe: insufficient non-saturated replicas: nonSaturated=0, required=2"}
{"level":"DEBUG","ts":"2025-11-28T21:01:52.246Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, totalReplicas=1, nonSaturated=0, avgSpareKv=0.000, avgSpareQueue=0.0, shouldScaleUp=true, scaleDownSafe=false"}
{"level":"INFO","ts":"2025-11-28T21:01:52.246Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, totalReplicas=1, nonSaturated=0, shouldScaleUp=true, scaleDownSafe=false"}
{"level":"INFO","ts":"2025-11-28T21:01:52.246Z","msg":"Capacity target: scale-up cheapest variant: variant=ms-inference-scheduling-llm-d-modelservice-decode, cost=10.00, currentReplicas=2, readyReplicas=1, target=2, reason=both KV spare (0.000 < 0.200) and queue spare (0.0 < 3.0)"}
{"level":"DEBUG","ts":"2025-11-28T21:01:52.246Z","msg":"Capacity targets calculated: modelID=unsloth/Meta-Llama-3.1-8B, targets=map[ms-inference-scheduling-llm-d-modelservice-decode:2]"}
{"level":"INFO","ts":"2025-11-28T21:01:52.246Z","msg":"Capacity-only decisions made for model: unsloth/Meta-Llama-3.1-8B - decision count: 1"}
{"level":"INFO","ts":"2025-11-28T21:01:52.246Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-11-28T21:01:52.246Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=2→target=2"}
{"level":"DEBUG","ts":"2025-11-28T21:01:52.246Z","msg":"Found VA in map: variant=ms-inference-scheduling-llm-d-modelservice-decode, hasCurrentAlloc=true, accelerator=H100"}
{"level":"INFO","ts":"2025-11-28T21:01:52.246Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 2, desired-replicas: 2, accelerator: H100"}
{"level":"INFO","ts":"2025-11-28T21:01:52.246Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=2, accelerator=H100, capacityOnly=true"}
{"level":"INFO","ts":"2025-11-28T21:01:52.252Z","msg":"Applied capacity decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=2, target=2, reason=capacity-only mode: no-change"}
{"level":"INFO","ts":"2025-11-28T21:01:52.252Z","msg":"Reconciliation completed successfully: mode=capacity-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-11-28T21:01:52.456Z","msg":"Operating in CAPACITY-ONLY mode: reactive capacity-based scaling only"}
{"level":"INFO","ts":"2025-11-28T21:01:52.456Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-11-28T21:01:52.456Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-11-28T21:01:52.497Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=2, accelerator=H100, ttft=68.75ms, itl=41.62ms, cost=20.00, arrivalRate=1742.00"}
{"level":"INFO","ts":"2025-11-28T21:01:52.499Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9l76md, queueLength=0"}
{"level":"DEBUG","ts":"2025-11-28T21:01:52.499Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"INFO","ts":"2025-11-28T21:01:52.500Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9l76md, usage=0.872 (87.2%)"}
{"level":"DEBUG","ts":"2025-11-28T21:01:52.500Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"DEBUG","ts":"2025-11-28T21:01:52.500Z","msg":"Pod-to-variant matching successful: totalPods=1, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"DEBUG","ts":"2025-11-28T21:01:52.500Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=1"}
{"level":"DEBUG","ts":"2025-11-28T21:01:52.500Z","msg":"Collected capacity metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, metricsCount=1"}
{"level":"DEBUG","ts":"2025-11-28T21:01:52.500Z","msg":"Variant analysis initialized: variant=ms-inference-scheduling-llm-d-modelservice-decode, accelerator=H100, cost=10.00, replicaCount=1"}
{"level":"DEBUG","ts":"2025-11-28T21:01:52.500Z","msg":"Scale-down unsafe: insufficient non-saturated replicas: nonSaturated=0, required=2"}
{"level":"DEBUG","ts":"2025-11-28T21:01:52.500Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, totalReplicas=1, nonSaturated=0, avgSpareKv=0.000, avgSpareQueue=0.0, shouldScaleUp=true, scaleDownSafe=false"}
{"level":"INFO","ts":"2025-11-28T21:01:52.500Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, totalReplicas=1, nonSaturated=0, shouldScaleUp=true, scaleDownSafe=false"}
{"level":"INFO","ts":"2025-11-28T21:01:52.500Z","msg":"Capacity target: scale-up cheapest variant: variant=ms-inference-scheduling-llm-d-modelservice-decode, cost=10.00, currentReplicas=2, readyReplicas=1, target=2, reason=both KV spare (0.000 < 0.200) and queue spare (0.0 < 3.0)"}
{"level":"DEBUG","ts":"2025-11-28T21:01:52.500Z","msg":"Capacity targets calculated: modelID=unsloth/Meta-Llama-3.1-8B, targets=map[ms-inference-scheduling-llm-d-modelservice-decode:2]"}
{"level":"INFO","ts":"2025-11-28T21:01:52.500Z","msg":"Capacity-only decisions made for model: unsloth/Meta-Llama-3.1-8B - decision count: 1"}
{"level":"INFO","ts":"2025-11-28T21:01:52.500Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-11-28T21:01:52.500Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=2→target=2"}
{"level":"DEBUG","ts":"2025-11-28T21:01:52.500Z","msg":"Found VA in map: variant=ms-inference-scheduling-llm-d-modelservice-decode, hasCurrentAlloc=true, accelerator=H100"}
{"level":"INFO","ts":"2025-11-28T21:01:52.500Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 2, desired-replicas: 2, accelerator: H100"}
{"level":"INFO","ts":"2025-11-28T21:01:52.500Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=2, accelerator=H100, capacityOnly=true"}
{"level":"INFO","ts":"2025-11-28T21:01:52.507Z","msg":"Applied capacity decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=2, target=2, reason=capacity-only mode: no-change"}
{"level":"INFO","ts":"2025-11-28T21:01:52.507Z","msg":"Reconciliation completed successfully: mode=capacity-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-11-28T21:02:22.253Z","msg":"Operating in CAPACITY-ONLY mode: reactive capacity-based scaling only"}
{"level":"INFO","ts":"2025-11-28T21:02:22.253Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-11-28T21:02:22.253Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-11-28T21:02:22.282Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=2, accelerator=H100, ttft=68.15ms, itl=41.69ms, cost=20.00, arrivalRate=1596.00"}
{"level":"INFO","ts":"2025-11-28T21:02:22.284Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9lshk5, queueLength=0"}
{"level":"INFO","ts":"2025-11-28T21:02:22.284Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9l76md, queueLength=0"}
{"level":"DEBUG","ts":"2025-11-28T21:02:22.284Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=2"}
{"level":"INFO","ts":"2025-11-28T21:02:22.284Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9lshk5, usage=0.191 (19.1%)"}
{"level":"INFO","ts":"2025-11-28T21:02:22.284Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9l76md, usage=0.719 (71.9%)"}
{"level":"DEBUG","ts":"2025-11-28T21:02:22.284Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=2"}
{"level":"DEBUG","ts":"2025-11-28T21:02:22.284Z","msg":"Pod-to-variant matching successful: totalPods=2, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:2]"}
{"level":"DEBUG","ts":"2025-11-28T21:02:22.284Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=2"}
{"level":"DEBUG","ts":"2025-11-28T21:02:22.284Z","msg":"Collected capacity metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, metricsCount=2"}
{"level":"DEBUG","ts":"2025-11-28T21:02:22.284Z","msg":"Variant analysis initialized: variant=ms-inference-scheduling-llm-d-modelservice-decode, accelerator=H100, cost=10.00, replicaCount=2"}
{"level":"DEBUG","ts":"2025-11-28T21:02:22.284Z","msg":"Scale-down unsafe: insufficient non-saturated replicas: nonSaturated=1, required=2"}
{"level":"DEBUG","ts":"2025-11-28T21:02:22.284Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, totalReplicas=2, nonSaturated=1, avgSpareKv=0.309, avgSpareQueue=5.0, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"INFO","ts":"2025-11-28T21:02:22.284Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, totalReplicas=2, nonSaturated=1, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"DEBUG","ts":"2025-11-28T21:02:22.284Z","msg":"Capacity targets: no scaling needed (avgSpareKv=0.309, avgSpareQueue=5.0, all variants stable)"}
{"level":"DEBUG","ts":"2025-11-28T21:02:22.284Z","msg":"Capacity targets calculated: modelID=unsloth/Meta-Llama-3.1-8B, targets=map[ms-inference-scheduling-llm-d-modelservice-decode:2]"}
{"level":"INFO","ts":"2025-11-28T21:02:22.284Z","msg":"Capacity-only decisions made for model: unsloth/Meta-Llama-3.1-8B - decision count: 1"}
{"level":"INFO","ts":"2025-11-28T21:02:22.284Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-11-28T21:02:22.284Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=2→target=2"}
{"level":"DEBUG","ts":"2025-11-28T21:02:22.284Z","msg":"Found VA in map: variant=ms-inference-scheduling-llm-d-modelservice-decode, hasCurrentAlloc=true, accelerator=H100"}
{"level":"INFO","ts":"2025-11-28T21:02:22.284Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 2, desired-replicas: 2, accelerator: H100"}
{"level":"INFO","ts":"2025-11-28T21:02:22.284Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=2, accelerator=H100, capacityOnly=true"}
{"level":"INFO","ts":"2025-11-28T21:02:22.292Z","msg":"Applied capacity decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=2, target=2, reason=capacity-only mode: no-change"}
{"level":"INFO","ts":"2025-11-28T21:02:22.292Z","msg":"Reconciliation completed successfully: mode=capacity-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-11-28T21:02:22.508Z","msg":"Operating in CAPACITY-ONLY mode: reactive capacity-based scaling only"}
{"level":"INFO","ts":"2025-11-28T21:02:22.508Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-11-28T21:02:22.508Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-11-28T21:02:22.519Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=2, accelerator=H100, ttft=68.15ms, itl=41.69ms, cost=20.00, arrivalRate=1596.00"}
{"level":"INFO","ts":"2025-11-28T21:02:22.520Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9lshk5, queueLength=0"}
{"level":"INFO","ts":"2025-11-28T21:02:22.520Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9l76md, queueLength=0"}
{"level":"DEBUG","ts":"2025-11-28T21:02:22.520Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=2"}
{"level":"INFO","ts":"2025-11-28T21:02:22.520Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9lshk5, usage=0.191 (19.1%)"}
{"level":"INFO","ts":"2025-11-28T21:02:22.520Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9l76md, usage=0.719 (71.9%)"}
{"level":"DEBUG","ts":"2025-11-28T21:02:22.520Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=2"}
{"level":"DEBUG","ts":"2025-11-28T21:02:22.520Z","msg":"Pod-to-variant matching successful: totalPods=2, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:2]"}
{"level":"DEBUG","ts":"2025-11-28T21:02:22.520Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=2"}
{"level":"DEBUG","ts":"2025-11-28T21:02:22.520Z","msg":"Collected capacity metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, metricsCount=2"}
{"level":"DEBUG","ts":"2025-11-28T21:02:22.520Z","msg":"Variant analysis initialized: variant=ms-inference-scheduling-llm-d-modelservice-decode, accelerator=H100, cost=10.00, replicaCount=2"}
{"level":"DEBUG","ts":"2025-11-28T21:02:22.520Z","msg":"Scale-down unsafe: insufficient non-saturated replicas: nonSaturated=1, required=2"}
{"level":"DEBUG","ts":"2025-11-28T21:02:22.520Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, totalReplicas=2, nonSaturated=1, avgSpareKv=0.309, avgSpareQueue=5.0, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"INFO","ts":"2025-11-28T21:02:22.520Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, totalReplicas=2, nonSaturated=1, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"DEBUG","ts":"2025-11-28T21:02:22.520Z","msg":"Capacity targets: no scaling needed (avgSpareKv=0.309, avgSpareQueue=5.0, all variants stable)"}
{"level":"DEBUG","ts":"2025-11-28T21:02:22.520Z","msg":"Capacity targets calculated: modelID=unsloth/Meta-Llama-3.1-8B, targets=map[ms-inference-scheduling-llm-d-modelservice-decode:2]"}
{"level":"INFO","ts":"2025-11-28T21:02:22.520Z","msg":"Capacity-only decisions made for model: unsloth/Meta-Llama-3.1-8B - decision count: 1"}
{"level":"INFO","ts":"2025-11-28T21:02:22.520Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-11-28T21:02:22.520Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=2→target=2"}
{"level":"DEBUG","ts":"2025-11-28T21:02:22.520Z","msg":"Found VA in map: variant=ms-inference-scheduling-llm-d-modelservice-decode, hasCurrentAlloc=true, accelerator=H100"}
{"level":"INFO","ts":"2025-11-28T21:02:22.520Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 2, desired-replicas: 2, accelerator: H100"}
{"level":"INFO","ts":"2025-11-28T21:02:22.520Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=2, accelerator=H100, capacityOnly=true"}
{"level":"INFO","ts":"2025-11-28T21:02:22.526Z","msg":"Applied capacity decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=2, target=2, reason=capacity-only mode: no-change"}
{"level":"INFO","ts":"2025-11-28T21:02:22.526Z","msg":"Reconciliation completed successfully: mode=capacity-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-11-28T21:02:52.293Z","msg":"Operating in CAPACITY-ONLY mode: reactive capacity-based scaling only"}
{"level":"INFO","ts":"2025-11-28T21:02:52.293Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-11-28T21:02:52.293Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-11-28T21:02:52.312Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=2, accelerator=H100, ttft=58.25ms, itl=34.73ms, cost=20.00, arrivalRate=2851.39"}
{"level":"INFO","ts":"2025-11-28T21:02:52.314Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9lshk5, usage=0.308 (30.8%)"}
{"level":"INFO","ts":"2025-11-28T21:02:52.314Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9l76md, usage=0.748 (74.8%)"}
{"level":"DEBUG","ts":"2025-11-28T21:02:52.314Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=2"}
{"level":"INFO","ts":"2025-11-28T21:02:52.314Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9lshk5, queueLength=0"}
{"level":"INFO","ts":"2025-11-28T21:02:52.314Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9l76md, queueLength=0"}
{"level":"DEBUG","ts":"2025-11-28T21:02:52.314Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=2"}
{"level":"DEBUG","ts":"2025-11-28T21:02:52.314Z","msg":"Pod-to-variant matching successful: totalPods=2, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:2]"}
{"level":"DEBUG","ts":"2025-11-28T21:02:52.314Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=2"}
{"level":"DEBUG","ts":"2025-11-28T21:02:52.314Z","msg":"Collected capacity metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, metricsCount=2"}
{"level":"DEBUG","ts":"2025-11-28T21:02:52.314Z","msg":"Variant analysis initialized: variant=ms-inference-scheduling-llm-d-modelservice-decode, accelerator=H100, cost=10.00, replicaCount=2"}
{"level":"DEBUG","ts":"2025-11-28T21:02:52.314Z","msg":"Scale-down unsafe: insufficient non-saturated replicas: nonSaturated=1, required=2"}
{"level":"DEBUG","ts":"2025-11-28T21:02:52.314Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, totalReplicas=2, nonSaturated=1, avgSpareKv=0.192, avgSpareQueue=5.0, shouldScaleUp=true, scaleDownSafe=false"}
{"level":"INFO","ts":"2025-11-28T21:02:52.314Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, totalReplicas=2, nonSaturated=1, shouldScaleUp=true, scaleDownSafe=false"}
{"level":"INFO","ts":"2025-11-28T21:02:52.314Z","msg":"Capacity target: scale-up cheapest variant: variant=ms-inference-scheduling-llm-d-modelservice-decode, cost=10.00, currentReplicas=2, readyReplicas=2, target=3, reason=KV spare capacity low (0.192 < 0.200)"}
{"level":"DEBUG","ts":"2025-11-28T21:02:52.314Z","msg":"Capacity targets calculated: modelID=unsloth/Meta-Llama-3.1-8B, targets=map[ms-inference-scheduling-llm-d-modelservice-decode:3]"}
{"level":"INFO","ts":"2025-11-28T21:02:52.314Z","msg":"Capacity-only decisions made for model: unsloth/Meta-Llama-3.1-8B - decision count: 1"}
{"level":"INFO","ts":"2025-11-28T21:02:52.314Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-11-28T21:02:52.314Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=scale-up, current=2→target=3"}
{"level":"DEBUG","ts":"2025-11-28T21:02:52.314Z","msg":"Found VA in map: variant=ms-inference-scheduling-llm-d-modelservice-decode, hasCurrentAlloc=true, accelerator=H100"}
{"level":"INFO","ts":"2025-11-28T21:02:52.314Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 2, desired-replicas: 3, accelerator: H100"}
{"level":"INFO","ts":"2025-11-28T21:02:52.314Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=3, accelerator=H100, capacityOnly=true"}
{"level":"INFO","ts":"2025-11-28T21:02:52.319Z","msg":"Applied capacity decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=scale-up, current=2, target=3, reason=capacity-only mode: scale-up"}
{"level":"INFO","ts":"2025-11-28T21:02:52.319Z","msg":"Reconciliation completed successfully: mode=capacity-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-11-28T21:02:52.527Z","msg":"Operating in CAPACITY-ONLY mode: reactive capacity-based scaling only"}
{"level":"INFO","ts":"2025-11-28T21:02:52.527Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-11-28T21:02:52.527Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-11-28T21:02:52.537Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=2, accelerator=H100, ttft=58.21ms, itl=34.70ms, cost=20.00, arrivalRate=2856.59"}
{"level":"INFO","ts":"2025-11-28T21:02:52.539Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9lshk5, queueLength=0"}
{"level":"INFO","ts":"2025-11-28T21:02:52.539Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9l76md, queueLength=0"}
{"level":"DEBUG","ts":"2025-11-28T21:02:52.539Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=2"}
{"level":"INFO","ts":"2025-11-28T21:02:52.539Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9lshk5, usage=0.308 (30.8%)"}
{"level":"INFO","ts":"2025-11-28T21:02:52.539Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9l76md, usage=0.748 (74.8%)"}
{"level":"DEBUG","ts":"2025-11-28T21:02:52.539Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=2"}
{"level":"DEBUG","ts":"2025-11-28T21:02:52.539Z","msg":"Pod-to-variant matching successful: totalPods=2, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:2]"}
{"level":"DEBUG","ts":"2025-11-28T21:02:52.539Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=2"}
{"level":"DEBUG","ts":"2025-11-28T21:02:52.539Z","msg":"Collected capacity metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, metricsCount=2"}
{"level":"DEBUG","ts":"2025-11-28T21:02:52.539Z","msg":"Variant analysis initialized: variant=ms-inference-scheduling-llm-d-modelservice-decode, accelerator=H100, cost=10.00, replicaCount=2"}
{"level":"DEBUG","ts":"2025-11-28T21:02:52.539Z","msg":"Scale-down unsafe: insufficient non-saturated replicas: nonSaturated=1, required=2"}
{"level":"DEBUG","ts":"2025-11-28T21:02:52.539Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, totalReplicas=2, nonSaturated=1, avgSpareKv=0.192, avgSpareQueue=5.0, shouldScaleUp=true, scaleDownSafe=false"}
{"level":"INFO","ts":"2025-11-28T21:02:52.539Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, totalReplicas=2, nonSaturated=1, shouldScaleUp=true, scaleDownSafe=false"}
{"level":"DEBUG","ts":"2025-11-28T21:02:52.539Z","msg":"Preserving desired replicas: variant=ms-inference-scheduling-llm-d-modelservice-decode, currentReplicas=2, readyReplicas=2, desired=3"}
{"level":"DEBUG","ts":"2025-11-28T21:02:52.539Z","msg":"Capacity targets calculated: modelID=unsloth/Meta-Llama-3.1-8B, targets=map[ms-inference-scheduling-llm-d-modelservice-decode:3]"}
{"level":"INFO","ts":"2025-11-28T21:02:52.539Z","msg":"Capacity-only decisions made for model: unsloth/Meta-Llama-3.1-8B - decision count: 1"}
{"level":"INFO","ts":"2025-11-28T21:02:52.539Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-11-28T21:02:52.539Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=scale-up, current=2→target=3"}
{"level":"DEBUG","ts":"2025-11-28T21:02:52.539Z","msg":"Found VA in map: variant=ms-inference-scheduling-llm-d-modelservice-decode, hasCurrentAlloc=true, accelerator=H100"}
{"level":"INFO","ts":"2025-11-28T21:02:52.539Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 2, desired-replicas: 3, accelerator: H100"}
{"level":"INFO","ts":"2025-11-28T21:02:52.539Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=3, accelerator=H100, capacityOnly=true"}
{"level":"INFO","ts":"2025-11-28T21:02:52.545Z","msg":"Applied capacity decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=scale-up, current=2, target=3, reason=capacity-only mode: scale-up"}
{"level":"INFO","ts":"2025-11-28T21:02:52.545Z","msg":"Reconciliation completed successfully: mode=capacity-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-11-28T21:03:22.320Z","msg":"Operating in CAPACITY-ONLY mode: reactive capacity-based scaling only"}
{"level":"INFO","ts":"2025-11-28T21:03:22.321Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-11-28T21:03:22.321Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-11-28T21:03:22.351Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=3, accelerator=H100, ttft=66.77ms, itl=41.91ms, cost=30.00, arrivalRate=2888.00"}
{"level":"INFO","ts":"2025-11-28T21:03:22.352Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9lshk5, queueLength=0"}
{"level":"INFO","ts":"2025-11-28T21:03:22.352Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9l76md, queueLength=0"}
{"level":"DEBUG","ts":"2025-11-28T21:03:22.352Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=2"}
{"level":"INFO","ts":"2025-11-28T21:03:22.353Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9lshk5, usage=0.549 (54.9%)"}
{"level":"INFO","ts":"2025-11-28T21:03:22.353Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9l76md, usage=0.972 (97.2%)"}
{"level":"DEBUG","ts":"2025-11-28T21:03:22.353Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=2"}
{"level":"DEBUG","ts":"2025-11-28T21:03:22.353Z","msg":"Pod-to-variant matching successful: totalPods=2, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:2]"}
{"level":"DEBUG","ts":"2025-11-28T21:03:22.353Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=2"}
{"level":"DEBUG","ts":"2025-11-28T21:03:22.353Z","msg":"Collected capacity metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, metricsCount=2"}
{"level":"DEBUG","ts":"2025-11-28T21:03:22.353Z","msg":"Variant analysis initialized: variant=ms-inference-scheduling-llm-d-modelservice-decode, accelerator=H100, cost=10.00, replicaCount=2"}
{"level":"DEBUG","ts":"2025-11-28T21:03:22.353Z","msg":"Scale-down unsafe: insufficient non-saturated replicas: nonSaturated=0, required=2"}
{"level":"DEBUG","ts":"2025-11-28T21:03:22.353Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, totalReplicas=2, nonSaturated=0, avgSpareKv=0.000, avgSpareQueue=0.0, shouldScaleUp=true, scaleDownSafe=false"}
{"level":"INFO","ts":"2025-11-28T21:03:22.353Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, totalReplicas=2, nonSaturated=0, shouldScaleUp=true, scaleDownSafe=false"}
{"level":"INFO","ts":"2025-11-28T21:03:22.353Z","msg":"Capacity target: scale-up cheapest variant: variant=ms-inference-scheduling-llm-d-modelservice-decode, cost=10.00, currentReplicas=3, readyReplicas=2, target=3, reason=both KV spare (0.000 < 0.200) and queue spare (0.0 < 3.0)"}
{"level":"DEBUG","ts":"2025-11-28T21:03:22.353Z","msg":"Capacity targets calculated: modelID=unsloth/Meta-Llama-3.1-8B, targets=map[ms-inference-scheduling-llm-d-modelservice-decode:3]"}
{"level":"INFO","ts":"2025-11-28T21:03:22.353Z","msg":"Capacity-only decisions made for model: unsloth/Meta-Llama-3.1-8B - decision count: 1"}
{"level":"INFO","ts":"2025-11-28T21:03:22.353Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-11-28T21:03:22.353Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=3→target=3"}
{"level":"DEBUG","ts":"2025-11-28T21:03:22.353Z","msg":"Found VA in map: variant=ms-inference-scheduling-llm-d-modelservice-decode, hasCurrentAlloc=true, accelerator=H100"}
{"level":"INFO","ts":"2025-11-28T21:03:22.353Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 3, desired-replicas: 3, accelerator: H100"}
{"level":"INFO","ts":"2025-11-28T21:03:22.353Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=3, accelerator=H100, capacityOnly=true"}
{"level":"INFO","ts":"2025-11-28T21:03:22.358Z","msg":"Applied capacity decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=3, target=3, reason=capacity-only mode: no-change"}
{"level":"INFO","ts":"2025-11-28T21:03:22.358Z","msg":"Reconciliation completed successfully: mode=capacity-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-11-28T21:03:22.547Z","msg":"Operating in CAPACITY-ONLY mode: reactive capacity-based scaling only"}
{"level":"INFO","ts":"2025-11-28T21:03:22.547Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-11-28T21:03:22.547Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-11-28T21:03:22.562Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=3, accelerator=H100, ttft=66.77ms, itl=41.91ms, cost=30.00, arrivalRate=2888.00"}
{"level":"INFO","ts":"2025-11-28T21:03:22.564Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9lshk5, usage=0.549 (54.9%)"}
{"level":"INFO","ts":"2025-11-28T21:03:22.564Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9l76md, usage=0.972 (97.2%)"}
{"level":"DEBUG","ts":"2025-11-28T21:03:22.564Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=2"}
{"level":"INFO","ts":"2025-11-28T21:03:22.564Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9lshk5, queueLength=0"}
{"level":"INFO","ts":"2025-11-28T21:03:22.564Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9l76md, queueLength=0"}
{"level":"DEBUG","ts":"2025-11-28T21:03:22.564Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=2"}
{"level":"DEBUG","ts":"2025-11-28T21:03:22.564Z","msg":"Pod-to-variant matching successful: totalPods=2, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:2]"}
{"level":"DEBUG","ts":"2025-11-28T21:03:22.564Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=2"}
{"level":"DEBUG","ts":"2025-11-28T21:03:22.564Z","msg":"Collected capacity metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, metricsCount=2"}
{"level":"DEBUG","ts":"2025-11-28T21:03:22.564Z","msg":"Variant analysis initialized: variant=ms-inference-scheduling-llm-d-modelservice-decode, accelerator=H100, cost=10.00, replicaCount=2"}
{"level":"DEBUG","ts":"2025-11-28T21:03:22.564Z","msg":"Scale-down unsafe: insufficient non-saturated replicas: nonSaturated=0, required=2"}
{"level":"DEBUG","ts":"2025-11-28T21:03:22.564Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, totalReplicas=2, nonSaturated=0, avgSpareKv=0.000, avgSpareQueue=0.0, shouldScaleUp=true, scaleDownSafe=false"}
{"level":"INFO","ts":"2025-11-28T21:03:22.564Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, totalReplicas=2, nonSaturated=0, shouldScaleUp=true, scaleDownSafe=false"}
{"level":"INFO","ts":"2025-11-28T21:03:22.564Z","msg":"Capacity target: scale-up cheapest variant: variant=ms-inference-scheduling-llm-d-modelservice-decode, cost=10.00, currentReplicas=3, readyReplicas=2, target=3, reason=both KV spare (0.000 < 0.200) and queue spare (0.0 < 3.0)"}
{"level":"DEBUG","ts":"2025-11-28T21:03:22.564Z","msg":"Capacity targets calculated: modelID=unsloth/Meta-Llama-3.1-8B, targets=map[ms-inference-scheduling-llm-d-modelservice-decode:3]"}
{"level":"INFO","ts":"2025-11-28T21:03:22.564Z","msg":"Capacity-only decisions made for model: unsloth/Meta-Llama-3.1-8B - decision count: 1"}
{"level":"INFO","ts":"2025-11-28T21:03:22.564Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-11-28T21:03:22.564Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=3→target=3"}
{"level":"DEBUG","ts":"2025-11-28T21:03:22.564Z","msg":"Found VA in map: variant=ms-inference-scheduling-llm-d-modelservice-decode, hasCurrentAlloc=true, accelerator=H100"}
{"level":"INFO","ts":"2025-11-28T21:03:22.564Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 3, desired-replicas: 3, accelerator: H100"}
{"level":"INFO","ts":"2025-11-28T21:03:22.564Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=3, accelerator=H100, capacityOnly=true"}
{"level":"INFO","ts":"2025-11-28T21:03:22.569Z","msg":"Applied capacity decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=3, target=3, reason=capacity-only mode: no-change"}
{"level":"INFO","ts":"2025-11-28T21:03:22.569Z","msg":"Reconciliation completed successfully: mode=capacity-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-11-28T21:03:52.359Z","msg":"Operating in CAPACITY-ONLY mode: reactive capacity-based scaling only"}
{"level":"INFO","ts":"2025-11-28T21:03:52.359Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-11-28T21:03:52.359Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-11-28T21:03:52.382Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=3, accelerator=H100, ttft=84.47ms, itl=40.46ms, cost=30.00, arrivalRate=3068.00"}
{"level":"INFO","ts":"2025-11-28T21:03:52.383Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9lshk5, queueLength=0"}
{"level":"INFO","ts":"2025-11-28T21:03:52.383Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9l76md, queueLength=0"}
{"level":"DEBUG","ts":"2025-11-28T21:03:52.383Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=2"}
{"level":"INFO","ts":"2025-11-28T21:03:52.383Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9lshk5, usage=0.549 (54.9%)"}
{"level":"INFO","ts":"2025-11-28T21:03:52.383Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9l76md, usage=0.972 (97.2%)"}
{"level":"DEBUG","ts":"2025-11-28T21:03:52.383Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=2"}
{"level":"DEBUG","ts":"2025-11-28T21:03:52.383Z","msg":"Pod-to-variant matching successful: totalPods=2, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:2]"}
{"level":"DEBUG","ts":"2025-11-28T21:03:52.383Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=2"}
{"level":"DEBUG","ts":"2025-11-28T21:03:52.383Z","msg":"Collected capacity metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, metricsCount=2"}
{"level":"DEBUG","ts":"2025-11-28T21:03:52.383Z","msg":"Variant analysis initialized: variant=ms-inference-scheduling-llm-d-modelservice-decode, accelerator=H100, cost=10.00, replicaCount=2"}
{"level":"DEBUG","ts":"2025-11-28T21:03:52.383Z","msg":"Scale-down unsafe: insufficient non-saturated replicas: nonSaturated=0, required=2"}
{"level":"DEBUG","ts":"2025-11-28T21:03:52.383Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, totalReplicas=2, nonSaturated=0, avgSpareKv=0.000, avgSpareQueue=0.0, shouldScaleUp=true, scaleDownSafe=false"}
{"level":"INFO","ts":"2025-11-28T21:03:52.383Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, totalReplicas=2, nonSaturated=0, shouldScaleUp=true, scaleDownSafe=false"}
{"level":"INFO","ts":"2025-11-28T21:03:52.384Z","msg":"Capacity target: scale-up cheapest variant: variant=ms-inference-scheduling-llm-d-modelservice-decode, cost=10.00, currentReplicas=3, readyReplicas=2, target=3, reason=both KV spare (0.000 < 0.200) and queue spare (0.0 < 3.0)"}
{"level":"DEBUG","ts":"2025-11-28T21:03:52.384Z","msg":"Capacity targets calculated: modelID=unsloth/Meta-Llama-3.1-8B, targets=map[ms-inference-scheduling-llm-d-modelservice-decode:3]"}
{"level":"INFO","ts":"2025-11-28T21:03:52.384Z","msg":"Capacity-only decisions made for model: unsloth/Meta-Llama-3.1-8B - decision count: 1"}
{"level":"INFO","ts":"2025-11-28T21:03:52.384Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-11-28T21:03:52.384Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=3→target=3"}
{"level":"DEBUG","ts":"2025-11-28T21:03:52.384Z","msg":"Found VA in map: variant=ms-inference-scheduling-llm-d-modelservice-decode, hasCurrentAlloc=true, accelerator=H100"}
{"level":"INFO","ts":"2025-11-28T21:03:52.384Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 3, desired-replicas: 3, accelerator: H100"}
{"level":"INFO","ts":"2025-11-28T21:03:52.384Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=3, accelerator=H100, capacityOnly=true"}
{"level":"INFO","ts":"2025-11-28T21:03:52.389Z","msg":"Applied capacity decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=3, target=3, reason=capacity-only mode: no-change"}
{"level":"INFO","ts":"2025-11-28T21:03:52.389Z","msg":"Reconciliation completed successfully: mode=capacity-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-11-28T21:03:52.570Z","msg":"Operating in CAPACITY-ONLY mode: reactive capacity-based scaling only"}
{"level":"INFO","ts":"2025-11-28T21:03:52.570Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-11-28T21:03:52.570Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-11-28T21:03:52.582Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=3, accelerator=H100, ttft=84.47ms, itl=40.46ms, cost=30.00, arrivalRate=3068.00"}
{"level":"INFO","ts":"2025-11-28T21:03:52.583Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9lshk5, queueLength=0"}
{"level":"INFO","ts":"2025-11-28T21:03:52.583Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9l76md, queueLength=0"}
{"level":"DEBUG","ts":"2025-11-28T21:03:52.583Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=2"}
{"level":"INFO","ts":"2025-11-28T21:03:52.583Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9lshk5, usage=0.549 (54.9%)"}
{"level":"INFO","ts":"2025-11-28T21:03:52.583Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9l76md, usage=0.972 (97.2%)"}
{"level":"DEBUG","ts":"2025-11-28T21:03:52.583Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=2"}
{"level":"DEBUG","ts":"2025-11-28T21:03:52.584Z","msg":"Pod-to-variant matching successful: totalPods=2, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:2]"}
{"level":"DEBUG","ts":"2025-11-28T21:03:52.584Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=2"}
{"level":"DEBUG","ts":"2025-11-28T21:03:52.584Z","msg":"Collected capacity metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, metricsCount=2"}
{"level":"DEBUG","ts":"2025-11-28T21:03:52.584Z","msg":"Variant analysis initialized: variant=ms-inference-scheduling-llm-d-modelservice-decode, accelerator=H100, cost=10.00, replicaCount=2"}
{"level":"DEBUG","ts":"2025-11-28T21:03:52.584Z","msg":"Scale-down unsafe: insufficient non-saturated replicas: nonSaturated=0, required=2"}
{"level":"DEBUG","ts":"2025-11-28T21:03:52.584Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, totalReplicas=2, nonSaturated=0, avgSpareKv=0.000, avgSpareQueue=0.0, shouldScaleUp=true, scaleDownSafe=false"}
{"level":"INFO","ts":"2025-11-28T21:03:52.584Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, totalReplicas=2, nonSaturated=0, shouldScaleUp=true, scaleDownSafe=false"}
{"level":"INFO","ts":"2025-11-28T21:03:52.584Z","msg":"Capacity target: scale-up cheapest variant: variant=ms-inference-scheduling-llm-d-modelservice-decode, cost=10.00, currentReplicas=3, readyReplicas=2, target=3, reason=both KV spare (0.000 < 0.200) and queue spare (0.0 < 3.0)"}
{"level":"DEBUG","ts":"2025-11-28T21:03:52.584Z","msg":"Capacity targets calculated: modelID=unsloth/Meta-Llama-3.1-8B, targets=map[ms-inference-scheduling-llm-d-modelservice-decode:3]"}
{"level":"INFO","ts":"2025-11-28T21:03:52.584Z","msg":"Capacity-only decisions made for model: unsloth/Meta-Llama-3.1-8B - decision count: 1"}
{"level":"INFO","ts":"2025-11-28T21:03:52.584Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-11-28T21:03:52.584Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=3→target=3"}
{"level":"DEBUG","ts":"2025-11-28T21:03:52.584Z","msg":"Found VA in map: variant=ms-inference-scheduling-llm-d-modelservice-decode, hasCurrentAlloc=true, accelerator=H100"}
{"level":"INFO","ts":"2025-11-28T21:03:52.584Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 3, desired-replicas: 3, accelerator: H100"}
{"level":"INFO","ts":"2025-11-28T21:03:52.584Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=3, accelerator=H100, capacityOnly=true"}
{"level":"INFO","ts":"2025-11-28T21:03:52.589Z","msg":"Applied capacity decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=3, target=3, reason=capacity-only mode: no-change"}
{"level":"INFO","ts":"2025-11-28T21:03:52.589Z","msg":"Reconciliation completed successfully: mode=capacity-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-11-28T21:04:22.393Z","msg":"Operating in CAPACITY-ONLY mode: reactive capacity-based scaling only"}
{"level":"INFO","ts":"2025-11-28T21:04:22.394Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-11-28T21:04:22.394Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-11-28T21:04:22.421Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=3, accelerator=H100, ttft=50.18ms, itl=29.32ms, cost=30.00, arrivalRate=3080.00"}
{"level":"INFO","ts":"2025-11-28T21:04:22.423Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9lshk5, queueLength=0"}
{"level":"INFO","ts":"2025-11-28T21:04:22.423Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9l76md, queueLength=0"}
{"level":"DEBUG","ts":"2025-11-28T21:04:22.423Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=2"}
{"level":"INFO","ts":"2025-11-28T21:04:22.423Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9lshk5, usage=0.490 (49.0%)"}
{"level":"INFO","ts":"2025-11-28T21:04:22.423Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9l76md, usage=0.655 (65.5%)"}
{"level":"DEBUG","ts":"2025-11-28T21:04:22.423Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=2"}
{"level":"DEBUG","ts":"2025-11-28T21:04:22.423Z","msg":"Pod-to-variant matching successful: totalPods=2, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:2]"}
{"level":"DEBUG","ts":"2025-11-28T21:04:22.423Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=2"}
{"level":"DEBUG","ts":"2025-11-28T21:04:22.423Z","msg":"Collected capacity metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, metricsCount=2"}
{"level":"DEBUG","ts":"2025-11-28T21:04:22.423Z","msg":"Variant analysis initialized: variant=ms-inference-scheduling-llm-d-modelservice-decode, accelerator=H100, cost=10.00, replicaCount=2"}
{"level":"DEBUG","ts":"2025-11-28T21:04:22.423Z","msg":"Scale-down unsafe: insufficient non-saturated replicas: nonSaturated=1, required=2"}
{"level":"DEBUG","ts":"2025-11-28T21:04:22.423Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, totalReplicas=2, nonSaturated=1, avgSpareKv=0.010, avgSpareQueue=5.0, shouldScaleUp=true, scaleDownSafe=false"}
{"level":"INFO","ts":"2025-11-28T21:04:22.423Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, totalReplicas=2, nonSaturated=1, shouldScaleUp=true, scaleDownSafe=false"}
{"level":"INFO","ts":"2025-11-28T21:04:22.423Z","msg":"Capacity target: scale-up cheapest variant: variant=ms-inference-scheduling-llm-d-modelservice-decode, cost=10.00, currentReplicas=3, readyReplicas=2, target=3, reason=KV spare capacity low (0.010 < 0.200)"}
{"level":"DEBUG","ts":"2025-11-28T21:04:22.423Z","msg":"Capacity targets calculated: modelID=unsloth/Meta-Llama-3.1-8B, targets=map[ms-inference-scheduling-llm-d-modelservice-decode:3]"}
{"level":"INFO","ts":"2025-11-28T21:04:22.423Z","msg":"Capacity-only decisions made for model: unsloth/Meta-Llama-3.1-8B - decision count: 1"}
{"level":"INFO","ts":"2025-11-28T21:04:22.423Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-11-28T21:04:22.423Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=3→target=3"}
{"level":"DEBUG","ts":"2025-11-28T21:04:22.423Z","msg":"Found VA in map: variant=ms-inference-scheduling-llm-d-modelservice-decode, hasCurrentAlloc=true, accelerator=H100"}
{"level":"INFO","ts":"2025-11-28T21:04:22.423Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 3, desired-replicas: 3, accelerator: H100"}
{"level":"INFO","ts":"2025-11-28T21:04:22.423Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=3, accelerator=H100, capacityOnly=true"}
{"level":"INFO","ts":"2025-11-28T21:04:22.429Z","msg":"Applied capacity decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=3, target=3, reason=capacity-only mode: no-change"}
{"level":"INFO","ts":"2025-11-28T21:04:22.429Z","msg":"Reconciliation completed successfully: mode=capacity-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-11-28T21:04:22.590Z","msg":"Operating in CAPACITY-ONLY mode: reactive capacity-based scaling only"}
{"level":"INFO","ts":"2025-11-28T21:04:22.590Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-11-28T21:04:22.590Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-11-28T21:04:22.602Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=3, accelerator=H100, ttft=50.18ms, itl=29.32ms, cost=30.00, arrivalRate=3080.00"}
{"level":"INFO","ts":"2025-11-28T21:04:22.603Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9lshk5, queueLength=0"}
{"level":"INFO","ts":"2025-11-28T21:04:22.603Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9l76md, queueLength=0"}
{"level":"DEBUG","ts":"2025-11-28T21:04:22.603Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=2"}
{"level":"INFO","ts":"2025-11-28T21:04:22.603Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9lshk5, usage=0.490 (49.0%)"}
{"level":"INFO","ts":"2025-11-28T21:04:22.603Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9l76md, usage=0.655 (65.5%)"}
{"level":"DEBUG","ts":"2025-11-28T21:04:22.603Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=2"}
{"level":"DEBUG","ts":"2025-11-28T21:04:22.603Z","msg":"Pod-to-variant matching successful: totalPods=2, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:2]"}
{"level":"DEBUG","ts":"2025-11-28T21:04:22.603Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=2"}
{"level":"DEBUG","ts":"2025-11-28T21:04:22.603Z","msg":"Collected capacity metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, metricsCount=2"}
{"level":"DEBUG","ts":"2025-11-28T21:04:22.603Z","msg":"Variant analysis initialized: variant=ms-inference-scheduling-llm-d-modelservice-decode, accelerator=H100, cost=10.00, replicaCount=2"}
{"level":"DEBUG","ts":"2025-11-28T21:04:22.603Z","msg":"Scale-down unsafe: insufficient non-saturated replicas: nonSaturated=1, required=2"}
{"level":"DEBUG","ts":"2025-11-28T21:04:22.603Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, totalReplicas=2, nonSaturated=1, avgSpareKv=0.010, avgSpareQueue=5.0, shouldScaleUp=true, scaleDownSafe=false"}
{"level":"INFO","ts":"2025-11-28T21:04:22.603Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, totalReplicas=2, nonSaturated=1, shouldScaleUp=true, scaleDownSafe=false"}
{"level":"INFO","ts":"2025-11-28T21:04:22.603Z","msg":"Capacity target: scale-up cheapest variant: variant=ms-inference-scheduling-llm-d-modelservice-decode, cost=10.00, currentReplicas=3, readyReplicas=2, target=3, reason=KV spare capacity low (0.010 < 0.200)"}
{"level":"DEBUG","ts":"2025-11-28T21:04:22.603Z","msg":"Capacity targets calculated: modelID=unsloth/Meta-Llama-3.1-8B, targets=map[ms-inference-scheduling-llm-d-modelservice-decode:3]"}
{"level":"INFO","ts":"2025-11-28T21:04:22.603Z","msg":"Capacity-only decisions made for model: unsloth/Meta-Llama-3.1-8B - decision count: 1"}
{"level":"INFO","ts":"2025-11-28T21:04:22.603Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-11-28T21:04:22.603Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=3→target=3"}
{"level":"DEBUG","ts":"2025-11-28T21:04:22.603Z","msg":"Found VA in map: variant=ms-inference-scheduling-llm-d-modelservice-decode, hasCurrentAlloc=true, accelerator=H100"}
{"level":"INFO","ts":"2025-11-28T21:04:22.603Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 3, desired-replicas: 3, accelerator: H100"}
{"level":"INFO","ts":"2025-11-28T21:04:22.604Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=3, accelerator=H100, capacityOnly=true"}
{"level":"INFO","ts":"2025-11-28T21:04:22.608Z","msg":"Applied capacity decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=3, target=3, reason=capacity-only mode: no-change"}
{"level":"INFO","ts":"2025-11-28T21:04:22.608Z","msg":"Reconciliation completed successfully: mode=capacity-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-11-28T21:04:52.430Z","msg":"Operating in CAPACITY-ONLY mode: reactive capacity-based scaling only"}
{"level":"INFO","ts":"2025-11-28T21:04:52.430Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-11-28T21:04:52.430Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-11-28T21:04:52.444Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=3, accelerator=H100, ttft=37.99ms, itl=21.15ms, cost=30.00, arrivalRate=2864.00"}
{"level":"INFO","ts":"2025-11-28T21:04:52.447Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9lshk5, queueLength=0"}
{"level":"INFO","ts":"2025-11-28T21:04:52.447Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9ww5mn, queueLength=0"}
{"level":"INFO","ts":"2025-11-28T21:04:52.447Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9l76md, queueLength=0"}
{"level":"DEBUG","ts":"2025-11-28T21:04:52.447Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=3"}
{"level":"INFO","ts":"2025-11-28T21:04:52.447Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9lshk5, usage=0.349 (34.9%)"}
{"level":"INFO","ts":"2025-11-28T21:04:52.447Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9ww5mn, usage=0.073 (7.3%)"}
{"level":"INFO","ts":"2025-11-28T21:04:52.447Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9l76md, usage=0.439 (43.9%)"}
{"level":"DEBUG","ts":"2025-11-28T21:04:52.447Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=3"}
{"level":"DEBUG","ts":"2025-11-28T21:04:52.447Z","msg":"Pod-to-variant matching successful: totalPods=3, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:3]"}
{"level":"DEBUG","ts":"2025-11-28T21:04:52.447Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=3"}
{"level":"DEBUG","ts":"2025-11-28T21:04:52.447Z","msg":"Collected capacity metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, metricsCount=3"}
{"level":"DEBUG","ts":"2025-11-28T21:04:52.447Z","msg":"Variant analysis initialized: variant=ms-inference-scheduling-llm-d-modelservice-decode, accelerator=H100, cost=10.00, replicaCount=3"}
{"level":"DEBUG","ts":"2025-11-28T21:04:52.447Z","msg":"Scale-down unsafe: insufficient headroom after redistribution: remainingSpareKv=0.069, kvTrigger=0.200, kvSafe=false, remainingSpareQueue=5.0, queueTrigger=3.0%!(EXTRA bool=true)"}
{"level":"DEBUG","ts":"2025-11-28T21:04:52.447Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, totalReplicas=3, nonSaturated=3, avgSpareKv=0.213, avgSpareQueue=5.0, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"INFO","ts":"2025-11-28T21:04:52.447Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, totalReplicas=3, nonSaturated=3, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"DEBUG","ts":"2025-11-28T21:04:52.447Z","msg":"Capacity targets: no scaling needed (avgSpareKv=0.213, avgSpareQueue=5.0, all variants stable)"}
{"level":"DEBUG","ts":"2025-11-28T21:04:52.447Z","msg":"Capacity targets calculated: modelID=unsloth/Meta-Llama-3.1-8B, targets=map[ms-inference-scheduling-llm-d-modelservice-decode:3]"}
{"level":"INFO","ts":"2025-11-28T21:04:52.447Z","msg":"Capacity-only decisions made for model: unsloth/Meta-Llama-3.1-8B - decision count: 1"}
{"level":"INFO","ts":"2025-11-28T21:04:52.447Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-11-28T21:04:52.447Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=3→target=3"}
{"level":"DEBUG","ts":"2025-11-28T21:04:52.447Z","msg":"Found VA in map: variant=ms-inference-scheduling-llm-d-modelservice-decode, hasCurrentAlloc=true, accelerator=H100"}
{"level":"INFO","ts":"2025-11-28T21:04:52.448Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 3, desired-replicas: 3, accelerator: H100"}
{"level":"INFO","ts":"2025-11-28T21:04:52.448Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=3, accelerator=H100, capacityOnly=true"}
{"level":"INFO","ts":"2025-11-28T21:04:52.454Z","msg":"Applied capacity decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=3, target=3, reason=capacity-only mode: no-change"}
{"level":"INFO","ts":"2025-11-28T21:04:52.454Z","msg":"Reconciliation completed successfully: mode=capacity-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-11-28T21:04:52.609Z","msg":"Operating in CAPACITY-ONLY mode: reactive capacity-based scaling only"}
{"level":"INFO","ts":"2025-11-28T21:04:52.609Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-11-28T21:04:52.609Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-11-28T21:04:52.620Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=3, accelerator=H100, ttft=37.99ms, itl=21.15ms, cost=30.00, arrivalRate=2864.00"}
{"level":"INFO","ts":"2025-11-28T21:04:52.621Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9lshk5, usage=0.349 (34.9%)"}
{"level":"INFO","ts":"2025-11-28T21:04:52.621Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9ww5mn, usage=0.073 (7.3%)"}
{"level":"INFO","ts":"2025-11-28T21:04:52.621Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9l76md, usage=0.439 (43.9%)"}
{"level":"DEBUG","ts":"2025-11-28T21:04:52.621Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=3"}
{"level":"INFO","ts":"2025-11-28T21:04:52.621Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9lshk5, queueLength=0"}
{"level":"INFO","ts":"2025-11-28T21:04:52.621Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9ww5mn, queueLength=0"}
{"level":"INFO","ts":"2025-11-28T21:04:52.621Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9l76md, queueLength=0"}
{"level":"DEBUG","ts":"2025-11-28T21:04:52.621Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=3"}
{"level":"DEBUG","ts":"2025-11-28T21:04:52.622Z","msg":"Pod-to-variant matching successful: totalPods=3, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:3]"}
{"level":"DEBUG","ts":"2025-11-28T21:04:52.622Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=3"}
{"level":"DEBUG","ts":"2025-11-28T21:04:52.622Z","msg":"Collected capacity metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, metricsCount=3"}
{"level":"DEBUG","ts":"2025-11-28T21:04:52.622Z","msg":"Variant analysis initialized: variant=ms-inference-scheduling-llm-d-modelservice-decode, accelerator=H100, cost=10.00, replicaCount=3"}
{"level":"DEBUG","ts":"2025-11-28T21:04:52.622Z","msg":"Scale-down unsafe: insufficient headroom after redistribution: remainingSpareKv=0.069, kvTrigger=0.200, kvSafe=false, remainingSpareQueue=5.0, queueTrigger=3.0%!(EXTRA bool=true)"}
{"level":"DEBUG","ts":"2025-11-28T21:04:52.622Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, totalReplicas=3, nonSaturated=3, avgSpareKv=0.213, avgSpareQueue=5.0, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"INFO","ts":"2025-11-28T21:04:52.622Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, totalReplicas=3, nonSaturated=3, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"DEBUG","ts":"2025-11-28T21:04:52.622Z","msg":"Capacity targets: no scaling needed (avgSpareKv=0.213, avgSpareQueue=5.0, all variants stable)"}
{"level":"DEBUG","ts":"2025-11-28T21:04:52.622Z","msg":"Capacity targets calculated: modelID=unsloth/Meta-Llama-3.1-8B, targets=map[ms-inference-scheduling-llm-d-modelservice-decode:3]"}
{"level":"INFO","ts":"2025-11-28T21:04:52.622Z","msg":"Capacity-only decisions made for model: unsloth/Meta-Llama-3.1-8B - decision count: 1"}
{"level":"INFO","ts":"2025-11-28T21:04:52.622Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-11-28T21:04:52.622Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=3→target=3"}
{"level":"DEBUG","ts":"2025-11-28T21:04:52.622Z","msg":"Found VA in map: variant=ms-inference-scheduling-llm-d-modelservice-decode, hasCurrentAlloc=true, accelerator=H100"}
{"level":"INFO","ts":"2025-11-28T21:04:52.622Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 3, desired-replicas: 3, accelerator: H100"}
{"level":"INFO","ts":"2025-11-28T21:04:52.622Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=3, accelerator=H100, capacityOnly=true"}
{"level":"INFO","ts":"2025-11-28T21:04:52.628Z","msg":"Applied capacity decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=3, target=3, reason=capacity-only mode: no-change"}
{"level":"INFO","ts":"2025-11-28T21:04:52.628Z","msg":"Reconciliation completed successfully: mode=capacity-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-11-28T21:05:22.454Z","msg":"Operating in CAPACITY-ONLY mode: reactive capacity-based scaling only"}
{"level":"INFO","ts":"2025-11-28T21:05:22.454Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-11-28T21:05:22.454Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-11-28T21:05:22.474Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=3, accelerator=H100, ttft=33.59ms, itl=18.11ms, cost=30.00, arrivalRate=2999.00"}
{"level":"INFO","ts":"2025-11-28T21:05:22.475Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9lshk5, usage=0.231 (23.1%)"}
{"level":"INFO","ts":"2025-11-28T21:05:22.475Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9ww5mn, usage=0.183 (18.3%)"}
{"level":"INFO","ts":"2025-11-28T21:05:22.475Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9l76md, usage=0.312 (31.2%)"}
{"level":"DEBUG","ts":"2025-11-28T21:05:22.475Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=3"}
{"level":"INFO","ts":"2025-11-28T21:05:22.475Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9lshk5, queueLength=0"}
{"level":"INFO","ts":"2025-11-28T21:05:22.475Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9ww5mn, queueLength=0"}
{"level":"INFO","ts":"2025-11-28T21:05:22.475Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9l76md, queueLength=0"}
{"level":"DEBUG","ts":"2025-11-28T21:05:22.475Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=3"}
{"level":"DEBUG","ts":"2025-11-28T21:05:22.476Z","msg":"Pod-to-variant matching successful: totalPods=3, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:3]"}
{"level":"DEBUG","ts":"2025-11-28T21:05:22.476Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=3"}
{"level":"DEBUG","ts":"2025-11-28T21:05:22.476Z","msg":"Collected capacity metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, metricsCount=3"}
{"level":"DEBUG","ts":"2025-11-28T21:05:22.476Z","msg":"Variant analysis initialized: variant=ms-inference-scheduling-llm-d-modelservice-decode, accelerator=H100, cost=10.00, replicaCount=3"}
{"level":"DEBUG","ts":"2025-11-28T21:05:22.476Z","msg":"Scale-down unsafe: insufficient headroom after redistribution: remainingSpareKv=0.137, kvTrigger=0.200, kvSafe=false, remainingSpareQueue=5.0, queueTrigger=3.0%!(EXTRA bool=true)"}
{"level":"DEBUG","ts":"2025-11-28T21:05:22.476Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, totalReplicas=3, nonSaturated=3, avgSpareKv=0.258, avgSpareQueue=5.0, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"INFO","ts":"2025-11-28T21:05:22.476Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, totalReplicas=3, nonSaturated=3, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"DEBUG","ts":"2025-11-28T21:05:22.476Z","msg":"Capacity targets: no scaling needed (avgSpareKv=0.258, avgSpareQueue=5.0, all variants stable)"}
{"level":"DEBUG","ts":"2025-11-28T21:05:22.476Z","msg":"Capacity targets calculated: modelID=unsloth/Meta-Llama-3.1-8B, targets=map[ms-inference-scheduling-llm-d-modelservice-decode:3]"}
{"level":"INFO","ts":"2025-11-28T21:05:22.476Z","msg":"Capacity-only decisions made for model: unsloth/Meta-Llama-3.1-8B - decision count: 1"}
{"level":"INFO","ts":"2025-11-28T21:05:22.476Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-11-28T21:05:22.476Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=3→target=3"}
{"level":"DEBUG","ts":"2025-11-28T21:05:22.476Z","msg":"Found VA in map: variant=ms-inference-scheduling-llm-d-modelservice-decode, hasCurrentAlloc=true, accelerator=H100"}
{"level":"INFO","ts":"2025-11-28T21:05:22.476Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 3, desired-replicas: 3, accelerator: H100"}
{"level":"INFO","ts":"2025-11-28T21:05:22.476Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=3, accelerator=H100, capacityOnly=true"}
{"level":"INFO","ts":"2025-11-28T21:05:22.480Z","msg":"Applied capacity decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=3, target=3, reason=capacity-only mode: no-change"}
{"level":"INFO","ts":"2025-11-28T21:05:22.480Z","msg":"Reconciliation completed successfully: mode=capacity-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-11-28T21:05:22.628Z","msg":"Operating in CAPACITY-ONLY mode: reactive capacity-based scaling only"}
{"level":"INFO","ts":"2025-11-28T21:05:22.628Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-11-28T21:05:22.628Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-11-28T21:05:22.641Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=3, accelerator=H100, ttft=33.59ms, itl=18.11ms, cost=30.00, arrivalRate=2999.00"}
{"level":"INFO","ts":"2025-11-28T21:05:22.644Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9lshk5, queueLength=0"}
{"level":"INFO","ts":"2025-11-28T21:05:22.644Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9ww5mn, queueLength=0"}
{"level":"INFO","ts":"2025-11-28T21:05:22.644Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9l76md, queueLength=0"}
{"level":"DEBUG","ts":"2025-11-28T21:05:22.644Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=3"}
{"level":"INFO","ts":"2025-11-28T21:05:22.644Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9lshk5, usage=0.231 (23.1%)"}
{"level":"INFO","ts":"2025-11-28T21:05:22.644Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9ww5mn, usage=0.183 (18.3%)"}
{"level":"INFO","ts":"2025-11-28T21:05:22.644Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9l76md, usage=0.312 (31.2%)"}
{"level":"DEBUG","ts":"2025-11-28T21:05:22.644Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=3"}
{"level":"DEBUG","ts":"2025-11-28T21:05:22.644Z","msg":"Pod-to-variant matching successful: totalPods=3, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:3]"}
{"level":"DEBUG","ts":"2025-11-28T21:05:22.644Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=3"}
{"level":"DEBUG","ts":"2025-11-28T21:05:22.644Z","msg":"Collected capacity metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, metricsCount=3"}
{"level":"DEBUG","ts":"2025-11-28T21:05:22.644Z","msg":"Variant analysis initialized: variant=ms-inference-scheduling-llm-d-modelservice-decode, accelerator=H100, cost=10.00, replicaCount=3"}
{"level":"DEBUG","ts":"2025-11-28T21:05:22.644Z","msg":"Scale-down unsafe: insufficient headroom after redistribution: remainingSpareKv=0.137, kvTrigger=0.200, kvSafe=false, remainingSpareQueue=5.0, queueTrigger=3.0%!(EXTRA bool=true)"}
{"level":"DEBUG","ts":"2025-11-28T21:05:22.644Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, totalReplicas=3, nonSaturated=3, avgSpareKv=0.258, avgSpareQueue=5.0, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"INFO","ts":"2025-11-28T21:05:22.644Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, totalReplicas=3, nonSaturated=3, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"DEBUG","ts":"2025-11-28T21:05:22.644Z","msg":"Capacity targets: no scaling needed (avgSpareKv=0.258, avgSpareQueue=5.0, all variants stable)"}
{"level":"DEBUG","ts":"2025-11-28T21:05:22.644Z","msg":"Capacity targets calculated: modelID=unsloth/Meta-Llama-3.1-8B, targets=map[ms-inference-scheduling-llm-d-modelservice-decode:3]"}
{"level":"INFO","ts":"2025-11-28T21:05:22.644Z","msg":"Capacity-only decisions made for model: unsloth/Meta-Llama-3.1-8B - decision count: 1"}
{"level":"INFO","ts":"2025-11-28T21:05:22.644Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-11-28T21:05:22.644Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=3→target=3"}
{"level":"DEBUG","ts":"2025-11-28T21:05:22.644Z","msg":"Found VA in map: variant=ms-inference-scheduling-llm-d-modelservice-decode, hasCurrentAlloc=true, accelerator=H100"}
{"level":"INFO","ts":"2025-11-28T21:05:22.644Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 3, desired-replicas: 3, accelerator: H100"}
{"level":"INFO","ts":"2025-11-28T21:05:22.644Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=3, accelerator=H100, capacityOnly=true"}
{"level":"INFO","ts":"2025-11-28T21:05:22.649Z","msg":"Applied capacity decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=3, target=3, reason=capacity-only mode: no-change"}
{"level":"INFO","ts":"2025-11-28T21:05:22.649Z","msg":"Reconciliation completed successfully: mode=capacity-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-11-28T21:05:52.481Z","msg":"Operating in CAPACITY-ONLY mode: reactive capacity-based scaling only"}
{"level":"INFO","ts":"2025-11-28T21:05:52.481Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-11-28T21:05:52.481Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-11-28T21:05:52.493Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=3, accelerator=H100, ttft=33.90ms, itl=18.15ms, cost=30.00, arrivalRate=3829.13"}
{"level":"INFO","ts":"2025-11-28T21:05:52.495Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9lshk5, usage=0.261 (26.1%)"}
{"level":"INFO","ts":"2025-11-28T21:05:52.495Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9ww5mn, usage=0.232 (23.2%)"}
{"level":"INFO","ts":"2025-11-28T21:05:52.495Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9l76md, usage=0.270 (27.0%)"}
{"level":"DEBUG","ts":"2025-11-28T21:05:52.495Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=3"}
{"level":"INFO","ts":"2025-11-28T21:05:52.495Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9lshk5, queueLength=0"}
{"level":"INFO","ts":"2025-11-28T21:05:52.495Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9ww5mn, queueLength=0"}
{"level":"INFO","ts":"2025-11-28T21:05:52.495Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9l76md, queueLength=0"}
{"level":"DEBUG","ts":"2025-11-28T21:05:52.495Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=3"}
{"level":"DEBUG","ts":"2025-11-28T21:05:52.495Z","msg":"Pod-to-variant matching successful: totalPods=3, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:3]"}
{"level":"DEBUG","ts":"2025-11-28T21:05:52.495Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=3"}
{"level":"DEBUG","ts":"2025-11-28T21:05:52.495Z","msg":"Collected capacity metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, metricsCount=3"}
{"level":"DEBUG","ts":"2025-11-28T21:05:52.495Z","msg":"Variant analysis initialized: variant=ms-inference-scheduling-llm-d-modelservice-decode, accelerator=H100, cost=10.00, replicaCount=3"}
{"level":"DEBUG","ts":"2025-11-28T21:05:52.495Z","msg":"Scale-down unsafe: insufficient headroom after redistribution: remainingSpareKv=0.118, kvTrigger=0.200, kvSafe=false, remainingSpareQueue=5.0, queueTrigger=3.0%!(EXTRA bool=true)"}
{"level":"DEBUG","ts":"2025-11-28T21:05:52.495Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, totalReplicas=3, nonSaturated=3, avgSpareKv=0.246, avgSpareQueue=5.0, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"INFO","ts":"2025-11-28T21:05:52.495Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, totalReplicas=3, nonSaturated=3, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"DEBUG","ts":"2025-11-28T21:05:52.495Z","msg":"Capacity targets: no scaling needed (avgSpareKv=0.246, avgSpareQueue=5.0, all variants stable)"}
{"level":"DEBUG","ts":"2025-11-28T21:05:52.495Z","msg":"Capacity targets calculated: modelID=unsloth/Meta-Llama-3.1-8B, targets=map[ms-inference-scheduling-llm-d-modelservice-decode:3]"}
{"level":"INFO","ts":"2025-11-28T21:05:52.495Z","msg":"Capacity-only decisions made for model: unsloth/Meta-Llama-3.1-8B - decision count: 1"}
{"level":"INFO","ts":"2025-11-28T21:05:52.495Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-11-28T21:05:52.495Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=3→target=3"}
{"level":"DEBUG","ts":"2025-11-28T21:05:52.495Z","msg":"Found VA in map: variant=ms-inference-scheduling-llm-d-modelservice-decode, hasCurrentAlloc=true, accelerator=H100"}
{"level":"INFO","ts":"2025-11-28T21:05:52.495Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 3, desired-replicas: 3, accelerator: H100"}
{"level":"INFO","ts":"2025-11-28T21:05:52.495Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=3, accelerator=H100, capacityOnly=true"}
{"level":"INFO","ts":"2025-11-28T21:05:52.502Z","msg":"Applied capacity decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=3, target=3, reason=capacity-only mode: no-change"}
{"level":"INFO","ts":"2025-11-28T21:05:52.502Z","msg":"Reconciliation completed successfully: mode=capacity-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-11-28T21:05:52.650Z","msg":"Operating in CAPACITY-ONLY mode: reactive capacity-based scaling only"}
{"level":"INFO","ts":"2025-11-28T21:05:52.650Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-11-28T21:05:52.650Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-11-28T21:05:52.660Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=3, accelerator=H100, ttft=33.90ms, itl=18.15ms, cost=30.00, arrivalRate=3829.13"}
{"level":"INFO","ts":"2025-11-28T21:05:52.662Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9lshk5, queueLength=0"}
{"level":"INFO","ts":"2025-11-28T21:05:52.662Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9ww5mn, queueLength=0"}
{"level":"INFO","ts":"2025-11-28T21:05:52.662Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9l76md, queueLength=0"}
{"level":"DEBUG","ts":"2025-11-28T21:05:52.662Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=3"}
{"level":"INFO","ts":"2025-11-28T21:05:52.662Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9lshk5, usage=0.261 (26.1%)"}
{"level":"INFO","ts":"2025-11-28T21:05:52.662Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9ww5mn, usage=0.232 (23.2%)"}
{"level":"INFO","ts":"2025-11-28T21:05:52.662Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9l76md, usage=0.270 (27.0%)"}
{"level":"DEBUG","ts":"2025-11-28T21:05:52.662Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=3"}
{"level":"DEBUG","ts":"2025-11-28T21:05:52.662Z","msg":"Pod-to-variant matching successful: totalPods=3, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:3]"}
{"level":"DEBUG","ts":"2025-11-28T21:05:52.662Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=3"}
{"level":"DEBUG","ts":"2025-11-28T21:05:52.662Z","msg":"Collected capacity metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, metricsCount=3"}
{"level":"DEBUG","ts":"2025-11-28T21:05:52.662Z","msg":"Variant analysis initialized: variant=ms-inference-scheduling-llm-d-modelservice-decode, accelerator=H100, cost=10.00, replicaCount=3"}
{"level":"DEBUG","ts":"2025-11-28T21:05:52.662Z","msg":"Scale-down unsafe: insufficient headroom after redistribution: remainingSpareKv=0.118, kvTrigger=0.200, kvSafe=false, remainingSpareQueue=5.0, queueTrigger=3.0%!(EXTRA bool=true)"}
{"level":"DEBUG","ts":"2025-11-28T21:05:52.662Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, totalReplicas=3, nonSaturated=3, avgSpareKv=0.246, avgSpareQueue=5.0, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"INFO","ts":"2025-11-28T21:05:52.662Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, totalReplicas=3, nonSaturated=3, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"DEBUG","ts":"2025-11-28T21:05:52.662Z","msg":"Capacity targets: no scaling needed (avgSpareKv=0.246, avgSpareQueue=5.0, all variants stable)"}
{"level":"DEBUG","ts":"2025-11-28T21:05:52.662Z","msg":"Capacity targets calculated: modelID=unsloth/Meta-Llama-3.1-8B, targets=map[ms-inference-scheduling-llm-d-modelservice-decode:3]"}
{"level":"INFO","ts":"2025-11-28T21:05:52.662Z","msg":"Capacity-only decisions made for model: unsloth/Meta-Llama-3.1-8B - decision count: 1"}
{"level":"INFO","ts":"2025-11-28T21:05:52.662Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-11-28T21:05:52.662Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=3→target=3"}
{"level":"DEBUG","ts":"2025-11-28T21:05:52.662Z","msg":"Found VA in map: variant=ms-inference-scheduling-llm-d-modelservice-decode, hasCurrentAlloc=true, accelerator=H100"}
{"level":"INFO","ts":"2025-11-28T21:05:52.662Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 3, desired-replicas: 3, accelerator: H100"}
{"level":"INFO","ts":"2025-11-28T21:05:52.662Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=3, accelerator=H100, capacityOnly=true"}
{"level":"INFO","ts":"2025-11-28T21:05:52.667Z","msg":"Applied capacity decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=3, target=3, reason=capacity-only mode: no-change"}
{"level":"INFO","ts":"2025-11-28T21:05:52.667Z","msg":"Reconciliation completed successfully: mode=capacity-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-11-28T21:06:22.503Z","msg":"Operating in CAPACITY-ONLY mode: reactive capacity-based scaling only"}
{"level":"INFO","ts":"2025-11-28T21:06:22.503Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-11-28T21:06:22.503Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-11-28T21:06:22.520Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=3, accelerator=H100, ttft=34.88ms, itl=18.88ms, cost=30.00, arrivalRate=3927.13"}
{"level":"INFO","ts":"2025-11-28T21:06:22.522Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9lshk5, queueLength=0"}
{"level":"INFO","ts":"2025-11-28T21:06:22.522Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9ww5mn, queueLength=0"}
{"level":"INFO","ts":"2025-11-28T21:06:22.522Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9l76md, queueLength=0"}
{"level":"DEBUG","ts":"2025-11-28T21:06:22.522Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=3"}
{"level":"INFO","ts":"2025-11-28T21:06:22.522Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9lshk5, usage=0.263 (26.3%)"}
{"level":"INFO","ts":"2025-11-28T21:06:22.522Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9ww5mn, usage=0.300 (30.0%)"}
{"level":"INFO","ts":"2025-11-28T21:06:22.522Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9l76md, usage=0.248 (24.8%)"}
{"level":"DEBUG","ts":"2025-11-28T21:06:22.522Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=3"}
{"level":"DEBUG","ts":"2025-11-28T21:06:22.522Z","msg":"Pod-to-variant matching successful: totalPods=3, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:3]"}
{"level":"DEBUG","ts":"2025-11-28T21:06:22.522Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=3"}
{"level":"DEBUG","ts":"2025-11-28T21:06:22.522Z","msg":"Collected capacity metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, metricsCount=3"}
{"level":"DEBUG","ts":"2025-11-28T21:06:22.522Z","msg":"Variant analysis initialized: variant=ms-inference-scheduling-llm-d-modelservice-decode, accelerator=H100, cost=10.00, replicaCount=3"}
{"level":"DEBUG","ts":"2025-11-28T21:06:22.522Z","msg":"Scale-down unsafe: insufficient headroom after redistribution: remainingSpareKv=0.095, kvTrigger=0.200, kvSafe=false, remainingSpareQueue=5.0, queueTrigger=3.0%!(EXTRA bool=true)"}
{"level":"DEBUG","ts":"2025-11-28T21:06:22.522Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, totalReplicas=3, nonSaturated=3, avgSpareKv=0.230, avgSpareQueue=5.0, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"INFO","ts":"2025-11-28T21:06:22.522Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, totalReplicas=3, nonSaturated=3, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"DEBUG","ts":"2025-11-28T21:06:22.522Z","msg":"Capacity targets: no scaling needed (avgSpareKv=0.230, avgSpareQueue=5.0, all variants stable)"}
{"level":"DEBUG","ts":"2025-11-28T21:06:22.522Z","msg":"Capacity targets calculated: modelID=unsloth/Meta-Llama-3.1-8B, targets=map[ms-inference-scheduling-llm-d-modelservice-decode:3]"}
{"level":"INFO","ts":"2025-11-28T21:06:22.522Z","msg":"Capacity-only decisions made for model: unsloth/Meta-Llama-3.1-8B - decision count: 1"}
{"level":"INFO","ts":"2025-11-28T21:06:22.522Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-11-28T21:06:22.522Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=3→target=3"}
{"level":"DEBUG","ts":"2025-11-28T21:06:22.522Z","msg":"Found VA in map: variant=ms-inference-scheduling-llm-d-modelservice-decode, hasCurrentAlloc=true, accelerator=H100"}
{"level":"INFO","ts":"2025-11-28T21:06:22.522Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 3, desired-replicas: 3, accelerator: H100"}
{"level":"INFO","ts":"2025-11-28T21:06:22.522Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=3, accelerator=H100, capacityOnly=true"}
{"level":"INFO","ts":"2025-11-28T21:06:22.527Z","msg":"Applied capacity decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=3, target=3, reason=capacity-only mode: no-change"}
{"level":"INFO","ts":"2025-11-28T21:06:22.527Z","msg":"Reconciliation completed successfully: mode=capacity-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-11-28T21:06:22.668Z","msg":"Operating in CAPACITY-ONLY mode: reactive capacity-based scaling only"}
{"level":"INFO","ts":"2025-11-28T21:06:22.668Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-11-28T21:06:22.668Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-11-28T21:06:22.679Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=3, accelerator=H100, ttft=34.88ms, itl=18.88ms, cost=30.00, arrivalRate=3927.13"}
{"level":"INFO","ts":"2025-11-28T21:06:22.680Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9lshk5, queueLength=0"}
{"level":"INFO","ts":"2025-11-28T21:06:22.680Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9ww5mn, queueLength=0"}
{"level":"INFO","ts":"2025-11-28T21:06:22.680Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9l76md, queueLength=0"}
{"level":"DEBUG","ts":"2025-11-28T21:06:22.680Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=3"}
{"level":"INFO","ts":"2025-11-28T21:06:22.680Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9lshk5, usage=0.263 (26.3%)"}
{"level":"INFO","ts":"2025-11-28T21:06:22.680Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9ww5mn, usage=0.300 (30.0%)"}
{"level":"INFO","ts":"2025-11-28T21:06:22.680Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9l76md, usage=0.248 (24.8%)"}
{"level":"DEBUG","ts":"2025-11-28T21:06:22.680Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=3"}
{"level":"DEBUG","ts":"2025-11-28T21:06:22.681Z","msg":"Pod-to-variant matching successful: totalPods=3, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:3]"}
{"level":"DEBUG","ts":"2025-11-28T21:06:22.681Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=3"}
{"level":"DEBUG","ts":"2025-11-28T21:06:22.681Z","msg":"Collected capacity metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, metricsCount=3"}
{"level":"DEBUG","ts":"2025-11-28T21:06:22.681Z","msg":"Variant analysis initialized: variant=ms-inference-scheduling-llm-d-modelservice-decode, accelerator=H100, cost=10.00, replicaCount=3"}
{"level":"DEBUG","ts":"2025-11-28T21:06:22.681Z","msg":"Scale-down unsafe: insufficient headroom after redistribution: remainingSpareKv=0.095, kvTrigger=0.200, kvSafe=false, remainingSpareQueue=5.0, queueTrigger=3.0%!(EXTRA bool=true)"}
{"level":"DEBUG","ts":"2025-11-28T21:06:22.681Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, totalReplicas=3, nonSaturated=3, avgSpareKv=0.230, avgSpareQueue=5.0, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"INFO","ts":"2025-11-28T21:06:22.681Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, totalReplicas=3, nonSaturated=3, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"DEBUG","ts":"2025-11-28T21:06:22.681Z","msg":"Capacity targets: no scaling needed (avgSpareKv=0.230, avgSpareQueue=5.0, all variants stable)"}
{"level":"DEBUG","ts":"2025-11-28T21:06:22.681Z","msg":"Capacity targets calculated: modelID=unsloth/Meta-Llama-3.1-8B, targets=map[ms-inference-scheduling-llm-d-modelservice-decode:3]"}
{"level":"INFO","ts":"2025-11-28T21:06:22.681Z","msg":"Capacity-only decisions made for model: unsloth/Meta-Llama-3.1-8B - decision count: 1"}
{"level":"INFO","ts":"2025-11-28T21:06:22.681Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-11-28T21:06:22.681Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=3→target=3"}
{"level":"DEBUG","ts":"2025-11-28T21:06:22.681Z","msg":"Found VA in map: variant=ms-inference-scheduling-llm-d-modelservice-decode, hasCurrentAlloc=true, accelerator=H100"}
{"level":"INFO","ts":"2025-11-28T21:06:22.681Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 3, desired-replicas: 3, accelerator: H100"}
{"level":"INFO","ts":"2025-11-28T21:06:22.681Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=3, accelerator=H100, capacityOnly=true"}
{"level":"INFO","ts":"2025-11-28T21:06:22.685Z","msg":"Applied capacity decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=3, target=3, reason=capacity-only mode: no-change"}
{"level":"INFO","ts":"2025-11-28T21:06:22.685Z","msg":"Reconciliation completed successfully: mode=capacity-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-11-28T21:06:52.527Z","msg":"Operating in CAPACITY-ONLY mode: reactive capacity-based scaling only"}
{"level":"INFO","ts":"2025-11-28T21:06:52.527Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-11-28T21:06:52.527Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-11-28T21:06:52.551Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=3, accelerator=H100, ttft=35.21ms, itl=19.05ms, cost=30.00, arrivalRate=3910.85"}
{"level":"INFO","ts":"2025-11-28T21:06:52.552Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9lshk5, queueLength=0"}
{"level":"INFO","ts":"2025-11-28T21:06:52.552Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9ww5mn, queueLength=0"}
{"level":"INFO","ts":"2025-11-28T21:06:52.552Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9l76md, queueLength=0"}
{"level":"DEBUG","ts":"2025-11-28T21:06:52.552Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=3"}
{"level":"INFO","ts":"2025-11-28T21:06:52.553Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9lshk5, usage=0.263 (26.3%)"}
{"level":"INFO","ts":"2025-11-28T21:06:52.553Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9ww5mn, usage=0.300 (30.0%)"}
{"level":"INFO","ts":"2025-11-28T21:06:52.553Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9l76md, usage=0.235 (23.5%)"}
{"level":"DEBUG","ts":"2025-11-28T21:06:52.553Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=3"}
{"level":"DEBUG","ts":"2025-11-28T21:06:52.553Z","msg":"Pod-to-variant matching successful: totalPods=3, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:3]"}
{"level":"DEBUG","ts":"2025-11-28T21:06:52.553Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=3"}
{"level":"DEBUG","ts":"2025-11-28T21:06:52.553Z","msg":"Collected capacity metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, metricsCount=3"}
{"level":"DEBUG","ts":"2025-11-28T21:06:52.553Z","msg":"Variant analysis initialized: variant=ms-inference-scheduling-llm-d-modelservice-decode, accelerator=H100, cost=10.00, replicaCount=3"}
{"level":"DEBUG","ts":"2025-11-28T21:06:52.553Z","msg":"Scale-down unsafe: insufficient headroom after redistribution: remainingSpareKv=0.101, kvTrigger=0.200, kvSafe=false, remainingSpareQueue=5.0, queueTrigger=3.0%!(EXTRA bool=true)"}
{"level":"DEBUG","ts":"2025-11-28T21:06:52.553Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, totalReplicas=3, nonSaturated=3, avgSpareKv=0.234, avgSpareQueue=5.0, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"INFO","ts":"2025-11-28T21:06:52.553Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, totalReplicas=3, nonSaturated=3, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"DEBUG","ts":"2025-11-28T21:06:52.553Z","msg":"Capacity targets: no scaling needed (avgSpareKv=0.234, avgSpareQueue=5.0, all variants stable)"}
{"level":"DEBUG","ts":"2025-11-28T21:06:52.553Z","msg":"Capacity targets calculated: modelID=unsloth/Meta-Llama-3.1-8B, targets=map[ms-inference-scheduling-llm-d-modelservice-decode:3]"}
{"level":"INFO","ts":"2025-11-28T21:06:52.553Z","msg":"Capacity-only decisions made for model: unsloth/Meta-Llama-3.1-8B - decision count: 1"}
{"level":"INFO","ts":"2025-11-28T21:06:52.553Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-11-28T21:06:52.553Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=3→target=3"}
{"level":"DEBUG","ts":"2025-11-28T21:06:52.553Z","msg":"Found VA in map: variant=ms-inference-scheduling-llm-d-modelservice-decode, hasCurrentAlloc=true, accelerator=H100"}
{"level":"INFO","ts":"2025-11-28T21:06:52.553Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 3, desired-replicas: 3, accelerator: H100"}
{"level":"INFO","ts":"2025-11-28T21:06:52.553Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=3, accelerator=H100, capacityOnly=true"}
{"level":"INFO","ts":"2025-11-28T21:06:52.559Z","msg":"Applied capacity decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=3, target=3, reason=capacity-only mode: no-change"}
{"level":"INFO","ts":"2025-11-28T21:06:52.559Z","msg":"Reconciliation completed successfully: mode=capacity-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-11-28T21:06:52.686Z","msg":"Operating in CAPACITY-ONLY mode: reactive capacity-based scaling only"}
{"level":"INFO","ts":"2025-11-28T21:06:52.686Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-11-28T21:06:52.686Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-11-28T21:06:52.697Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=3, accelerator=H100, ttft=35.21ms, itl=19.05ms, cost=30.00, arrivalRate=3910.85"}
{"level":"INFO","ts":"2025-11-28T21:06:52.699Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9lshk5, usage=0.263 (26.3%)"}
{"level":"INFO","ts":"2025-11-28T21:06:52.699Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9ww5mn, usage=0.300 (30.0%)"}
{"level":"INFO","ts":"2025-11-28T21:06:52.699Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9l76md, usage=0.235 (23.5%)"}
{"level":"DEBUG","ts":"2025-11-28T21:06:52.699Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=3"}
{"level":"INFO","ts":"2025-11-28T21:06:52.699Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9lshk5, queueLength=0"}
{"level":"INFO","ts":"2025-11-28T21:06:52.699Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9ww5mn, queueLength=0"}
{"level":"INFO","ts":"2025-11-28T21:06:52.699Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9l76md, queueLength=0"}
{"level":"DEBUG","ts":"2025-11-28T21:06:52.699Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=3"}
{"level":"DEBUG","ts":"2025-11-28T21:06:52.699Z","msg":"Pod-to-variant matching successful: totalPods=3, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:3]"}
{"level":"DEBUG","ts":"2025-11-28T21:06:52.699Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=3"}
{"level":"DEBUG","ts":"2025-11-28T21:06:52.699Z","msg":"Collected capacity metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, metricsCount=3"}
{"level":"DEBUG","ts":"2025-11-28T21:06:52.699Z","msg":"Variant analysis initialized: variant=ms-inference-scheduling-llm-d-modelservice-decode, accelerator=H100, cost=10.00, replicaCount=3"}
{"level":"DEBUG","ts":"2025-11-28T21:06:52.699Z","msg":"Scale-down unsafe: insufficient headroom after redistribution: remainingSpareKv=0.101, kvTrigger=0.200, kvSafe=false, remainingSpareQueue=5.0, queueTrigger=3.0%!(EXTRA bool=true)"}
{"level":"DEBUG","ts":"2025-11-28T21:06:52.699Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, totalReplicas=3, nonSaturated=3, avgSpareKv=0.234, avgSpareQueue=5.0, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"INFO","ts":"2025-11-28T21:06:52.699Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, totalReplicas=3, nonSaturated=3, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"DEBUG","ts":"2025-11-28T21:06:52.699Z","msg":"Capacity targets: no scaling needed (avgSpareKv=0.234, avgSpareQueue=5.0, all variants stable)"}
{"level":"DEBUG","ts":"2025-11-28T21:06:52.699Z","msg":"Capacity targets calculated: modelID=unsloth/Meta-Llama-3.1-8B, targets=map[ms-inference-scheduling-llm-d-modelservice-decode:3]"}
{"level":"INFO","ts":"2025-11-28T21:06:52.699Z","msg":"Capacity-only decisions made for model: unsloth/Meta-Llama-3.1-8B - decision count: 1"}
{"level":"INFO","ts":"2025-11-28T21:06:52.699Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-11-28T21:06:52.699Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=3→target=3"}
{"level":"DEBUG","ts":"2025-11-28T21:06:52.699Z","msg":"Found VA in map: variant=ms-inference-scheduling-llm-d-modelservice-decode, hasCurrentAlloc=true, accelerator=H100"}
{"level":"INFO","ts":"2025-11-28T21:06:52.699Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 3, desired-replicas: 3, accelerator: H100"}
{"level":"INFO","ts":"2025-11-28T21:06:52.699Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=3, accelerator=H100, capacityOnly=true"}
{"level":"INFO","ts":"2025-11-28T21:06:52.703Z","msg":"Applied capacity decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=3, target=3, reason=capacity-only mode: no-change"}
{"level":"INFO","ts":"2025-11-28T21:06:52.703Z","msg":"Reconciliation completed successfully: mode=capacity-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-11-28T21:07:22.559Z","msg":"Operating in CAPACITY-ONLY mode: reactive capacity-based scaling only"}
{"level":"INFO","ts":"2025-11-28T21:07:22.560Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-11-28T21:07:22.560Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-11-28T21:07:22.591Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=3, accelerator=H100, ttft=34.31ms, itl=18.61ms, cost=30.00, arrivalRate=3786.35"}
{"level":"INFO","ts":"2025-11-28T21:07:22.592Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9lshk5, queueLength=0"}
{"level":"INFO","ts":"2025-11-28T21:07:22.592Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9ww5mn, queueLength=0"}
{"level":"INFO","ts":"2025-11-28T21:07:22.592Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9l76md, queueLength=0"}
{"level":"DEBUG","ts":"2025-11-28T21:07:22.592Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=3"}
{"level":"INFO","ts":"2025-11-28T21:07:22.592Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9lshk5, usage=0.264 (26.4%)"}
{"level":"INFO","ts":"2025-11-28T21:07:22.592Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9ww5mn, usage=0.304 (30.4%)"}
{"level":"INFO","ts":"2025-11-28T21:07:22.592Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9l76md, usage=0.265 (26.5%)"}
{"level":"DEBUG","ts":"2025-11-28T21:07:22.592Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=3"}
{"level":"DEBUG","ts":"2025-11-28T21:07:22.593Z","msg":"Pod-to-variant matching successful: totalPods=3, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:3]"}
{"level":"DEBUG","ts":"2025-11-28T21:07:22.593Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=3"}
{"level":"DEBUG","ts":"2025-11-28T21:07:22.593Z","msg":"Collected capacity metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, metricsCount=3"}
{"level":"DEBUG","ts":"2025-11-28T21:07:22.593Z","msg":"Variant analysis initialized: variant=ms-inference-scheduling-llm-d-modelservice-decode, accelerator=H100, cost=10.00, replicaCount=3"}
{"level":"DEBUG","ts":"2025-11-28T21:07:22.593Z","msg":"Scale-down unsafe: insufficient headroom after redistribution: remainingSpareKv=0.083, kvTrigger=0.200, kvSafe=false, remainingSpareQueue=5.0, queueTrigger=3.0%!(EXTRA bool=true)"}
{"level":"DEBUG","ts":"2025-11-28T21:07:22.593Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, totalReplicas=3, nonSaturated=3, avgSpareKv=0.222, avgSpareQueue=5.0, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"INFO","ts":"2025-11-28T21:07:22.593Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, totalReplicas=3, nonSaturated=3, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"DEBUG","ts":"2025-11-28T21:07:22.593Z","msg":"Capacity targets: no scaling needed (avgSpareKv=0.222, avgSpareQueue=5.0, all variants stable)"}
{"level":"DEBUG","ts":"2025-11-28T21:07:22.593Z","msg":"Capacity targets calculated: modelID=unsloth/Meta-Llama-3.1-8B, targets=map[ms-inference-scheduling-llm-d-modelservice-decode:3]"}
{"level":"INFO","ts":"2025-11-28T21:07:22.593Z","msg":"Capacity-only decisions made for model: unsloth/Meta-Llama-3.1-8B - decision count: 1"}
{"level":"INFO","ts":"2025-11-28T21:07:22.593Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-11-28T21:07:22.593Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=3→target=3"}
{"level":"DEBUG","ts":"2025-11-28T21:07:22.593Z","msg":"Found VA in map: variant=ms-inference-scheduling-llm-d-modelservice-decode, hasCurrentAlloc=true, accelerator=H100"}
{"level":"INFO","ts":"2025-11-28T21:07:22.593Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 3, desired-replicas: 3, accelerator: H100"}
{"level":"INFO","ts":"2025-11-28T21:07:22.593Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=3, accelerator=H100, capacityOnly=true"}
{"level":"INFO","ts":"2025-11-28T21:07:22.598Z","msg":"Applied capacity decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=3, target=3, reason=capacity-only mode: no-change"}
{"level":"INFO","ts":"2025-11-28T21:07:22.598Z","msg":"Reconciliation completed successfully: mode=capacity-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-11-28T21:07:22.704Z","msg":"Operating in CAPACITY-ONLY mode: reactive capacity-based scaling only"}
{"level":"INFO","ts":"2025-11-28T21:07:22.704Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-11-28T21:07:22.704Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-11-28T21:07:22.717Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=3, accelerator=H100, ttft=34.31ms, itl=18.61ms, cost=30.00, arrivalRate=3786.35"}
{"level":"INFO","ts":"2025-11-28T21:07:22.719Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9lshk5, usage=0.264 (26.4%)"}
{"level":"INFO","ts":"2025-11-28T21:07:22.719Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9ww5mn, usage=0.304 (30.4%)"}
{"level":"INFO","ts":"2025-11-28T21:07:22.719Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9l76md, usage=0.265 (26.5%)"}
{"level":"DEBUG","ts":"2025-11-28T21:07:22.719Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=3"}
{"level":"INFO","ts":"2025-11-28T21:07:22.719Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9lshk5, queueLength=0"}
{"level":"INFO","ts":"2025-11-28T21:07:22.719Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9ww5mn, queueLength=0"}
{"level":"INFO","ts":"2025-11-28T21:07:22.719Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9l76md, queueLength=0"}
{"level":"DEBUG","ts":"2025-11-28T21:07:22.719Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=3"}
{"level":"DEBUG","ts":"2025-11-28T21:07:22.719Z","msg":"Pod-to-variant matching successful: totalPods=3, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:3]"}
{"level":"DEBUG","ts":"2025-11-28T21:07:22.719Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=3"}
{"level":"DEBUG","ts":"2025-11-28T21:07:22.719Z","msg":"Collected capacity metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, metricsCount=3"}
{"level":"DEBUG","ts":"2025-11-28T21:07:22.719Z","msg":"Variant analysis initialized: variant=ms-inference-scheduling-llm-d-modelservice-decode, accelerator=H100, cost=10.00, replicaCount=3"}
{"level":"DEBUG","ts":"2025-11-28T21:07:22.719Z","msg":"Scale-down unsafe: insufficient headroom after redistribution: remainingSpareKv=0.083, kvTrigger=0.200, kvSafe=false, remainingSpareQueue=5.0, queueTrigger=3.0%!(EXTRA bool=true)"}
{"level":"DEBUG","ts":"2025-11-28T21:07:22.719Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, totalReplicas=3, nonSaturated=3, avgSpareKv=0.222, avgSpareQueue=5.0, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"INFO","ts":"2025-11-28T21:07:22.719Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, totalReplicas=3, nonSaturated=3, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"DEBUG","ts":"2025-11-28T21:07:22.719Z","msg":"Capacity targets: no scaling needed (avgSpareKv=0.222, avgSpareQueue=5.0, all variants stable)"}
{"level":"DEBUG","ts":"2025-11-28T21:07:22.719Z","msg":"Capacity targets calculated: modelID=unsloth/Meta-Llama-3.1-8B, targets=map[ms-inference-scheduling-llm-d-modelservice-decode:3]"}
{"level":"INFO","ts":"2025-11-28T21:07:22.719Z","msg":"Capacity-only decisions made for model: unsloth/Meta-Llama-3.1-8B - decision count: 1"}
{"level":"INFO","ts":"2025-11-28T21:07:22.719Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-11-28T21:07:22.719Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=3→target=3"}
{"level":"DEBUG","ts":"2025-11-28T21:07:22.719Z","msg":"Found VA in map: variant=ms-inference-scheduling-llm-d-modelservice-decode, hasCurrentAlloc=true, accelerator=H100"}
{"level":"INFO","ts":"2025-11-28T21:07:22.719Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 3, desired-replicas: 3, accelerator: H100"}
{"level":"INFO","ts":"2025-11-28T21:07:22.719Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=3, accelerator=H100, capacityOnly=true"}
{"level":"INFO","ts":"2025-11-28T21:07:22.723Z","msg":"Applied capacity decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=3, target=3, reason=capacity-only mode: no-change"}
{"level":"INFO","ts":"2025-11-28T21:07:22.723Z","msg":"Reconciliation completed successfully: mode=capacity-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-11-28T21:07:52.599Z","msg":"Operating in CAPACITY-ONLY mode: reactive capacity-based scaling only"}
{"level":"INFO","ts":"2025-11-28T21:07:52.599Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-11-28T21:07:52.599Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-11-28T21:07:52.619Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=3, accelerator=H100, ttft=36.73ms, itl=19.70ms, cost=30.00, arrivalRate=4003.71"}
{"level":"INFO","ts":"2025-11-28T21:07:52.620Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9lshk5, queueLength=0"}
{"level":"INFO","ts":"2025-11-28T21:07:52.620Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9ww5mn, queueLength=0"}
{"level":"INFO","ts":"2025-11-28T21:07:52.620Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9l76md, queueLength=0"}
{"level":"DEBUG","ts":"2025-11-28T21:07:52.620Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=3"}
{"level":"INFO","ts":"2025-11-28T21:07:52.620Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9lshk5, usage=0.287 (28.7%)"}
{"level":"INFO","ts":"2025-11-28T21:07:52.621Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9ww5mn, usage=0.293 (29.3%)"}
{"level":"INFO","ts":"2025-11-28T21:07:52.621Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9l76md, usage=0.292 (29.2%)"}
{"level":"DEBUG","ts":"2025-11-28T21:07:52.621Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=3"}
{"level":"DEBUG","ts":"2025-11-28T21:07:52.621Z","msg":"Pod-to-variant matching successful: totalPods=3, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:3]"}
{"level":"DEBUG","ts":"2025-11-28T21:07:52.621Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=3"}
{"level":"DEBUG","ts":"2025-11-28T21:07:52.621Z","msg":"Collected capacity metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, metricsCount=3"}
{"level":"DEBUG","ts":"2025-11-28T21:07:52.621Z","msg":"Variant analysis initialized: variant=ms-inference-scheduling-llm-d-modelservice-decode, accelerator=H100, cost=10.00, replicaCount=3"}
{"level":"DEBUG","ts":"2025-11-28T21:07:52.621Z","msg":"Scale-down unsafe: insufficient headroom after redistribution: remainingSpareKv=0.064, kvTrigger=0.200, kvSafe=false, remainingSpareQueue=5.0, queueTrigger=3.0%!(EXTRA bool=true)"}
{"level":"DEBUG","ts":"2025-11-28T21:07:52.621Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, totalReplicas=3, nonSaturated=3, avgSpareKv=0.210, avgSpareQueue=5.0, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"INFO","ts":"2025-11-28T21:07:52.621Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, totalReplicas=3, nonSaturated=3, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"DEBUG","ts":"2025-11-28T21:07:52.621Z","msg":"Capacity targets: no scaling needed (avgSpareKv=0.210, avgSpareQueue=5.0, all variants stable)"}
{"level":"DEBUG","ts":"2025-11-28T21:07:52.621Z","msg":"Capacity targets calculated: modelID=unsloth/Meta-Llama-3.1-8B, targets=map[ms-inference-scheduling-llm-d-modelservice-decode:3]"}
{"level":"INFO","ts":"2025-11-28T21:07:52.621Z","msg":"Capacity-only decisions made for model: unsloth/Meta-Llama-3.1-8B - decision count: 1"}
{"level":"INFO","ts":"2025-11-28T21:07:52.621Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-11-28T21:07:52.621Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=3→target=3"}
{"level":"DEBUG","ts":"2025-11-28T21:07:52.621Z","msg":"Found VA in map: variant=ms-inference-scheduling-llm-d-modelservice-decode, hasCurrentAlloc=true, accelerator=H100"}
{"level":"INFO","ts":"2025-11-28T21:07:52.621Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 3, desired-replicas: 3, accelerator: H100"}
{"level":"INFO","ts":"2025-11-28T21:07:52.621Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=3, accelerator=H100, capacityOnly=true"}
{"level":"INFO","ts":"2025-11-28T21:07:52.627Z","msg":"Applied capacity decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=3, target=3, reason=capacity-only mode: no-change"}
{"level":"INFO","ts":"2025-11-28T21:07:52.627Z","msg":"Reconciliation completed successfully: mode=capacity-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-11-28T21:07:52.724Z","msg":"Operating in CAPACITY-ONLY mode: reactive capacity-based scaling only"}
{"level":"INFO","ts":"2025-11-28T21:07:52.724Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-11-28T21:07:52.724Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-11-28T21:07:52.741Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=3, accelerator=H100, ttft=36.73ms, itl=19.70ms, cost=30.00, arrivalRate=4003.71"}
{"level":"INFO","ts":"2025-11-28T21:07:52.743Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9lshk5, queueLength=0"}
{"level":"INFO","ts":"2025-11-28T21:07:52.743Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9ww5mn, queueLength=0"}
{"level":"INFO","ts":"2025-11-28T21:07:52.743Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9l76md, queueLength=0"}
{"level":"DEBUG","ts":"2025-11-28T21:07:52.743Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=3"}
{"level":"INFO","ts":"2025-11-28T21:07:52.743Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9lshk5, usage=0.287 (28.7%)"}
{"level":"INFO","ts":"2025-11-28T21:07:52.743Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9ww5mn, usage=0.293 (29.3%)"}
{"level":"INFO","ts":"2025-11-28T21:07:52.743Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9l76md, usage=0.292 (29.2%)"}
{"level":"DEBUG","ts":"2025-11-28T21:07:52.743Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=3"}
{"level":"DEBUG","ts":"2025-11-28T21:07:52.743Z","msg":"Pod-to-variant matching successful: totalPods=3, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:3]"}
{"level":"DEBUG","ts":"2025-11-28T21:07:52.743Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=3"}
{"level":"DEBUG","ts":"2025-11-28T21:07:52.743Z","msg":"Collected capacity metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, metricsCount=3"}
{"level":"DEBUG","ts":"2025-11-28T21:07:52.743Z","msg":"Variant analysis initialized: variant=ms-inference-scheduling-llm-d-modelservice-decode, accelerator=H100, cost=10.00, replicaCount=3"}
{"level":"DEBUG","ts":"2025-11-28T21:07:52.743Z","msg":"Scale-down unsafe: insufficient headroom after redistribution: remainingSpareKv=0.064, kvTrigger=0.200, kvSafe=false, remainingSpareQueue=5.0, queueTrigger=3.0%!(EXTRA bool=true)"}
{"level":"DEBUG","ts":"2025-11-28T21:07:52.743Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, totalReplicas=3, nonSaturated=3, avgSpareKv=0.210, avgSpareQueue=5.0, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"INFO","ts":"2025-11-28T21:07:52.743Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, totalReplicas=3, nonSaturated=3, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"DEBUG","ts":"2025-11-28T21:07:52.743Z","msg":"Capacity targets: no scaling needed (avgSpareKv=0.210, avgSpareQueue=5.0, all variants stable)"}
{"level":"DEBUG","ts":"2025-11-28T21:07:52.743Z","msg":"Capacity targets calculated: modelID=unsloth/Meta-Llama-3.1-8B, targets=map[ms-inference-scheduling-llm-d-modelservice-decode:3]"}
{"level":"INFO","ts":"2025-11-28T21:07:52.743Z","msg":"Capacity-only decisions made for model: unsloth/Meta-Llama-3.1-8B - decision count: 1"}
{"level":"INFO","ts":"2025-11-28T21:07:52.743Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-11-28T21:07:52.743Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=3→target=3"}
{"level":"DEBUG","ts":"2025-11-28T21:07:52.743Z","msg":"Found VA in map: variant=ms-inference-scheduling-llm-d-modelservice-decode, hasCurrentAlloc=true, accelerator=H100"}
{"level":"INFO","ts":"2025-11-28T21:07:52.743Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 3, desired-replicas: 3, accelerator: H100"}
{"level":"INFO","ts":"2025-11-28T21:07:52.743Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=3, accelerator=H100, capacityOnly=true"}
{"level":"INFO","ts":"2025-11-28T21:07:52.748Z","msg":"Applied capacity decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=3, target=3, reason=capacity-only mode: no-change"}
{"level":"INFO","ts":"2025-11-28T21:07:52.748Z","msg":"Reconciliation completed successfully: mode=capacity-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-11-28T21:08:22.627Z","msg":"Operating in CAPACITY-ONLY mode: reactive capacity-based scaling only"}
{"level":"INFO","ts":"2025-11-28T21:08:22.627Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-11-28T21:08:22.627Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-11-28T21:08:22.648Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=3, accelerator=H100, ttft=35.49ms, itl=19.53ms, cost=30.00, arrivalRate=3995.64"}
{"level":"INFO","ts":"2025-11-28T21:08:22.650Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9lshk5, usage=0.287 (28.7%)"}
{"level":"INFO","ts":"2025-11-28T21:08:22.650Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9ww5mn, usage=0.293 (29.3%)"}
{"level":"INFO","ts":"2025-11-28T21:08:22.650Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9l76md, usage=0.292 (29.2%)"}
{"level":"DEBUG","ts":"2025-11-28T21:08:22.650Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=3"}
{"level":"INFO","ts":"2025-11-28T21:08:22.650Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9lshk5, queueLength=0"}
{"level":"INFO","ts":"2025-11-28T21:08:22.650Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9ww5mn, queueLength=0"}
{"level":"INFO","ts":"2025-11-28T21:08:22.650Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9l76md, queueLength=0"}
{"level":"DEBUG","ts":"2025-11-28T21:08:22.650Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=3"}
{"level":"DEBUG","ts":"2025-11-28T21:08:22.650Z","msg":"Pod-to-variant matching successful: totalPods=3, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:3]"}
{"level":"DEBUG","ts":"2025-11-28T21:08:22.650Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=3"}
{"level":"DEBUG","ts":"2025-11-28T21:08:22.650Z","msg":"Collected capacity metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, metricsCount=3"}
{"level":"DEBUG","ts":"2025-11-28T21:08:22.650Z","msg":"Variant analysis initialized: variant=ms-inference-scheduling-llm-d-modelservice-decode, accelerator=H100, cost=10.00, replicaCount=3"}
{"level":"DEBUG","ts":"2025-11-28T21:08:22.650Z","msg":"Scale-down unsafe: insufficient headroom after redistribution: remainingSpareKv=0.064, kvTrigger=0.200, kvSafe=false, remainingSpareQueue=5.0, queueTrigger=3.0%!(EXTRA bool=true)"}
{"level":"DEBUG","ts":"2025-11-28T21:08:22.650Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, totalReplicas=3, nonSaturated=3, avgSpareKv=0.210, avgSpareQueue=5.0, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"INFO","ts":"2025-11-28T21:08:22.650Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, totalReplicas=3, nonSaturated=3, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"DEBUG","ts":"2025-11-28T21:08:22.650Z","msg":"Capacity targets: no scaling needed (avgSpareKv=0.210, avgSpareQueue=5.0, all variants stable)"}
{"level":"DEBUG","ts":"2025-11-28T21:08:22.650Z","msg":"Capacity targets calculated: modelID=unsloth/Meta-Llama-3.1-8B, targets=map[ms-inference-scheduling-llm-d-modelservice-decode:3]"}
{"level":"INFO","ts":"2025-11-28T21:08:22.650Z","msg":"Capacity-only decisions made for model: unsloth/Meta-Llama-3.1-8B - decision count: 1"}
{"level":"INFO","ts":"2025-11-28T21:08:22.650Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-11-28T21:08:22.650Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=3→target=3"}
{"level":"DEBUG","ts":"2025-11-28T21:08:22.650Z","msg":"Found VA in map: variant=ms-inference-scheduling-llm-d-modelservice-decode, hasCurrentAlloc=true, accelerator=H100"}
{"level":"INFO","ts":"2025-11-28T21:08:22.650Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 3, desired-replicas: 3, accelerator: H100"}
{"level":"INFO","ts":"2025-11-28T21:08:22.650Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=3, accelerator=H100, capacityOnly=true"}
{"level":"INFO","ts":"2025-11-28T21:08:22.656Z","msg":"Applied capacity decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=3, target=3, reason=capacity-only mode: no-change"}
{"level":"INFO","ts":"2025-11-28T21:08:22.656Z","msg":"Reconciliation completed successfully: mode=capacity-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-11-28T21:08:22.749Z","msg":"Operating in CAPACITY-ONLY mode: reactive capacity-based scaling only"}
{"level":"INFO","ts":"2025-11-28T21:08:22.749Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-11-28T21:08:22.749Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-11-28T21:08:22.760Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=3, accelerator=H100, ttft=35.49ms, itl=19.53ms, cost=30.00, arrivalRate=3995.64"}
{"level":"INFO","ts":"2025-11-28T21:08:22.762Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9lshk5, queueLength=0"}
{"level":"INFO","ts":"2025-11-28T21:08:22.762Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9ww5mn, queueLength=0"}
{"level":"INFO","ts":"2025-11-28T21:08:22.762Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9l76md, queueLength=0"}
{"level":"DEBUG","ts":"2025-11-28T21:08:22.762Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=3"}
{"level":"INFO","ts":"2025-11-28T21:08:22.762Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9lshk5, usage=0.287 (28.7%)"}
{"level":"INFO","ts":"2025-11-28T21:08:22.762Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9ww5mn, usage=0.293 (29.3%)"}
{"level":"INFO","ts":"2025-11-28T21:08:22.762Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9l76md, usage=0.292 (29.2%)"}
{"level":"DEBUG","ts":"2025-11-28T21:08:22.762Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=3"}
{"level":"DEBUG","ts":"2025-11-28T21:08:22.762Z","msg":"Pod-to-variant matching successful: totalPods=3, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:3]"}
{"level":"DEBUG","ts":"2025-11-28T21:08:22.762Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=3"}
{"level":"DEBUG","ts":"2025-11-28T21:08:22.762Z","msg":"Collected capacity metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, metricsCount=3"}
{"level":"DEBUG","ts":"2025-11-28T21:08:22.762Z","msg":"Variant analysis initialized: variant=ms-inference-scheduling-llm-d-modelservice-decode, accelerator=H100, cost=10.00, replicaCount=3"}
{"level":"DEBUG","ts":"2025-11-28T21:08:22.762Z","msg":"Scale-down unsafe: insufficient headroom after redistribution: remainingSpareKv=0.064, kvTrigger=0.200, kvSafe=false, remainingSpareQueue=5.0, queueTrigger=3.0%!(EXTRA bool=true)"}
{"level":"DEBUG","ts":"2025-11-28T21:08:22.762Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, totalReplicas=3, nonSaturated=3, avgSpareKv=0.210, avgSpareQueue=5.0, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"INFO","ts":"2025-11-28T21:08:22.762Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, totalReplicas=3, nonSaturated=3, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"DEBUG","ts":"2025-11-28T21:08:22.762Z","msg":"Capacity targets: no scaling needed (avgSpareKv=0.210, avgSpareQueue=5.0, all variants stable)"}
{"level":"DEBUG","ts":"2025-11-28T21:08:22.762Z","msg":"Capacity targets calculated: modelID=unsloth/Meta-Llama-3.1-8B, targets=map[ms-inference-scheduling-llm-d-modelservice-decode:3]"}
{"level":"INFO","ts":"2025-11-28T21:08:22.762Z","msg":"Capacity-only decisions made for model: unsloth/Meta-Llama-3.1-8B - decision count: 1"}
{"level":"INFO","ts":"2025-11-28T21:08:22.762Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-11-28T21:08:22.762Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=3→target=3"}
{"level":"DEBUG","ts":"2025-11-28T21:08:22.762Z","msg":"Found VA in map: variant=ms-inference-scheduling-llm-d-modelservice-decode, hasCurrentAlloc=true, accelerator=H100"}
{"level":"INFO","ts":"2025-11-28T21:08:22.762Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 3, desired-replicas: 3, accelerator: H100"}
{"level":"INFO","ts":"2025-11-28T21:08:22.762Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=3, accelerator=H100, capacityOnly=true"}
{"level":"INFO","ts":"2025-11-28T21:08:22.766Z","msg":"Applied capacity decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=3, target=3, reason=capacity-only mode: no-change"}
{"level":"INFO","ts":"2025-11-28T21:08:22.766Z","msg":"Reconciliation completed successfully: mode=capacity-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-11-28T21:08:52.657Z","msg":"Operating in CAPACITY-ONLY mode: reactive capacity-based scaling only"}
{"level":"INFO","ts":"2025-11-28T21:08:52.657Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-11-28T21:08:52.657Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-11-28T21:08:52.680Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=3, accelerator=H100, ttft=34.76ms, itl=19.00ms, cost=30.00, arrivalRate=3783.56"}
{"level":"INFO","ts":"2025-11-28T21:08:52.681Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9lshk5, usage=0.255 (25.5%)"}
{"level":"INFO","ts":"2025-11-28T21:08:52.681Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9ww5mn, usage=0.343 (34.3%)"}
{"level":"INFO","ts":"2025-11-28T21:08:52.681Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9l76md, usage=0.232 (23.2%)"}
{"level":"DEBUG","ts":"2025-11-28T21:08:52.681Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=3"}
{"level":"INFO","ts":"2025-11-28T21:08:52.681Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9lshk5, queueLength=0"}
{"level":"INFO","ts":"2025-11-28T21:08:52.682Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9ww5mn, queueLength=0"}
{"level":"INFO","ts":"2025-11-28T21:08:52.682Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9l76md, queueLength=0"}
{"level":"DEBUG","ts":"2025-11-28T21:08:52.682Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=3"}
{"level":"DEBUG","ts":"2025-11-28T21:08:52.682Z","msg":"Pod-to-variant matching successful: totalPods=3, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:3]"}
{"level":"DEBUG","ts":"2025-11-28T21:08:52.682Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=3"}
{"level":"DEBUG","ts":"2025-11-28T21:08:52.682Z","msg":"Collected capacity metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, metricsCount=3"}
{"level":"DEBUG","ts":"2025-11-28T21:08:52.682Z","msg":"Variant analysis initialized: variant=ms-inference-scheduling-llm-d-modelservice-decode, accelerator=H100, cost=10.00, replicaCount=3"}
{"level":"DEBUG","ts":"2025-11-28T21:08:52.682Z","msg":"Scale-down unsafe: insufficient headroom after redistribution: remainingSpareKv=0.085, kvTrigger=0.200, kvSafe=false, remainingSpareQueue=5.0, queueTrigger=3.0%!(EXTRA bool=true)"}
{"level":"DEBUG","ts":"2025-11-28T21:08:52.682Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, totalReplicas=3, nonSaturated=3, avgSpareKv=0.223, avgSpareQueue=5.0, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"INFO","ts":"2025-11-28T21:08:52.682Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, totalReplicas=3, nonSaturated=3, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"DEBUG","ts":"2025-11-28T21:08:52.682Z","msg":"Capacity targets: no scaling needed (avgSpareKv=0.223, avgSpareQueue=5.0, all variants stable)"}
{"level":"DEBUG","ts":"2025-11-28T21:08:52.682Z","msg":"Capacity targets calculated: modelID=unsloth/Meta-Llama-3.1-8B, targets=map[ms-inference-scheduling-llm-d-modelservice-decode:3]"}
{"level":"INFO","ts":"2025-11-28T21:08:52.682Z","msg":"Capacity-only decisions made for model: unsloth/Meta-Llama-3.1-8B - decision count: 1"}
{"level":"INFO","ts":"2025-11-28T21:08:52.682Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-11-28T21:08:52.682Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=3→target=3"}
{"level":"DEBUG","ts":"2025-11-28T21:08:52.682Z","msg":"Found VA in map: variant=ms-inference-scheduling-llm-d-modelservice-decode, hasCurrentAlloc=true, accelerator=H100"}
{"level":"INFO","ts":"2025-11-28T21:08:52.682Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 3, desired-replicas: 3, accelerator: H100"}
{"level":"INFO","ts":"2025-11-28T21:08:52.682Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=3, accelerator=H100, capacityOnly=true"}
{"level":"INFO","ts":"2025-11-28T21:08:52.688Z","msg":"Applied capacity decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=3, target=3, reason=capacity-only mode: no-change"}
{"level":"INFO","ts":"2025-11-28T21:08:52.688Z","msg":"Reconciliation completed successfully: mode=capacity-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-11-28T21:08:52.767Z","msg":"Operating in CAPACITY-ONLY mode: reactive capacity-based scaling only"}
{"level":"INFO","ts":"2025-11-28T21:08:52.767Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-11-28T21:08:52.767Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-11-28T21:08:52.779Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=3, accelerator=H100, ttft=34.76ms, itl=19.00ms, cost=30.00, arrivalRate=3783.56"}
{"level":"INFO","ts":"2025-11-28T21:08:52.780Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9lshk5, queueLength=0"}
{"level":"INFO","ts":"2025-11-28T21:08:52.780Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9ww5mn, queueLength=0"}
{"level":"INFO","ts":"2025-11-28T21:08:52.780Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9l76md, queueLength=0"}
{"level":"DEBUG","ts":"2025-11-28T21:08:52.780Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=3"}
{"level":"INFO","ts":"2025-11-28T21:08:52.781Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9lshk5, usage=0.255 (25.5%)"}
{"level":"INFO","ts":"2025-11-28T21:08:52.781Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9ww5mn, usage=0.343 (34.3%)"}
{"level":"INFO","ts":"2025-11-28T21:08:52.781Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9l76md, usage=0.232 (23.2%)"}
{"level":"DEBUG","ts":"2025-11-28T21:08:52.781Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=3"}
{"level":"DEBUG","ts":"2025-11-28T21:08:52.781Z","msg":"Pod-to-variant matching successful: totalPods=3, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:3]"}
{"level":"DEBUG","ts":"2025-11-28T21:08:52.781Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=3"}
{"level":"DEBUG","ts":"2025-11-28T21:08:52.781Z","msg":"Collected capacity metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, metricsCount=3"}
{"level":"DEBUG","ts":"2025-11-28T21:08:52.781Z","msg":"Variant analysis initialized: variant=ms-inference-scheduling-llm-d-modelservice-decode, accelerator=H100, cost=10.00, replicaCount=3"}
{"level":"DEBUG","ts":"2025-11-28T21:08:52.781Z","msg":"Scale-down unsafe: insufficient headroom after redistribution: remainingSpareKv=0.085, kvTrigger=0.200, kvSafe=false, remainingSpareQueue=5.0, queueTrigger=3.0%!(EXTRA bool=true)"}
{"level":"DEBUG","ts":"2025-11-28T21:08:52.781Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, totalReplicas=3, nonSaturated=3, avgSpareKv=0.223, avgSpareQueue=5.0, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"INFO","ts":"2025-11-28T21:08:52.781Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, totalReplicas=3, nonSaturated=3, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"DEBUG","ts":"2025-11-28T21:08:52.781Z","msg":"Capacity targets: no scaling needed (avgSpareKv=0.223, avgSpareQueue=5.0, all variants stable)"}
{"level":"DEBUG","ts":"2025-11-28T21:08:52.781Z","msg":"Capacity targets calculated: modelID=unsloth/Meta-Llama-3.1-8B, targets=map[ms-inference-scheduling-llm-d-modelservice-decode:3]"}
{"level":"INFO","ts":"2025-11-28T21:08:52.781Z","msg":"Capacity-only decisions made for model: unsloth/Meta-Llama-3.1-8B - decision count: 1"}
{"level":"INFO","ts":"2025-11-28T21:08:52.781Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-11-28T21:08:52.781Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=3→target=3"}
{"level":"DEBUG","ts":"2025-11-28T21:08:52.781Z","msg":"Found VA in map: variant=ms-inference-scheduling-llm-d-modelservice-decode, hasCurrentAlloc=true, accelerator=H100"}
{"level":"INFO","ts":"2025-11-28T21:08:52.781Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 3, desired-replicas: 3, accelerator: H100"}
{"level":"INFO","ts":"2025-11-28T21:08:52.781Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=3, accelerator=H100, capacityOnly=true"}
{"level":"INFO","ts":"2025-11-28T21:08:52.786Z","msg":"Applied capacity decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=3, target=3, reason=capacity-only mode: no-change"}
{"level":"INFO","ts":"2025-11-28T21:08:52.786Z","msg":"Reconciliation completed successfully: mode=capacity-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-11-28T21:09:22.688Z","msg":"Operating in CAPACITY-ONLY mode: reactive capacity-based scaling only"}
{"level":"INFO","ts":"2025-11-28T21:09:22.688Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-11-28T21:09:22.688Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-11-28T21:09:22.704Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=3, accelerator=H100, ttft=34.91ms, itl=18.92ms, cost=30.00, arrivalRate=3833.55"}
{"level":"INFO","ts":"2025-11-28T21:09:22.706Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9lshk5, queueLength=0"}
{"level":"INFO","ts":"2025-11-28T21:09:22.706Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9ww5mn, queueLength=0"}
{"level":"INFO","ts":"2025-11-28T21:09:22.706Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9l76md, queueLength=0"}
{"level":"DEBUG","ts":"2025-11-28T21:09:22.706Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=3"}
{"level":"INFO","ts":"2025-11-28T21:09:22.706Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9lshk5, usage=0.238 (23.8%)"}
{"level":"INFO","ts":"2025-11-28T21:09:22.706Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9ww5mn, usage=0.343 (34.3%)"}
{"level":"INFO","ts":"2025-11-28T21:09:22.706Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9l76md, usage=0.232 (23.2%)"}
{"level":"DEBUG","ts":"2025-11-28T21:09:22.706Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=3"}
{"level":"DEBUG","ts":"2025-11-28T21:09:22.706Z","msg":"Pod-to-variant matching successful: totalPods=3, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:3]"}
{"level":"DEBUG","ts":"2025-11-28T21:09:22.706Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=3"}
{"level":"DEBUG","ts":"2025-11-28T21:09:22.706Z","msg":"Collected capacity metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, metricsCount=3"}
{"level":"DEBUG","ts":"2025-11-28T21:09:22.706Z","msg":"Variant analysis initialized: variant=ms-inference-scheduling-llm-d-modelservice-decode, accelerator=H100, cost=10.00, replicaCount=3"}
{"level":"DEBUG","ts":"2025-11-28T21:09:22.706Z","msg":"Scale-down unsafe: insufficient headroom after redistribution: remainingSpareKv=0.093, kvTrigger=0.200, kvSafe=false, remainingSpareQueue=5.0, queueTrigger=3.0%!(EXTRA bool=true)"}
{"level":"DEBUG","ts":"2025-11-28T21:09:22.706Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, totalReplicas=3, nonSaturated=3, avgSpareKv=0.229, avgSpareQueue=5.0, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"INFO","ts":"2025-11-28T21:09:22.706Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, totalReplicas=3, nonSaturated=3, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"DEBUG","ts":"2025-11-28T21:09:22.706Z","msg":"Capacity targets: no scaling needed (avgSpareKv=0.229, avgSpareQueue=5.0, all variants stable)"}
{"level":"DEBUG","ts":"2025-11-28T21:09:22.706Z","msg":"Capacity targets calculated: modelID=unsloth/Meta-Llama-3.1-8B, targets=map[ms-inference-scheduling-llm-d-modelservice-decode:3]"}
{"level":"INFO","ts":"2025-11-28T21:09:22.706Z","msg":"Capacity-only decisions made for model: unsloth/Meta-Llama-3.1-8B - decision count: 1"}
{"level":"INFO","ts":"2025-11-28T21:09:22.706Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-11-28T21:09:22.706Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=3→target=3"}
{"level":"DEBUG","ts":"2025-11-28T21:09:22.706Z","msg":"Found VA in map: variant=ms-inference-scheduling-llm-d-modelservice-decode, hasCurrentAlloc=true, accelerator=H100"}
{"level":"INFO","ts":"2025-11-28T21:09:22.706Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 3, desired-replicas: 3, accelerator: H100"}
{"level":"INFO","ts":"2025-11-28T21:09:22.706Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=3, accelerator=H100, capacityOnly=true"}
{"level":"INFO","ts":"2025-11-28T21:09:22.711Z","msg":"Applied capacity decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=3, target=3, reason=capacity-only mode: no-change"}
{"level":"INFO","ts":"2025-11-28T21:09:22.711Z","msg":"Reconciliation completed successfully: mode=capacity-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-11-28T21:09:22.787Z","msg":"Operating in CAPACITY-ONLY mode: reactive capacity-based scaling only"}
{"level":"INFO","ts":"2025-11-28T21:09:22.787Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-11-28T21:09:22.787Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-11-28T21:09:22.797Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=3, accelerator=H100, ttft=34.91ms, itl=18.92ms, cost=30.00, arrivalRate=3833.55"}
{"level":"INFO","ts":"2025-11-28T21:09:22.799Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9lshk5, usage=0.238 (23.8%)"}
{"level":"INFO","ts":"2025-11-28T21:09:22.799Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9ww5mn, usage=0.343 (34.3%)"}
{"level":"INFO","ts":"2025-11-28T21:09:22.799Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9l76md, usage=0.232 (23.2%)"}
{"level":"DEBUG","ts":"2025-11-28T21:09:22.799Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=3"}
{"level":"INFO","ts":"2025-11-28T21:09:22.799Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9lshk5, queueLength=0"}
{"level":"INFO","ts":"2025-11-28T21:09:22.799Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9ww5mn, queueLength=0"}
{"level":"INFO","ts":"2025-11-28T21:09:22.799Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9l76md, queueLength=0"}
{"level":"DEBUG","ts":"2025-11-28T21:09:22.799Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=3"}
{"level":"DEBUG","ts":"2025-11-28T21:09:22.799Z","msg":"Pod-to-variant matching successful: totalPods=3, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:3]"}
{"level":"DEBUG","ts":"2025-11-28T21:09:22.799Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=3"}
{"level":"DEBUG","ts":"2025-11-28T21:09:22.799Z","msg":"Collected capacity metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, metricsCount=3"}
{"level":"DEBUG","ts":"2025-11-28T21:09:22.799Z","msg":"Variant analysis initialized: variant=ms-inference-scheduling-llm-d-modelservice-decode, accelerator=H100, cost=10.00, replicaCount=3"}
{"level":"DEBUG","ts":"2025-11-28T21:09:22.799Z","msg":"Scale-down unsafe: insufficient headroom after redistribution: remainingSpareKv=0.093, kvTrigger=0.200, kvSafe=false, remainingSpareQueue=5.0, queueTrigger=3.0%!(EXTRA bool=true)"}
{"level":"DEBUG","ts":"2025-11-28T21:09:22.799Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, totalReplicas=3, nonSaturated=3, avgSpareKv=0.229, avgSpareQueue=5.0, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"INFO","ts":"2025-11-28T21:09:22.799Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, totalReplicas=3, nonSaturated=3, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"DEBUG","ts":"2025-11-28T21:09:22.799Z","msg":"Capacity targets: no scaling needed (avgSpareKv=0.229, avgSpareQueue=5.0, all variants stable)"}
{"level":"DEBUG","ts":"2025-11-28T21:09:22.799Z","msg":"Capacity targets calculated: modelID=unsloth/Meta-Llama-3.1-8B, targets=map[ms-inference-scheduling-llm-d-modelservice-decode:3]"}
{"level":"INFO","ts":"2025-11-28T21:09:22.799Z","msg":"Capacity-only decisions made for model: unsloth/Meta-Llama-3.1-8B - decision count: 1"}
{"level":"INFO","ts":"2025-11-28T21:09:22.799Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-11-28T21:09:22.799Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=3→target=3"}
{"level":"DEBUG","ts":"2025-11-28T21:09:22.799Z","msg":"Found VA in map: variant=ms-inference-scheduling-llm-d-modelservice-decode, hasCurrentAlloc=true, accelerator=H100"}
{"level":"INFO","ts":"2025-11-28T21:09:22.799Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 3, desired-replicas: 3, accelerator: H100"}
{"level":"INFO","ts":"2025-11-28T21:09:22.799Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=3, accelerator=H100, capacityOnly=true"}
{"level":"INFO","ts":"2025-11-28T21:09:22.804Z","msg":"Applied capacity decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=3, target=3, reason=capacity-only mode: no-change"}
{"level":"INFO","ts":"2025-11-28T21:09:22.804Z","msg":"Reconciliation completed successfully: mode=capacity-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-11-28T21:09:52.711Z","msg":"Operating in CAPACITY-ONLY mode: reactive capacity-based scaling only"}
{"level":"INFO","ts":"2025-11-28T21:09:52.712Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-11-28T21:09:52.712Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-11-28T21:09:52.724Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=3, accelerator=H100, ttft=34.56ms, itl=18.93ms, cost=30.00, arrivalRate=3860.36"}
{"level":"INFO","ts":"2025-11-28T21:09:52.726Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9lshk5, queueLength=0"}
{"level":"INFO","ts":"2025-11-28T21:09:52.726Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9ww5mn, queueLength=0"}
{"level":"INFO","ts":"2025-11-28T21:09:52.726Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9l76md, queueLength=0"}
{"level":"DEBUG","ts":"2025-11-28T21:09:52.726Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=3"}
{"level":"INFO","ts":"2025-11-28T21:09:52.726Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9lshk5, usage=0.232 (23.2%)"}
{"level":"INFO","ts":"2025-11-28T21:09:52.726Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9ww5mn, usage=0.335 (33.5%)"}
{"level":"INFO","ts":"2025-11-28T21:09:52.726Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9l76md, usage=0.231 (23.1%)"}
{"level":"DEBUG","ts":"2025-11-28T21:09:52.726Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=3"}
{"level":"DEBUG","ts":"2025-11-28T21:09:52.726Z","msg":"Pod-to-variant matching successful: totalPods=3, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:3]"}
{"level":"DEBUG","ts":"2025-11-28T21:09:52.726Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=3"}
{"level":"DEBUG","ts":"2025-11-28T21:09:52.726Z","msg":"Collected capacity metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, metricsCount=3"}
{"level":"DEBUG","ts":"2025-11-28T21:09:52.726Z","msg":"Variant analysis initialized: variant=ms-inference-scheduling-llm-d-modelservice-decode, accelerator=H100, cost=10.00, replicaCount=3"}
{"level":"DEBUG","ts":"2025-11-28T21:09:52.726Z","msg":"Scale-down unsafe: insufficient headroom after redistribution: remainingSpareKv=0.101, kvTrigger=0.200, kvSafe=false, remainingSpareQueue=5.0, queueTrigger=3.0%!(EXTRA bool=true)"}
{"level":"DEBUG","ts":"2025-11-28T21:09:52.726Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, totalReplicas=3, nonSaturated=3, avgSpareKv=0.234, avgSpareQueue=5.0, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"INFO","ts":"2025-11-28T21:09:52.726Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, totalReplicas=3, nonSaturated=3, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"DEBUG","ts":"2025-11-28T21:09:52.726Z","msg":"Capacity targets: no scaling needed (avgSpareKv=0.234, avgSpareQueue=5.0, all variants stable)"}
{"level":"DEBUG","ts":"2025-11-28T21:09:52.726Z","msg":"Capacity targets calculated: modelID=unsloth/Meta-Llama-3.1-8B, targets=map[ms-inference-scheduling-llm-d-modelservice-decode:3]"}
{"level":"INFO","ts":"2025-11-28T21:09:52.726Z","msg":"Capacity-only decisions made for model: unsloth/Meta-Llama-3.1-8B - decision count: 1"}
{"level":"INFO","ts":"2025-11-28T21:09:52.726Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-11-28T21:09:52.726Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=3→target=3"}
{"level":"DEBUG","ts":"2025-11-28T21:09:52.726Z","msg":"Found VA in map: variant=ms-inference-scheduling-llm-d-modelservice-decode, hasCurrentAlloc=true, accelerator=H100"}
{"level":"INFO","ts":"2025-11-28T21:09:52.726Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 3, desired-replicas: 3, accelerator: H100"}
{"level":"INFO","ts":"2025-11-28T21:09:52.726Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=3, accelerator=H100, capacityOnly=true"}
{"level":"INFO","ts":"2025-11-28T21:09:52.731Z","msg":"Applied capacity decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=3, target=3, reason=capacity-only mode: no-change"}
{"level":"INFO","ts":"2025-11-28T21:09:52.731Z","msg":"Reconciliation completed successfully: mode=capacity-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-11-28T21:09:52.805Z","msg":"Operating in CAPACITY-ONLY mode: reactive capacity-based scaling only"}
{"level":"INFO","ts":"2025-11-28T21:09:52.805Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-11-28T21:09:52.805Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-11-28T21:09:52.841Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=3, accelerator=H100, ttft=34.56ms, itl=18.93ms, cost=30.00, arrivalRate=3860.36"}
{"level":"INFO","ts":"2025-11-28T21:09:52.843Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9lshk5, queueLength=0"}
{"level":"INFO","ts":"2025-11-28T21:09:52.843Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9ww5mn, queueLength=0"}
{"level":"INFO","ts":"2025-11-28T21:09:52.843Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9l76md, queueLength=0"}
{"level":"DEBUG","ts":"2025-11-28T21:09:52.843Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=3"}
{"level":"INFO","ts":"2025-11-28T21:09:52.844Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9lshk5, usage=0.232 (23.2%)"}
{"level":"INFO","ts":"2025-11-28T21:09:52.844Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9ww5mn, usage=0.335 (33.5%)"}
{"level":"INFO","ts":"2025-11-28T21:09:52.844Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9l76md, usage=0.231 (23.1%)"}
{"level":"DEBUG","ts":"2025-11-28T21:09:52.844Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=3"}
{"level":"DEBUG","ts":"2025-11-28T21:09:52.844Z","msg":"Pod-to-variant matching successful: totalPods=3, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:3]"}
{"level":"DEBUG","ts":"2025-11-28T21:09:52.844Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=3"}
{"level":"DEBUG","ts":"2025-11-28T21:09:52.844Z","msg":"Collected capacity metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, metricsCount=3"}
{"level":"DEBUG","ts":"2025-11-28T21:09:52.844Z","msg":"Variant analysis initialized: variant=ms-inference-scheduling-llm-d-modelservice-decode, accelerator=H100, cost=10.00, replicaCount=3"}
{"level":"DEBUG","ts":"2025-11-28T21:09:52.844Z","msg":"Scale-down unsafe: insufficient headroom after redistribution: remainingSpareKv=0.101, kvTrigger=0.200, kvSafe=false, remainingSpareQueue=5.0, queueTrigger=3.0%!(EXTRA bool=true)"}
{"level":"DEBUG","ts":"2025-11-28T21:09:52.844Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, totalReplicas=3, nonSaturated=3, avgSpareKv=0.234, avgSpareQueue=5.0, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"INFO","ts":"2025-11-28T21:09:52.844Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, totalReplicas=3, nonSaturated=3, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"DEBUG","ts":"2025-11-28T21:09:52.844Z","msg":"Capacity targets: no scaling needed (avgSpareKv=0.234, avgSpareQueue=5.0, all variants stable)"}
{"level":"DEBUG","ts":"2025-11-28T21:09:52.844Z","msg":"Capacity targets calculated: modelID=unsloth/Meta-Llama-3.1-8B, targets=map[ms-inference-scheduling-llm-d-modelservice-decode:3]"}
{"level":"INFO","ts":"2025-11-28T21:09:52.844Z","msg":"Capacity-only decisions made for model: unsloth/Meta-Llama-3.1-8B - decision count: 1"}
{"level":"INFO","ts":"2025-11-28T21:09:52.844Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-11-28T21:09:52.844Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=3→target=3"}
{"level":"DEBUG","ts":"2025-11-28T21:09:52.844Z","msg":"Found VA in map: variant=ms-inference-scheduling-llm-d-modelservice-decode, hasCurrentAlloc=true, accelerator=H100"}
{"level":"INFO","ts":"2025-11-28T21:09:52.844Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 3, desired-replicas: 3, accelerator: H100"}
{"level":"INFO","ts":"2025-11-28T21:09:52.844Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=3, accelerator=H100, capacityOnly=true"}
{"level":"INFO","ts":"2025-11-28T21:09:52.849Z","msg":"Applied capacity decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=3, target=3, reason=capacity-only mode: no-change"}
{"level":"INFO","ts":"2025-11-28T21:09:52.849Z","msg":"Reconciliation completed successfully: mode=capacity-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-11-28T21:10:22.731Z","msg":"Operating in CAPACITY-ONLY mode: reactive capacity-based scaling only"}
{"level":"INFO","ts":"2025-11-28T21:10:22.731Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-11-28T21:10:22.731Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-11-28T21:10:22.747Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=3, accelerator=H100, ttft=34.78ms, itl=18.74ms, cost=30.00, arrivalRate=3986.01"}
{"level":"INFO","ts":"2025-11-28T21:10:22.749Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9lshk5, usage=0.223 (22.3%)"}
{"level":"INFO","ts":"2025-11-28T21:10:22.749Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9ww5mn, usage=0.308 (30.8%)"}
{"level":"INFO","ts":"2025-11-28T21:10:22.749Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9l76md, usage=0.231 (23.1%)"}
{"level":"DEBUG","ts":"2025-11-28T21:10:22.749Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=3"}
{"level":"INFO","ts":"2025-11-28T21:10:22.749Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9lshk5, queueLength=0"}
{"level":"INFO","ts":"2025-11-28T21:10:22.749Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9ww5mn, queueLength=0"}
{"level":"INFO","ts":"2025-11-28T21:10:22.749Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9l76md, queueLength=0"}
{"level":"DEBUG","ts":"2025-11-28T21:10:22.749Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=3"}
{"level":"DEBUG","ts":"2025-11-28T21:10:22.750Z","msg":"Pod-to-variant matching successful: totalPods=3, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:3]"}
{"level":"DEBUG","ts":"2025-11-28T21:10:22.750Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=3"}
{"level":"DEBUG","ts":"2025-11-28T21:10:22.750Z","msg":"Collected capacity metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, metricsCount=3"}
{"level":"DEBUG","ts":"2025-11-28T21:10:22.750Z","msg":"Variant analysis initialized: variant=ms-inference-scheduling-llm-d-modelservice-decode, accelerator=H100, cost=10.00, replicaCount=3"}
{"level":"DEBUG","ts":"2025-11-28T21:10:22.750Z","msg":"Scale-down unsafe: insufficient headroom after redistribution: remainingSpareKv=0.119, kvTrigger=0.200, kvSafe=false, remainingSpareQueue=5.0, queueTrigger=3.0%!(EXTRA bool=true)"}
{"level":"DEBUG","ts":"2025-11-28T21:10:22.750Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, totalReplicas=3, nonSaturated=3, avgSpareKv=0.246, avgSpareQueue=5.0, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"INFO","ts":"2025-11-28T21:10:22.750Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, totalReplicas=3, nonSaturated=3, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"DEBUG","ts":"2025-11-28T21:10:22.750Z","msg":"Capacity targets: no scaling needed (avgSpareKv=0.246, avgSpareQueue=5.0, all variants stable)"}
{"level":"DEBUG","ts":"2025-11-28T21:10:22.750Z","msg":"Capacity targets calculated: modelID=unsloth/Meta-Llama-3.1-8B, targets=map[ms-inference-scheduling-llm-d-modelservice-decode:3]"}
{"level":"INFO","ts":"2025-11-28T21:10:22.750Z","msg":"Capacity-only decisions made for model: unsloth/Meta-Llama-3.1-8B - decision count: 1"}
{"level":"INFO","ts":"2025-11-28T21:10:22.750Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-11-28T21:10:22.750Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=3→target=3"}
{"level":"DEBUG","ts":"2025-11-28T21:10:22.750Z","msg":"Found VA in map: variant=ms-inference-scheduling-llm-d-modelservice-decode, hasCurrentAlloc=true, accelerator=H100"}
{"level":"INFO","ts":"2025-11-28T21:10:22.750Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 3, desired-replicas: 3, accelerator: H100"}
{"level":"INFO","ts":"2025-11-28T21:10:22.750Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=3, accelerator=H100, capacityOnly=true"}
{"level":"INFO","ts":"2025-11-28T21:10:22.756Z","msg":"Applied capacity decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=3, target=3, reason=capacity-only mode: no-change"}
{"level":"INFO","ts":"2025-11-28T21:10:22.757Z","msg":"Reconciliation completed successfully: mode=capacity-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-11-28T21:10:22.850Z","msg":"Operating in CAPACITY-ONLY mode: reactive capacity-based scaling only"}
{"level":"INFO","ts":"2025-11-28T21:10:22.850Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-11-28T21:10:22.850Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-11-28T21:10:22.861Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=3, accelerator=H100, ttft=34.78ms, itl=18.74ms, cost=30.00, arrivalRate=3986.01"}
{"level":"INFO","ts":"2025-11-28T21:10:22.862Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9lshk5, usage=0.223 (22.3%)"}
{"level":"INFO","ts":"2025-11-28T21:10:22.862Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9ww5mn, usage=0.308 (30.8%)"}
{"level":"INFO","ts":"2025-11-28T21:10:22.862Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9l76md, usage=0.231 (23.1%)"}
{"level":"DEBUG","ts":"2025-11-28T21:10:22.862Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=3"}
{"level":"INFO","ts":"2025-11-28T21:10:22.862Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9lshk5, queueLength=0"}
{"level":"INFO","ts":"2025-11-28T21:10:22.862Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9ww5mn, queueLength=0"}
{"level":"INFO","ts":"2025-11-28T21:10:22.862Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9l76md, queueLength=0"}
{"level":"DEBUG","ts":"2025-11-28T21:10:22.862Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=3"}
{"level":"DEBUG","ts":"2025-11-28T21:10:22.863Z","msg":"Pod-to-variant matching successful: totalPods=3, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:3]"}
{"level":"DEBUG","ts":"2025-11-28T21:10:22.863Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=3"}
{"level":"DEBUG","ts":"2025-11-28T21:10:22.863Z","msg":"Collected capacity metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, metricsCount=3"}
{"level":"DEBUG","ts":"2025-11-28T21:10:22.863Z","msg":"Variant analysis initialized: variant=ms-inference-scheduling-llm-d-modelservice-decode, accelerator=H100, cost=10.00, replicaCount=3"}
{"level":"DEBUG","ts":"2025-11-28T21:10:22.863Z","msg":"Scale-down unsafe: insufficient headroom after redistribution: remainingSpareKv=0.119, kvTrigger=0.200, kvSafe=false, remainingSpareQueue=5.0, queueTrigger=3.0%!(EXTRA bool=true)"}
{"level":"DEBUG","ts":"2025-11-28T21:10:22.863Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, totalReplicas=3, nonSaturated=3, avgSpareKv=0.246, avgSpareQueue=5.0, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"INFO","ts":"2025-11-28T21:10:22.863Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, totalReplicas=3, nonSaturated=3, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"DEBUG","ts":"2025-11-28T21:10:22.863Z","msg":"Capacity targets: no scaling needed (avgSpareKv=0.246, avgSpareQueue=5.0, all variants stable)"}
{"level":"DEBUG","ts":"2025-11-28T21:10:22.863Z","msg":"Capacity targets calculated: modelID=unsloth/Meta-Llama-3.1-8B, targets=map[ms-inference-scheduling-llm-d-modelservice-decode:3]"}
{"level":"INFO","ts":"2025-11-28T21:10:22.863Z","msg":"Capacity-only decisions made for model: unsloth/Meta-Llama-3.1-8B - decision count: 1"}
{"level":"INFO","ts":"2025-11-28T21:10:22.863Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-11-28T21:10:22.863Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=3→target=3"}
{"level":"DEBUG","ts":"2025-11-28T21:10:22.863Z","msg":"Found VA in map: variant=ms-inference-scheduling-llm-d-modelservice-decode, hasCurrentAlloc=true, accelerator=H100"}
{"level":"INFO","ts":"2025-11-28T21:10:22.863Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 3, desired-replicas: 3, accelerator: H100"}
{"level":"INFO","ts":"2025-11-28T21:10:22.863Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=3, accelerator=H100, capacityOnly=true"}
{"level":"INFO","ts":"2025-11-28T21:10:22.867Z","msg":"Applied capacity decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=3, target=3, reason=capacity-only mode: no-change"}
{"level":"INFO","ts":"2025-11-28T21:10:22.867Z","msg":"Reconciliation completed successfully: mode=capacity-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-11-28T21:10:52.759Z","msg":"Operating in CAPACITY-ONLY mode: reactive capacity-based scaling only"}
{"level":"INFO","ts":"2025-11-28T21:10:52.759Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-11-28T21:10:52.759Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-11-28T21:10:52.783Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=3, accelerator=H100, ttft=32.24ms, itl=17.26ms, cost=30.00, arrivalRate=3745.00"}
{"level":"INFO","ts":"2025-11-28T21:10:52.785Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9lshk5, queueLength=0"}
{"level":"INFO","ts":"2025-11-28T21:10:52.785Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9ww5mn, queueLength=0"}
{"level":"INFO","ts":"2025-11-28T21:10:52.785Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9l76md, queueLength=0"}
{"level":"DEBUG","ts":"2025-11-28T21:10:52.785Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=3"}
{"level":"INFO","ts":"2025-11-28T21:10:52.785Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9lshk5, usage=0.209 (20.9%)"}
{"level":"INFO","ts":"2025-11-28T21:10:52.785Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9ww5mn, usage=0.266 (26.6%)"}
{"level":"INFO","ts":"2025-11-28T21:10:52.785Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9l76md, usage=0.246 (24.6%)"}
{"level":"DEBUG","ts":"2025-11-28T21:10:52.785Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=3"}
{"level":"DEBUG","ts":"2025-11-28T21:10:52.785Z","msg":"Pod-to-variant matching successful: totalPods=3, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:3]"}
{"level":"DEBUG","ts":"2025-11-28T21:10:52.785Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=3"}
{"level":"DEBUG","ts":"2025-11-28T21:10:52.785Z","msg":"Collected capacity metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, metricsCount=3"}
{"level":"DEBUG","ts":"2025-11-28T21:10:52.785Z","msg":"Variant analysis initialized: variant=ms-inference-scheduling-llm-d-modelservice-decode, accelerator=H100, cost=10.00, replicaCount=3"}
{"level":"DEBUG","ts":"2025-11-28T21:10:52.785Z","msg":"Scale-down unsafe: insufficient headroom after redistribution: remainingSpareKv=0.140, kvTrigger=0.200, kvSafe=false, remainingSpareQueue=5.0, queueTrigger=3.0%!(EXTRA bool=true)"}
{"level":"DEBUG","ts":"2025-11-28T21:10:52.785Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, totalReplicas=3, nonSaturated=3, avgSpareKv=0.260, avgSpareQueue=5.0, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"INFO","ts":"2025-11-28T21:10:52.785Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, totalReplicas=3, nonSaturated=3, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"DEBUG","ts":"2025-11-28T21:10:52.785Z","msg":"Capacity targets: no scaling needed (avgSpareKv=0.260, avgSpareQueue=5.0, all variants stable)"}
{"level":"DEBUG","ts":"2025-11-28T21:10:52.785Z","msg":"Capacity targets calculated: modelID=unsloth/Meta-Llama-3.1-8B, targets=map[ms-inference-scheduling-llm-d-modelservice-decode:3]"}
{"level":"INFO","ts":"2025-11-28T21:10:52.785Z","msg":"Capacity-only decisions made for model: unsloth/Meta-Llama-3.1-8B - decision count: 1"}
{"level":"INFO","ts":"2025-11-28T21:10:52.785Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-11-28T21:10:52.785Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=3→target=3"}
{"level":"DEBUG","ts":"2025-11-28T21:10:52.785Z","msg":"Found VA in map: variant=ms-inference-scheduling-llm-d-modelservice-decode, hasCurrentAlloc=true, accelerator=H100"}
{"level":"INFO","ts":"2025-11-28T21:10:52.785Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 3, desired-replicas: 3, accelerator: H100"}
{"level":"INFO","ts":"2025-11-28T21:10:52.785Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=3, accelerator=H100, capacityOnly=true"}
{"level":"INFO","ts":"2025-11-28T21:10:52.793Z","msg":"Applied capacity decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=3, target=3, reason=capacity-only mode: no-change"}
{"level":"INFO","ts":"2025-11-28T21:10:52.793Z","msg":"Reconciliation completed successfully: mode=capacity-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-11-28T21:10:52.868Z","msg":"Operating in CAPACITY-ONLY mode: reactive capacity-based scaling only"}
{"level":"INFO","ts":"2025-11-28T21:10:52.868Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-11-28T21:10:52.868Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-11-28T21:10:52.878Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=3, accelerator=H100, ttft=32.24ms, itl=17.26ms, cost=30.00, arrivalRate=3745.00"}
{"level":"INFO","ts":"2025-11-28T21:10:52.880Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9lshk5, queueLength=0"}
{"level":"INFO","ts":"2025-11-28T21:10:52.880Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9ww5mn, queueLength=0"}
{"level":"INFO","ts":"2025-11-28T21:10:52.880Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9l76md, queueLength=0"}
{"level":"DEBUG","ts":"2025-11-28T21:10:52.880Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=3"}
{"level":"INFO","ts":"2025-11-28T21:10:52.880Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9lshk5, usage=0.209 (20.9%)"}
{"level":"INFO","ts":"2025-11-28T21:10:52.880Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9ww5mn, usage=0.266 (26.6%)"}
{"level":"INFO","ts":"2025-11-28T21:10:52.880Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9l76md, usage=0.246 (24.6%)"}
{"level":"DEBUG","ts":"2025-11-28T21:10:52.880Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=3"}
{"level":"DEBUG","ts":"2025-11-28T21:10:52.880Z","msg":"Pod-to-variant matching successful: totalPods=3, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:3]"}
{"level":"DEBUG","ts":"2025-11-28T21:10:52.880Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=3"}
{"level":"DEBUG","ts":"2025-11-28T21:10:52.880Z","msg":"Collected capacity metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, metricsCount=3"}
{"level":"DEBUG","ts":"2025-11-28T21:10:52.880Z","msg":"Variant analysis initialized: variant=ms-inference-scheduling-llm-d-modelservice-decode, accelerator=H100, cost=10.00, replicaCount=3"}
{"level":"DEBUG","ts":"2025-11-28T21:10:52.880Z","msg":"Scale-down unsafe: insufficient headroom after redistribution: remainingSpareKv=0.140, kvTrigger=0.200, kvSafe=false, remainingSpareQueue=5.0, queueTrigger=3.0%!(EXTRA bool=true)"}
{"level":"DEBUG","ts":"2025-11-28T21:10:52.880Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, totalReplicas=3, nonSaturated=3, avgSpareKv=0.260, avgSpareQueue=5.0, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"INFO","ts":"2025-11-28T21:10:52.880Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, totalReplicas=3, nonSaturated=3, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"DEBUG","ts":"2025-11-28T21:10:52.880Z","msg":"Capacity targets: no scaling needed (avgSpareKv=0.260, avgSpareQueue=5.0, all variants stable)"}
{"level":"DEBUG","ts":"2025-11-28T21:10:52.880Z","msg":"Capacity targets calculated: modelID=unsloth/Meta-Llama-3.1-8B, targets=map[ms-inference-scheduling-llm-d-modelservice-decode:3]"}
{"level":"INFO","ts":"2025-11-28T21:10:52.880Z","msg":"Capacity-only decisions made for model: unsloth/Meta-Llama-3.1-8B - decision count: 1"}
{"level":"INFO","ts":"2025-11-28T21:10:52.880Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-11-28T21:10:52.880Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=3→target=3"}
{"level":"DEBUG","ts":"2025-11-28T21:10:52.880Z","msg":"Found VA in map: variant=ms-inference-scheduling-llm-d-modelservice-decode, hasCurrentAlloc=true, accelerator=H100"}
{"level":"INFO","ts":"2025-11-28T21:10:52.880Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 3, desired-replicas: 3, accelerator: H100"}
{"level":"INFO","ts":"2025-11-28T21:10:52.880Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=3, accelerator=H100, capacityOnly=true"}
{"level":"INFO","ts":"2025-11-28T21:10:52.885Z","msg":"Applied capacity decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=3, target=3, reason=capacity-only mode: no-change"}
{"level":"INFO","ts":"2025-11-28T21:10:52.885Z","msg":"Reconciliation completed successfully: mode=capacity-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-11-28T21:11:22.793Z","msg":"Operating in CAPACITY-ONLY mode: reactive capacity-based scaling only"}
{"level":"INFO","ts":"2025-11-28T21:11:22.793Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-11-28T21:11:22.793Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-11-28T21:11:22.810Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=3, accelerator=H100, ttft=33.53ms, itl=18.20ms, cost=30.00, arrivalRate=3904.42"}
{"level":"INFO","ts":"2025-11-28T21:11:22.811Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9lshk5, usage=0.232 (23.2%)"}
{"level":"INFO","ts":"2025-11-28T21:11:22.811Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9ww5mn, usage=0.248 (24.8%)"}
{"level":"INFO","ts":"2025-11-28T21:11:22.811Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9l76md, usage=0.267 (26.7%)"}
{"level":"DEBUG","ts":"2025-11-28T21:11:22.811Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=3"}
{"level":"INFO","ts":"2025-11-28T21:11:22.812Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9lshk5, queueLength=0"}
{"level":"INFO","ts":"2025-11-28T21:11:22.812Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9ww5mn, queueLength=0"}
{"level":"INFO","ts":"2025-11-28T21:11:22.812Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9l76md, queueLength=0"}
{"level":"DEBUG","ts":"2025-11-28T21:11:22.812Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=3"}
{"level":"DEBUG","ts":"2025-11-28T21:11:22.812Z","msg":"Pod-to-variant matching successful: totalPods=3, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:3]"}
{"level":"DEBUG","ts":"2025-11-28T21:11:22.812Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=3"}
{"level":"DEBUG","ts":"2025-11-28T21:11:22.812Z","msg":"Collected capacity metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, metricsCount=3"}
{"level":"DEBUG","ts":"2025-11-28T21:11:22.812Z","msg":"Variant analysis initialized: variant=ms-inference-scheduling-llm-d-modelservice-decode, accelerator=H100, cost=10.00, replicaCount=3"}
{"level":"DEBUG","ts":"2025-11-28T21:11:22.812Z","msg":"Scale-down unsafe: insufficient headroom after redistribution: remainingSpareKv=0.126, kvTrigger=0.200, kvSafe=false, remainingSpareQueue=5.0, queueTrigger=3.0%!(EXTRA bool=true)"}
{"level":"DEBUG","ts":"2025-11-28T21:11:22.812Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, totalReplicas=3, nonSaturated=3, avgSpareKv=0.251, avgSpareQueue=5.0, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"INFO","ts":"2025-11-28T21:11:22.812Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, totalReplicas=3, nonSaturated=3, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"DEBUG","ts":"2025-11-28T21:11:22.812Z","msg":"Capacity targets: no scaling needed (avgSpareKv=0.251, avgSpareQueue=5.0, all variants stable)"}
{"level":"DEBUG","ts":"2025-11-28T21:11:22.812Z","msg":"Capacity targets calculated: modelID=unsloth/Meta-Llama-3.1-8B, targets=map[ms-inference-scheduling-llm-d-modelservice-decode:3]"}
{"level":"INFO","ts":"2025-11-28T21:11:22.812Z","msg":"Capacity-only decisions made for model: unsloth/Meta-Llama-3.1-8B - decision count: 1"}
{"level":"INFO","ts":"2025-11-28T21:11:22.812Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-11-28T21:11:22.812Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=3→target=3"}
{"level":"DEBUG","ts":"2025-11-28T21:11:22.812Z","msg":"Found VA in map: variant=ms-inference-scheduling-llm-d-modelservice-decode, hasCurrentAlloc=true, accelerator=H100"}
{"level":"INFO","ts":"2025-11-28T21:11:22.812Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 3, desired-replicas: 3, accelerator: H100"}
{"level":"INFO","ts":"2025-11-28T21:11:22.812Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=3, accelerator=H100, capacityOnly=true"}
{"level":"INFO","ts":"2025-11-28T21:11:22.817Z","msg":"Applied capacity decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=3, target=3, reason=capacity-only mode: no-change"}
{"level":"INFO","ts":"2025-11-28T21:11:22.817Z","msg":"Reconciliation completed successfully: mode=capacity-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-11-28T21:11:22.886Z","msg":"Operating in CAPACITY-ONLY mode: reactive capacity-based scaling only"}
{"level":"INFO","ts":"2025-11-28T21:11:22.886Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-11-28T21:11:22.886Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-11-28T21:11:22.897Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=3, accelerator=H100, ttft=33.53ms, itl=18.20ms, cost=30.00, arrivalRate=3904.42"}
{"level":"INFO","ts":"2025-11-28T21:11:22.898Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9lshk5, queueLength=0"}
{"level":"INFO","ts":"2025-11-28T21:11:22.898Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9ww5mn, queueLength=0"}
{"level":"INFO","ts":"2025-11-28T21:11:22.898Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9l76md, queueLength=0"}
{"level":"DEBUG","ts":"2025-11-28T21:11:22.898Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=3"}
{"level":"INFO","ts":"2025-11-28T21:11:22.898Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9lshk5, usage=0.232 (23.2%)"}
{"level":"INFO","ts":"2025-11-28T21:11:22.898Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9ww5mn, usage=0.248 (24.8%)"}
{"level":"INFO","ts":"2025-11-28T21:11:22.898Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9l76md, usage=0.267 (26.7%)"}
{"level":"DEBUG","ts":"2025-11-28T21:11:22.898Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=3"}
{"level":"DEBUG","ts":"2025-11-28T21:11:22.899Z","msg":"Pod-to-variant matching successful: totalPods=3, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:3]"}
{"level":"DEBUG","ts":"2025-11-28T21:11:22.899Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=3"}
{"level":"DEBUG","ts":"2025-11-28T21:11:22.899Z","msg":"Collected capacity metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, metricsCount=3"}
{"level":"DEBUG","ts":"2025-11-28T21:11:22.899Z","msg":"Variant analysis initialized: variant=ms-inference-scheduling-llm-d-modelservice-decode, accelerator=H100, cost=10.00, replicaCount=3"}
{"level":"DEBUG","ts":"2025-11-28T21:11:22.899Z","msg":"Scale-down unsafe: insufficient headroom after redistribution: remainingSpareKv=0.126, kvTrigger=0.200, kvSafe=false, remainingSpareQueue=5.0, queueTrigger=3.0%!(EXTRA bool=true)"}
{"level":"DEBUG","ts":"2025-11-28T21:11:22.899Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, totalReplicas=3, nonSaturated=3, avgSpareKv=0.251, avgSpareQueue=5.0, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"INFO","ts":"2025-11-28T21:11:22.899Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, totalReplicas=3, nonSaturated=3, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"DEBUG","ts":"2025-11-28T21:11:22.899Z","msg":"Capacity targets: no scaling needed (avgSpareKv=0.251, avgSpareQueue=5.0, all variants stable)"}
{"level":"DEBUG","ts":"2025-11-28T21:11:22.899Z","msg":"Capacity targets calculated: modelID=unsloth/Meta-Llama-3.1-8B, targets=map[ms-inference-scheduling-llm-d-modelservice-decode:3]"}
{"level":"INFO","ts":"2025-11-28T21:11:22.899Z","msg":"Capacity-only decisions made for model: unsloth/Meta-Llama-3.1-8B - decision count: 1"}
{"level":"INFO","ts":"2025-11-28T21:11:22.899Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-11-28T21:11:22.899Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=3→target=3"}
{"level":"DEBUG","ts":"2025-11-28T21:11:22.899Z","msg":"Found VA in map: variant=ms-inference-scheduling-llm-d-modelservice-decode, hasCurrentAlloc=true, accelerator=H100"}
{"level":"INFO","ts":"2025-11-28T21:11:22.899Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 3, desired-replicas: 3, accelerator: H100"}
{"level":"INFO","ts":"2025-11-28T21:11:22.899Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=3, accelerator=H100, capacityOnly=true"}
{"level":"INFO","ts":"2025-11-28T21:11:22.903Z","msg":"Applied capacity decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=3, target=3, reason=capacity-only mode: no-change"}
{"level":"INFO","ts":"2025-11-28T21:11:22.903Z","msg":"Reconciliation completed successfully: mode=capacity-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-11-28T21:11:52.817Z","msg":"Operating in CAPACITY-ONLY mode: reactive capacity-based scaling only"}
{"level":"INFO","ts":"2025-11-28T21:11:52.817Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-11-28T21:11:52.817Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-11-28T21:11:52.830Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=3, accelerator=H100, ttft=32.87ms, itl=18.01ms, cost=30.00, arrivalRate=3821.85"}
{"level":"INFO","ts":"2025-11-28T21:11:52.832Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9lshk5, usage=0.260 (26.0%)"}
{"level":"INFO","ts":"2025-11-28T21:11:52.832Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9ww5mn, usage=0.240 (24.0%)"}
{"level":"INFO","ts":"2025-11-28T21:11:52.832Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9l76md, usage=0.267 (26.7%)"}
{"level":"DEBUG","ts":"2025-11-28T21:11:52.832Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=3"}
{"level":"INFO","ts":"2025-11-28T21:11:52.832Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9lshk5, queueLength=0"}
{"level":"INFO","ts":"2025-11-28T21:11:52.832Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9ww5mn, queueLength=0"}
{"level":"INFO","ts":"2025-11-28T21:11:52.832Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9l76md, queueLength=0"}
{"level":"DEBUG","ts":"2025-11-28T21:11:52.832Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=3"}
{"level":"DEBUG","ts":"2025-11-28T21:11:52.832Z","msg":"Pod-to-variant matching successful: totalPods=3, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:3]"}
{"level":"DEBUG","ts":"2025-11-28T21:11:52.832Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=3"}
{"level":"DEBUG","ts":"2025-11-28T21:11:52.832Z","msg":"Collected capacity metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, metricsCount=3"}
{"level":"DEBUG","ts":"2025-11-28T21:11:52.832Z","msg":"Variant analysis initialized: variant=ms-inference-scheduling-llm-d-modelservice-decode, accelerator=H100, cost=10.00, replicaCount=3"}
{"level":"DEBUG","ts":"2025-11-28T21:11:52.832Z","msg":"Scale-down unsafe: insufficient headroom after redistribution: remainingSpareKv=0.116, kvTrigger=0.200, kvSafe=false, remainingSpareQueue=5.0, queueTrigger=3.0%!(EXTRA bool=true)"}
{"level":"DEBUG","ts":"2025-11-28T21:11:52.832Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, totalReplicas=3, nonSaturated=3, avgSpareKv=0.244, avgSpareQueue=5.0, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"INFO","ts":"2025-11-28T21:11:52.832Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, totalReplicas=3, nonSaturated=3, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"DEBUG","ts":"2025-11-28T21:11:52.833Z","msg":"Capacity targets: no scaling needed (avgSpareKv=0.244, avgSpareQueue=5.0, all variants stable)"}
{"level":"DEBUG","ts":"2025-11-28T21:11:52.833Z","msg":"Capacity targets calculated: modelID=unsloth/Meta-Llama-3.1-8B, targets=map[ms-inference-scheduling-llm-d-modelservice-decode:3]"}
{"level":"INFO","ts":"2025-11-28T21:11:52.833Z","msg":"Capacity-only decisions made for model: unsloth/Meta-Llama-3.1-8B - decision count: 1"}
{"level":"INFO","ts":"2025-11-28T21:11:52.833Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-11-28T21:11:52.833Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=3→target=3"}
{"level":"DEBUG","ts":"2025-11-28T21:11:52.833Z","msg":"Found VA in map: variant=ms-inference-scheduling-llm-d-modelservice-decode, hasCurrentAlloc=true, accelerator=H100"}
{"level":"INFO","ts":"2025-11-28T21:11:52.833Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 3, desired-replicas: 3, accelerator: H100"}
{"level":"INFO","ts":"2025-11-28T21:11:52.833Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=3, accelerator=H100, capacityOnly=true"}
{"level":"INFO","ts":"2025-11-28T21:11:52.845Z","msg":"Applied capacity decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=3, target=3, reason=capacity-only mode: no-change"}
{"level":"INFO","ts":"2025-11-28T21:11:52.845Z","msg":"Reconciliation completed successfully: mode=capacity-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-11-28T21:11:52.903Z","msg":"Operating in CAPACITY-ONLY mode: reactive capacity-based scaling only"}
{"level":"INFO","ts":"2025-11-28T21:11:52.903Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-11-28T21:11:52.903Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-11-28T21:11:52.914Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=3, accelerator=H100, ttft=32.87ms, itl=18.01ms, cost=30.00, arrivalRate=3821.85"}
{"level":"INFO","ts":"2025-11-28T21:11:52.915Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9lshk5, queueLength=0"}
{"level":"INFO","ts":"2025-11-28T21:11:52.915Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9ww5mn, queueLength=0"}
{"level":"INFO","ts":"2025-11-28T21:11:52.915Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9l76md, queueLength=0"}
{"level":"DEBUG","ts":"2025-11-28T21:11:52.915Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=3"}
{"level":"INFO","ts":"2025-11-28T21:11:52.915Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9lshk5, usage=0.260 (26.0%)"}
{"level":"INFO","ts":"2025-11-28T21:11:52.915Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9ww5mn, usage=0.240 (24.0%)"}
{"level":"INFO","ts":"2025-11-28T21:11:52.915Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9l76md, usage=0.267 (26.7%)"}
{"level":"DEBUG","ts":"2025-11-28T21:11:52.915Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=3"}
{"level":"DEBUG","ts":"2025-11-28T21:11:52.916Z","msg":"Pod-to-variant matching successful: totalPods=3, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:3]"}
{"level":"DEBUG","ts":"2025-11-28T21:11:52.916Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=3"}
{"level":"DEBUG","ts":"2025-11-28T21:11:52.916Z","msg":"Collected capacity metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, metricsCount=3"}
{"level":"DEBUG","ts":"2025-11-28T21:11:52.916Z","msg":"Variant analysis initialized: variant=ms-inference-scheduling-llm-d-modelservice-decode, accelerator=H100, cost=10.00, replicaCount=3"}
{"level":"DEBUG","ts":"2025-11-28T21:11:52.916Z","msg":"Scale-down unsafe: insufficient headroom after redistribution: remainingSpareKv=0.116, kvTrigger=0.200, kvSafe=false, remainingSpareQueue=5.0, queueTrigger=3.0%!(EXTRA bool=true)"}
{"level":"DEBUG","ts":"2025-11-28T21:11:52.916Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, totalReplicas=3, nonSaturated=3, avgSpareKv=0.244, avgSpareQueue=5.0, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"INFO","ts":"2025-11-28T21:11:52.916Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, totalReplicas=3, nonSaturated=3, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"DEBUG","ts":"2025-11-28T21:11:52.916Z","msg":"Capacity targets: no scaling needed (avgSpareKv=0.244, avgSpareQueue=5.0, all variants stable)"}
{"level":"DEBUG","ts":"2025-11-28T21:11:52.916Z","msg":"Capacity targets calculated: modelID=unsloth/Meta-Llama-3.1-8B, targets=map[ms-inference-scheduling-llm-d-modelservice-decode:3]"}
{"level":"INFO","ts":"2025-11-28T21:11:52.916Z","msg":"Capacity-only decisions made for model: unsloth/Meta-Llama-3.1-8B - decision count: 1"}
{"level":"INFO","ts":"2025-11-28T21:11:52.916Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-11-28T21:11:52.916Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=3→target=3"}
{"level":"DEBUG","ts":"2025-11-28T21:11:52.916Z","msg":"Found VA in map: variant=ms-inference-scheduling-llm-d-modelservice-decode, hasCurrentAlloc=true, accelerator=H100"}
{"level":"INFO","ts":"2025-11-28T21:11:52.916Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 3, desired-replicas: 3, accelerator: H100"}
{"level":"INFO","ts":"2025-11-28T21:11:52.916Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=3, accelerator=H100, capacityOnly=true"}
{"level":"INFO","ts":"2025-11-28T21:11:52.924Z","msg":"Applied capacity decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=3, target=3, reason=capacity-only mode: no-change"}
{"level":"INFO","ts":"2025-11-28T21:11:52.924Z","msg":"Reconciliation completed successfully: mode=capacity-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-11-28T21:12:22.846Z","msg":"Operating in CAPACITY-ONLY mode: reactive capacity-based scaling only"}
{"level":"INFO","ts":"2025-11-28T21:12:22.846Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-11-28T21:12:22.846Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-11-28T21:12:22.859Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=3, accelerator=H100, ttft=33.44ms, itl=18.22ms, cost=30.00, arrivalRate=3817.99"}
{"level":"INFO","ts":"2025-11-28T21:12:22.861Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9lshk5, queueLength=0"}
{"level":"INFO","ts":"2025-11-28T21:12:22.861Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9ww5mn, queueLength=0"}
{"level":"INFO","ts":"2025-11-28T21:12:22.861Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9l76md, queueLength=0"}
{"level":"DEBUG","ts":"2025-11-28T21:12:22.861Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=3"}
{"level":"INFO","ts":"2025-11-28T21:12:22.861Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9lshk5, usage=0.260 (26.0%)"}
{"level":"INFO","ts":"2025-11-28T21:12:22.861Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9ww5mn, usage=0.245 (24.5%)"}
{"level":"INFO","ts":"2025-11-28T21:12:22.861Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9l76md, usage=0.224 (22.4%)"}
{"level":"DEBUG","ts":"2025-11-28T21:12:22.861Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=3"}
{"level":"DEBUG","ts":"2025-11-28T21:12:22.861Z","msg":"Pod-to-variant matching successful: totalPods=3, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:3]"}
{"level":"DEBUG","ts":"2025-11-28T21:12:22.861Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=3"}
{"level":"DEBUG","ts":"2025-11-28T21:12:22.861Z","msg":"Collected capacity metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, metricsCount=3"}
{"level":"DEBUG","ts":"2025-11-28T21:12:22.861Z","msg":"Variant analysis initialized: variant=ms-inference-scheduling-llm-d-modelservice-decode, accelerator=H100, cost=10.00, replicaCount=3"}
{"level":"DEBUG","ts":"2025-11-28T21:12:22.861Z","msg":"Scale-down unsafe: insufficient headroom after redistribution: remainingSpareKv=0.136, kvTrigger=0.200, kvSafe=false, remainingSpareQueue=5.0, queueTrigger=3.0%!(EXTRA bool=true)"}
{"level":"DEBUG","ts":"2025-11-28T21:12:22.861Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, totalReplicas=3, nonSaturated=3, avgSpareKv=0.257, avgSpareQueue=5.0, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"INFO","ts":"2025-11-28T21:12:22.861Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, totalReplicas=3, nonSaturated=3, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"DEBUG","ts":"2025-11-28T21:12:22.861Z","msg":"Capacity targets: no scaling needed (avgSpareKv=0.257, avgSpareQueue=5.0, all variants stable)"}
{"level":"DEBUG","ts":"2025-11-28T21:12:22.861Z","msg":"Capacity targets calculated: modelID=unsloth/Meta-Llama-3.1-8B, targets=map[ms-inference-scheduling-llm-d-modelservice-decode:3]"}
{"level":"INFO","ts":"2025-11-28T21:12:22.861Z","msg":"Capacity-only decisions made for model: unsloth/Meta-Llama-3.1-8B - decision count: 1"}
{"level":"INFO","ts":"2025-11-28T21:12:22.861Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-11-28T21:12:22.861Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=3→target=3"}
{"level":"DEBUG","ts":"2025-11-28T21:12:22.861Z","msg":"Found VA in map: variant=ms-inference-scheduling-llm-d-modelservice-decode, hasCurrentAlloc=true, accelerator=H100"}
{"level":"INFO","ts":"2025-11-28T21:12:22.861Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 3, desired-replicas: 3, accelerator: H100"}
{"level":"INFO","ts":"2025-11-28T21:12:22.861Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=3, accelerator=H100, capacityOnly=true"}
{"level":"INFO","ts":"2025-11-28T21:12:22.865Z","msg":"Applied capacity decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=3, target=3, reason=capacity-only mode: no-change"}
{"level":"INFO","ts":"2025-11-28T21:12:22.865Z","msg":"Reconciliation completed successfully: mode=capacity-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-11-28T21:12:22.925Z","msg":"Operating in CAPACITY-ONLY mode: reactive capacity-based scaling only"}
{"level":"INFO","ts":"2025-11-28T21:12:22.925Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-11-28T21:12:22.925Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-11-28T21:12:22.935Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=3, accelerator=H100, ttft=33.44ms, itl=18.22ms, cost=30.00, arrivalRate=3817.99"}
{"level":"INFO","ts":"2025-11-28T21:12:22.937Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9lshk5, usage=0.260 (26.0%)"}
{"level":"INFO","ts":"2025-11-28T21:12:22.937Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9ww5mn, usage=0.245 (24.5%)"}
{"level":"INFO","ts":"2025-11-28T21:12:22.937Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9l76md, usage=0.224 (22.4%)"}
{"level":"DEBUG","ts":"2025-11-28T21:12:22.937Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=3"}
{"level":"INFO","ts":"2025-11-28T21:12:22.937Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9lshk5, queueLength=0"}
{"level":"INFO","ts":"2025-11-28T21:12:22.937Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9ww5mn, queueLength=0"}
{"level":"INFO","ts":"2025-11-28T21:12:22.937Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9l76md, queueLength=0"}
{"level":"DEBUG","ts":"2025-11-28T21:12:22.937Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=3"}
{"level":"DEBUG","ts":"2025-11-28T21:12:22.938Z","msg":"Pod-to-variant matching successful: totalPods=3, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:3]"}
{"level":"DEBUG","ts":"2025-11-28T21:12:22.938Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=3"}
{"level":"DEBUG","ts":"2025-11-28T21:12:22.938Z","msg":"Collected capacity metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, metricsCount=3"}
{"level":"DEBUG","ts":"2025-11-28T21:12:22.938Z","msg":"Variant analysis initialized: variant=ms-inference-scheduling-llm-d-modelservice-decode, accelerator=H100, cost=10.00, replicaCount=3"}
{"level":"DEBUG","ts":"2025-11-28T21:12:22.938Z","msg":"Scale-down unsafe: insufficient headroom after redistribution: remainingSpareKv=0.136, kvTrigger=0.200, kvSafe=false, remainingSpareQueue=5.0, queueTrigger=3.0%!(EXTRA bool=true)"}
{"level":"DEBUG","ts":"2025-11-28T21:12:22.938Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, totalReplicas=3, nonSaturated=3, avgSpareKv=0.257, avgSpareQueue=5.0, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"INFO","ts":"2025-11-28T21:12:22.938Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, totalReplicas=3, nonSaturated=3, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"DEBUG","ts":"2025-11-28T21:12:22.938Z","msg":"Capacity targets: no scaling needed (avgSpareKv=0.257, avgSpareQueue=5.0, all variants stable)"}
{"level":"DEBUG","ts":"2025-11-28T21:12:22.938Z","msg":"Capacity targets calculated: modelID=unsloth/Meta-Llama-3.1-8B, targets=map[ms-inference-scheduling-llm-d-modelservice-decode:3]"}
{"level":"INFO","ts":"2025-11-28T21:12:22.938Z","msg":"Capacity-only decisions made for model: unsloth/Meta-Llama-3.1-8B - decision count: 1"}
{"level":"INFO","ts":"2025-11-28T21:12:22.938Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-11-28T21:12:22.938Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=3→target=3"}
{"level":"DEBUG","ts":"2025-11-28T21:12:22.938Z","msg":"Found VA in map: variant=ms-inference-scheduling-llm-d-modelservice-decode, hasCurrentAlloc=true, accelerator=H100"}
{"level":"INFO","ts":"2025-11-28T21:12:22.938Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 3, desired-replicas: 3, accelerator: H100"}
{"level":"INFO","ts":"2025-11-28T21:12:22.938Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=3, accelerator=H100, capacityOnly=true"}
{"level":"INFO","ts":"2025-11-28T21:12:22.942Z","msg":"Applied capacity decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=3, target=3, reason=capacity-only mode: no-change"}
{"level":"INFO","ts":"2025-11-28T21:12:22.942Z","msg":"Reconciliation completed successfully: mode=capacity-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-11-28T21:12:52.866Z","msg":"Operating in CAPACITY-ONLY mode: reactive capacity-based scaling only"}
{"level":"INFO","ts":"2025-11-28T21:12:52.866Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-11-28T21:12:52.866Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-11-28T21:12:52.879Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=3, accelerator=H100, ttft=33.66ms, itl=18.20ms, cost=30.00, arrivalRate=3900.64"}
{"level":"INFO","ts":"2025-11-28T21:12:52.881Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9lshk5, queueLength=0"}
{"level":"INFO","ts":"2025-11-28T21:12:52.881Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9ww5mn, queueLength=0"}
{"level":"INFO","ts":"2025-11-28T21:12:52.881Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9l76md, queueLength=0"}
{"level":"DEBUG","ts":"2025-11-28T21:12:52.881Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=3"}
{"level":"INFO","ts":"2025-11-28T21:12:52.881Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9lshk5, usage=0.259 (25.9%)"}
{"level":"INFO","ts":"2025-11-28T21:12:52.881Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9ww5mn, usage=0.247 (24.7%)"}
{"level":"INFO","ts":"2025-11-28T21:12:52.881Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9l76md, usage=0.224 (22.4%)"}
{"level":"DEBUG","ts":"2025-11-28T21:12:52.881Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=3"}
{"level":"DEBUG","ts":"2025-11-28T21:12:52.881Z","msg":"Pod-to-variant matching successful: totalPods=3, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:3]"}
{"level":"DEBUG","ts":"2025-11-28T21:12:52.881Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=3"}
{"level":"DEBUG","ts":"2025-11-28T21:12:52.881Z","msg":"Collected capacity metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, metricsCount=3"}
{"level":"DEBUG","ts":"2025-11-28T21:12:52.881Z","msg":"Variant analysis initialized: variant=ms-inference-scheduling-llm-d-modelservice-decode, accelerator=H100, cost=10.00, replicaCount=3"}
{"level":"DEBUG","ts":"2025-11-28T21:12:52.881Z","msg":"Scale-down unsafe: insufficient headroom after redistribution: remainingSpareKv=0.135, kvTrigger=0.200, kvSafe=false, remainingSpareQueue=5.0, queueTrigger=3.0%!(EXTRA bool=true)"}
{"level":"DEBUG","ts":"2025-11-28T21:12:52.881Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, totalReplicas=3, nonSaturated=3, avgSpareKv=0.257, avgSpareQueue=5.0, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"INFO","ts":"2025-11-28T21:12:52.881Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, totalReplicas=3, nonSaturated=3, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"DEBUG","ts":"2025-11-28T21:12:52.881Z","msg":"Capacity targets: no scaling needed (avgSpareKv=0.257, avgSpareQueue=5.0, all variants stable)"}
{"level":"DEBUG","ts":"2025-11-28T21:12:52.881Z","msg":"Capacity targets calculated: modelID=unsloth/Meta-Llama-3.1-8B, targets=map[ms-inference-scheduling-llm-d-modelservice-decode:3]"}
{"level":"INFO","ts":"2025-11-28T21:12:52.881Z","msg":"Capacity-only decisions made for model: unsloth/Meta-Llama-3.1-8B - decision count: 1"}
{"level":"INFO","ts":"2025-11-28T21:12:52.881Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-11-28T21:12:52.881Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=3→target=3"}
{"level":"DEBUG","ts":"2025-11-28T21:12:52.881Z","msg":"Found VA in map: variant=ms-inference-scheduling-llm-d-modelservice-decode, hasCurrentAlloc=true, accelerator=H100"}
{"level":"INFO","ts":"2025-11-28T21:12:52.881Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 3, desired-replicas: 3, accelerator: H100"}
{"level":"INFO","ts":"2025-11-28T21:12:52.881Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=3, accelerator=H100, capacityOnly=true"}
{"level":"INFO","ts":"2025-11-28T21:12:52.885Z","msg":"Applied capacity decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=3, target=3, reason=capacity-only mode: no-change"}
{"level":"INFO","ts":"2025-11-28T21:12:52.885Z","msg":"Reconciliation completed successfully: mode=capacity-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-11-28T21:12:52.943Z","msg":"Operating in CAPACITY-ONLY mode: reactive capacity-based scaling only"}
{"level":"INFO","ts":"2025-11-28T21:12:52.943Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-11-28T21:12:52.943Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-11-28T21:12:52.954Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=3, accelerator=H100, ttft=33.66ms, itl=18.20ms, cost=30.00, arrivalRate=3900.64"}
{"level":"INFO","ts":"2025-11-28T21:12:52.955Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9lshk5, queueLength=0"}
{"level":"INFO","ts":"2025-11-28T21:12:52.955Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9ww5mn, queueLength=0"}
{"level":"INFO","ts":"2025-11-28T21:12:52.955Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9l76md, queueLength=0"}
{"level":"DEBUG","ts":"2025-11-28T21:12:52.955Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=3"}
{"level":"INFO","ts":"2025-11-28T21:12:52.955Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9lshk5, usage=0.259 (25.9%)"}
{"level":"INFO","ts":"2025-11-28T21:12:52.955Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9ww5mn, usage=0.247 (24.7%)"}
{"level":"INFO","ts":"2025-11-28T21:12:52.955Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9l76md, usage=0.224 (22.4%)"}
{"level":"DEBUG","ts":"2025-11-28T21:12:52.955Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=3"}
{"level":"DEBUG","ts":"2025-11-28T21:12:52.955Z","msg":"Pod-to-variant matching successful: totalPods=3, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:3]"}
{"level":"DEBUG","ts":"2025-11-28T21:12:52.955Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=3"}
{"level":"DEBUG","ts":"2025-11-28T21:12:52.955Z","msg":"Collected capacity metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, metricsCount=3"}
{"level":"DEBUG","ts":"2025-11-28T21:12:52.955Z","msg":"Variant analysis initialized: variant=ms-inference-scheduling-llm-d-modelservice-decode, accelerator=H100, cost=10.00, replicaCount=3"}
{"level":"DEBUG","ts":"2025-11-28T21:12:52.955Z","msg":"Scale-down unsafe: insufficient headroom after redistribution: remainingSpareKv=0.135, kvTrigger=0.200, kvSafe=false, remainingSpareQueue=5.0, queueTrigger=3.0%!(EXTRA bool=true)"}
{"level":"DEBUG","ts":"2025-11-28T21:12:52.955Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, totalReplicas=3, nonSaturated=3, avgSpareKv=0.257, avgSpareQueue=5.0, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"INFO","ts":"2025-11-28T21:12:52.955Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, totalReplicas=3, nonSaturated=3, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"DEBUG","ts":"2025-11-28T21:12:52.955Z","msg":"Capacity targets: no scaling needed (avgSpareKv=0.257, avgSpareQueue=5.0, all variants stable)"}
{"level":"DEBUG","ts":"2025-11-28T21:12:52.955Z","msg":"Capacity targets calculated: modelID=unsloth/Meta-Llama-3.1-8B, targets=map[ms-inference-scheduling-llm-d-modelservice-decode:3]"}
{"level":"INFO","ts":"2025-11-28T21:12:52.955Z","msg":"Capacity-only decisions made for model: unsloth/Meta-Llama-3.1-8B - decision count: 1"}
{"level":"INFO","ts":"2025-11-28T21:12:52.955Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-11-28T21:12:52.955Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=3→target=3"}
{"level":"DEBUG","ts":"2025-11-28T21:12:52.955Z","msg":"Found VA in map: variant=ms-inference-scheduling-llm-d-modelservice-decode, hasCurrentAlloc=true, accelerator=H100"}
{"level":"INFO","ts":"2025-11-28T21:12:52.955Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 3, desired-replicas: 3, accelerator: H100"}
{"level":"INFO","ts":"2025-11-28T21:12:52.955Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=3, accelerator=H100, capacityOnly=true"}
{"level":"INFO","ts":"2025-11-28T21:12:52.960Z","msg":"Applied capacity decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=3, target=3, reason=capacity-only mode: no-change"}
{"level":"INFO","ts":"2025-11-28T21:12:52.960Z","msg":"Reconciliation completed successfully: mode=capacity-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-11-28T21:13:22.887Z","msg":"Operating in CAPACITY-ONLY mode: reactive capacity-based scaling only"}
{"level":"INFO","ts":"2025-11-28T21:13:22.887Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-11-28T21:13:22.887Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-11-28T21:13:22.907Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=3, accelerator=H100, ttft=34.52ms, itl=18.57ms, cost=30.00, arrivalRate=3837.57"}
{"level":"INFO","ts":"2025-11-28T21:13:22.908Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9lshk5, queueLength=0"}
{"level":"INFO","ts":"2025-11-28T21:13:22.908Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9ww5mn, queueLength=0"}
{"level":"INFO","ts":"2025-11-28T21:13:22.908Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9l76md, queueLength=0"}
{"level":"DEBUG","ts":"2025-11-28T21:13:22.908Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=3"}
{"level":"INFO","ts":"2025-11-28T21:13:22.909Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9lshk5, usage=0.262 (26.2%)"}
{"level":"INFO","ts":"2025-11-28T21:13:22.909Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9ww5mn, usage=0.253 (25.3%)"}
{"level":"INFO","ts":"2025-11-28T21:13:22.909Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9l76md, usage=0.296 (29.6%)"}
{"level":"DEBUG","ts":"2025-11-28T21:13:22.909Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=3"}
{"level":"DEBUG","ts":"2025-11-28T21:13:22.909Z","msg":"Pod-to-variant matching successful: totalPods=3, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:3]"}
{"level":"DEBUG","ts":"2025-11-28T21:13:22.909Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=3"}
{"level":"DEBUG","ts":"2025-11-28T21:13:22.909Z","msg":"Collected capacity metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, metricsCount=3"}
{"level":"DEBUG","ts":"2025-11-28T21:13:22.909Z","msg":"Variant analysis initialized: variant=ms-inference-scheduling-llm-d-modelservice-decode, accelerator=H100, cost=10.00, replicaCount=3"}
{"level":"DEBUG","ts":"2025-11-28T21:13:22.909Z","msg":"Scale-down unsafe: insufficient headroom after redistribution: remainingSpareKv=0.094, kvTrigger=0.200, kvSafe=false, remainingSpareQueue=5.0, queueTrigger=3.0%!(EXTRA bool=true)"}
{"level":"DEBUG","ts":"2025-11-28T21:13:22.909Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, totalReplicas=3, nonSaturated=3, avgSpareKv=0.230, avgSpareQueue=5.0, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"INFO","ts":"2025-11-28T21:13:22.909Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, totalReplicas=3, nonSaturated=3, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"DEBUG","ts":"2025-11-28T21:13:22.909Z","msg":"Capacity targets: no scaling needed (avgSpareKv=0.230, avgSpareQueue=5.0, all variants stable)"}
{"level":"DEBUG","ts":"2025-11-28T21:13:22.909Z","msg":"Capacity targets calculated: modelID=unsloth/Meta-Llama-3.1-8B, targets=map[ms-inference-scheduling-llm-d-modelservice-decode:3]"}
{"level":"INFO","ts":"2025-11-28T21:13:22.909Z","msg":"Capacity-only decisions made for model: unsloth/Meta-Llama-3.1-8B - decision count: 1"}
{"level":"INFO","ts":"2025-11-28T21:13:22.909Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-11-28T21:13:22.909Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=3→target=3"}
{"level":"DEBUG","ts":"2025-11-28T21:13:22.909Z","msg":"Found VA in map: variant=ms-inference-scheduling-llm-d-modelservice-decode, hasCurrentAlloc=true, accelerator=H100"}
{"level":"INFO","ts":"2025-11-28T21:13:22.909Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 3, desired-replicas: 3, accelerator: H100"}
{"level":"INFO","ts":"2025-11-28T21:13:22.909Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=3, accelerator=H100, capacityOnly=true"}
{"level":"INFO","ts":"2025-11-28T21:13:22.914Z","msg":"Applied capacity decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=3, target=3, reason=capacity-only mode: no-change"}
{"level":"INFO","ts":"2025-11-28T21:13:22.914Z","msg":"Reconciliation completed successfully: mode=capacity-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-11-28T21:13:22.961Z","msg":"Operating in CAPACITY-ONLY mode: reactive capacity-based scaling only"}
{"level":"INFO","ts":"2025-11-28T21:13:22.961Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-11-28T21:13:22.961Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-11-28T21:13:22.972Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=3, accelerator=H100, ttft=34.52ms, itl=18.57ms, cost=30.00, arrivalRate=3837.57"}
{"level":"INFO","ts":"2025-11-28T21:13:22.973Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9lshk5, queueLength=0"}
{"level":"INFO","ts":"2025-11-28T21:13:22.973Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9ww5mn, queueLength=0"}
{"level":"INFO","ts":"2025-11-28T21:13:22.973Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9l76md, queueLength=0"}
{"level":"DEBUG","ts":"2025-11-28T21:13:22.973Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=3"}
{"level":"INFO","ts":"2025-11-28T21:13:22.974Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9lshk5, usage=0.262 (26.2%)"}
{"level":"INFO","ts":"2025-11-28T21:13:22.974Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9ww5mn, usage=0.253 (25.3%)"}
{"level":"INFO","ts":"2025-11-28T21:13:22.974Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9l76md, usage=0.296 (29.6%)"}
{"level":"DEBUG","ts":"2025-11-28T21:13:22.974Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=3"}
{"level":"DEBUG","ts":"2025-11-28T21:13:22.974Z","msg":"Pod-to-variant matching successful: totalPods=3, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:3]"}
{"level":"DEBUG","ts":"2025-11-28T21:13:22.974Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=3"}
{"level":"DEBUG","ts":"2025-11-28T21:13:22.974Z","msg":"Collected capacity metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, metricsCount=3"}
{"level":"DEBUG","ts":"2025-11-28T21:13:22.974Z","msg":"Variant analysis initialized: variant=ms-inference-scheduling-llm-d-modelservice-decode, accelerator=H100, cost=10.00, replicaCount=3"}
{"level":"DEBUG","ts":"2025-11-28T21:13:22.974Z","msg":"Scale-down unsafe: insufficient headroom after redistribution: remainingSpareKv=0.094, kvTrigger=0.200, kvSafe=false, remainingSpareQueue=5.0, queueTrigger=3.0%!(EXTRA bool=true)"}
{"level":"DEBUG","ts":"2025-11-28T21:13:22.974Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, totalReplicas=3, nonSaturated=3, avgSpareKv=0.230, avgSpareQueue=5.0, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"INFO","ts":"2025-11-28T21:13:22.974Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, totalReplicas=3, nonSaturated=3, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"DEBUG","ts":"2025-11-28T21:13:22.974Z","msg":"Capacity targets: no scaling needed (avgSpareKv=0.230, avgSpareQueue=5.0, all variants stable)"}
{"level":"DEBUG","ts":"2025-11-28T21:13:22.974Z","msg":"Capacity targets calculated: modelID=unsloth/Meta-Llama-3.1-8B, targets=map[ms-inference-scheduling-llm-d-modelservice-decode:3]"}
{"level":"INFO","ts":"2025-11-28T21:13:22.974Z","msg":"Capacity-only decisions made for model: unsloth/Meta-Llama-3.1-8B - decision count: 1"}
{"level":"INFO","ts":"2025-11-28T21:13:22.974Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-11-28T21:13:22.974Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=3→target=3"}
{"level":"DEBUG","ts":"2025-11-28T21:13:22.974Z","msg":"Found VA in map: variant=ms-inference-scheduling-llm-d-modelservice-decode, hasCurrentAlloc=true, accelerator=H100"}
{"level":"INFO","ts":"2025-11-28T21:13:22.974Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 3, desired-replicas: 3, accelerator: H100"}
{"level":"INFO","ts":"2025-11-28T21:13:22.974Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=3, accelerator=H100, capacityOnly=true"}
{"level":"INFO","ts":"2025-11-28T21:13:22.978Z","msg":"Applied capacity decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=3, target=3, reason=capacity-only mode: no-change"}
{"level":"INFO","ts":"2025-11-28T21:13:22.978Z","msg":"Reconciliation completed successfully: mode=capacity-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-11-28T21:13:52.915Z","msg":"Operating in CAPACITY-ONLY mode: reactive capacity-based scaling only"}
{"level":"INFO","ts":"2025-11-28T21:13:52.915Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-11-28T21:13:52.915Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-11-28T21:13:52.937Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=3, accelerator=H100, ttft=35.50ms, itl=19.40ms, cost=30.00, arrivalRate=3935.14"}
{"level":"INFO","ts":"2025-11-28T21:13:52.938Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9lshk5, queueLength=0"}
{"level":"INFO","ts":"2025-11-28T21:13:52.938Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9ww5mn, queueLength=0"}
{"level":"INFO","ts":"2025-11-28T21:13:52.938Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9l76md, queueLength=0"}
{"level":"DEBUG","ts":"2025-11-28T21:13:52.938Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=3"}
{"level":"INFO","ts":"2025-11-28T21:13:52.939Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9lshk5, usage=0.321 (32.1%)"}
{"level":"INFO","ts":"2025-11-28T21:13:52.939Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9ww5mn, usage=0.253 (25.3%)"}
{"level":"INFO","ts":"2025-11-28T21:13:52.939Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9l76md, usage=0.296 (29.6%)"}
{"level":"DEBUG","ts":"2025-11-28T21:13:52.939Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=3"}
{"level":"DEBUG","ts":"2025-11-28T21:13:52.939Z","msg":"Pod-to-variant matching successful: totalPods=3, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:3]"}
{"level":"DEBUG","ts":"2025-11-28T21:13:52.939Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=3"}
{"level":"DEBUG","ts":"2025-11-28T21:13:52.939Z","msg":"Collected capacity metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, metricsCount=3"}
{"level":"DEBUG","ts":"2025-11-28T21:13:52.939Z","msg":"Variant analysis initialized: variant=ms-inference-scheduling-llm-d-modelservice-decode, accelerator=H100, cost=10.00, replicaCount=3"}
{"level":"DEBUG","ts":"2025-11-28T21:13:52.939Z","msg":"Scale-down unsafe: insufficient headroom after redistribution: remainingSpareKv=0.065, kvTrigger=0.200, kvSafe=false, remainingSpareQueue=5.0, queueTrigger=3.0%!(EXTRA bool=true)"}
{"level":"DEBUG","ts":"2025-11-28T21:13:52.939Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, totalReplicas=3, nonSaturated=3, avgSpareKv=0.210, avgSpareQueue=5.0, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"INFO","ts":"2025-11-28T21:13:52.939Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, totalReplicas=3, nonSaturated=3, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"DEBUG","ts":"2025-11-28T21:13:52.939Z","msg":"Capacity targets: no scaling needed (avgSpareKv=0.210, avgSpareQueue=5.0, all variants stable)"}
{"level":"DEBUG","ts":"2025-11-28T21:13:52.939Z","msg":"Capacity targets calculated: modelID=unsloth/Meta-Llama-3.1-8B, targets=map[ms-inference-scheduling-llm-d-modelservice-decode:3]"}
{"level":"INFO","ts":"2025-11-28T21:13:52.939Z","msg":"Capacity-only decisions made for model: unsloth/Meta-Llama-3.1-8B - decision count: 1"}
{"level":"INFO","ts":"2025-11-28T21:13:52.939Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-11-28T21:13:52.939Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=3→target=3"}
{"level":"DEBUG","ts":"2025-11-28T21:13:52.939Z","msg":"Found VA in map: variant=ms-inference-scheduling-llm-d-modelservice-decode, hasCurrentAlloc=true, accelerator=H100"}
{"level":"INFO","ts":"2025-11-28T21:13:52.939Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 3, desired-replicas: 3, accelerator: H100"}
{"level":"INFO","ts":"2025-11-28T21:13:52.939Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=3, accelerator=H100, capacityOnly=true"}
{"level":"INFO","ts":"2025-11-28T21:13:52.944Z","msg":"Applied capacity decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=3, target=3, reason=capacity-only mode: no-change"}
{"level":"INFO","ts":"2025-11-28T21:13:52.944Z","msg":"Reconciliation completed successfully: mode=capacity-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-11-28T21:13:52.979Z","msg":"Operating in CAPACITY-ONLY mode: reactive capacity-based scaling only"}
{"level":"INFO","ts":"2025-11-28T21:13:52.979Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-11-28T21:13:52.979Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-11-28T21:13:52.990Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=3, accelerator=H100, ttft=35.50ms, itl=19.40ms, cost=30.00, arrivalRate=3935.14"}
{"level":"INFO","ts":"2025-11-28T21:13:52.992Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9lshk5, usage=0.321 (32.1%)"}
{"level":"INFO","ts":"2025-11-28T21:13:52.992Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9ww5mn, usage=0.253 (25.3%)"}
{"level":"INFO","ts":"2025-11-28T21:13:52.992Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9l76md, usage=0.296 (29.6%)"}
{"level":"DEBUG","ts":"2025-11-28T21:13:52.992Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=3"}
{"level":"INFO","ts":"2025-11-28T21:13:52.992Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9lshk5, queueLength=0"}
{"level":"INFO","ts":"2025-11-28T21:13:52.992Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9ww5mn, queueLength=0"}
{"level":"INFO","ts":"2025-11-28T21:13:52.992Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9l76md, queueLength=0"}
{"level":"DEBUG","ts":"2025-11-28T21:13:52.992Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=3"}
{"level":"DEBUG","ts":"2025-11-28T21:13:52.992Z","msg":"Pod-to-variant matching successful: totalPods=3, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:3]"}
{"level":"DEBUG","ts":"2025-11-28T21:13:52.992Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=3"}
{"level":"DEBUG","ts":"2025-11-28T21:13:52.992Z","msg":"Collected capacity metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, metricsCount=3"}
{"level":"DEBUG","ts":"2025-11-28T21:13:52.992Z","msg":"Variant analysis initialized: variant=ms-inference-scheduling-llm-d-modelservice-decode, accelerator=H100, cost=10.00, replicaCount=3"}
{"level":"DEBUG","ts":"2025-11-28T21:13:52.992Z","msg":"Scale-down unsafe: insufficient headroom after redistribution: remainingSpareKv=0.065, kvTrigger=0.200, kvSafe=false, remainingSpareQueue=5.0, queueTrigger=3.0%!(EXTRA bool=true)"}
{"level":"DEBUG","ts":"2025-11-28T21:13:52.992Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, totalReplicas=3, nonSaturated=3, avgSpareKv=0.210, avgSpareQueue=5.0, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"INFO","ts":"2025-11-28T21:13:52.992Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, totalReplicas=3, nonSaturated=3, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"DEBUG","ts":"2025-11-28T21:13:52.992Z","msg":"Capacity targets: no scaling needed (avgSpareKv=0.210, avgSpareQueue=5.0, all variants stable)"}
{"level":"DEBUG","ts":"2025-11-28T21:13:52.992Z","msg":"Capacity targets calculated: modelID=unsloth/Meta-Llama-3.1-8B, targets=map[ms-inference-scheduling-llm-d-modelservice-decode:3]"}
{"level":"INFO","ts":"2025-11-28T21:13:52.992Z","msg":"Capacity-only decisions made for model: unsloth/Meta-Llama-3.1-8B - decision count: 1"}
{"level":"INFO","ts":"2025-11-28T21:13:52.992Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-11-28T21:13:52.992Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=3→target=3"}
{"level":"DEBUG","ts":"2025-11-28T21:13:52.992Z","msg":"Found VA in map: variant=ms-inference-scheduling-llm-d-modelservice-decode, hasCurrentAlloc=true, accelerator=H100"}
{"level":"INFO","ts":"2025-11-28T21:13:52.992Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 3, desired-replicas: 3, accelerator: H100"}
{"level":"INFO","ts":"2025-11-28T21:13:52.992Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=3, accelerator=H100, capacityOnly=true"}
{"level":"INFO","ts":"2025-11-28T21:13:52.997Z","msg":"Applied capacity decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=3, target=3, reason=capacity-only mode: no-change"}
{"level":"INFO","ts":"2025-11-28T21:13:52.997Z","msg":"Reconciliation completed successfully: mode=capacity-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-11-28T21:14:22.945Z","msg":"Operating in CAPACITY-ONLY mode: reactive capacity-based scaling only"}
{"level":"INFO","ts":"2025-11-28T21:14:22.945Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-11-28T21:14:22.945Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-11-28T21:14:22.960Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=3, accelerator=H100, ttft=36.10ms, itl=19.44ms, cost=30.00, arrivalRate=3976.57"}
{"level":"INFO","ts":"2025-11-28T21:14:22.961Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9lshk5, queueLength=0"}
{"level":"INFO","ts":"2025-11-28T21:14:22.961Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9ww5mn, queueLength=0"}
{"level":"INFO","ts":"2025-11-28T21:14:22.961Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9l76md, queueLength=0"}
{"level":"DEBUG","ts":"2025-11-28T21:14:22.961Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=3"}
{"level":"INFO","ts":"2025-11-28T21:14:22.962Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9lshk5, usage=0.321 (32.1%)"}
{"level":"INFO","ts":"2025-11-28T21:14:22.962Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9ww5mn, usage=0.244 (24.4%)"}
{"level":"INFO","ts":"2025-11-28T21:14:22.962Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9l76md, usage=0.266 (26.6%)"}
{"level":"DEBUG","ts":"2025-11-28T21:14:22.962Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=3"}
{"level":"DEBUG","ts":"2025-11-28T21:14:22.962Z","msg":"Pod-to-variant matching successful: totalPods=3, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:3]"}
{"level":"DEBUG","ts":"2025-11-28T21:14:22.962Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=3"}
{"level":"DEBUG","ts":"2025-11-28T21:14:22.962Z","msg":"Collected capacity metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, metricsCount=3"}
{"level":"DEBUG","ts":"2025-11-28T21:14:22.962Z","msg":"Variant analysis initialized: variant=ms-inference-scheduling-llm-d-modelservice-decode, accelerator=H100, cost=10.00, replicaCount=3"}
{"level":"DEBUG","ts":"2025-11-28T21:14:22.962Z","msg":"Scale-down unsafe: insufficient headroom after redistribution: remainingSpareKv=0.085, kvTrigger=0.200, kvSafe=false, remainingSpareQueue=5.0, queueTrigger=3.0%!(EXTRA bool=true)"}
{"level":"DEBUG","ts":"2025-11-28T21:14:22.962Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, totalReplicas=3, nonSaturated=3, avgSpareKv=0.223, avgSpareQueue=5.0, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"INFO","ts":"2025-11-28T21:14:22.962Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, totalReplicas=3, nonSaturated=3, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"DEBUG","ts":"2025-11-28T21:14:22.962Z","msg":"Capacity targets: no scaling needed (avgSpareKv=0.223, avgSpareQueue=5.0, all variants stable)"}
{"level":"DEBUG","ts":"2025-11-28T21:14:22.962Z","msg":"Capacity targets calculated: modelID=unsloth/Meta-Llama-3.1-8B, targets=map[ms-inference-scheduling-llm-d-modelservice-decode:3]"}
{"level":"INFO","ts":"2025-11-28T21:14:22.962Z","msg":"Capacity-only decisions made for model: unsloth/Meta-Llama-3.1-8B - decision count: 1"}
{"level":"INFO","ts":"2025-11-28T21:14:22.962Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-11-28T21:14:22.962Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=3→target=3"}
{"level":"DEBUG","ts":"2025-11-28T21:14:22.962Z","msg":"Found VA in map: variant=ms-inference-scheduling-llm-d-modelservice-decode, hasCurrentAlloc=true, accelerator=H100"}
{"level":"INFO","ts":"2025-11-28T21:14:22.962Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 3, desired-replicas: 3, accelerator: H100"}
{"level":"INFO","ts":"2025-11-28T21:14:22.962Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=3, accelerator=H100, capacityOnly=true"}
{"level":"INFO","ts":"2025-11-28T21:14:22.967Z","msg":"Applied capacity decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=3, target=3, reason=capacity-only mode: no-change"}
{"level":"INFO","ts":"2025-11-28T21:14:22.967Z","msg":"Reconciliation completed successfully: mode=capacity-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-11-28T21:14:22.998Z","msg":"Operating in CAPACITY-ONLY mode: reactive capacity-based scaling only"}
{"level":"INFO","ts":"2025-11-28T21:14:22.998Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-11-28T21:14:22.998Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-11-28T21:14:23.008Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=3, accelerator=H100, ttft=36.10ms, itl=19.44ms, cost=30.00, arrivalRate=3976.57"}
{"level":"INFO","ts":"2025-11-28T21:14:23.009Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9lshk5, queueLength=0"}
{"level":"INFO","ts":"2025-11-28T21:14:23.009Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9ww5mn, queueLength=0"}
{"level":"INFO","ts":"2025-11-28T21:14:23.009Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9l76md, queueLength=0"}
{"level":"DEBUG","ts":"2025-11-28T21:14:23.009Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=3"}
{"level":"INFO","ts":"2025-11-28T21:14:23.009Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9lshk5, usage=0.321 (32.1%)"}
{"level":"INFO","ts":"2025-11-28T21:14:23.009Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9ww5mn, usage=0.244 (24.4%)"}
{"level":"INFO","ts":"2025-11-28T21:14:23.009Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9l76md, usage=0.266 (26.6%)"}
{"level":"DEBUG","ts":"2025-11-28T21:14:23.009Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=3"}
{"level":"DEBUG","ts":"2025-11-28T21:14:23.009Z","msg":"Pod-to-variant matching successful: totalPods=3, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:3]"}
{"level":"DEBUG","ts":"2025-11-28T21:14:23.009Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=3"}
{"level":"DEBUG","ts":"2025-11-28T21:14:23.009Z","msg":"Collected capacity metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, metricsCount=3"}
{"level":"DEBUG","ts":"2025-11-28T21:14:23.009Z","msg":"Variant analysis initialized: variant=ms-inference-scheduling-llm-d-modelservice-decode, accelerator=H100, cost=10.00, replicaCount=3"}
{"level":"DEBUG","ts":"2025-11-28T21:14:23.009Z","msg":"Scale-down unsafe: insufficient headroom after redistribution: remainingSpareKv=0.085, kvTrigger=0.200, kvSafe=false, remainingSpareQueue=5.0, queueTrigger=3.0%!(EXTRA bool=true)"}
{"level":"DEBUG","ts":"2025-11-28T21:14:23.009Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, totalReplicas=3, nonSaturated=3, avgSpareKv=0.223, avgSpareQueue=5.0, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"INFO","ts":"2025-11-28T21:14:23.009Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, totalReplicas=3, nonSaturated=3, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"DEBUG","ts":"2025-11-28T21:14:23.009Z","msg":"Capacity targets: no scaling needed (avgSpareKv=0.223, avgSpareQueue=5.0, all variants stable)"}
{"level":"DEBUG","ts":"2025-11-28T21:14:23.009Z","msg":"Capacity targets calculated: modelID=unsloth/Meta-Llama-3.1-8B, targets=map[ms-inference-scheduling-llm-d-modelservice-decode:3]"}
{"level":"INFO","ts":"2025-11-28T21:14:23.009Z","msg":"Capacity-only decisions made for model: unsloth/Meta-Llama-3.1-8B - decision count: 1"}
{"level":"INFO","ts":"2025-11-28T21:14:23.009Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-11-28T21:14:23.009Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=3→target=3"}
{"level":"DEBUG","ts":"2025-11-28T21:14:23.009Z","msg":"Found VA in map: variant=ms-inference-scheduling-llm-d-modelservice-decode, hasCurrentAlloc=true, accelerator=H100"}
{"level":"INFO","ts":"2025-11-28T21:14:23.009Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 3, desired-replicas: 3, accelerator: H100"}
{"level":"INFO","ts":"2025-11-28T21:14:23.009Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=3, accelerator=H100, capacityOnly=true"}
{"level":"INFO","ts":"2025-11-28T21:14:23.015Z","msg":"Applied capacity decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=3, target=3, reason=capacity-only mode: no-change"}
{"level":"INFO","ts":"2025-11-28T21:14:23.015Z","msg":"Reconciliation completed successfully: mode=capacity-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-11-28T21:14:52.969Z","msg":"Operating in CAPACITY-ONLY mode: reactive capacity-based scaling only"}
{"level":"INFO","ts":"2025-11-28T21:14:52.969Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-11-28T21:14:52.969Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-11-28T21:14:52.985Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=3, accelerator=H100, ttft=34.72ms, itl=18.79ms, cost=30.00, arrivalRate=4037.00"}
{"level":"INFO","ts":"2025-11-28T21:14:52.987Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9lshk5, usage=0.287 (28.7%)"}
{"level":"INFO","ts":"2025-11-28T21:14:52.987Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9ww5mn, usage=0.277 (27.7%)"}
{"level":"INFO","ts":"2025-11-28T21:14:52.987Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9l76md, usage=0.230 (23.0%)"}
{"level":"DEBUG","ts":"2025-11-28T21:14:52.987Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=3"}
{"level":"INFO","ts":"2025-11-28T21:14:52.987Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9lshk5, queueLength=0"}
{"level":"INFO","ts":"2025-11-28T21:14:52.987Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9ww5mn, queueLength=0"}
{"level":"INFO","ts":"2025-11-28T21:14:52.987Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9l76md, queueLength=0"}
{"level":"DEBUG","ts":"2025-11-28T21:14:52.987Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=3"}
{"level":"DEBUG","ts":"2025-11-28T21:14:52.987Z","msg":"Pod-to-variant matching successful: totalPods=3, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:3]"}
{"level":"DEBUG","ts":"2025-11-28T21:14:52.987Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=3"}
{"level":"DEBUG","ts":"2025-11-28T21:14:52.987Z","msg":"Collected capacity metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, metricsCount=3"}
{"level":"DEBUG","ts":"2025-11-28T21:14:52.987Z","msg":"Variant analysis initialized: variant=ms-inference-scheduling-llm-d-modelservice-decode, accelerator=H100, cost=10.00, replicaCount=3"}
{"level":"DEBUG","ts":"2025-11-28T21:14:52.987Z","msg":"Scale-down unsafe: insufficient headroom after redistribution: remainingSpareKv=0.103, kvTrigger=0.200, kvSafe=false, remainingSpareQueue=5.0, queueTrigger=3.0%!(EXTRA bool=true)"}
{"level":"DEBUG","ts":"2025-11-28T21:14:52.987Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, totalReplicas=3, nonSaturated=3, avgSpareKv=0.235, avgSpareQueue=5.0, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"INFO","ts":"2025-11-28T21:14:52.987Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, totalReplicas=3, nonSaturated=3, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"DEBUG","ts":"2025-11-28T21:14:52.987Z","msg":"Capacity targets: no scaling needed (avgSpareKv=0.235, avgSpareQueue=5.0, all variants stable)"}
{"level":"DEBUG","ts":"2025-11-28T21:14:52.987Z","msg":"Capacity targets calculated: modelID=unsloth/Meta-Llama-3.1-8B, targets=map[ms-inference-scheduling-llm-d-modelservice-decode:3]"}
{"level":"INFO","ts":"2025-11-28T21:14:52.987Z","msg":"Capacity-only decisions made for model: unsloth/Meta-Llama-3.1-8B - decision count: 1"}
{"level":"INFO","ts":"2025-11-28T21:14:52.987Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-11-28T21:14:52.987Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=3→target=3"}
{"level":"DEBUG","ts":"2025-11-28T21:14:52.987Z","msg":"Found VA in map: variant=ms-inference-scheduling-llm-d-modelservice-decode, hasCurrentAlloc=true, accelerator=H100"}
{"level":"INFO","ts":"2025-11-28T21:14:52.987Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 3, desired-replicas: 3, accelerator: H100"}
{"level":"INFO","ts":"2025-11-28T21:14:52.987Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=3, accelerator=H100, capacityOnly=true"}
{"level":"INFO","ts":"2025-11-28T21:14:52.993Z","msg":"Applied capacity decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=3, target=3, reason=capacity-only mode: no-change"}
{"level":"INFO","ts":"2025-11-28T21:14:52.993Z","msg":"Reconciliation completed successfully: mode=capacity-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-11-28T21:14:53.016Z","msg":"Operating in CAPACITY-ONLY mode: reactive capacity-based scaling only"}
{"level":"INFO","ts":"2025-11-28T21:14:53.016Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-11-28T21:14:53.016Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-11-28T21:14:53.051Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=3, accelerator=H100, ttft=34.72ms, itl=18.79ms, cost=30.00, arrivalRate=4037.00"}
{"level":"INFO","ts":"2025-11-28T21:14:53.053Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9lshk5, queueLength=0"}
{"level":"INFO","ts":"2025-11-28T21:14:53.053Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9ww5mn, queueLength=0"}
{"level":"INFO","ts":"2025-11-28T21:14:53.053Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9l76md, queueLength=0"}
{"level":"DEBUG","ts":"2025-11-28T21:14:53.053Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=3"}
{"level":"INFO","ts":"2025-11-28T21:14:53.053Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9lshk5, usage=0.287 (28.7%)"}
{"level":"INFO","ts":"2025-11-28T21:14:53.053Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9ww5mn, usage=0.277 (27.7%)"}
{"level":"INFO","ts":"2025-11-28T21:14:53.053Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9l76md, usage=0.230 (23.0%)"}
{"level":"DEBUG","ts":"2025-11-28T21:14:53.053Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=3"}
{"level":"DEBUG","ts":"2025-11-28T21:14:53.053Z","msg":"Pod-to-variant matching successful: totalPods=3, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:3]"}
{"level":"DEBUG","ts":"2025-11-28T21:14:53.053Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=3"}
{"level":"DEBUG","ts":"2025-11-28T21:14:53.053Z","msg":"Collected capacity metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, metricsCount=3"}
{"level":"DEBUG","ts":"2025-11-28T21:14:53.053Z","msg":"Variant analysis initialized: variant=ms-inference-scheduling-llm-d-modelservice-decode, accelerator=H100, cost=10.00, replicaCount=3"}
{"level":"DEBUG","ts":"2025-11-28T21:14:53.053Z","msg":"Scale-down unsafe: insufficient headroom after redistribution: remainingSpareKv=0.103, kvTrigger=0.200, kvSafe=false, remainingSpareQueue=5.0, queueTrigger=3.0%!(EXTRA bool=true)"}
{"level":"DEBUG","ts":"2025-11-28T21:14:53.053Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, totalReplicas=3, nonSaturated=3, avgSpareKv=0.235, avgSpareQueue=5.0, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"INFO","ts":"2025-11-28T21:14:53.053Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, totalReplicas=3, nonSaturated=3, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"DEBUG","ts":"2025-11-28T21:14:53.053Z","msg":"Capacity targets: no scaling needed (avgSpareKv=0.235, avgSpareQueue=5.0, all variants stable)"}
{"level":"DEBUG","ts":"2025-11-28T21:14:53.053Z","msg":"Capacity targets calculated: modelID=unsloth/Meta-Llama-3.1-8B, targets=map[ms-inference-scheduling-llm-d-modelservice-decode:3]"}
{"level":"INFO","ts":"2025-11-28T21:14:53.053Z","msg":"Capacity-only decisions made for model: unsloth/Meta-Llama-3.1-8B - decision count: 1"}
{"level":"INFO","ts":"2025-11-28T21:14:53.053Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-11-28T21:14:53.053Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=3→target=3"}
{"level":"DEBUG","ts":"2025-11-28T21:14:53.053Z","msg":"Found VA in map: variant=ms-inference-scheduling-llm-d-modelservice-decode, hasCurrentAlloc=true, accelerator=H100"}
{"level":"INFO","ts":"2025-11-28T21:14:53.053Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 3, desired-replicas: 3, accelerator: H100"}
{"level":"INFO","ts":"2025-11-28T21:14:53.053Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=3, accelerator=H100, capacityOnly=true"}
{"level":"INFO","ts":"2025-11-28T21:14:53.060Z","msg":"Applied capacity decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=3, target=3, reason=capacity-only mode: no-change"}
{"level":"INFO","ts":"2025-11-28T21:14:53.060Z","msg":"Reconciliation completed successfully: mode=capacity-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-11-28T21:15:22.993Z","msg":"Operating in CAPACITY-ONLY mode: reactive capacity-based scaling only"}
{"level":"INFO","ts":"2025-11-28T21:15:22.993Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-11-28T21:15:22.993Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-11-28T21:15:23.014Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=3, accelerator=H100, ttft=34.15ms, itl=18.73ms, cost=30.00, arrivalRate=3847.00"}
{"level":"INFO","ts":"2025-11-28T21:15:23.015Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9lshk5, queueLength=0"}
{"level":"INFO","ts":"2025-11-28T21:15:23.015Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9ww5mn, queueLength=0"}
{"level":"INFO","ts":"2025-11-28T21:15:23.015Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9l76md, queueLength=0"}
{"level":"DEBUG","ts":"2025-11-28T21:15:23.015Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=3"}
{"level":"INFO","ts":"2025-11-28T21:15:23.015Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9lshk5, usage=0.262 (26.2%)"}
{"level":"INFO","ts":"2025-11-28T21:15:23.015Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9ww5mn, usage=0.248 (24.8%)"}
{"level":"INFO","ts":"2025-11-28T21:15:23.015Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9l76md, usage=0.277 (27.7%)"}
{"level":"DEBUG","ts":"2025-11-28T21:15:23.015Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=3"}
{"level":"DEBUG","ts":"2025-11-28T21:15:23.015Z","msg":"Pod-to-variant matching successful: totalPods=3, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:3]"}
{"level":"DEBUG","ts":"2025-11-28T21:15:23.015Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=3"}
{"level":"DEBUG","ts":"2025-11-28T21:15:23.016Z","msg":"Collected capacity metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, metricsCount=3"}
{"level":"DEBUG","ts":"2025-11-28T21:15:23.016Z","msg":"Variant analysis initialized: variant=ms-inference-scheduling-llm-d-modelservice-decode, accelerator=H100, cost=10.00, replicaCount=3"}
{"level":"DEBUG","ts":"2025-11-28T21:15:23.016Z","msg":"Scale-down unsafe: insufficient headroom after redistribution: remainingSpareKv=0.107, kvTrigger=0.200, kvSafe=false, remainingSpareQueue=5.0, queueTrigger=3.0%!(EXTRA bool=true)"}
{"level":"DEBUG","ts":"2025-11-28T21:15:23.016Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, totalReplicas=3, nonSaturated=3, avgSpareKv=0.238, avgSpareQueue=5.0, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"INFO","ts":"2025-11-28T21:15:23.016Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, totalReplicas=3, nonSaturated=3, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"DEBUG","ts":"2025-11-28T21:15:23.016Z","msg":"Capacity targets: no scaling needed (avgSpareKv=0.238, avgSpareQueue=5.0, all variants stable)"}
{"level":"DEBUG","ts":"2025-11-28T21:15:23.016Z","msg":"Capacity targets calculated: modelID=unsloth/Meta-Llama-3.1-8B, targets=map[ms-inference-scheduling-llm-d-modelservice-decode:3]"}
{"level":"INFO","ts":"2025-11-28T21:15:23.016Z","msg":"Capacity-only decisions made for model: unsloth/Meta-Llama-3.1-8B - decision count: 1"}
{"level":"INFO","ts":"2025-11-28T21:15:23.016Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-11-28T21:15:23.016Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=3→target=3"}
{"level":"DEBUG","ts":"2025-11-28T21:15:23.016Z","msg":"Found VA in map: variant=ms-inference-scheduling-llm-d-modelservice-decode, hasCurrentAlloc=true, accelerator=H100"}
{"level":"INFO","ts":"2025-11-28T21:15:23.016Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 3, desired-replicas: 3, accelerator: H100"}
{"level":"INFO","ts":"2025-11-28T21:15:23.016Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=3, accelerator=H100, capacityOnly=true"}
{"level":"INFO","ts":"2025-11-28T21:15:23.022Z","msg":"Applied capacity decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=3, target=3, reason=capacity-only mode: no-change"}
{"level":"INFO","ts":"2025-11-28T21:15:23.022Z","msg":"Reconciliation completed successfully: mode=capacity-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-11-28T21:15:23.061Z","msg":"Operating in CAPACITY-ONLY mode: reactive capacity-based scaling only"}
{"level":"INFO","ts":"2025-11-28T21:15:23.061Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-11-28T21:15:23.061Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-11-28T21:15:23.071Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=3, accelerator=H100, ttft=34.15ms, itl=18.73ms, cost=30.00, arrivalRate=3847.00"}
{"level":"INFO","ts":"2025-11-28T21:15:23.072Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9lshk5, queueLength=0"}
{"level":"INFO","ts":"2025-11-28T21:15:23.072Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9ww5mn, queueLength=0"}
{"level":"INFO","ts":"2025-11-28T21:15:23.072Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9l76md, queueLength=0"}
{"level":"DEBUG","ts":"2025-11-28T21:15:23.072Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=3"}
{"level":"INFO","ts":"2025-11-28T21:15:23.072Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9lshk5, usage=0.262 (26.2%)"}
{"level":"INFO","ts":"2025-11-28T21:15:23.072Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9ww5mn, usage=0.248 (24.8%)"}
{"level":"INFO","ts":"2025-11-28T21:15:23.072Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9l76md, usage=0.277 (27.7%)"}
{"level":"DEBUG","ts":"2025-11-28T21:15:23.072Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=3"}
{"level":"DEBUG","ts":"2025-11-28T21:15:23.072Z","msg":"Pod-to-variant matching successful: totalPods=3, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:3]"}
{"level":"DEBUG","ts":"2025-11-28T21:15:23.072Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=3"}
{"level":"DEBUG","ts":"2025-11-28T21:15:23.072Z","msg":"Collected capacity metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, metricsCount=3"}
{"level":"DEBUG","ts":"2025-11-28T21:15:23.072Z","msg":"Variant analysis initialized: variant=ms-inference-scheduling-llm-d-modelservice-decode, accelerator=H100, cost=10.00, replicaCount=3"}
{"level":"DEBUG","ts":"2025-11-28T21:15:23.072Z","msg":"Scale-down unsafe: insufficient headroom after redistribution: remainingSpareKv=0.107, kvTrigger=0.200, kvSafe=false, remainingSpareQueue=5.0, queueTrigger=3.0%!(EXTRA bool=true)"}
{"level":"DEBUG","ts":"2025-11-28T21:15:23.072Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, totalReplicas=3, nonSaturated=3, avgSpareKv=0.238, avgSpareQueue=5.0, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"INFO","ts":"2025-11-28T21:15:23.072Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, totalReplicas=3, nonSaturated=3, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"DEBUG","ts":"2025-11-28T21:15:23.072Z","msg":"Capacity targets: no scaling needed (avgSpareKv=0.238, avgSpareQueue=5.0, all variants stable)"}
{"level":"DEBUG","ts":"2025-11-28T21:15:23.072Z","msg":"Capacity targets calculated: modelID=unsloth/Meta-Llama-3.1-8B, targets=map[ms-inference-scheduling-llm-d-modelservice-decode:3]"}
{"level":"INFO","ts":"2025-11-28T21:15:23.072Z","msg":"Capacity-only decisions made for model: unsloth/Meta-Llama-3.1-8B - decision count: 1"}
{"level":"INFO","ts":"2025-11-28T21:15:23.072Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-11-28T21:15:23.072Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=3→target=3"}
{"level":"DEBUG","ts":"2025-11-28T21:15:23.072Z","msg":"Found VA in map: variant=ms-inference-scheduling-llm-d-modelservice-decode, hasCurrentAlloc=true, accelerator=H100"}
{"level":"INFO","ts":"2025-11-28T21:15:23.073Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 3, desired-replicas: 3, accelerator: H100"}
{"level":"INFO","ts":"2025-11-28T21:15:23.073Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=3, accelerator=H100, capacityOnly=true"}
{"level":"INFO","ts":"2025-11-28T21:15:23.078Z","msg":"Applied capacity decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=3, target=3, reason=capacity-only mode: no-change"}
{"level":"INFO","ts":"2025-11-28T21:15:23.078Z","msg":"Reconciliation completed successfully: mode=capacity-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-11-28T21:15:53.023Z","msg":"Operating in CAPACITY-ONLY mode: reactive capacity-based scaling only"}
{"level":"INFO","ts":"2025-11-28T21:15:53.023Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-11-28T21:15:53.023Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-11-28T21:15:53.044Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=3, accelerator=H100, ttft=34.48ms, itl=18.62ms, cost=30.00, arrivalRate=3814.57"}
{"level":"INFO","ts":"2025-11-28T21:15:53.045Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9lshk5, queueLength=0"}
{"level":"INFO","ts":"2025-11-28T21:15:53.045Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9ww5mn, queueLength=0"}
{"level":"INFO","ts":"2025-11-28T21:15:53.045Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9l76md, queueLength=0"}
{"level":"DEBUG","ts":"2025-11-28T21:15:53.045Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=3"}
{"level":"INFO","ts":"2025-11-28T21:15:53.046Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9lshk5, usage=0.257 (25.7%)"}
{"level":"INFO","ts":"2025-11-28T21:15:53.046Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9ww5mn, usage=0.283 (28.3%)"}
{"level":"INFO","ts":"2025-11-28T21:15:53.046Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9l76md, usage=0.277 (27.7%)"}
{"level":"DEBUG","ts":"2025-11-28T21:15:53.046Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=3"}
{"level":"DEBUG","ts":"2025-11-28T21:15:53.046Z","msg":"Pod-to-variant matching successful: totalPods=3, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:3]"}
{"level":"DEBUG","ts":"2025-11-28T21:15:53.046Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=3"}
{"level":"DEBUG","ts":"2025-11-28T21:15:53.046Z","msg":"Collected capacity metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, metricsCount=3"}
{"level":"DEBUG","ts":"2025-11-28T21:15:53.046Z","msg":"Variant analysis initialized: variant=ms-inference-scheduling-llm-d-modelservice-decode, accelerator=H100, cost=10.00, replicaCount=3"}
{"level":"DEBUG","ts":"2025-11-28T21:15:53.046Z","msg":"Scale-down unsafe: insufficient headroom after redistribution: remainingSpareKv=0.091, kvTrigger=0.200, kvSafe=false, remainingSpareQueue=5.0, queueTrigger=3.0%!(EXTRA bool=true)"}
{"level":"DEBUG","ts":"2025-11-28T21:15:53.046Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, totalReplicas=3, nonSaturated=3, avgSpareKv=0.228, avgSpareQueue=5.0, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"INFO","ts":"2025-11-28T21:15:53.046Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, totalReplicas=3, nonSaturated=3, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"DEBUG","ts":"2025-11-28T21:15:53.046Z","msg":"Capacity targets: no scaling needed (avgSpareKv=0.228, avgSpareQueue=5.0, all variants stable)"}
{"level":"DEBUG","ts":"2025-11-28T21:15:53.046Z","msg":"Capacity targets calculated: modelID=unsloth/Meta-Llama-3.1-8B, targets=map[ms-inference-scheduling-llm-d-modelservice-decode:3]"}
{"level":"INFO","ts":"2025-11-28T21:15:53.046Z","msg":"Capacity-only decisions made for model: unsloth/Meta-Llama-3.1-8B - decision count: 1"}
{"level":"INFO","ts":"2025-11-28T21:15:53.046Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-11-28T21:15:53.046Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=3→target=3"}
{"level":"DEBUG","ts":"2025-11-28T21:15:53.046Z","msg":"Found VA in map: variant=ms-inference-scheduling-llm-d-modelservice-decode, hasCurrentAlloc=true, accelerator=H100"}
{"level":"INFO","ts":"2025-11-28T21:15:53.046Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 3, desired-replicas: 3, accelerator: H100"}
{"level":"INFO","ts":"2025-11-28T21:15:53.046Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=3, accelerator=H100, capacityOnly=true"}
{"level":"INFO","ts":"2025-11-28T21:15:53.051Z","msg":"Applied capacity decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=3, target=3, reason=capacity-only mode: no-change"}
{"level":"INFO","ts":"2025-11-28T21:15:53.051Z","msg":"Reconciliation completed successfully: mode=capacity-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-11-28T21:15:53.078Z","msg":"Operating in CAPACITY-ONLY mode: reactive capacity-based scaling only"}
{"level":"INFO","ts":"2025-11-28T21:15:53.078Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-11-28T21:15:53.078Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-11-28T21:15:53.087Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=3, accelerator=H100, ttft=34.48ms, itl=18.62ms, cost=30.00, arrivalRate=3814.57"}
{"level":"INFO","ts":"2025-11-28T21:15:53.089Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9lshk5, queueLength=0"}
{"level":"INFO","ts":"2025-11-28T21:15:53.089Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9ww5mn, queueLength=0"}
{"level":"INFO","ts":"2025-11-28T21:15:53.089Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9l76md, queueLength=0"}
{"level":"DEBUG","ts":"2025-11-28T21:15:53.089Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=3"}
{"level":"INFO","ts":"2025-11-28T21:15:53.089Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9lshk5, usage=0.257 (25.7%)"}
{"level":"INFO","ts":"2025-11-28T21:15:53.089Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9ww5mn, usage=0.283 (28.3%)"}
{"level":"INFO","ts":"2025-11-28T21:15:53.089Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9l76md, usage=0.277 (27.7%)"}
{"level":"DEBUG","ts":"2025-11-28T21:15:53.089Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=3"}
{"level":"DEBUG","ts":"2025-11-28T21:15:53.089Z","msg":"Pod-to-variant matching successful: totalPods=3, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:3]"}
{"level":"DEBUG","ts":"2025-11-28T21:15:53.089Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=3"}
{"level":"DEBUG","ts":"2025-11-28T21:15:53.089Z","msg":"Collected capacity metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, metricsCount=3"}
{"level":"DEBUG","ts":"2025-11-28T21:15:53.089Z","msg":"Variant analysis initialized: variant=ms-inference-scheduling-llm-d-modelservice-decode, accelerator=H100, cost=10.00, replicaCount=3"}
{"level":"DEBUG","ts":"2025-11-28T21:15:53.089Z","msg":"Scale-down unsafe: insufficient headroom after redistribution: remainingSpareKv=0.091, kvTrigger=0.200, kvSafe=false, remainingSpareQueue=5.0, queueTrigger=3.0%!(EXTRA bool=true)"}
{"level":"DEBUG","ts":"2025-11-28T21:15:53.089Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, totalReplicas=3, nonSaturated=3, avgSpareKv=0.228, avgSpareQueue=5.0, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"INFO","ts":"2025-11-28T21:15:53.089Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, totalReplicas=3, nonSaturated=3, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"DEBUG","ts":"2025-11-28T21:15:53.089Z","msg":"Capacity targets: no scaling needed (avgSpareKv=0.228, avgSpareQueue=5.0, all variants stable)"}
{"level":"DEBUG","ts":"2025-11-28T21:15:53.089Z","msg":"Capacity targets calculated: modelID=unsloth/Meta-Llama-3.1-8B, targets=map[ms-inference-scheduling-llm-d-modelservice-decode:3]"}
{"level":"INFO","ts":"2025-11-28T21:15:53.089Z","msg":"Capacity-only decisions made for model: unsloth/Meta-Llama-3.1-8B - decision count: 1"}
{"level":"INFO","ts":"2025-11-28T21:15:53.089Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-11-28T21:15:53.089Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=3→target=3"}
{"level":"DEBUG","ts":"2025-11-28T21:15:53.089Z","msg":"Found VA in map: variant=ms-inference-scheduling-llm-d-modelservice-decode, hasCurrentAlloc=true, accelerator=H100"}
{"level":"INFO","ts":"2025-11-28T21:15:53.089Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 3, desired-replicas: 3, accelerator: H100"}
{"level":"INFO","ts":"2025-11-28T21:15:53.089Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=3, accelerator=H100, capacityOnly=true"}
{"level":"INFO","ts":"2025-11-28T21:15:53.095Z","msg":"Applied capacity decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=3, target=3, reason=capacity-only mode: no-change"}
{"level":"INFO","ts":"2025-11-28T21:15:53.095Z","msg":"Reconciliation completed successfully: mode=capacity-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-11-28T21:16:23.052Z","msg":"Operating in CAPACITY-ONLY mode: reactive capacity-based scaling only"}
{"level":"INFO","ts":"2025-11-28T21:16:23.053Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-11-28T21:16:23.053Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-11-28T21:16:23.068Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=3, accelerator=H100, ttft=33.49ms, itl=18.24ms, cost=30.00, arrivalRate=3836.21"}
{"level":"INFO","ts":"2025-11-28T21:16:23.070Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9lshk5, queueLength=0"}
{"level":"INFO","ts":"2025-11-28T21:16:23.070Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9ww5mn, queueLength=0"}
{"level":"INFO","ts":"2025-11-28T21:16:23.070Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9l76md, queueLength=0"}
{"level":"DEBUG","ts":"2025-11-28T21:16:23.070Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=3"}
{"level":"INFO","ts":"2025-11-28T21:16:23.070Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9lshk5, usage=0.258 (25.8%)"}
{"level":"INFO","ts":"2025-11-28T21:16:23.070Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9ww5mn, usage=0.302 (30.2%)"}
{"level":"INFO","ts":"2025-11-28T21:16:23.070Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9l76md, usage=0.208 (20.8%)"}
{"level":"DEBUG","ts":"2025-11-28T21:16:23.070Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=3"}
{"level":"DEBUG","ts":"2025-11-28T21:16:23.070Z","msg":"Pod-to-variant matching successful: totalPods=3, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:3]"}
{"level":"DEBUG","ts":"2025-11-28T21:16:23.070Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=3"}
{"level":"DEBUG","ts":"2025-11-28T21:16:23.070Z","msg":"Collected capacity metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, metricsCount=3"}
{"level":"DEBUG","ts":"2025-11-28T21:16:23.070Z","msg":"Variant analysis initialized: variant=ms-inference-scheduling-llm-d-modelservice-decode, accelerator=H100, cost=10.00, replicaCount=3"}
{"level":"DEBUG","ts":"2025-11-28T21:16:23.070Z","msg":"Scale-down unsafe: insufficient headroom after redistribution: remainingSpareKv=0.116, kvTrigger=0.200, kvSafe=false, remainingSpareQueue=5.0, queueTrigger=3.0%!(EXTRA bool=true)"}
{"level":"DEBUG","ts":"2025-11-28T21:16:23.070Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, totalReplicas=3, nonSaturated=3, avgSpareKv=0.244, avgSpareQueue=5.0, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"INFO","ts":"2025-11-28T21:16:23.070Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, totalReplicas=3, nonSaturated=3, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"DEBUG","ts":"2025-11-28T21:16:23.070Z","msg":"Capacity targets: no scaling needed (avgSpareKv=0.244, avgSpareQueue=5.0, all variants stable)"}
{"level":"DEBUG","ts":"2025-11-28T21:16:23.070Z","msg":"Capacity targets calculated: modelID=unsloth/Meta-Llama-3.1-8B, targets=map[ms-inference-scheduling-llm-d-modelservice-decode:3]"}
{"level":"INFO","ts":"2025-11-28T21:16:23.070Z","msg":"Capacity-only decisions made for model: unsloth/Meta-Llama-3.1-8B - decision count: 1"}
{"level":"INFO","ts":"2025-11-28T21:16:23.070Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-11-28T21:16:23.070Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=3→target=3"}
{"level":"DEBUG","ts":"2025-11-28T21:16:23.070Z","msg":"Found VA in map: variant=ms-inference-scheduling-llm-d-modelservice-decode, hasCurrentAlloc=true, accelerator=H100"}
{"level":"INFO","ts":"2025-11-28T21:16:23.070Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 3, desired-replicas: 3, accelerator: H100"}
{"level":"INFO","ts":"2025-11-28T21:16:23.070Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=3, accelerator=H100, capacityOnly=true"}
{"level":"INFO","ts":"2025-11-28T21:16:23.076Z","msg":"Applied capacity decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=3, target=3, reason=capacity-only mode: no-change"}
{"level":"INFO","ts":"2025-11-28T21:16:23.076Z","msg":"Reconciliation completed successfully: mode=capacity-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-11-28T21:16:23.095Z","msg":"Operating in CAPACITY-ONLY mode: reactive capacity-based scaling only"}
{"level":"INFO","ts":"2025-11-28T21:16:23.095Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-11-28T21:16:23.095Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-11-28T21:16:23.105Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=3, accelerator=H100, ttft=33.49ms, itl=18.24ms, cost=30.00, arrivalRate=3836.21"}
{"level":"INFO","ts":"2025-11-28T21:16:23.106Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9lshk5, queueLength=0"}
{"level":"INFO","ts":"2025-11-28T21:16:23.106Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9ww5mn, queueLength=0"}
{"level":"INFO","ts":"2025-11-28T21:16:23.106Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9l76md, queueLength=0"}
{"level":"DEBUG","ts":"2025-11-28T21:16:23.106Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=3"}
{"level":"INFO","ts":"2025-11-28T21:16:23.107Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9lshk5, usage=0.258 (25.8%)"}
{"level":"INFO","ts":"2025-11-28T21:16:23.107Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9ww5mn, usage=0.302 (30.2%)"}
{"level":"INFO","ts":"2025-11-28T21:16:23.107Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9l76md, usage=0.208 (20.8%)"}
{"level":"DEBUG","ts":"2025-11-28T21:16:23.107Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=3"}
{"level":"DEBUG","ts":"2025-11-28T21:16:23.107Z","msg":"Pod-to-variant matching successful: totalPods=3, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:3]"}
{"level":"DEBUG","ts":"2025-11-28T21:16:23.107Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=3"}
{"level":"DEBUG","ts":"2025-11-28T21:16:23.107Z","msg":"Collected capacity metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, metricsCount=3"}
{"level":"DEBUG","ts":"2025-11-28T21:16:23.107Z","msg":"Variant analysis initialized: variant=ms-inference-scheduling-llm-d-modelservice-decode, accelerator=H100, cost=10.00, replicaCount=3"}
{"level":"DEBUG","ts":"2025-11-28T21:16:23.107Z","msg":"Scale-down unsafe: insufficient headroom after redistribution: remainingSpareKv=0.116, kvTrigger=0.200, kvSafe=false, remainingSpareQueue=5.0, queueTrigger=3.0%!(EXTRA bool=true)"}
{"level":"DEBUG","ts":"2025-11-28T21:16:23.107Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, totalReplicas=3, nonSaturated=3, avgSpareKv=0.244, avgSpareQueue=5.0, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"INFO","ts":"2025-11-28T21:16:23.107Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, totalReplicas=3, nonSaturated=3, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"DEBUG","ts":"2025-11-28T21:16:23.107Z","msg":"Capacity targets: no scaling needed (avgSpareKv=0.244, avgSpareQueue=5.0, all variants stable)"}
{"level":"DEBUG","ts":"2025-11-28T21:16:23.107Z","msg":"Capacity targets calculated: modelID=unsloth/Meta-Llama-3.1-8B, targets=map[ms-inference-scheduling-llm-d-modelservice-decode:3]"}
{"level":"INFO","ts":"2025-11-28T21:16:23.107Z","msg":"Capacity-only decisions made for model: unsloth/Meta-Llama-3.1-8B - decision count: 1"}
{"level":"INFO","ts":"2025-11-28T21:16:23.107Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-11-28T21:16:23.107Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=3→target=3"}
{"level":"DEBUG","ts":"2025-11-28T21:16:23.107Z","msg":"Found VA in map: variant=ms-inference-scheduling-llm-d-modelservice-decode, hasCurrentAlloc=true, accelerator=H100"}
{"level":"INFO","ts":"2025-11-28T21:16:23.107Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 3, desired-replicas: 3, accelerator: H100"}
{"level":"INFO","ts":"2025-11-28T21:16:23.107Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=3, accelerator=H100, capacityOnly=true"}
{"level":"INFO","ts":"2025-11-28T21:16:23.111Z","msg":"Applied capacity decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=3, target=3, reason=capacity-only mode: no-change"}
{"level":"INFO","ts":"2025-11-28T21:16:23.111Z","msg":"Reconciliation completed successfully: mode=capacity-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-11-28T21:16:53.076Z","msg":"Operating in CAPACITY-ONLY mode: reactive capacity-based scaling only"}
{"level":"INFO","ts":"2025-11-28T21:16:53.076Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-11-28T21:16:53.076Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-11-28T21:16:53.097Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=3, accelerator=H100, ttft=33.94ms, itl=18.34ms, cost=30.00, arrivalRate=4018.93"}
{"level":"INFO","ts":"2025-11-28T21:16:53.098Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9lshk5, queueLength=0"}
{"level":"INFO","ts":"2025-11-28T21:16:53.098Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9ww5mn, queueLength=0"}
{"level":"INFO","ts":"2025-11-28T21:16:53.098Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9l76md, queueLength=0"}
{"level":"DEBUG","ts":"2025-11-28T21:16:53.098Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=3"}
{"level":"INFO","ts":"2025-11-28T21:16:53.099Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9lshk5, usage=0.283 (28.3%)"}
{"level":"INFO","ts":"2025-11-28T21:16:53.099Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9ww5mn, usage=0.238 (23.8%)"}
{"level":"INFO","ts":"2025-11-28T21:16:53.099Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9l76md, usage=0.238 (23.8%)"}
{"level":"DEBUG","ts":"2025-11-28T21:16:53.099Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=3"}
{"level":"DEBUG","ts":"2025-11-28T21:16:53.099Z","msg":"Pod-to-variant matching successful: totalPods=3, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:3]"}
{"level":"DEBUG","ts":"2025-11-28T21:16:53.099Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=3"}
{"level":"DEBUG","ts":"2025-11-28T21:16:53.099Z","msg":"Collected capacity metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, metricsCount=3"}
{"level":"DEBUG","ts":"2025-11-28T21:16:53.099Z","msg":"Variant analysis initialized: variant=ms-inference-scheduling-llm-d-modelservice-decode, accelerator=H100, cost=10.00, replicaCount=3"}
{"level":"DEBUG","ts":"2025-11-28T21:16:53.099Z","msg":"Scale-down unsafe: insufficient headroom after redistribution: remainingSpareKv=0.120, kvTrigger=0.200, kvSafe=false, remainingSpareQueue=5.0, queueTrigger=3.0%!(EXTRA bool=true)"}
{"level":"DEBUG","ts":"2025-11-28T21:16:53.099Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, totalReplicas=3, nonSaturated=3, avgSpareKv=0.247, avgSpareQueue=5.0, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"INFO","ts":"2025-11-28T21:16:53.099Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, totalReplicas=3, nonSaturated=3, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"DEBUG","ts":"2025-11-28T21:16:53.099Z","msg":"Capacity targets: no scaling needed (avgSpareKv=0.247, avgSpareQueue=5.0, all variants stable)"}
{"level":"DEBUG","ts":"2025-11-28T21:16:53.099Z","msg":"Capacity targets calculated: modelID=unsloth/Meta-Llama-3.1-8B, targets=map[ms-inference-scheduling-llm-d-modelservice-decode:3]"}
{"level":"INFO","ts":"2025-11-28T21:16:53.099Z","msg":"Capacity-only decisions made for model: unsloth/Meta-Llama-3.1-8B - decision count: 1"}
{"level":"INFO","ts":"2025-11-28T21:16:53.099Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-11-28T21:16:53.099Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=3→target=3"}
{"level":"DEBUG","ts":"2025-11-28T21:16:53.099Z","msg":"Found VA in map: variant=ms-inference-scheduling-llm-d-modelservice-decode, hasCurrentAlloc=true, accelerator=H100"}
{"level":"INFO","ts":"2025-11-28T21:16:53.099Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 3, desired-replicas: 3, accelerator: H100"}
{"level":"INFO","ts":"2025-11-28T21:16:53.099Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=3, accelerator=H100, capacityOnly=true"}
{"level":"INFO","ts":"2025-11-28T21:16:53.105Z","msg":"Applied capacity decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=3, target=3, reason=capacity-only mode: no-change"}
{"level":"INFO","ts":"2025-11-28T21:16:53.105Z","msg":"Reconciliation completed successfully: mode=capacity-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-11-28T21:16:53.112Z","msg":"Operating in CAPACITY-ONLY mode: reactive capacity-based scaling only"}
{"level":"INFO","ts":"2025-11-28T21:16:53.112Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-11-28T21:16:53.112Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-11-28T21:16:53.121Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=3, accelerator=H100, ttft=33.94ms, itl=18.34ms, cost=30.00, arrivalRate=4018.93"}
{"level":"INFO","ts":"2025-11-28T21:16:53.123Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9lshk5, queueLength=0"}
{"level":"INFO","ts":"2025-11-28T21:16:53.123Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9ww5mn, queueLength=0"}
{"level":"INFO","ts":"2025-11-28T21:16:53.123Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9l76md, queueLength=0"}
{"level":"DEBUG","ts":"2025-11-28T21:16:53.123Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=3"}
{"level":"INFO","ts":"2025-11-28T21:16:53.123Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9lshk5, usage=0.283 (28.3%)"}
{"level":"INFO","ts":"2025-11-28T21:16:53.123Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9ww5mn, usage=0.238 (23.8%)"}
{"level":"INFO","ts":"2025-11-28T21:16:53.123Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9l76md, usage=0.238 (23.8%)"}
{"level":"DEBUG","ts":"2025-11-28T21:16:53.123Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=3"}
{"level":"DEBUG","ts":"2025-11-28T21:16:53.123Z","msg":"Pod-to-variant matching successful: totalPods=3, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:3]"}
{"level":"DEBUG","ts":"2025-11-28T21:16:53.123Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=3"}
{"level":"DEBUG","ts":"2025-11-28T21:16:53.123Z","msg":"Collected capacity metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, metricsCount=3"}
{"level":"DEBUG","ts":"2025-11-28T21:16:53.123Z","msg":"Variant analysis initialized: variant=ms-inference-scheduling-llm-d-modelservice-decode, accelerator=H100, cost=10.00, replicaCount=3"}
{"level":"DEBUG","ts":"2025-11-28T21:16:53.123Z","msg":"Scale-down unsafe: insufficient headroom after redistribution: remainingSpareKv=0.120, kvTrigger=0.200, kvSafe=false, remainingSpareQueue=5.0, queueTrigger=3.0%!(EXTRA bool=true)"}
{"level":"DEBUG","ts":"2025-11-28T21:16:53.123Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, totalReplicas=3, nonSaturated=3, avgSpareKv=0.247, avgSpareQueue=5.0, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"INFO","ts":"2025-11-28T21:16:53.123Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, totalReplicas=3, nonSaturated=3, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"DEBUG","ts":"2025-11-28T21:16:53.123Z","msg":"Capacity targets: no scaling needed (avgSpareKv=0.247, avgSpareQueue=5.0, all variants stable)"}
{"level":"DEBUG","ts":"2025-11-28T21:16:53.123Z","msg":"Capacity targets calculated: modelID=unsloth/Meta-Llama-3.1-8B, targets=map[ms-inference-scheduling-llm-d-modelservice-decode:3]"}
{"level":"INFO","ts":"2025-11-28T21:16:53.123Z","msg":"Capacity-only decisions made for model: unsloth/Meta-Llama-3.1-8B - decision count: 1"}
{"level":"INFO","ts":"2025-11-28T21:16:53.123Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-11-28T21:16:53.123Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=3→target=3"}
{"level":"DEBUG","ts":"2025-11-28T21:16:53.123Z","msg":"Found VA in map: variant=ms-inference-scheduling-llm-d-modelservice-decode, hasCurrentAlloc=true, accelerator=H100"}
{"level":"INFO","ts":"2025-11-28T21:16:53.123Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 3, desired-replicas: 3, accelerator: H100"}
{"level":"INFO","ts":"2025-11-28T21:16:53.123Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=3, accelerator=H100, capacityOnly=true"}
{"level":"INFO","ts":"2025-11-28T21:16:53.127Z","msg":"Applied capacity decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=3, target=3, reason=capacity-only mode: no-change"}
{"level":"INFO","ts":"2025-11-28T21:16:53.127Z","msg":"Reconciliation completed successfully: mode=capacity-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-11-28T21:17:23.106Z","msg":"Operating in CAPACITY-ONLY mode: reactive capacity-based scaling only"}
{"level":"INFO","ts":"2025-11-28T21:17:23.106Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-11-28T21:17:23.106Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-11-28T21:17:23.125Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=3, accelerator=H100, ttft=34.92ms, itl=19.10ms, cost=30.00, arrivalRate=3961.07"}
{"level":"INFO","ts":"2025-11-28T21:17:23.127Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9lshk5, usage=0.283 (28.3%)"}
{"level":"INFO","ts":"2025-11-28T21:17:23.127Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9ww5mn, usage=0.290 (29.0%)"}
{"level":"INFO","ts":"2025-11-28T21:17:23.127Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9l76md, usage=0.298 (29.8%)"}
{"level":"DEBUG","ts":"2025-11-28T21:17:23.127Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=3"}
{"level":"INFO","ts":"2025-11-28T21:17:23.127Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9lshk5, queueLength=0"}
{"level":"INFO","ts":"2025-11-28T21:17:23.127Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9ww5mn, queueLength=0"}
{"level":"INFO","ts":"2025-11-28T21:17:23.127Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9l76md, queueLength=0"}
{"level":"DEBUG","ts":"2025-11-28T21:17:23.127Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=3"}
{"level":"DEBUG","ts":"2025-11-28T21:17:23.127Z","msg":"Pod-to-variant matching successful: totalPods=3, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:3]"}
{"level":"DEBUG","ts":"2025-11-28T21:17:23.127Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=3"}
{"level":"DEBUG","ts":"2025-11-28T21:17:23.127Z","msg":"Collected capacity metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, metricsCount=3"}
{"level":"DEBUG","ts":"2025-11-28T21:17:23.127Z","msg":"Variant analysis initialized: variant=ms-inference-scheduling-llm-d-modelservice-decode, accelerator=H100, cost=10.00, replicaCount=3"}
{"level":"DEBUG","ts":"2025-11-28T21:17:23.127Z","msg":"Scale-down unsafe: insufficient headroom after redistribution: remainingSpareKv=0.065, kvTrigger=0.200, kvSafe=false, remainingSpareQueue=5.0, queueTrigger=3.0%!(EXTRA bool=true)"}
{"level":"DEBUG","ts":"2025-11-28T21:17:23.127Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, totalReplicas=3, nonSaturated=3, avgSpareKv=0.210, avgSpareQueue=5.0, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"INFO","ts":"2025-11-28T21:17:23.127Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, totalReplicas=3, nonSaturated=3, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"DEBUG","ts":"2025-11-28T21:17:23.127Z","msg":"Capacity targets: no scaling needed (avgSpareKv=0.210, avgSpareQueue=5.0, all variants stable)"}
{"level":"DEBUG","ts":"2025-11-28T21:17:23.127Z","msg":"Capacity targets calculated: modelID=unsloth/Meta-Llama-3.1-8B, targets=map[ms-inference-scheduling-llm-d-modelservice-decode:3]"}
{"level":"INFO","ts":"2025-11-28T21:17:23.127Z","msg":"Capacity-only decisions made for model: unsloth/Meta-Llama-3.1-8B - decision count: 1"}
{"level":"INFO","ts":"2025-11-28T21:17:23.127Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-11-28T21:17:23.127Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=3→target=3"}
{"level":"DEBUG","ts":"2025-11-28T21:17:23.127Z","msg":"Found VA in map: variant=ms-inference-scheduling-llm-d-modelservice-decode, hasCurrentAlloc=true, accelerator=H100"}
{"level":"INFO","ts":"2025-11-28T21:17:23.127Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 3, desired-replicas: 3, accelerator: H100"}
{"level":"INFO","ts":"2025-11-28T21:17:23.127Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=3, accelerator=H100, capacityOnly=true"}
{"level":"INFO","ts":"2025-11-28T21:17:23.133Z","msg":"Applied capacity decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=3, target=3, reason=capacity-only mode: no-change"}
{"level":"INFO","ts":"2025-11-28T21:17:23.133Z","msg":"Reconciliation completed successfully: mode=capacity-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-11-28T21:17:23.133Z","msg":"Operating in CAPACITY-ONLY mode: reactive capacity-based scaling only"}
{"level":"INFO","ts":"2025-11-28T21:17:23.133Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-11-28T21:17:23.133Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-11-28T21:17:23.144Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=3, accelerator=H100, ttft=34.92ms, itl=19.10ms, cost=30.00, arrivalRate=3961.07"}
{"level":"INFO","ts":"2025-11-28T21:17:23.148Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9lshk5, queueLength=0"}
{"level":"INFO","ts":"2025-11-28T21:17:23.148Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9ww5mn, queueLength=0"}
{"level":"INFO","ts":"2025-11-28T21:17:23.148Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9lshk5, usage=0.283 (28.3%)"}
{"level":"INFO","ts":"2025-11-28T21:17:23.148Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9l76md, queueLength=0"}
{"level":"DEBUG","ts":"2025-11-28T21:17:23.148Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=3"}
{"level":"INFO","ts":"2025-11-28T21:17:23.148Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9ww5mn, usage=0.290 (29.0%)"}
{"level":"INFO","ts":"2025-11-28T21:17:23.148Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9l76md, usage=0.298 (29.8%)"}
{"level":"DEBUG","ts":"2025-11-28T21:17:23.148Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=3"}
{"level":"DEBUG","ts":"2025-11-28T21:17:23.149Z","msg":"Pod-to-variant matching successful: totalPods=3, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:3]"}
{"level":"DEBUG","ts":"2025-11-28T21:17:23.149Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=3"}
{"level":"DEBUG","ts":"2025-11-28T21:17:23.149Z","msg":"Collected capacity metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, metricsCount=3"}
{"level":"DEBUG","ts":"2025-11-28T21:17:23.149Z","msg":"Variant analysis initialized: variant=ms-inference-scheduling-llm-d-modelservice-decode, accelerator=H100, cost=10.00, replicaCount=3"}
{"level":"DEBUG","ts":"2025-11-28T21:17:23.149Z","msg":"Scale-down unsafe: insufficient headroom after redistribution: remainingSpareKv=0.065, kvTrigger=0.200, kvSafe=false, remainingSpareQueue=5.0, queueTrigger=3.0%!(EXTRA bool=true)"}
{"level":"DEBUG","ts":"2025-11-28T21:17:23.149Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, totalReplicas=3, nonSaturated=3, avgSpareKv=0.210, avgSpareQueue=5.0, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"INFO","ts":"2025-11-28T21:17:23.149Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, totalReplicas=3, nonSaturated=3, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"DEBUG","ts":"2025-11-28T21:17:23.149Z","msg":"Capacity targets: no scaling needed (avgSpareKv=0.210, avgSpareQueue=5.0, all variants stable)"}
{"level":"DEBUG","ts":"2025-11-28T21:17:23.149Z","msg":"Capacity targets calculated: modelID=unsloth/Meta-Llama-3.1-8B, targets=map[ms-inference-scheduling-llm-d-modelservice-decode:3]"}
{"level":"INFO","ts":"2025-11-28T21:17:23.149Z","msg":"Capacity-only decisions made for model: unsloth/Meta-Llama-3.1-8B - decision count: 1"}
{"level":"INFO","ts":"2025-11-28T21:17:23.149Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-11-28T21:17:23.149Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=3→target=3"}
{"level":"DEBUG","ts":"2025-11-28T21:17:23.149Z","msg":"Found VA in map: variant=ms-inference-scheduling-llm-d-modelservice-decode, hasCurrentAlloc=true, accelerator=H100"}
{"level":"INFO","ts":"2025-11-28T21:17:23.150Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 3, desired-replicas: 3, accelerator: H100"}
{"level":"INFO","ts":"2025-11-28T21:17:23.150Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=3, accelerator=H100, capacityOnly=true"}
{"level":"INFO","ts":"2025-11-28T21:17:23.158Z","msg":"Applied capacity decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=3, target=3, reason=capacity-only mode: no-change"}
{"level":"INFO","ts":"2025-11-28T21:17:23.158Z","msg":"Reconciliation completed successfully: mode=capacity-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-11-28T21:17:53.134Z","msg":"Operating in CAPACITY-ONLY mode: reactive capacity-based scaling only"}
{"level":"INFO","ts":"2025-11-28T21:17:53.134Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-11-28T21:17:53.134Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-11-28T21:17:53.149Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=3, accelerator=H100, ttft=35.64ms, itl=19.24ms, cost=30.00, arrivalRate=3959.07"}
{"level":"INFO","ts":"2025-11-28T21:17:53.151Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9lshk5, queueLength=0"}
{"level":"INFO","ts":"2025-11-28T21:17:53.151Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9ww5mn, queueLength=0"}
{"level":"INFO","ts":"2025-11-28T21:17:53.151Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9l76md, queueLength=0"}
{"level":"DEBUG","ts":"2025-11-28T21:17:53.151Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=3"}
{"level":"INFO","ts":"2025-11-28T21:17:53.151Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9lshk5, usage=0.242 (24.2%)"}
{"level":"INFO","ts":"2025-11-28T21:17:53.151Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9ww5mn, usage=0.290 (29.0%)"}
{"level":"INFO","ts":"2025-11-28T21:17:53.151Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9l76md, usage=0.298 (29.8%)"}
{"level":"DEBUG","ts":"2025-11-28T21:17:53.151Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=3"}
{"level":"DEBUG","ts":"2025-11-28T21:17:53.151Z","msg":"Pod-to-variant matching successful: totalPods=3, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:3]"}
{"level":"DEBUG","ts":"2025-11-28T21:17:53.151Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=3"}
{"level":"DEBUG","ts":"2025-11-28T21:17:53.151Z","msg":"Collected capacity metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, metricsCount=3"}
{"level":"DEBUG","ts":"2025-11-28T21:17:53.151Z","msg":"Variant analysis initialized: variant=ms-inference-scheduling-llm-d-modelservice-decode, accelerator=H100, cost=10.00, replicaCount=3"}
{"level":"DEBUG","ts":"2025-11-28T21:17:53.151Z","msg":"Scale-down unsafe: insufficient headroom after redistribution: remainingSpareKv=0.085, kvTrigger=0.200, kvSafe=false, remainingSpareQueue=5.0, queueTrigger=3.0%!(EXTRA bool=true)"}
{"level":"DEBUG","ts":"2025-11-28T21:17:53.151Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, totalReplicas=3, nonSaturated=3, avgSpareKv=0.223, avgSpareQueue=5.0, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"INFO","ts":"2025-11-28T21:17:53.151Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, totalReplicas=3, nonSaturated=3, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"DEBUG","ts":"2025-11-28T21:17:53.151Z","msg":"Capacity targets: no scaling needed (avgSpareKv=0.223, avgSpareQueue=5.0, all variants stable)"}
{"level":"DEBUG","ts":"2025-11-28T21:17:53.151Z","msg":"Capacity targets calculated: modelID=unsloth/Meta-Llama-3.1-8B, targets=map[ms-inference-scheduling-llm-d-modelservice-decode:3]"}
{"level":"INFO","ts":"2025-11-28T21:17:53.151Z","msg":"Capacity-only decisions made for model: unsloth/Meta-Llama-3.1-8B - decision count: 1"}
{"level":"INFO","ts":"2025-11-28T21:17:53.151Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-11-28T21:17:53.151Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=3→target=3"}
{"level":"DEBUG","ts":"2025-11-28T21:17:53.151Z","msg":"Found VA in map: variant=ms-inference-scheduling-llm-d-modelservice-decode, hasCurrentAlloc=true, accelerator=H100"}
{"level":"INFO","ts":"2025-11-28T21:17:53.151Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 3, desired-replicas: 3, accelerator: H100"}
{"level":"INFO","ts":"2025-11-28T21:17:53.151Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=3, accelerator=H100, capacityOnly=true"}
{"level":"INFO","ts":"2025-11-28T21:17:53.159Z","msg":"Applied capacity decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=3, target=3, reason=capacity-only mode: no-change"}
{"level":"INFO","ts":"2025-11-28T21:17:53.159Z","msg":"Reconciliation completed successfully: mode=capacity-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-11-28T21:17:53.159Z","msg":"Operating in CAPACITY-ONLY mode: reactive capacity-based scaling only"}
{"level":"INFO","ts":"2025-11-28T21:17:53.159Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-11-28T21:17:53.159Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-11-28T21:17:53.168Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=3, accelerator=H100, ttft=35.64ms, itl=19.24ms, cost=30.00, arrivalRate=3959.07"}
{"level":"INFO","ts":"2025-11-28T21:17:53.170Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9lshk5, queueLength=0"}
{"level":"INFO","ts":"2025-11-28T21:17:53.170Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9ww5mn, queueLength=0"}
{"level":"INFO","ts":"2025-11-28T21:17:53.170Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9l76md, queueLength=0"}
{"level":"DEBUG","ts":"2025-11-28T21:17:53.170Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=3"}
{"level":"INFO","ts":"2025-11-28T21:17:53.170Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9lshk5, usage=0.242 (24.2%)"}
{"level":"INFO","ts":"2025-11-28T21:17:53.170Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9ww5mn, usage=0.290 (29.0%)"}
{"level":"INFO","ts":"2025-11-28T21:17:53.170Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9l76md, usage=0.298 (29.8%)"}
{"level":"DEBUG","ts":"2025-11-28T21:17:53.170Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=3"}
{"level":"DEBUG","ts":"2025-11-28T21:17:53.170Z","msg":"Pod-to-variant matching successful: totalPods=3, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:3]"}
{"level":"DEBUG","ts":"2025-11-28T21:17:53.170Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=3"}
{"level":"DEBUG","ts":"2025-11-28T21:17:53.170Z","msg":"Collected capacity metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, metricsCount=3"}
{"level":"DEBUG","ts":"2025-11-28T21:17:53.170Z","msg":"Variant analysis initialized: variant=ms-inference-scheduling-llm-d-modelservice-decode, accelerator=H100, cost=10.00, replicaCount=3"}
{"level":"DEBUG","ts":"2025-11-28T21:17:53.170Z","msg":"Scale-down unsafe: insufficient headroom after redistribution: remainingSpareKv=0.085, kvTrigger=0.200, kvSafe=false, remainingSpareQueue=5.0, queueTrigger=3.0%!(EXTRA bool=true)"}
{"level":"DEBUG","ts":"2025-11-28T21:17:53.170Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, totalReplicas=3, nonSaturated=3, avgSpareKv=0.223, avgSpareQueue=5.0, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"INFO","ts":"2025-11-28T21:17:53.170Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, totalReplicas=3, nonSaturated=3, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"DEBUG","ts":"2025-11-28T21:17:53.170Z","msg":"Capacity targets: no scaling needed (avgSpareKv=0.223, avgSpareQueue=5.0, all variants stable)"}
{"level":"DEBUG","ts":"2025-11-28T21:17:53.170Z","msg":"Capacity targets calculated: modelID=unsloth/Meta-Llama-3.1-8B, targets=map[ms-inference-scheduling-llm-d-modelservice-decode:3]"}
{"level":"INFO","ts":"2025-11-28T21:17:53.170Z","msg":"Capacity-only decisions made for model: unsloth/Meta-Llama-3.1-8B - decision count: 1"}
{"level":"INFO","ts":"2025-11-28T21:17:53.170Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-11-28T21:17:53.170Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=3→target=3"}
{"level":"DEBUG","ts":"2025-11-28T21:17:53.170Z","msg":"Found VA in map: variant=ms-inference-scheduling-llm-d-modelservice-decode, hasCurrentAlloc=true, accelerator=H100"}
{"level":"INFO","ts":"2025-11-28T21:17:53.170Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 3, desired-replicas: 3, accelerator: H100"}
{"level":"INFO","ts":"2025-11-28T21:17:53.170Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=3, accelerator=H100, capacityOnly=true"}
{"level":"INFO","ts":"2025-11-28T21:17:53.174Z","msg":"Applied capacity decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=3, target=3, reason=capacity-only mode: no-change"}
{"level":"INFO","ts":"2025-11-28T21:17:53.174Z","msg":"Reconciliation completed successfully: mode=capacity-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-11-28T21:18:23.159Z","msg":"Operating in CAPACITY-ONLY mode: reactive capacity-based scaling only"}
{"level":"INFO","ts":"2025-11-28T21:18:23.159Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-11-28T21:18:23.159Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-11-28T21:18:23.173Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=3, accelerator=H100, ttft=36.26ms, itl=19.80ms, cost=30.00, arrivalRate=3857.99"}
{"level":"INFO","ts":"2025-11-28T21:18:23.175Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9lshk5, usage=0.303 (30.3%)"}
{"level":"INFO","ts":"2025-11-28T21:18:23.175Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9ww5mn, usage=0.294 (29.4%)"}
{"level":"INFO","ts":"2025-11-28T21:18:23.175Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9l76md, usage=0.290 (29.0%)"}
{"level":"DEBUG","ts":"2025-11-28T21:18:23.175Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=3"}
{"level":"INFO","ts":"2025-11-28T21:18:23.175Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9lshk5, queueLength=0"}
{"level":"INFO","ts":"2025-11-28T21:18:23.175Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9ww5mn, queueLength=0"}
{"level":"INFO","ts":"2025-11-28T21:18:23.175Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9l76md, queueLength=0"}
{"level":"DEBUG","ts":"2025-11-28T21:18:23.175Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=3"}
{"level":"DEBUG","ts":"2025-11-28T21:18:23.175Z","msg":"Pod-to-variant matching successful: totalPods=3, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:3]"}
{"level":"DEBUG","ts":"2025-11-28T21:18:23.175Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=3"}
{"level":"DEBUG","ts":"2025-11-28T21:18:23.175Z","msg":"Collected capacity metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, metricsCount=3"}
{"level":"DEBUG","ts":"2025-11-28T21:18:23.175Z","msg":"Variant analysis initialized: variant=ms-inference-scheduling-llm-d-modelservice-decode, accelerator=H100, cost=10.00, replicaCount=3"}
{"level":"DEBUG","ts":"2025-11-28T21:18:23.175Z","msg":"Scale-down unsafe: insufficient headroom after redistribution: remainingSpareKv=0.057, kvTrigger=0.200, kvSafe=false, remainingSpareQueue=5.0, queueTrigger=3.0%!(EXTRA bool=true)"}
{"level":"DEBUG","ts":"2025-11-28T21:18:23.175Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, totalReplicas=3, nonSaturated=3, avgSpareKv=0.204, avgSpareQueue=5.0, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"INFO","ts":"2025-11-28T21:18:23.175Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, totalReplicas=3, nonSaturated=3, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"DEBUG","ts":"2025-11-28T21:18:23.175Z","msg":"Capacity targets: no scaling needed (avgSpareKv=0.204, avgSpareQueue=5.0, all variants stable)"}
{"level":"DEBUG","ts":"2025-11-28T21:18:23.175Z","msg":"Capacity targets calculated: modelID=unsloth/Meta-Llama-3.1-8B, targets=map[ms-inference-scheduling-llm-d-modelservice-decode:3]"}
{"level":"INFO","ts":"2025-11-28T21:18:23.175Z","msg":"Capacity-only decisions made for model: unsloth/Meta-Llama-3.1-8B - decision count: 1"}
{"level":"INFO","ts":"2025-11-28T21:18:23.175Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-11-28T21:18:23.175Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=3→target=3"}
{"level":"DEBUG","ts":"2025-11-28T21:18:23.175Z","msg":"Found VA in map: variant=ms-inference-scheduling-llm-d-modelservice-decode, hasCurrentAlloc=true, accelerator=H100"}
{"level":"INFO","ts":"2025-11-28T21:18:23.175Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 3, desired-replicas: 3, accelerator: H100"}
{"level":"INFO","ts":"2025-11-28T21:18:23.175Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=3, accelerator=H100, capacityOnly=true"}
{"level":"INFO","ts":"2025-11-28T21:18:23.182Z","msg":"Applied capacity decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=3, target=3, reason=capacity-only mode: no-change"}
{"level":"INFO","ts":"2025-11-28T21:18:23.182Z","msg":"Reconciliation completed successfully: mode=capacity-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-11-28T21:18:23.182Z","msg":"Operating in CAPACITY-ONLY mode: reactive capacity-based scaling only"}
{"level":"INFO","ts":"2025-11-28T21:18:23.182Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-11-28T21:18:23.182Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-11-28T21:18:23.192Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=3, accelerator=H100, ttft=36.26ms, itl=19.80ms, cost=30.00, arrivalRate=3857.99"}
{"level":"INFO","ts":"2025-11-28T21:18:23.193Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9lshk5, queueLength=0"}
{"level":"INFO","ts":"2025-11-28T21:18:23.193Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9ww5mn, queueLength=0"}
{"level":"INFO","ts":"2025-11-28T21:18:23.193Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9l76md, queueLength=0"}
{"level":"DEBUG","ts":"2025-11-28T21:18:23.193Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=3"}
{"level":"INFO","ts":"2025-11-28T21:18:23.193Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9lshk5, usage=0.303 (30.3%)"}
{"level":"INFO","ts":"2025-11-28T21:18:23.193Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9ww5mn, usage=0.294 (29.4%)"}
{"level":"INFO","ts":"2025-11-28T21:18:23.193Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9l76md, usage=0.290 (29.0%)"}
{"level":"DEBUG","ts":"2025-11-28T21:18:23.193Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=3"}
{"level":"DEBUG","ts":"2025-11-28T21:18:23.193Z","msg":"Pod-to-variant matching successful: totalPods=3, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:3]"}
{"level":"DEBUG","ts":"2025-11-28T21:18:23.193Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=3"}
{"level":"DEBUG","ts":"2025-11-28T21:18:23.193Z","msg":"Collected capacity metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, metricsCount=3"}
{"level":"DEBUG","ts":"2025-11-28T21:18:23.193Z","msg":"Variant analysis initialized: variant=ms-inference-scheduling-llm-d-modelservice-decode, accelerator=H100, cost=10.00, replicaCount=3"}
{"level":"DEBUG","ts":"2025-11-28T21:18:23.193Z","msg":"Scale-down unsafe: insufficient headroom after redistribution: remainingSpareKv=0.057, kvTrigger=0.200, kvSafe=false, remainingSpareQueue=5.0, queueTrigger=3.0%!(EXTRA bool=true)"}
{"level":"DEBUG","ts":"2025-11-28T21:18:23.193Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, totalReplicas=3, nonSaturated=3, avgSpareKv=0.204, avgSpareQueue=5.0, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"INFO","ts":"2025-11-28T21:18:23.193Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, totalReplicas=3, nonSaturated=3, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"DEBUG","ts":"2025-11-28T21:18:23.193Z","msg":"Capacity targets: no scaling needed (avgSpareKv=0.204, avgSpareQueue=5.0, all variants stable)"}
{"level":"DEBUG","ts":"2025-11-28T21:18:23.193Z","msg":"Capacity targets calculated: modelID=unsloth/Meta-Llama-3.1-8B, targets=map[ms-inference-scheduling-llm-d-modelservice-decode:3]"}
{"level":"INFO","ts":"2025-11-28T21:18:23.193Z","msg":"Capacity-only decisions made for model: unsloth/Meta-Llama-3.1-8B - decision count: 1"}
{"level":"INFO","ts":"2025-11-28T21:18:23.193Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-11-28T21:18:23.193Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=3→target=3"}
{"level":"DEBUG","ts":"2025-11-28T21:18:23.193Z","msg":"Found VA in map: variant=ms-inference-scheduling-llm-d-modelservice-decode, hasCurrentAlloc=true, accelerator=H100"}
{"level":"INFO","ts":"2025-11-28T21:18:23.193Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 3, desired-replicas: 3, accelerator: H100"}
{"level":"INFO","ts":"2025-11-28T21:18:23.193Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=3, accelerator=H100, capacityOnly=true"}
{"level":"INFO","ts":"2025-11-28T21:18:23.200Z","msg":"Applied capacity decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=3, target=3, reason=capacity-only mode: no-change"}
{"level":"INFO","ts":"2025-11-28T21:18:23.200Z","msg":"Reconciliation completed successfully: mode=capacity-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-11-28T21:18:53.182Z","msg":"Operating in CAPACITY-ONLY mode: reactive capacity-based scaling only"}
{"level":"INFO","ts":"2025-11-28T21:18:53.182Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-11-28T21:18:53.182Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-11-28T21:18:53.198Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=3, accelerator=H100, ttft=36.06ms, itl=19.36ms, cost=30.00, arrivalRate=4063.63"}
{"level":"INFO","ts":"2025-11-28T21:18:53.200Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9lshk5, queueLength=0"}
{"level":"INFO","ts":"2025-11-28T21:18:53.200Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9ww5mn, queueLength=0"}
{"level":"INFO","ts":"2025-11-28T21:18:53.200Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9l76md, queueLength=0"}
{"level":"DEBUG","ts":"2025-11-28T21:18:53.200Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=3"}
{"level":"INFO","ts":"2025-11-28T21:18:53.200Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9lshk5, usage=0.303 (30.3%)"}
{"level":"INFO","ts":"2025-11-28T21:18:53.200Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9ww5mn, usage=0.287 (28.7%)"}
{"level":"INFO","ts":"2025-11-28T21:18:53.200Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9l76md, usage=0.290 (29.0%)"}
{"level":"DEBUG","ts":"2025-11-28T21:18:53.200Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=3"}
{"level":"DEBUG","ts":"2025-11-28T21:18:53.200Z","msg":"Pod-to-variant matching successful: totalPods=3, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:3]"}
{"level":"DEBUG","ts":"2025-11-28T21:18:53.200Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=3"}
{"level":"DEBUG","ts":"2025-11-28T21:18:53.200Z","msg":"Collected capacity metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, metricsCount=3"}
{"level":"DEBUG","ts":"2025-11-28T21:18:53.200Z","msg":"Variant analysis initialized: variant=ms-inference-scheduling-llm-d-modelservice-decode, accelerator=H100, cost=10.00, replicaCount=3"}
{"level":"DEBUG","ts":"2025-11-28T21:18:53.200Z","msg":"Scale-down unsafe: insufficient headroom after redistribution: remainingSpareKv=0.060, kvTrigger=0.200, kvSafe=false, remainingSpareQueue=5.0, queueTrigger=3.0%!(EXTRA bool=true)"}
{"level":"DEBUG","ts":"2025-11-28T21:18:53.200Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, totalReplicas=3, nonSaturated=3, avgSpareKv=0.207, avgSpareQueue=5.0, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"INFO","ts":"2025-11-28T21:18:53.200Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, totalReplicas=3, nonSaturated=3, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"DEBUG","ts":"2025-11-28T21:18:53.200Z","msg":"Capacity targets: no scaling needed (avgSpareKv=0.207, avgSpareQueue=5.0, all variants stable)"}
{"level":"DEBUG","ts":"2025-11-28T21:18:53.200Z","msg":"Capacity targets calculated: modelID=unsloth/Meta-Llama-3.1-8B, targets=map[ms-inference-scheduling-llm-d-modelservice-decode:3]"}
{"level":"INFO","ts":"2025-11-28T21:18:53.200Z","msg":"Capacity-only decisions made for model: unsloth/Meta-Llama-3.1-8B - decision count: 1"}
{"level":"INFO","ts":"2025-11-28T21:18:53.200Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-11-28T21:18:53.200Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=3→target=3"}
{"level":"DEBUG","ts":"2025-11-28T21:18:53.200Z","msg":"Found VA in map: variant=ms-inference-scheduling-llm-d-modelservice-decode, hasCurrentAlloc=true, accelerator=H100"}
{"level":"INFO","ts":"2025-11-28T21:18:53.200Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 3, desired-replicas: 3, accelerator: H100"}
{"level":"INFO","ts":"2025-11-28T21:18:53.200Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=3, accelerator=H100, capacityOnly=true"}
{"level":"INFO","ts":"2025-11-28T21:18:53.205Z","msg":"Applied capacity decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=3, target=3, reason=capacity-only mode: no-change"}
{"level":"INFO","ts":"2025-11-28T21:18:53.205Z","msg":"Reconciliation completed successfully: mode=capacity-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-11-28T21:18:53.205Z","msg":"Operating in CAPACITY-ONLY mode: reactive capacity-based scaling only"}
{"level":"INFO","ts":"2025-11-28T21:18:53.206Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-11-28T21:18:53.206Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-11-28T21:18:53.215Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=3, accelerator=H100, ttft=36.06ms, itl=19.36ms, cost=30.00, arrivalRate=4063.63"}
{"level":"INFO","ts":"2025-11-28T21:18:53.216Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9lshk5, usage=0.303 (30.3%)"}
{"level":"INFO","ts":"2025-11-28T21:18:53.216Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9ww5mn, usage=0.287 (28.7%)"}
{"level":"INFO","ts":"2025-11-28T21:18:53.216Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9l76md, usage=0.290 (29.0%)"}
{"level":"DEBUG","ts":"2025-11-28T21:18:53.216Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=3"}
{"level":"INFO","ts":"2025-11-28T21:18:53.216Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9lshk5, queueLength=0"}
{"level":"INFO","ts":"2025-11-28T21:18:53.216Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9ww5mn, queueLength=0"}
{"level":"INFO","ts":"2025-11-28T21:18:53.216Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9l76md, queueLength=0"}
{"level":"DEBUG","ts":"2025-11-28T21:18:53.216Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=3"}
{"level":"DEBUG","ts":"2025-11-28T21:18:53.216Z","msg":"Pod-to-variant matching successful: totalPods=3, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:3]"}
{"level":"DEBUG","ts":"2025-11-28T21:18:53.216Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=3"}
{"level":"DEBUG","ts":"2025-11-28T21:18:53.216Z","msg":"Collected capacity metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, metricsCount=3"}
{"level":"DEBUG","ts":"2025-11-28T21:18:53.216Z","msg":"Variant analysis initialized: variant=ms-inference-scheduling-llm-d-modelservice-decode, accelerator=H100, cost=10.00, replicaCount=3"}
{"level":"DEBUG","ts":"2025-11-28T21:18:53.216Z","msg":"Scale-down unsafe: insufficient headroom after redistribution: remainingSpareKv=0.060, kvTrigger=0.200, kvSafe=false, remainingSpareQueue=5.0, queueTrigger=3.0%!(EXTRA bool=true)"}
{"level":"DEBUG","ts":"2025-11-28T21:18:53.216Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, totalReplicas=3, nonSaturated=3, avgSpareKv=0.207, avgSpareQueue=5.0, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"INFO","ts":"2025-11-28T21:18:53.216Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, totalReplicas=3, nonSaturated=3, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"DEBUG","ts":"2025-11-28T21:18:53.216Z","msg":"Capacity targets: no scaling needed (avgSpareKv=0.207, avgSpareQueue=5.0, all variants stable)"}
{"level":"DEBUG","ts":"2025-11-28T21:18:53.216Z","msg":"Capacity targets calculated: modelID=unsloth/Meta-Llama-3.1-8B, targets=map[ms-inference-scheduling-llm-d-modelservice-decode:3]"}
{"level":"INFO","ts":"2025-11-28T21:18:53.216Z","msg":"Capacity-only decisions made for model: unsloth/Meta-Llama-3.1-8B - decision count: 1"}
{"level":"INFO","ts":"2025-11-28T21:18:53.216Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-11-28T21:18:53.216Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=3→target=3"}
{"level":"DEBUG","ts":"2025-11-28T21:18:53.216Z","msg":"Found VA in map: variant=ms-inference-scheduling-llm-d-modelservice-decode, hasCurrentAlloc=true, accelerator=H100"}
{"level":"INFO","ts":"2025-11-28T21:18:53.216Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 3, desired-replicas: 3, accelerator: H100"}
{"level":"INFO","ts":"2025-11-28T21:18:53.216Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=3, accelerator=H100, capacityOnly=true"}
{"level":"INFO","ts":"2025-11-28T21:18:53.224Z","msg":"Applied capacity decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=3, target=3, reason=capacity-only mode: no-change"}
{"level":"INFO","ts":"2025-11-28T21:18:53.224Z","msg":"Reconciliation completed successfully: mode=capacity-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-11-28T21:19:23.206Z","msg":"Operating in CAPACITY-ONLY mode: reactive capacity-based scaling only"}
{"level":"INFO","ts":"2025-11-28T21:19:23.206Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-11-28T21:19:23.206Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-11-28T21:19:23.231Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=3, accelerator=H100, ttft=33.24ms, itl=17.80ms, cost=30.00, arrivalRate=3953.56"}
{"level":"INFO","ts":"2025-11-28T21:19:23.233Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9lshk5, queueLength=0"}
{"level":"INFO","ts":"2025-11-28T21:19:23.233Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9ww5mn, queueLength=0"}
{"level":"INFO","ts":"2025-11-28T21:19:23.233Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9l76md, queueLength=0"}
{"level":"DEBUG","ts":"2025-11-28T21:19:23.233Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=3"}
{"level":"INFO","ts":"2025-11-28T21:19:23.233Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9lshk5, usage=0.230 (23.0%)"}
{"level":"INFO","ts":"2025-11-28T21:19:23.233Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9ww5mn, usage=0.297 (29.7%)"}
{"level":"INFO","ts":"2025-11-28T21:19:23.233Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9l76md, usage=0.247 (24.7%)"}
{"level":"DEBUG","ts":"2025-11-28T21:19:23.233Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=3"}
{"level":"DEBUG","ts":"2025-11-28T21:19:23.233Z","msg":"Pod-to-variant matching successful: totalPods=3, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:3]"}
{"level":"DEBUG","ts":"2025-11-28T21:19:23.233Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=3"}
{"level":"DEBUG","ts":"2025-11-28T21:19:23.233Z","msg":"Collected capacity metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, metricsCount=3"}
{"level":"DEBUG","ts":"2025-11-28T21:19:23.233Z","msg":"Variant analysis initialized: variant=ms-inference-scheduling-llm-d-modelservice-decode, accelerator=H100, cost=10.00, replicaCount=3"}
{"level":"DEBUG","ts":"2025-11-28T21:19:23.233Z","msg":"Scale-down unsafe: insufficient headroom after redistribution: remainingSpareKv=0.113, kvTrigger=0.200, kvSafe=false, remainingSpareQueue=5.0, queueTrigger=3.0%!(EXTRA bool=true)"}
{"level":"DEBUG","ts":"2025-11-28T21:19:23.233Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, totalReplicas=3, nonSaturated=3, avgSpareKv=0.242, avgSpareQueue=5.0, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"INFO","ts":"2025-11-28T21:19:23.233Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, totalReplicas=3, nonSaturated=3, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"DEBUG","ts":"2025-11-28T21:19:23.233Z","msg":"Capacity targets: no scaling needed (avgSpareKv=0.242, avgSpareQueue=5.0, all variants stable)"}
{"level":"DEBUG","ts":"2025-11-28T21:19:23.233Z","msg":"Capacity targets calculated: modelID=unsloth/Meta-Llama-3.1-8B, targets=map[ms-inference-scheduling-llm-d-modelservice-decode:3]"}
{"level":"INFO","ts":"2025-11-28T21:19:23.233Z","msg":"Capacity-only decisions made for model: unsloth/Meta-Llama-3.1-8B - decision count: 1"}
{"level":"INFO","ts":"2025-11-28T21:19:23.233Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-11-28T21:19:23.233Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=3→target=3"}
{"level":"DEBUG","ts":"2025-11-28T21:19:23.233Z","msg":"Found VA in map: variant=ms-inference-scheduling-llm-d-modelservice-decode, hasCurrentAlloc=true, accelerator=H100"}
{"level":"INFO","ts":"2025-11-28T21:19:23.233Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 3, desired-replicas: 3, accelerator: H100"}
{"level":"INFO","ts":"2025-11-28T21:19:23.233Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=3, accelerator=H100, capacityOnly=true"}
{"level":"INFO","ts":"2025-11-28T21:19:23.239Z","msg":"Applied capacity decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=3, target=3, reason=capacity-only mode: no-change"}
{"level":"INFO","ts":"2025-11-28T21:19:23.239Z","msg":"Reconciliation completed successfully: mode=capacity-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-11-28T21:19:23.239Z","msg":"Operating in CAPACITY-ONLY mode: reactive capacity-based scaling only"}
{"level":"INFO","ts":"2025-11-28T21:19:23.239Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-11-28T21:19:23.239Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-11-28T21:19:23.250Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=3, accelerator=H100, ttft=33.24ms, itl=17.80ms, cost=30.00, arrivalRate=3953.56"}
{"level":"INFO","ts":"2025-11-28T21:19:23.251Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9lshk5, queueLength=0"}
{"level":"INFO","ts":"2025-11-28T21:19:23.251Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9ww5mn, queueLength=0"}
{"level":"INFO","ts":"2025-11-28T21:19:23.251Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9l76md, queueLength=0"}
{"level":"DEBUG","ts":"2025-11-28T21:19:23.251Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=3"}
{"level":"INFO","ts":"2025-11-28T21:19:23.251Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9lshk5, usage=0.230 (23.0%)"}
{"level":"INFO","ts":"2025-11-28T21:19:23.251Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9ww5mn, usage=0.297 (29.7%)"}
{"level":"INFO","ts":"2025-11-28T21:19:23.251Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9l76md, usage=0.247 (24.7%)"}
{"level":"DEBUG","ts":"2025-11-28T21:19:23.251Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=3"}
{"level":"DEBUG","ts":"2025-11-28T21:19:23.251Z","msg":"Pod-to-variant matching successful: totalPods=3, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:3]"}
{"level":"DEBUG","ts":"2025-11-28T21:19:23.251Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=3"}
{"level":"DEBUG","ts":"2025-11-28T21:19:23.251Z","msg":"Collected capacity metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, metricsCount=3"}
{"level":"DEBUG","ts":"2025-11-28T21:19:23.251Z","msg":"Variant analysis initialized: variant=ms-inference-scheduling-llm-d-modelservice-decode, accelerator=H100, cost=10.00, replicaCount=3"}
{"level":"DEBUG","ts":"2025-11-28T21:19:23.251Z","msg":"Scale-down unsafe: insufficient headroom after redistribution: remainingSpareKv=0.113, kvTrigger=0.200, kvSafe=false, remainingSpareQueue=5.0, queueTrigger=3.0%!(EXTRA bool=true)"}
{"level":"DEBUG","ts":"2025-11-28T21:19:23.251Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, totalReplicas=3, nonSaturated=3, avgSpareKv=0.242, avgSpareQueue=5.0, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"INFO","ts":"2025-11-28T21:19:23.251Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, totalReplicas=3, nonSaturated=3, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"DEBUG","ts":"2025-11-28T21:19:23.251Z","msg":"Capacity targets: no scaling needed (avgSpareKv=0.242, avgSpareQueue=5.0, all variants stable)"}
{"level":"DEBUG","ts":"2025-11-28T21:19:23.251Z","msg":"Capacity targets calculated: modelID=unsloth/Meta-Llama-3.1-8B, targets=map[ms-inference-scheduling-llm-d-modelservice-decode:3]"}
{"level":"INFO","ts":"2025-11-28T21:19:23.251Z","msg":"Capacity-only decisions made for model: unsloth/Meta-Llama-3.1-8B - decision count: 1"}
{"level":"INFO","ts":"2025-11-28T21:19:23.251Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-11-28T21:19:23.251Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=3→target=3"}
{"level":"DEBUG","ts":"2025-11-28T21:19:23.251Z","msg":"Found VA in map: variant=ms-inference-scheduling-llm-d-modelservice-decode, hasCurrentAlloc=true, accelerator=H100"}
{"level":"INFO","ts":"2025-11-28T21:19:23.251Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 3, desired-replicas: 3, accelerator: H100"}
{"level":"INFO","ts":"2025-11-28T21:19:23.251Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=3, accelerator=H100, capacityOnly=true"}
{"level":"INFO","ts":"2025-11-28T21:19:23.256Z","msg":"Applied capacity decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=3, target=3, reason=capacity-only mode: no-change"}
{"level":"INFO","ts":"2025-11-28T21:19:23.256Z","msg":"Reconciliation completed successfully: mode=capacity-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-11-28T21:19:53.240Z","msg":"Operating in CAPACITY-ONLY mode: reactive capacity-based scaling only"}
{"level":"INFO","ts":"2025-11-28T21:19:53.240Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-11-28T21:19:53.240Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-11-28T21:19:53.252Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=3, accelerator=H100, ttft=32.95ms, itl=17.82ms, cost=30.00, arrivalRate=3743.13"}
{"level":"INFO","ts":"2025-11-28T21:19:53.253Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9lshk5, queueLength=0"}
{"level":"INFO","ts":"2025-11-28T21:19:53.253Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9ww5mn, queueLength=0"}
{"level":"INFO","ts":"2025-11-28T21:19:53.253Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9l76md, queueLength=0"}
{"level":"DEBUG","ts":"2025-11-28T21:19:53.253Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=3"}
{"level":"INFO","ts":"2025-11-28T21:19:53.254Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9lshk5, usage=0.262 (26.2%)"}
{"level":"INFO","ts":"2025-11-28T21:19:53.254Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9ww5mn, usage=0.220 (22.0%)"}
{"level":"INFO","ts":"2025-11-28T21:19:53.254Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9l76md, usage=0.296 (29.6%)"}
{"level":"DEBUG","ts":"2025-11-28T21:19:53.254Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=3"}
{"level":"DEBUG","ts":"2025-11-28T21:19:53.254Z","msg":"Pod-to-variant matching successful: totalPods=3, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:3]"}
{"level":"DEBUG","ts":"2025-11-28T21:19:53.254Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=3"}
{"level":"DEBUG","ts":"2025-11-28T21:19:53.254Z","msg":"Collected capacity metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, metricsCount=3"}
{"level":"DEBUG","ts":"2025-11-28T21:19:53.254Z","msg":"Variant analysis initialized: variant=ms-inference-scheduling-llm-d-modelservice-decode, accelerator=H100, cost=10.00, replicaCount=3"}
{"level":"DEBUG","ts":"2025-11-28T21:19:53.254Z","msg":"Scale-down unsafe: insufficient headroom after redistribution: remainingSpareKv=0.111, kvTrigger=0.200, kvSafe=false, remainingSpareQueue=5.0, queueTrigger=3.0%!(EXTRA bool=true)"}
{"level":"DEBUG","ts":"2025-11-28T21:19:53.254Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, totalReplicas=3, nonSaturated=3, avgSpareKv=0.241, avgSpareQueue=5.0, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"INFO","ts":"2025-11-28T21:19:53.254Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, totalReplicas=3, nonSaturated=3, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"DEBUG","ts":"2025-11-28T21:19:53.254Z","msg":"Capacity targets: no scaling needed (avgSpareKv=0.241, avgSpareQueue=5.0, all variants stable)"}
{"level":"DEBUG","ts":"2025-11-28T21:19:53.254Z","msg":"Capacity targets calculated: modelID=unsloth/Meta-Llama-3.1-8B, targets=map[ms-inference-scheduling-llm-d-modelservice-decode:3]"}
{"level":"INFO","ts":"2025-11-28T21:19:53.254Z","msg":"Capacity-only decisions made for model: unsloth/Meta-Llama-3.1-8B - decision count: 1"}
{"level":"INFO","ts":"2025-11-28T21:19:53.254Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-11-28T21:19:53.254Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=3→target=3"}
{"level":"DEBUG","ts":"2025-11-28T21:19:53.254Z","msg":"Found VA in map: variant=ms-inference-scheduling-llm-d-modelservice-decode, hasCurrentAlloc=true, accelerator=H100"}
{"level":"INFO","ts":"2025-11-28T21:19:53.254Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 3, desired-replicas: 3, accelerator: H100"}
{"level":"INFO","ts":"2025-11-28T21:19:53.254Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=3, accelerator=H100, capacityOnly=true"}
{"level":"INFO","ts":"2025-11-28T21:19:53.259Z","msg":"Applied capacity decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=3, target=3, reason=capacity-only mode: no-change"}
{"level":"INFO","ts":"2025-11-28T21:19:53.259Z","msg":"Reconciliation completed successfully: mode=capacity-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-11-28T21:19:53.259Z","msg":"Operating in CAPACITY-ONLY mode: reactive capacity-based scaling only"}
{"level":"INFO","ts":"2025-11-28T21:19:53.259Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-11-28T21:19:53.259Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-11-28T21:19:53.268Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=3, accelerator=H100, ttft=32.95ms, itl=17.82ms, cost=30.00, arrivalRate=3743.13"}
{"level":"INFO","ts":"2025-11-28T21:19:53.270Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9lshk5, queueLength=0"}
{"level":"INFO","ts":"2025-11-28T21:19:53.270Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9ww5mn, queueLength=0"}
{"level":"INFO","ts":"2025-11-28T21:19:53.270Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9l76md, queueLength=0"}
{"level":"DEBUG","ts":"2025-11-28T21:19:53.270Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=3"}
{"level":"INFO","ts":"2025-11-28T21:19:53.270Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9lshk5, usage=0.262 (26.2%)"}
{"level":"INFO","ts":"2025-11-28T21:19:53.270Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9ww5mn, usage=0.220 (22.0%)"}
{"level":"INFO","ts":"2025-11-28T21:19:53.270Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9l76md, usage=0.296 (29.6%)"}
{"level":"DEBUG","ts":"2025-11-28T21:19:53.270Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=3"}
{"level":"DEBUG","ts":"2025-11-28T21:19:53.270Z","msg":"Pod-to-variant matching successful: totalPods=3, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:3]"}
{"level":"DEBUG","ts":"2025-11-28T21:19:53.270Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=3"}
{"level":"DEBUG","ts":"2025-11-28T21:19:53.270Z","msg":"Collected capacity metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, metricsCount=3"}
{"level":"DEBUG","ts":"2025-11-28T21:19:53.270Z","msg":"Variant analysis initialized: variant=ms-inference-scheduling-llm-d-modelservice-decode, accelerator=H100, cost=10.00, replicaCount=3"}
{"level":"DEBUG","ts":"2025-11-28T21:19:53.270Z","msg":"Scale-down unsafe: insufficient headroom after redistribution: remainingSpareKv=0.111, kvTrigger=0.200, kvSafe=false, remainingSpareQueue=5.0, queueTrigger=3.0%!(EXTRA bool=true)"}
{"level":"DEBUG","ts":"2025-11-28T21:19:53.270Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, totalReplicas=3, nonSaturated=3, avgSpareKv=0.241, avgSpareQueue=5.0, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"INFO","ts":"2025-11-28T21:19:53.270Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, totalReplicas=3, nonSaturated=3, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"DEBUG","ts":"2025-11-28T21:19:53.270Z","msg":"Capacity targets: no scaling needed (avgSpareKv=0.241, avgSpareQueue=5.0, all variants stable)"}
{"level":"DEBUG","ts":"2025-11-28T21:19:53.270Z","msg":"Capacity targets calculated: modelID=unsloth/Meta-Llama-3.1-8B, targets=map[ms-inference-scheduling-llm-d-modelservice-decode:3]"}
{"level":"INFO","ts":"2025-11-28T21:19:53.270Z","msg":"Capacity-only decisions made for model: unsloth/Meta-Llama-3.1-8B - decision count: 1"}
{"level":"INFO","ts":"2025-11-28T21:19:53.270Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-11-28T21:19:53.270Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=3→target=3"}
{"level":"DEBUG","ts":"2025-11-28T21:19:53.270Z","msg":"Found VA in map: variant=ms-inference-scheduling-llm-d-modelservice-decode, hasCurrentAlloc=true, accelerator=H100"}
{"level":"INFO","ts":"2025-11-28T21:19:53.270Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 3, desired-replicas: 3, accelerator: H100"}
{"level":"INFO","ts":"2025-11-28T21:19:53.270Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=3, accelerator=H100, capacityOnly=true"}
{"level":"INFO","ts":"2025-11-28T21:19:53.275Z","msg":"Applied capacity decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=3, target=3, reason=capacity-only mode: no-change"}
{"level":"INFO","ts":"2025-11-28T21:19:53.275Z","msg":"Reconciliation completed successfully: mode=capacity-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-11-28T21:20:23.259Z","msg":"Operating in CAPACITY-ONLY mode: reactive capacity-based scaling only"}
{"level":"INFO","ts":"2025-11-28T21:20:23.260Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-11-28T21:20:23.260Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-11-28T21:20:23.294Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=3, accelerator=H100, ttft=33.49ms, itl=18.16ms, cost=30.00, arrivalRate=3975.29"}
{"level":"INFO","ts":"2025-11-28T21:20:23.295Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9lshk5, usage=0.262 (26.2%)"}
{"level":"INFO","ts":"2025-11-28T21:20:23.295Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9ww5mn, usage=0.254 (25.4%)"}
{"level":"INFO","ts":"2025-11-28T21:20:23.295Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9l76md, usage=0.296 (29.6%)"}
{"level":"DEBUG","ts":"2025-11-28T21:20:23.295Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=3"}
{"level":"INFO","ts":"2025-11-28T21:20:23.295Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9lshk5, queueLength=0"}
{"level":"INFO","ts":"2025-11-28T21:20:23.295Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9ww5mn, queueLength=0"}
{"level":"INFO","ts":"2025-11-28T21:20:23.295Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9l76md, queueLength=0"}
{"level":"DEBUG","ts":"2025-11-28T21:20:23.295Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=3"}
{"level":"DEBUG","ts":"2025-11-28T21:20:23.296Z","msg":"Pod-to-variant matching successful: totalPods=3, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:3]"}
{"level":"DEBUG","ts":"2025-11-28T21:20:23.296Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=3"}
{"level":"DEBUG","ts":"2025-11-28T21:20:23.296Z","msg":"Collected capacity metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, metricsCount=3"}
{"level":"DEBUG","ts":"2025-11-28T21:20:23.296Z","msg":"Variant analysis initialized: variant=ms-inference-scheduling-llm-d-modelservice-decode, accelerator=H100, cost=10.00, replicaCount=3"}
{"level":"DEBUG","ts":"2025-11-28T21:20:23.296Z","msg":"Scale-down unsafe: insufficient headroom after redistribution: remainingSpareKv=0.094, kvTrigger=0.200, kvSafe=false, remainingSpareQueue=5.0, queueTrigger=3.0%!(EXTRA bool=true)"}
{"level":"DEBUG","ts":"2025-11-28T21:20:23.296Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, totalReplicas=3, nonSaturated=3, avgSpareKv=0.229, avgSpareQueue=5.0, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"INFO","ts":"2025-11-28T21:20:23.296Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, totalReplicas=3, nonSaturated=3, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"DEBUG","ts":"2025-11-28T21:20:23.296Z","msg":"Capacity targets: no scaling needed (avgSpareKv=0.229, avgSpareQueue=5.0, all variants stable)"}
{"level":"DEBUG","ts":"2025-11-28T21:20:23.296Z","msg":"Capacity targets calculated: modelID=unsloth/Meta-Llama-3.1-8B, targets=map[ms-inference-scheduling-llm-d-modelservice-decode:3]"}
{"level":"INFO","ts":"2025-11-28T21:20:23.296Z","msg":"Capacity-only decisions made for model: unsloth/Meta-Llama-3.1-8B - decision count: 1"}
{"level":"INFO","ts":"2025-11-28T21:20:23.296Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-11-28T21:20:23.296Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=3→target=3"}
{"level":"DEBUG","ts":"2025-11-28T21:20:23.296Z","msg":"Found VA in map: variant=ms-inference-scheduling-llm-d-modelservice-decode, hasCurrentAlloc=true, accelerator=H100"}
{"level":"INFO","ts":"2025-11-28T21:20:23.296Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 3, desired-replicas: 3, accelerator: H100"}
{"level":"INFO","ts":"2025-11-28T21:20:23.296Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=3, accelerator=H100, capacityOnly=true"}
{"level":"INFO","ts":"2025-11-28T21:20:23.301Z","msg":"Applied capacity decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=3, target=3, reason=capacity-only mode: no-change"}
{"level":"INFO","ts":"2025-11-28T21:20:23.301Z","msg":"Reconciliation completed successfully: mode=capacity-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-11-28T21:20:23.301Z","msg":"Operating in CAPACITY-ONLY mode: reactive capacity-based scaling only"}
{"level":"INFO","ts":"2025-11-28T21:20:23.301Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-11-28T21:20:23.301Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-11-28T21:20:23.311Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=3, accelerator=H100, ttft=33.49ms, itl=18.16ms, cost=30.00, arrivalRate=3975.29"}
{"level":"INFO","ts":"2025-11-28T21:20:23.312Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9lshk5, queueLength=0"}
{"level":"INFO","ts":"2025-11-28T21:20:23.312Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9ww5mn, queueLength=0"}
{"level":"INFO","ts":"2025-11-28T21:20:23.312Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9l76md, queueLength=0"}
{"level":"DEBUG","ts":"2025-11-28T21:20:23.312Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=3"}
{"level":"INFO","ts":"2025-11-28T21:20:23.313Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9lshk5, usage=0.262 (26.2%)"}
{"level":"INFO","ts":"2025-11-28T21:20:23.313Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9ww5mn, usage=0.254 (25.4%)"}
{"level":"INFO","ts":"2025-11-28T21:20:23.313Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9l76md, usage=0.296 (29.6%)"}
{"level":"DEBUG","ts":"2025-11-28T21:20:23.313Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=3"}
{"level":"DEBUG","ts":"2025-11-28T21:20:23.313Z","msg":"Pod-to-variant matching successful: totalPods=3, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:3]"}
{"level":"DEBUG","ts":"2025-11-28T21:20:23.313Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=3"}
{"level":"DEBUG","ts":"2025-11-28T21:20:23.313Z","msg":"Collected capacity metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, metricsCount=3"}
{"level":"DEBUG","ts":"2025-11-28T21:20:23.313Z","msg":"Variant analysis initialized: variant=ms-inference-scheduling-llm-d-modelservice-decode, accelerator=H100, cost=10.00, replicaCount=3"}
{"level":"DEBUG","ts":"2025-11-28T21:20:23.313Z","msg":"Scale-down unsafe: insufficient headroom after redistribution: remainingSpareKv=0.094, kvTrigger=0.200, kvSafe=false, remainingSpareQueue=5.0, queueTrigger=3.0%!(EXTRA bool=true)"}
{"level":"DEBUG","ts":"2025-11-28T21:20:23.313Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, totalReplicas=3, nonSaturated=3, avgSpareKv=0.229, avgSpareQueue=5.0, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"INFO","ts":"2025-11-28T21:20:23.313Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, totalReplicas=3, nonSaturated=3, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"DEBUG","ts":"2025-11-28T21:20:23.313Z","msg":"Capacity targets: no scaling needed (avgSpareKv=0.229, avgSpareQueue=5.0, all variants stable)"}
{"level":"DEBUG","ts":"2025-11-28T21:20:23.313Z","msg":"Capacity targets calculated: modelID=unsloth/Meta-Llama-3.1-8B, targets=map[ms-inference-scheduling-llm-d-modelservice-decode:3]"}
{"level":"INFO","ts":"2025-11-28T21:20:23.313Z","msg":"Capacity-only decisions made for model: unsloth/Meta-Llama-3.1-8B - decision count: 1"}
{"level":"INFO","ts":"2025-11-28T21:20:23.313Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-11-28T21:20:23.313Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=3→target=3"}
{"level":"DEBUG","ts":"2025-11-28T21:20:23.313Z","msg":"Found VA in map: variant=ms-inference-scheduling-llm-d-modelservice-decode, hasCurrentAlloc=true, accelerator=H100"}
{"level":"INFO","ts":"2025-11-28T21:20:23.313Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 3, desired-replicas: 3, accelerator: H100"}
{"level":"INFO","ts":"2025-11-28T21:20:23.313Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=3, accelerator=H100, capacityOnly=true"}
{"level":"INFO","ts":"2025-11-28T21:20:23.317Z","msg":"Applied capacity decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=3, target=3, reason=capacity-only mode: no-change"}
{"level":"INFO","ts":"2025-11-28T21:20:23.317Z","msg":"Reconciliation completed successfully: mode=capacity-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-11-28T21:20:53.302Z","msg":"Operating in CAPACITY-ONLY mode: reactive capacity-based scaling only"}
{"level":"INFO","ts":"2025-11-28T21:20:53.302Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-11-28T21:20:53.302Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-11-28T21:20:53.322Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=3, accelerator=H100, ttft=33.72ms, itl=18.22ms, cost=30.00, arrivalRate=3890.14"}
{"level":"INFO","ts":"2025-11-28T21:20:53.323Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9lshk5, queueLength=0"}
{"level":"INFO","ts":"2025-11-28T21:20:53.323Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9ww5mn, queueLength=0"}
{"level":"INFO","ts":"2025-11-28T21:20:53.323Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9l76md, queueLength=0"}
{"level":"DEBUG","ts":"2025-11-28T21:20:53.323Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=3"}
{"level":"INFO","ts":"2025-11-28T21:20:53.323Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9lshk5, usage=0.257 (25.7%)"}
{"level":"INFO","ts":"2025-11-28T21:20:53.323Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9ww5mn, usage=0.247 (24.7%)"}
{"level":"INFO","ts":"2025-11-28T21:20:53.323Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9l76md, usage=0.274 (27.4%)"}
{"level":"DEBUG","ts":"2025-11-28T21:20:53.323Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=3"}
{"level":"DEBUG","ts":"2025-11-28T21:20:53.324Z","msg":"Pod-to-variant matching successful: totalPods=3, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:3]"}
{"level":"DEBUG","ts":"2025-11-28T21:20:53.324Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=3"}
{"level":"DEBUG","ts":"2025-11-28T21:20:53.324Z","msg":"Collected capacity metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, metricsCount=3"}
{"level":"DEBUG","ts":"2025-11-28T21:20:53.324Z","msg":"Variant analysis initialized: variant=ms-inference-scheduling-llm-d-modelservice-decode, accelerator=H100, cost=10.00, replicaCount=3"}
{"level":"DEBUG","ts":"2025-11-28T21:20:53.324Z","msg":"Scale-down unsafe: insufficient headroom after redistribution: remainingSpareKv=0.111, kvTrigger=0.200, kvSafe=false, remainingSpareQueue=5.0, queueTrigger=3.0%!(EXTRA bool=true)"}
{"level":"DEBUG","ts":"2025-11-28T21:20:53.324Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, totalReplicas=3, nonSaturated=3, avgSpareKv=0.241, avgSpareQueue=5.0, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"INFO","ts":"2025-11-28T21:20:53.324Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, totalReplicas=3, nonSaturated=3, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"DEBUG","ts":"2025-11-28T21:20:53.324Z","msg":"Capacity targets: no scaling needed (avgSpareKv=0.241, avgSpareQueue=5.0, all variants stable)"}
{"level":"DEBUG","ts":"2025-11-28T21:20:53.324Z","msg":"Capacity targets calculated: modelID=unsloth/Meta-Llama-3.1-8B, targets=map[ms-inference-scheduling-llm-d-modelservice-decode:3]"}
{"level":"INFO","ts":"2025-11-28T21:20:53.324Z","msg":"Capacity-only decisions made for model: unsloth/Meta-Llama-3.1-8B - decision count: 1"}
{"level":"INFO","ts":"2025-11-28T21:20:53.324Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-11-28T21:20:53.324Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=3→target=3"}
{"level":"DEBUG","ts":"2025-11-28T21:20:53.324Z","msg":"Found VA in map: variant=ms-inference-scheduling-llm-d-modelservice-decode, hasCurrentAlloc=true, accelerator=H100"}
{"level":"INFO","ts":"2025-11-28T21:20:53.324Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 3, desired-replicas: 3, accelerator: H100"}
{"level":"INFO","ts":"2025-11-28T21:20:53.324Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=3, accelerator=H100, capacityOnly=true"}
{"level":"INFO","ts":"2025-11-28T21:20:53.330Z","msg":"Applied capacity decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=3, target=3, reason=capacity-only mode: no-change"}
{"level":"INFO","ts":"2025-11-28T21:20:53.330Z","msg":"Reconciliation completed successfully: mode=capacity-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-11-28T21:20:53.330Z","msg":"Operating in CAPACITY-ONLY mode: reactive capacity-based scaling only"}
{"level":"INFO","ts":"2025-11-28T21:20:53.330Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-11-28T21:20:53.330Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-11-28T21:20:53.339Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=3, accelerator=H100, ttft=33.72ms, itl=18.22ms, cost=30.00, arrivalRate=3890.14"}
{"level":"INFO","ts":"2025-11-28T21:20:53.340Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9lshk5, queueLength=0"}
{"level":"INFO","ts":"2025-11-28T21:20:53.340Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9ww5mn, queueLength=0"}
{"level":"INFO","ts":"2025-11-28T21:20:53.340Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9l76md, queueLength=0"}
{"level":"DEBUG","ts":"2025-11-28T21:20:53.340Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=3"}
{"level":"INFO","ts":"2025-11-28T21:20:53.340Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9lshk5, usage=0.257 (25.7%)"}
{"level":"INFO","ts":"2025-11-28T21:20:53.340Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9ww5mn, usage=0.247 (24.7%)"}
{"level":"INFO","ts":"2025-11-28T21:20:53.340Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9l76md, usage=0.274 (27.4%)"}
{"level":"DEBUG","ts":"2025-11-28T21:20:53.340Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=3"}
{"level":"DEBUG","ts":"2025-11-28T21:20:53.341Z","msg":"Pod-to-variant matching successful: totalPods=3, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:3]"}
{"level":"DEBUG","ts":"2025-11-28T21:20:53.341Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=3"}
{"level":"DEBUG","ts":"2025-11-28T21:20:53.341Z","msg":"Collected capacity metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, metricsCount=3"}
{"level":"DEBUG","ts":"2025-11-28T21:20:53.341Z","msg":"Variant analysis initialized: variant=ms-inference-scheduling-llm-d-modelservice-decode, accelerator=H100, cost=10.00, replicaCount=3"}
{"level":"DEBUG","ts":"2025-11-28T21:20:53.341Z","msg":"Scale-down unsafe: insufficient headroom after redistribution: remainingSpareKv=0.111, kvTrigger=0.200, kvSafe=false, remainingSpareQueue=5.0, queueTrigger=3.0%!(EXTRA bool=true)"}
{"level":"DEBUG","ts":"2025-11-28T21:20:53.341Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, totalReplicas=3, nonSaturated=3, avgSpareKv=0.241, avgSpareQueue=5.0, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"INFO","ts":"2025-11-28T21:20:53.341Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, totalReplicas=3, nonSaturated=3, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"DEBUG","ts":"2025-11-28T21:20:53.341Z","msg":"Capacity targets: no scaling needed (avgSpareKv=0.241, avgSpareQueue=5.0, all variants stable)"}
{"level":"DEBUG","ts":"2025-11-28T21:20:53.341Z","msg":"Capacity targets calculated: modelID=unsloth/Meta-Llama-3.1-8B, targets=map[ms-inference-scheduling-llm-d-modelservice-decode:3]"}
{"level":"INFO","ts":"2025-11-28T21:20:53.341Z","msg":"Capacity-only decisions made for model: unsloth/Meta-Llama-3.1-8B - decision count: 1"}
{"level":"INFO","ts":"2025-11-28T21:20:53.341Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-11-28T21:20:53.341Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=3→target=3"}
{"level":"DEBUG","ts":"2025-11-28T21:20:53.341Z","msg":"Found VA in map: variant=ms-inference-scheduling-llm-d-modelservice-decode, hasCurrentAlloc=true, accelerator=H100"}
{"level":"INFO","ts":"2025-11-28T21:20:53.341Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 3, desired-replicas: 3, accelerator: H100"}
{"level":"INFO","ts":"2025-11-28T21:20:53.341Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=3, accelerator=H100, capacityOnly=true"}
{"level":"INFO","ts":"2025-11-28T21:20:53.345Z","msg":"Applied capacity decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=3, target=3, reason=capacity-only mode: no-change"}
{"level":"INFO","ts":"2025-11-28T21:20:53.345Z","msg":"Reconciliation completed successfully: mode=capacity-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-11-28T21:21:23.332Z","msg":"Operating in CAPACITY-ONLY mode: reactive capacity-based scaling only"}
{"level":"INFO","ts":"2025-11-28T21:21:23.332Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-11-28T21:21:23.332Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-11-28T21:21:23.353Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=3, accelerator=H100, ttft=34.39ms, itl=18.55ms, cost=30.00, arrivalRate=3841.99"}
{"level":"INFO","ts":"2025-11-28T21:21:23.355Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9lshk5, queueLength=0"}
{"level":"INFO","ts":"2025-11-28T21:21:23.355Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9ww5mn, queueLength=0"}
{"level":"INFO","ts":"2025-11-28T21:21:23.355Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9l76md, queueLength=0"}
{"level":"DEBUG","ts":"2025-11-28T21:21:23.355Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=3"}
{"level":"INFO","ts":"2025-11-28T21:21:23.355Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9lshk5, usage=0.257 (25.7%)"}
{"level":"INFO","ts":"2025-11-28T21:21:23.355Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9ww5mn, usage=0.288 (28.8%)"}
{"level":"INFO","ts":"2025-11-28T21:21:23.355Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9l76md, usage=0.274 (27.4%)"}
{"level":"DEBUG","ts":"2025-11-28T21:21:23.355Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=3"}
{"level":"DEBUG","ts":"2025-11-28T21:21:23.355Z","msg":"Pod-to-variant matching successful: totalPods=3, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:3]"}
{"level":"DEBUG","ts":"2025-11-28T21:21:23.355Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=3"}
{"level":"DEBUG","ts":"2025-11-28T21:21:23.355Z","msg":"Collected capacity metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, metricsCount=3"}
{"level":"DEBUG","ts":"2025-11-28T21:21:23.355Z","msg":"Variant analysis initialized: variant=ms-inference-scheduling-llm-d-modelservice-decode, accelerator=H100, cost=10.00, replicaCount=3"}
{"level":"DEBUG","ts":"2025-11-28T21:21:23.355Z","msg":"Scale-down unsafe: insufficient headroom after redistribution: remainingSpareKv=0.091, kvTrigger=0.200, kvSafe=false, remainingSpareQueue=5.0, queueTrigger=3.0%!(EXTRA bool=true)"}
{"level":"DEBUG","ts":"2025-11-28T21:21:23.355Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, totalReplicas=3, nonSaturated=3, avgSpareKv=0.227, avgSpareQueue=5.0, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"INFO","ts":"2025-11-28T21:21:23.355Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, totalReplicas=3, nonSaturated=3, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"DEBUG","ts":"2025-11-28T21:21:23.355Z","msg":"Capacity targets: no scaling needed (avgSpareKv=0.227, avgSpareQueue=5.0, all variants stable)"}
{"level":"DEBUG","ts":"2025-11-28T21:21:23.355Z","msg":"Capacity targets calculated: modelID=unsloth/Meta-Llama-3.1-8B, targets=map[ms-inference-scheduling-llm-d-modelservice-decode:3]"}
{"level":"INFO","ts":"2025-11-28T21:21:23.355Z","msg":"Capacity-only decisions made for model: unsloth/Meta-Llama-3.1-8B - decision count: 1"}
{"level":"INFO","ts":"2025-11-28T21:21:23.355Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-11-28T21:21:23.355Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=3→target=3"}
{"level":"DEBUG","ts":"2025-11-28T21:21:23.355Z","msg":"Found VA in map: variant=ms-inference-scheduling-llm-d-modelservice-decode, hasCurrentAlloc=true, accelerator=H100"}
{"level":"INFO","ts":"2025-11-28T21:21:23.355Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 3, desired-replicas: 3, accelerator: H100"}
{"level":"INFO","ts":"2025-11-28T21:21:23.355Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=3, accelerator=H100, capacityOnly=true"}
{"level":"INFO","ts":"2025-11-28T21:21:23.362Z","msg":"Applied capacity decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=3, target=3, reason=capacity-only mode: no-change"}
{"level":"INFO","ts":"2025-11-28T21:21:23.362Z","msg":"Reconciliation completed successfully: mode=capacity-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-11-28T21:21:23.362Z","msg":"Operating in CAPACITY-ONLY mode: reactive capacity-based scaling only"}
{"level":"INFO","ts":"2025-11-28T21:21:23.362Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-11-28T21:21:23.362Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-11-28T21:21:23.371Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=3, accelerator=H100, ttft=34.39ms, itl=18.55ms, cost=30.00, arrivalRate=3841.99"}
{"level":"INFO","ts":"2025-11-28T21:21:23.372Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9lshk5, queueLength=0"}
{"level":"INFO","ts":"2025-11-28T21:21:23.372Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9ww5mn, queueLength=0"}
{"level":"INFO","ts":"2025-11-28T21:21:23.372Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9l76md, queueLength=0"}
{"level":"DEBUG","ts":"2025-11-28T21:21:23.372Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=3"}
{"level":"INFO","ts":"2025-11-28T21:21:23.372Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9lshk5, usage=0.257 (25.7%)"}
{"level":"INFO","ts":"2025-11-28T21:21:23.372Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9ww5mn, usage=0.288 (28.8%)"}
{"level":"INFO","ts":"2025-11-28T21:21:23.372Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9l76md, usage=0.274 (27.4%)"}
{"level":"DEBUG","ts":"2025-11-28T21:21:23.372Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=3"}
{"level":"DEBUG","ts":"2025-11-28T21:21:23.372Z","msg":"Pod-to-variant matching successful: totalPods=3, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:3]"}
{"level":"DEBUG","ts":"2025-11-28T21:21:23.372Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=3"}
{"level":"DEBUG","ts":"2025-11-28T21:21:23.372Z","msg":"Collected capacity metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, metricsCount=3"}
{"level":"DEBUG","ts":"2025-11-28T21:21:23.372Z","msg":"Variant analysis initialized: variant=ms-inference-scheduling-llm-d-modelservice-decode, accelerator=H100, cost=10.00, replicaCount=3"}
{"level":"DEBUG","ts":"2025-11-28T21:21:23.372Z","msg":"Scale-down unsafe: insufficient headroom after redistribution: remainingSpareKv=0.091, kvTrigger=0.200, kvSafe=false, remainingSpareQueue=5.0, queueTrigger=3.0%!(EXTRA bool=true)"}
{"level":"DEBUG","ts":"2025-11-28T21:21:23.372Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, totalReplicas=3, nonSaturated=3, avgSpareKv=0.227, avgSpareQueue=5.0, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"INFO","ts":"2025-11-28T21:21:23.372Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, totalReplicas=3, nonSaturated=3, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"DEBUG","ts":"2025-11-28T21:21:23.372Z","msg":"Capacity targets: no scaling needed (avgSpareKv=0.227, avgSpareQueue=5.0, all variants stable)"}
{"level":"DEBUG","ts":"2025-11-28T21:21:23.372Z","msg":"Capacity targets calculated: modelID=unsloth/Meta-Llama-3.1-8B, targets=map[ms-inference-scheduling-llm-d-modelservice-decode:3]"}
{"level":"INFO","ts":"2025-11-28T21:21:23.372Z","msg":"Capacity-only decisions made for model: unsloth/Meta-Llama-3.1-8B - decision count: 1"}
{"level":"INFO","ts":"2025-11-28T21:21:23.372Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-11-28T21:21:23.372Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=3→target=3"}
{"level":"DEBUG","ts":"2025-11-28T21:21:23.372Z","msg":"Found VA in map: variant=ms-inference-scheduling-llm-d-modelservice-decode, hasCurrentAlloc=true, accelerator=H100"}
{"level":"INFO","ts":"2025-11-28T21:21:23.372Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 3, desired-replicas: 3, accelerator: H100"}
{"level":"INFO","ts":"2025-11-28T21:21:23.372Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=3, accelerator=H100, capacityOnly=true"}
{"level":"INFO","ts":"2025-11-28T21:21:23.377Z","msg":"Applied capacity decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=3, target=3, reason=capacity-only mode: no-change"}
{"level":"INFO","ts":"2025-11-28T21:21:23.377Z","msg":"Reconciliation completed successfully: mode=capacity-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-11-28T21:21:53.363Z","msg":"Operating in CAPACITY-ONLY mode: reactive capacity-based scaling only"}
{"level":"INFO","ts":"2025-11-28T21:21:53.363Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-11-28T21:21:53.363Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-11-28T21:21:53.383Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=3, accelerator=H100, ttft=34.02ms, itl=18.41ms, cost=30.00, arrivalRate=3870.86"}
{"level":"INFO","ts":"2025-11-28T21:21:53.384Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9lshk5, queueLength=0"}
{"level":"INFO","ts":"2025-11-28T21:21:53.384Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9ww5mn, queueLength=0"}
{"level":"INFO","ts":"2025-11-28T21:21:53.384Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9l76md, queueLength=0"}
{"level":"DEBUG","ts":"2025-11-28T21:21:53.384Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=3"}
{"level":"INFO","ts":"2025-11-28T21:21:53.384Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9lshk5, usage=0.281 (28.1%)"}
{"level":"INFO","ts":"2025-11-28T21:21:53.384Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9ww5mn, usage=0.288 (28.8%)"}
{"level":"INFO","ts":"2025-11-28T21:21:53.384Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9l76md, usage=0.256 (25.6%)"}
{"level":"DEBUG","ts":"2025-11-28T21:21:53.384Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=3"}
{"level":"DEBUG","ts":"2025-11-28T21:21:53.385Z","msg":"Pod-to-variant matching successful: totalPods=3, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:3]"}
{"level":"DEBUG","ts":"2025-11-28T21:21:53.385Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=3"}
{"level":"DEBUG","ts":"2025-11-28T21:21:53.385Z","msg":"Collected capacity metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, metricsCount=3"}
{"level":"DEBUG","ts":"2025-11-28T21:21:53.385Z","msg":"Variant analysis initialized: variant=ms-inference-scheduling-llm-d-modelservice-decode, accelerator=H100, cost=10.00, replicaCount=3"}
{"level":"DEBUG","ts":"2025-11-28T21:21:53.385Z","msg":"Scale-down unsafe: insufficient headroom after redistribution: remainingSpareKv=0.087, kvTrigger=0.200, kvSafe=false, remainingSpareQueue=5.0, queueTrigger=3.0%!(EXTRA bool=true)"}
{"level":"DEBUG","ts":"2025-11-28T21:21:53.385Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, totalReplicas=3, nonSaturated=3, avgSpareKv=0.225, avgSpareQueue=5.0, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"INFO","ts":"2025-11-28T21:21:53.385Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, totalReplicas=3, nonSaturated=3, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"DEBUG","ts":"2025-11-28T21:21:53.385Z","msg":"Capacity targets: no scaling needed (avgSpareKv=0.225, avgSpareQueue=5.0, all variants stable)"}
{"level":"DEBUG","ts":"2025-11-28T21:21:53.385Z","msg":"Capacity targets calculated: modelID=unsloth/Meta-Llama-3.1-8B, targets=map[ms-inference-scheduling-llm-d-modelservice-decode:3]"}
{"level":"INFO","ts":"2025-11-28T21:21:53.385Z","msg":"Capacity-only decisions made for model: unsloth/Meta-Llama-3.1-8B - decision count: 1"}
{"level":"INFO","ts":"2025-11-28T21:21:53.385Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-11-28T21:21:53.385Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=3→target=3"}
{"level":"DEBUG","ts":"2025-11-28T21:21:53.385Z","msg":"Found VA in map: variant=ms-inference-scheduling-llm-d-modelservice-decode, hasCurrentAlloc=true, accelerator=H100"}
{"level":"INFO","ts":"2025-11-28T21:21:53.385Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 3, desired-replicas: 3, accelerator: H100"}
{"level":"INFO","ts":"2025-11-28T21:21:53.385Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=3, accelerator=H100, capacityOnly=true"}
{"level":"INFO","ts":"2025-11-28T21:21:53.391Z","msg":"Applied capacity decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=3, target=3, reason=capacity-only mode: no-change"}
{"level":"INFO","ts":"2025-11-28T21:21:53.391Z","msg":"Reconciliation completed successfully: mode=capacity-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-11-28T21:21:53.391Z","msg":"Operating in CAPACITY-ONLY mode: reactive capacity-based scaling only"}
{"level":"INFO","ts":"2025-11-28T21:21:53.391Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-11-28T21:21:53.391Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-11-28T21:21:53.399Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=3, accelerator=H100, ttft=34.02ms, itl=18.41ms, cost=30.00, arrivalRate=3870.86"}
{"level":"INFO","ts":"2025-11-28T21:21:53.401Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9lshk5, usage=0.281 (28.1%)"}
{"level":"INFO","ts":"2025-11-28T21:21:53.401Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9ww5mn, usage=0.288 (28.8%)"}
{"level":"INFO","ts":"2025-11-28T21:21:53.401Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9l76md, usage=0.256 (25.6%)"}
{"level":"DEBUG","ts":"2025-11-28T21:21:53.401Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=3"}
{"level":"INFO","ts":"2025-11-28T21:21:53.401Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9lshk5, queueLength=0"}
{"level":"INFO","ts":"2025-11-28T21:21:53.401Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9ww5mn, queueLength=0"}
{"level":"INFO","ts":"2025-11-28T21:21:53.401Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9l76md, queueLength=0"}
{"level":"DEBUG","ts":"2025-11-28T21:21:53.401Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=3"}
{"level":"DEBUG","ts":"2025-11-28T21:21:53.401Z","msg":"Pod-to-variant matching successful: totalPods=3, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:3]"}
{"level":"DEBUG","ts":"2025-11-28T21:21:53.401Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=3"}
{"level":"DEBUG","ts":"2025-11-28T21:21:53.401Z","msg":"Collected capacity metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, metricsCount=3"}
{"level":"DEBUG","ts":"2025-11-28T21:21:53.401Z","msg":"Variant analysis initialized: variant=ms-inference-scheduling-llm-d-modelservice-decode, accelerator=H100, cost=10.00, replicaCount=3"}
{"level":"DEBUG","ts":"2025-11-28T21:21:53.401Z","msg":"Scale-down unsafe: insufficient headroom after redistribution: remainingSpareKv=0.087, kvTrigger=0.200, kvSafe=false, remainingSpareQueue=5.0, queueTrigger=3.0%!(EXTRA bool=true)"}
{"level":"DEBUG","ts":"2025-11-28T21:21:53.401Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, totalReplicas=3, nonSaturated=3, avgSpareKv=0.225, avgSpareQueue=5.0, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"INFO","ts":"2025-11-28T21:21:53.401Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, totalReplicas=3, nonSaturated=3, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"DEBUG","ts":"2025-11-28T21:21:53.401Z","msg":"Capacity targets: no scaling needed (avgSpareKv=0.225, avgSpareQueue=5.0, all variants stable)"}
{"level":"DEBUG","ts":"2025-11-28T21:21:53.401Z","msg":"Capacity targets calculated: modelID=unsloth/Meta-Llama-3.1-8B, targets=map[ms-inference-scheduling-llm-d-modelservice-decode:3]"}
{"level":"INFO","ts":"2025-11-28T21:21:53.401Z","msg":"Capacity-only decisions made for model: unsloth/Meta-Llama-3.1-8B - decision count: 1"}
{"level":"INFO","ts":"2025-11-28T21:21:53.401Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-11-28T21:21:53.401Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=3→target=3"}
{"level":"DEBUG","ts":"2025-11-28T21:21:53.401Z","msg":"Found VA in map: variant=ms-inference-scheduling-llm-d-modelservice-decode, hasCurrentAlloc=true, accelerator=H100"}
{"level":"INFO","ts":"2025-11-28T21:21:53.401Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 3, desired-replicas: 3, accelerator: H100"}
{"level":"INFO","ts":"2025-11-28T21:21:53.401Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=3, accelerator=H100, capacityOnly=true"}
{"level":"INFO","ts":"2025-11-28T21:21:53.405Z","msg":"Applied capacity decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=3, target=3, reason=capacity-only mode: no-change"}
{"level":"INFO","ts":"2025-11-28T21:21:53.405Z","msg":"Reconciliation completed successfully: mode=capacity-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-11-28T21:22:23.392Z","msg":"Operating in CAPACITY-ONLY mode: reactive capacity-based scaling only"}
{"level":"INFO","ts":"2025-11-28T21:22:23.392Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-11-28T21:22:23.392Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-11-28T21:22:23.405Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=3, accelerator=H100, ttft=33.02ms, itl=18.08ms, cost=30.00, arrivalRate=3912.06"}
{"level":"INFO","ts":"2025-11-28T21:22:23.406Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9lshk5, usage=0.281 (28.1%)"}
{"level":"INFO","ts":"2025-11-28T21:22:23.406Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9lshk5, queueLength=0"}
{"level":"INFO","ts":"2025-11-28T21:22:23.406Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9ww5mn, queueLength=0"}
{"level":"INFO","ts":"2025-11-28T21:22:23.406Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9l76md, queueLength=0"}
{"level":"DEBUG","ts":"2025-11-28T21:22:23.406Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=3"}
{"level":"INFO","ts":"2025-11-28T21:22:23.406Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9ww5mn, usage=0.275 (27.5%)"}
{"level":"INFO","ts":"2025-11-28T21:22:23.407Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9l76md, usage=0.219 (21.9%)"}
{"level":"DEBUG","ts":"2025-11-28T21:22:23.407Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=3"}
{"level":"DEBUG","ts":"2025-11-28T21:22:23.407Z","msg":"Pod-to-variant matching successful: totalPods=3, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:3]"}
{"level":"DEBUG","ts":"2025-11-28T21:22:23.407Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=3"}
{"level":"DEBUG","ts":"2025-11-28T21:22:23.407Z","msg":"Collected capacity metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, metricsCount=3"}
{"level":"DEBUG","ts":"2025-11-28T21:22:23.407Z","msg":"Variant analysis initialized: variant=ms-inference-scheduling-llm-d-modelservice-decode, accelerator=H100, cost=10.00, replicaCount=3"}
{"level":"DEBUG","ts":"2025-11-28T21:22:23.407Z","msg":"Scale-down unsafe: insufficient headroom after redistribution: remainingSpareKv=0.112, kvTrigger=0.200, kvSafe=false, remainingSpareQueue=5.0, queueTrigger=3.0%!(EXTRA bool=true)"}
{"level":"DEBUG","ts":"2025-11-28T21:22:23.407Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, totalReplicas=3, nonSaturated=3, avgSpareKv=0.242, avgSpareQueue=5.0, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"INFO","ts":"2025-11-28T21:22:23.407Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, totalReplicas=3, nonSaturated=3, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"DEBUG","ts":"2025-11-28T21:22:23.407Z","msg":"Capacity targets: no scaling needed (avgSpareKv=0.242, avgSpareQueue=5.0, all variants stable)"}
{"level":"DEBUG","ts":"2025-11-28T21:22:23.407Z","msg":"Capacity targets calculated: modelID=unsloth/Meta-Llama-3.1-8B, targets=map[ms-inference-scheduling-llm-d-modelservice-decode:3]"}
{"level":"INFO","ts":"2025-11-28T21:22:23.407Z","msg":"Capacity-only decisions made for model: unsloth/Meta-Llama-3.1-8B - decision count: 1"}
{"level":"INFO","ts":"2025-11-28T21:22:23.407Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-11-28T21:22:23.407Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=3→target=3"}
{"level":"DEBUG","ts":"2025-11-28T21:22:23.407Z","msg":"Found VA in map: variant=ms-inference-scheduling-llm-d-modelservice-decode, hasCurrentAlloc=true, accelerator=H100"}
{"level":"INFO","ts":"2025-11-28T21:22:23.407Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 3, desired-replicas: 3, accelerator: H100"}
{"level":"INFO","ts":"2025-11-28T21:22:23.407Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=3, accelerator=H100, capacityOnly=true"}
{"level":"INFO","ts":"2025-11-28T21:22:23.411Z","msg":"Applied capacity decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=3, target=3, reason=capacity-only mode: no-change"}
{"level":"INFO","ts":"2025-11-28T21:22:23.411Z","msg":"Reconciliation completed successfully: mode=capacity-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-11-28T21:22:23.411Z","msg":"Operating in CAPACITY-ONLY mode: reactive capacity-based scaling only"}
{"level":"INFO","ts":"2025-11-28T21:22:23.411Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-11-28T21:22:23.411Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-11-28T21:22:23.421Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=3, accelerator=H100, ttft=33.02ms, itl=18.08ms, cost=30.00, arrivalRate=3912.06"}
{"level":"INFO","ts":"2025-11-28T21:22:23.422Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9lshk5, queueLength=0"}
{"level":"INFO","ts":"2025-11-28T21:22:23.422Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9ww5mn, queueLength=0"}
{"level":"INFO","ts":"2025-11-28T21:22:23.422Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9l76md, queueLength=0"}
{"level":"INFO","ts":"2025-11-28T21:22:23.422Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9lshk5, usage=0.281 (28.1%)"}
{"level":"INFO","ts":"2025-11-28T21:22:23.422Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9ww5mn, usage=0.275 (27.5%)"}
{"level":"INFO","ts":"2025-11-28T21:22:23.422Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9l76md, usage=0.219 (21.9%)"}
{"level":"DEBUG","ts":"2025-11-28T21:22:23.422Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=3"}
{"level":"DEBUG","ts":"2025-11-28T21:22:23.422Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=3"}
{"level":"DEBUG","ts":"2025-11-28T21:22:23.422Z","msg":"Pod-to-variant matching successful: totalPods=3, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:3]"}
{"level":"DEBUG","ts":"2025-11-28T21:22:23.422Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=3"}
{"level":"DEBUG","ts":"2025-11-28T21:22:23.422Z","msg":"Collected capacity metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, metricsCount=3"}
{"level":"DEBUG","ts":"2025-11-28T21:22:23.422Z","msg":"Variant analysis initialized: variant=ms-inference-scheduling-llm-d-modelservice-decode, accelerator=H100, cost=10.00, replicaCount=3"}
{"level":"DEBUG","ts":"2025-11-28T21:22:23.422Z","msg":"Scale-down unsafe: insufficient headroom after redistribution: remainingSpareKv=0.112, kvTrigger=0.200, kvSafe=false, remainingSpareQueue=5.0, queueTrigger=3.0%!(EXTRA bool=true)"}
{"level":"DEBUG","ts":"2025-11-28T21:22:23.422Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, totalReplicas=3, nonSaturated=3, avgSpareKv=0.242, avgSpareQueue=5.0, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"INFO","ts":"2025-11-28T21:22:23.422Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, totalReplicas=3, nonSaturated=3, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"DEBUG","ts":"2025-11-28T21:22:23.422Z","msg":"Capacity targets: no scaling needed (avgSpareKv=0.242, avgSpareQueue=5.0, all variants stable)"}
{"level":"DEBUG","ts":"2025-11-28T21:22:23.423Z","msg":"Capacity targets calculated: modelID=unsloth/Meta-Llama-3.1-8B, targets=map[ms-inference-scheduling-llm-d-modelservice-decode:3]"}
{"level":"INFO","ts":"2025-11-28T21:22:23.423Z","msg":"Capacity-only decisions made for model: unsloth/Meta-Llama-3.1-8B - decision count: 1"}
{"level":"INFO","ts":"2025-11-28T21:22:23.423Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-11-28T21:22:23.423Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=3→target=3"}
{"level":"DEBUG","ts":"2025-11-28T21:22:23.423Z","msg":"Found VA in map: variant=ms-inference-scheduling-llm-d-modelservice-decode, hasCurrentAlloc=true, accelerator=H100"}
{"level":"INFO","ts":"2025-11-28T21:22:23.423Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 3, desired-replicas: 3, accelerator: H100"}
{"level":"INFO","ts":"2025-11-28T21:22:23.423Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=3, accelerator=H100, capacityOnly=true"}
{"level":"INFO","ts":"2025-11-28T21:22:23.428Z","msg":"Applied capacity decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=3, target=3, reason=capacity-only mode: no-change"}
{"level":"INFO","ts":"2025-11-28T21:22:23.428Z","msg":"Reconciliation completed successfully: mode=capacity-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-11-28T21:22:53.412Z","msg":"Operating in CAPACITY-ONLY mode: reactive capacity-based scaling only"}
{"level":"INFO","ts":"2025-11-28T21:22:53.412Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-11-28T21:22:53.412Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-11-28T21:22:53.431Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=3, accelerator=H100, ttft=33.77ms, itl=18.55ms, cost=30.00, arrivalRate=3737.56"}
{"level":"INFO","ts":"2025-11-28T21:22:53.438Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9lshk5, queueLength=0"}
{"level":"INFO","ts":"2025-11-28T21:22:53.439Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9ww5mn, queueLength=0"}
{"level":"INFO","ts":"2025-11-28T21:22:53.439Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9l76md, queueLength=0"}
{"level":"DEBUG","ts":"2025-11-28T21:22:53.439Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=3"}
{"level":"INFO","ts":"2025-11-28T21:22:53.439Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9lshk5, usage=0.209 (20.9%)"}
{"level":"INFO","ts":"2025-11-28T21:22:53.439Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9ww5mn, usage=0.331 (33.1%)"}
{"level":"INFO","ts":"2025-11-28T21:22:53.439Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9l76md, usage=0.229 (22.9%)"}
{"level":"DEBUG","ts":"2025-11-28T21:22:53.439Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=3"}
{"level":"DEBUG","ts":"2025-11-28T21:22:53.441Z","msg":"Pod-to-variant matching successful: totalPods=3, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:3]"}
{"level":"DEBUG","ts":"2025-11-28T21:22:53.441Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=3"}
{"level":"DEBUG","ts":"2025-11-28T21:22:53.441Z","msg":"Collected capacity metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, metricsCount=3"}
{"level":"DEBUG","ts":"2025-11-28T21:22:53.441Z","msg":"Variant analysis initialized: variant=ms-inference-scheduling-llm-d-modelservice-decode, accelerator=H100, cost=10.00, replicaCount=3"}
{"level":"DEBUG","ts":"2025-11-28T21:22:53.441Z","msg":"Scale-down unsafe: insufficient headroom after redistribution: remainingSpareKv=0.116, kvTrigger=0.200, kvSafe=false, remainingSpareQueue=5.0, queueTrigger=3.0%!(EXTRA bool=true)"}
{"level":"DEBUG","ts":"2025-11-28T21:22:53.441Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, totalReplicas=3, nonSaturated=3, avgSpareKv=0.244, avgSpareQueue=5.0, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"INFO","ts":"2025-11-28T21:22:53.441Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, totalReplicas=3, nonSaturated=3, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"DEBUG","ts":"2025-11-28T21:22:53.441Z","msg":"Capacity targets: no scaling needed (avgSpareKv=0.244, avgSpareQueue=5.0, all variants stable)"}
{"level":"DEBUG","ts":"2025-11-28T21:22:53.441Z","msg":"Capacity targets calculated: modelID=unsloth/Meta-Llama-3.1-8B, targets=map[ms-inference-scheduling-llm-d-modelservice-decode:3]"}
{"level":"INFO","ts":"2025-11-28T21:22:53.441Z","msg":"Capacity-only decisions made for model: unsloth/Meta-Llama-3.1-8B - decision count: 1"}
{"level":"INFO","ts":"2025-11-28T21:22:53.441Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-11-28T21:22:53.441Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=3→target=3"}
{"level":"DEBUG","ts":"2025-11-28T21:22:53.441Z","msg":"Found VA in map: variant=ms-inference-scheduling-llm-d-modelservice-decode, hasCurrentAlloc=true, accelerator=H100"}
{"level":"INFO","ts":"2025-11-28T21:22:53.441Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 3, desired-replicas: 3, accelerator: H100"}
{"level":"INFO","ts":"2025-11-28T21:22:53.441Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=3, accelerator=H100, capacityOnly=true"}
{"level":"INFO","ts":"2025-11-28T21:22:53.449Z","msg":"Applied capacity decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=3, target=3, reason=capacity-only mode: no-change"}
{"level":"INFO","ts":"2025-11-28T21:22:53.449Z","msg":"Reconciliation completed successfully: mode=capacity-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-11-28T21:22:53.449Z","msg":"Operating in CAPACITY-ONLY mode: reactive capacity-based scaling only"}
{"level":"INFO","ts":"2025-11-28T21:22:53.449Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-11-28T21:22:53.449Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-11-28T21:22:53.461Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=3, accelerator=H100, ttft=33.77ms, itl=18.55ms, cost=30.00, arrivalRate=3737.56"}
{"level":"INFO","ts":"2025-11-28T21:22:53.463Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9lshk5, usage=0.209 (20.9%)"}
{"level":"INFO","ts":"2025-11-28T21:22:53.463Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9ww5mn, usage=0.331 (33.1%)"}
{"level":"INFO","ts":"2025-11-28T21:22:53.463Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9l76md, usage=0.229 (22.9%)"}
{"level":"DEBUG","ts":"2025-11-28T21:22:53.463Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=3"}
{"level":"INFO","ts":"2025-11-28T21:22:53.463Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9lshk5, queueLength=0"}
{"level":"INFO","ts":"2025-11-28T21:22:53.463Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9ww5mn, queueLength=0"}
{"level":"INFO","ts":"2025-11-28T21:22:53.463Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9l76md, queueLength=0"}
{"level":"DEBUG","ts":"2025-11-28T21:22:53.463Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=3"}
{"level":"DEBUG","ts":"2025-11-28T21:22:53.463Z","msg":"Pod-to-variant matching successful: totalPods=3, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:3]"}
{"level":"DEBUG","ts":"2025-11-28T21:22:53.463Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=3"}
{"level":"DEBUG","ts":"2025-11-28T21:22:53.463Z","msg":"Collected capacity metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, metricsCount=3"}
{"level":"DEBUG","ts":"2025-11-28T21:22:53.463Z","msg":"Variant analysis initialized: variant=ms-inference-scheduling-llm-d-modelservice-decode, accelerator=H100, cost=10.00, replicaCount=3"}
{"level":"DEBUG","ts":"2025-11-28T21:22:53.463Z","msg":"Scale-down unsafe: insufficient headroom after redistribution: remainingSpareKv=0.116, kvTrigger=0.200, kvSafe=false, remainingSpareQueue=5.0, queueTrigger=3.0%!(EXTRA bool=true)"}
{"level":"DEBUG","ts":"2025-11-28T21:22:53.463Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, totalReplicas=3, nonSaturated=3, avgSpareKv=0.244, avgSpareQueue=5.0, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"INFO","ts":"2025-11-28T21:22:53.463Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, totalReplicas=3, nonSaturated=3, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"DEBUG","ts":"2025-11-28T21:22:53.463Z","msg":"Capacity targets: no scaling needed (avgSpareKv=0.244, avgSpareQueue=5.0, all variants stable)"}
{"level":"DEBUG","ts":"2025-11-28T21:22:53.463Z","msg":"Capacity targets calculated: modelID=unsloth/Meta-Llama-3.1-8B, targets=map[ms-inference-scheduling-llm-d-modelservice-decode:3]"}
{"level":"INFO","ts":"2025-11-28T21:22:53.463Z","msg":"Capacity-only decisions made for model: unsloth/Meta-Llama-3.1-8B - decision count: 1"}
{"level":"INFO","ts":"2025-11-28T21:22:53.463Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-11-28T21:22:53.463Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=3→target=3"}
{"level":"DEBUG","ts":"2025-11-28T21:22:53.463Z","msg":"Found VA in map: variant=ms-inference-scheduling-llm-d-modelservice-decode, hasCurrentAlloc=true, accelerator=H100"}
{"level":"INFO","ts":"2025-11-28T21:22:53.463Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 3, desired-replicas: 3, accelerator: H100"}
{"level":"INFO","ts":"2025-11-28T21:22:53.463Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=3, accelerator=H100, capacityOnly=true"}
{"level":"INFO","ts":"2025-11-28T21:22:53.467Z","msg":"Applied capacity decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=3, target=3, reason=capacity-only mode: no-change"}
{"level":"INFO","ts":"2025-11-28T21:22:53.467Z","msg":"Reconciliation completed successfully: mode=capacity-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-11-28T21:23:23.450Z","msg":"Operating in CAPACITY-ONLY mode: reactive capacity-based scaling only"}
{"level":"INFO","ts":"2025-11-28T21:23:23.450Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-11-28T21:23:23.450Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-11-28T21:23:23.479Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=3, accelerator=H100, ttft=35.76ms, itl=19.44ms, cost=30.00, arrivalRate=3855.73"}
{"level":"INFO","ts":"2025-11-28T21:23:23.481Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9lshk5, queueLength=0"}
{"level":"INFO","ts":"2025-11-28T21:23:23.481Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9ww5mn, queueLength=0"}
{"level":"INFO","ts":"2025-11-28T21:23:23.481Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9l76md, queueLength=0"}
{"level":"DEBUG","ts":"2025-11-28T21:23:23.481Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=3"}
{"level":"INFO","ts":"2025-11-28T21:23:23.481Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9lshk5, usage=0.247 (24.7%)"}
{"level":"INFO","ts":"2025-11-28T21:23:23.481Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9ww5mn, usage=0.333 (33.3%)"}
{"level":"INFO","ts":"2025-11-28T21:23:23.481Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9l76md, usage=0.257 (25.7%)"}
{"level":"DEBUG","ts":"2025-11-28T21:23:23.481Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=3"}
{"level":"DEBUG","ts":"2025-11-28T21:23:23.481Z","msg":"Pod-to-variant matching successful: totalPods=3, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:3]"}
{"level":"DEBUG","ts":"2025-11-28T21:23:23.481Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=3"}
{"level":"DEBUG","ts":"2025-11-28T21:23:23.481Z","msg":"Collected capacity metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, metricsCount=3"}
{"level":"DEBUG","ts":"2025-11-28T21:23:23.481Z","msg":"Variant analysis initialized: variant=ms-inference-scheduling-llm-d-modelservice-decode, accelerator=H100, cost=10.00, replicaCount=3"}
{"level":"DEBUG","ts":"2025-11-28T21:23:23.481Z","msg":"Scale-down unsafe: insufficient headroom after redistribution: remainingSpareKv=0.082, kvTrigger=0.200, kvSafe=false, remainingSpareQueue=5.0, queueTrigger=3.0%!(EXTRA bool=true)"}
{"level":"DEBUG","ts":"2025-11-28T21:23:23.481Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, totalReplicas=3, nonSaturated=3, avgSpareKv=0.221, avgSpareQueue=5.0, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"INFO","ts":"2025-11-28T21:23:23.481Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, totalReplicas=3, nonSaturated=3, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"DEBUG","ts":"2025-11-28T21:23:23.481Z","msg":"Capacity targets: no scaling needed (avgSpareKv=0.221, avgSpareQueue=5.0, all variants stable)"}
{"level":"DEBUG","ts":"2025-11-28T21:23:23.481Z","msg":"Capacity targets calculated: modelID=unsloth/Meta-Llama-3.1-8B, targets=map[ms-inference-scheduling-llm-d-modelservice-decode:3]"}
{"level":"INFO","ts":"2025-11-28T21:23:23.481Z","msg":"Capacity-only decisions made for model: unsloth/Meta-Llama-3.1-8B - decision count: 1"}
{"level":"INFO","ts":"2025-11-28T21:23:23.481Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-11-28T21:23:23.481Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=3→target=3"}
{"level":"DEBUG","ts":"2025-11-28T21:23:23.481Z","msg":"Found VA in map: variant=ms-inference-scheduling-llm-d-modelservice-decode, hasCurrentAlloc=true, accelerator=H100"}
{"level":"INFO","ts":"2025-11-28T21:23:23.481Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 3, desired-replicas: 3, accelerator: H100"}
{"level":"INFO","ts":"2025-11-28T21:23:23.481Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=3, accelerator=H100, capacityOnly=true"}
{"level":"INFO","ts":"2025-11-28T21:23:23.486Z","msg":"Applied capacity decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=3, target=3, reason=capacity-only mode: no-change"}
{"level":"INFO","ts":"2025-11-28T21:23:23.487Z","msg":"Reconciliation completed successfully: mode=capacity-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-11-28T21:23:23.487Z","msg":"Operating in CAPACITY-ONLY mode: reactive capacity-based scaling only"}
{"level":"INFO","ts":"2025-11-28T21:23:23.487Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-11-28T21:23:23.487Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-11-28T21:23:23.496Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=3, accelerator=H100, ttft=35.76ms, itl=19.44ms, cost=30.00, arrivalRate=3855.73"}
{"level":"INFO","ts":"2025-11-28T21:23:23.497Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9lshk5, queueLength=0"}
{"level":"INFO","ts":"2025-11-28T21:23:23.497Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9ww5mn, queueLength=0"}
{"level":"INFO","ts":"2025-11-28T21:23:23.497Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9l76md, queueLength=0"}
{"level":"DEBUG","ts":"2025-11-28T21:23:23.497Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=3"}
{"level":"INFO","ts":"2025-11-28T21:23:23.497Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9lshk5, usage=0.247 (24.7%)"}
{"level":"INFO","ts":"2025-11-28T21:23:23.497Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9ww5mn, usage=0.333 (33.3%)"}
{"level":"INFO","ts":"2025-11-28T21:23:23.497Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9l76md, usage=0.257 (25.7%)"}
{"level":"DEBUG","ts":"2025-11-28T21:23:23.497Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=3"}
{"level":"DEBUG","ts":"2025-11-28T21:23:23.497Z","msg":"Pod-to-variant matching successful: totalPods=3, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:3]"}
{"level":"DEBUG","ts":"2025-11-28T21:23:23.497Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=3"}
{"level":"DEBUG","ts":"2025-11-28T21:23:23.497Z","msg":"Collected capacity metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, metricsCount=3"}
{"level":"DEBUG","ts":"2025-11-28T21:23:23.497Z","msg":"Variant analysis initialized: variant=ms-inference-scheduling-llm-d-modelservice-decode, accelerator=H100, cost=10.00, replicaCount=3"}
{"level":"DEBUG","ts":"2025-11-28T21:23:23.497Z","msg":"Scale-down unsafe: insufficient headroom after redistribution: remainingSpareKv=0.082, kvTrigger=0.200, kvSafe=false, remainingSpareQueue=5.0, queueTrigger=3.0%!(EXTRA bool=true)"}
{"level":"DEBUG","ts":"2025-11-28T21:23:23.497Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, totalReplicas=3, nonSaturated=3, avgSpareKv=0.221, avgSpareQueue=5.0, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"INFO","ts":"2025-11-28T21:23:23.497Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, totalReplicas=3, nonSaturated=3, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"DEBUG","ts":"2025-11-28T21:23:23.497Z","msg":"Capacity targets: no scaling needed (avgSpareKv=0.221, avgSpareQueue=5.0, all variants stable)"}
{"level":"DEBUG","ts":"2025-11-28T21:23:23.497Z","msg":"Capacity targets calculated: modelID=unsloth/Meta-Llama-3.1-8B, targets=map[ms-inference-scheduling-llm-d-modelservice-decode:3]"}
{"level":"INFO","ts":"2025-11-28T21:23:23.497Z","msg":"Capacity-only decisions made for model: unsloth/Meta-Llama-3.1-8B - decision count: 1"}
{"level":"INFO","ts":"2025-11-28T21:23:23.497Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-11-28T21:23:23.497Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=3→target=3"}
{"level":"DEBUG","ts":"2025-11-28T21:23:23.497Z","msg":"Found VA in map: variant=ms-inference-scheduling-llm-d-modelservice-decode, hasCurrentAlloc=true, accelerator=H100"}
{"level":"INFO","ts":"2025-11-28T21:23:23.497Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 3, desired-replicas: 3, accelerator: H100"}
{"level":"INFO","ts":"2025-11-28T21:23:23.497Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=3, accelerator=H100, capacityOnly=true"}
{"level":"INFO","ts":"2025-11-28T21:23:23.502Z","msg":"Applied capacity decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=3, target=3, reason=capacity-only mode: no-change"}
{"level":"INFO","ts":"2025-11-28T21:23:23.502Z","msg":"Reconciliation completed successfully: mode=capacity-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-11-28T21:23:53.487Z","msg":"Operating in CAPACITY-ONLY mode: reactive capacity-based scaling only"}
{"level":"INFO","ts":"2025-11-28T21:23:53.488Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-11-28T21:23:53.488Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-11-28T21:23:53.507Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=3, accelerator=H100, ttft=37.96ms, itl=19.83ms, cost=30.00, arrivalRate=3970.01"}
{"level":"INFO","ts":"2025-11-28T21:23:53.509Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9lshk5, queueLength=0"}
{"level":"INFO","ts":"2025-11-28T21:23:53.509Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9ww5mn, queueLength=0"}
{"level":"INFO","ts":"2025-11-28T21:23:53.509Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9l76md, queueLength=0"}
{"level":"DEBUG","ts":"2025-11-28T21:23:53.509Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=3"}
{"level":"INFO","ts":"2025-11-28T21:23:53.509Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9lshk5, usage=0.295 (29.5%)"}
{"level":"INFO","ts":"2025-11-28T21:23:53.509Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9ww5mn, usage=0.335 (33.5%)"}
{"level":"INFO","ts":"2025-11-28T21:23:53.509Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9l76md, usage=0.257 (25.7%)"}
{"level":"DEBUG","ts":"2025-11-28T21:23:53.509Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=3"}
{"level":"DEBUG","ts":"2025-11-28T21:23:53.509Z","msg":"Pod-to-variant matching successful: totalPods=3, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:3]"}
{"level":"DEBUG","ts":"2025-11-28T21:23:53.509Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=3"}
{"level":"DEBUG","ts":"2025-11-28T21:23:53.509Z","msg":"Collected capacity metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, metricsCount=3"}
{"level":"DEBUG","ts":"2025-11-28T21:23:53.509Z","msg":"Variant analysis initialized: variant=ms-inference-scheduling-llm-d-modelservice-decode, accelerator=H100, cost=10.00, replicaCount=3"}
{"level":"DEBUG","ts":"2025-11-28T21:23:53.509Z","msg":"Scale-down unsafe: insufficient headroom after redistribution: remainingSpareKv=0.057, kvTrigger=0.200, kvSafe=false, remainingSpareQueue=5.0, queueTrigger=3.0%!(EXTRA bool=true)"}
{"level":"DEBUG","ts":"2025-11-28T21:23:53.509Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, totalReplicas=3, nonSaturated=3, avgSpareKv=0.205, avgSpareQueue=5.0, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"INFO","ts":"2025-11-28T21:23:53.509Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, totalReplicas=3, nonSaturated=3, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"DEBUG","ts":"2025-11-28T21:23:53.509Z","msg":"Capacity targets: no scaling needed (avgSpareKv=0.205, avgSpareQueue=5.0, all variants stable)"}
{"level":"DEBUG","ts":"2025-11-28T21:23:53.509Z","msg":"Capacity targets calculated: modelID=unsloth/Meta-Llama-3.1-8B, targets=map[ms-inference-scheduling-llm-d-modelservice-decode:3]"}
{"level":"INFO","ts":"2025-11-28T21:23:53.509Z","msg":"Capacity-only decisions made for model: unsloth/Meta-Llama-3.1-8B - decision count: 1"}
{"level":"INFO","ts":"2025-11-28T21:23:53.509Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-11-28T21:23:53.509Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=3→target=3"}
{"level":"DEBUG","ts":"2025-11-28T21:23:53.509Z","msg":"Found VA in map: variant=ms-inference-scheduling-llm-d-modelservice-decode, hasCurrentAlloc=true, accelerator=H100"}
{"level":"INFO","ts":"2025-11-28T21:23:53.509Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 3, desired-replicas: 3, accelerator: H100"}
{"level":"INFO","ts":"2025-11-28T21:23:53.509Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=3, accelerator=H100, capacityOnly=true"}
{"level":"INFO","ts":"2025-11-28T21:23:53.516Z","msg":"Applied capacity decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=3, target=3, reason=capacity-only mode: no-change"}
{"level":"INFO","ts":"2025-11-28T21:23:53.516Z","msg":"Reconciliation completed successfully: mode=capacity-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-11-28T21:23:53.516Z","msg":"Operating in CAPACITY-ONLY mode: reactive capacity-based scaling only"}
{"level":"INFO","ts":"2025-11-28T21:23:53.516Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-11-28T21:23:53.516Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-11-28T21:23:53.528Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=3, accelerator=H100, ttft=37.96ms, itl=19.83ms, cost=30.00, arrivalRate=3970.01"}
{"level":"INFO","ts":"2025-11-28T21:23:53.530Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9lshk5, queueLength=0"}
{"level":"INFO","ts":"2025-11-28T21:23:53.530Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9ww5mn, queueLength=0"}
{"level":"INFO","ts":"2025-11-28T21:23:53.530Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9l76md, queueLength=0"}
{"level":"DEBUG","ts":"2025-11-28T21:23:53.530Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=3"}
{"level":"INFO","ts":"2025-11-28T21:23:53.530Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9lshk5, usage=0.295 (29.5%)"}
{"level":"INFO","ts":"2025-11-28T21:23:53.530Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9ww5mn, usage=0.335 (33.5%)"}
{"level":"INFO","ts":"2025-11-28T21:23:53.530Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9l76md, usage=0.257 (25.7%)"}
{"level":"DEBUG","ts":"2025-11-28T21:23:53.530Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=3"}
{"level":"DEBUG","ts":"2025-11-28T21:23:53.530Z","msg":"Pod-to-variant matching successful: totalPods=3, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:3]"}
{"level":"DEBUG","ts":"2025-11-28T21:23:53.530Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=3"}
{"level":"DEBUG","ts":"2025-11-28T21:23:53.530Z","msg":"Collected capacity metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, metricsCount=3"}
{"level":"DEBUG","ts":"2025-11-28T21:23:53.530Z","msg":"Variant analysis initialized: variant=ms-inference-scheduling-llm-d-modelservice-decode, accelerator=H100, cost=10.00, replicaCount=3"}
{"level":"DEBUG","ts":"2025-11-28T21:23:53.530Z","msg":"Scale-down unsafe: insufficient headroom after redistribution: remainingSpareKv=0.057, kvTrigger=0.200, kvSafe=false, remainingSpareQueue=5.0, queueTrigger=3.0%!(EXTRA bool=true)"}
{"level":"DEBUG","ts":"2025-11-28T21:23:53.530Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, totalReplicas=3, nonSaturated=3, avgSpareKv=0.205, avgSpareQueue=5.0, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"INFO","ts":"2025-11-28T21:23:53.530Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, totalReplicas=3, nonSaturated=3, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"DEBUG","ts":"2025-11-28T21:23:53.530Z","msg":"Capacity targets: no scaling needed (avgSpareKv=0.205, avgSpareQueue=5.0, all variants stable)"}
{"level":"DEBUG","ts":"2025-11-28T21:23:53.530Z","msg":"Capacity targets calculated: modelID=unsloth/Meta-Llama-3.1-8B, targets=map[ms-inference-scheduling-llm-d-modelservice-decode:3]"}
{"level":"INFO","ts":"2025-11-28T21:23:53.530Z","msg":"Capacity-only decisions made for model: unsloth/Meta-Llama-3.1-8B - decision count: 1"}
{"level":"INFO","ts":"2025-11-28T21:23:53.530Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-11-28T21:23:53.530Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=3→target=3"}
{"level":"DEBUG","ts":"2025-11-28T21:23:53.530Z","msg":"Found VA in map: variant=ms-inference-scheduling-llm-d-modelservice-decode, hasCurrentAlloc=true, accelerator=H100"}
{"level":"INFO","ts":"2025-11-28T21:23:53.530Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 3, desired-replicas: 3, accelerator: H100"}
{"level":"INFO","ts":"2025-11-28T21:23:53.530Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=3, accelerator=H100, capacityOnly=true"}
{"level":"INFO","ts":"2025-11-28T21:23:53.534Z","msg":"Applied capacity decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=3, target=3, reason=capacity-only mode: no-change"}
{"level":"INFO","ts":"2025-11-28T21:23:53.534Z","msg":"Reconciliation completed successfully: mode=capacity-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-11-28T21:24:23.516Z","msg":"Operating in CAPACITY-ONLY mode: reactive capacity-based scaling only"}
{"level":"INFO","ts":"2025-11-28T21:24:23.516Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-11-28T21:24:23.516Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-11-28T21:24:23.529Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=3, accelerator=H100, ttft=39.63ms, itl=19.81ms, cost=30.00, arrivalRate=2327.85"}
{"level":"INFO","ts":"2025-11-28T21:24:23.530Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9lshk5, queueLength=0"}
{"level":"INFO","ts":"2025-11-28T21:24:23.530Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9ww5mn, queueLength=0"}
{"level":"INFO","ts":"2025-11-28T21:24:23.530Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9l76md, queueLength=0"}
{"level":"DEBUG","ts":"2025-11-28T21:24:23.530Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=3"}
{"level":"INFO","ts":"2025-11-28T21:24:23.531Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9lshk5, usage=0.295 (29.5%)"}
{"level":"INFO","ts":"2025-11-28T21:24:23.531Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9ww5mn, usage=0.329 (32.9%)"}
{"level":"INFO","ts":"2025-11-28T21:24:23.531Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9l76md, usage=0.196 (19.6%)"}
{"level":"DEBUG","ts":"2025-11-28T21:24:23.531Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=3"}
{"level":"DEBUG","ts":"2025-11-28T21:24:23.531Z","msg":"Pod-to-variant matching successful: totalPods=3, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:3]"}
{"level":"DEBUG","ts":"2025-11-28T21:24:23.531Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=3"}
{"level":"DEBUG","ts":"2025-11-28T21:24:23.531Z","msg":"Collected capacity metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, metricsCount=3"}
{"level":"DEBUG","ts":"2025-11-28T21:24:23.531Z","msg":"Variant analysis initialized: variant=ms-inference-scheduling-llm-d-modelservice-decode, accelerator=H100, cost=10.00, replicaCount=3"}
{"level":"DEBUG","ts":"2025-11-28T21:24:23.531Z","msg":"Scale-down unsafe: insufficient headroom after redistribution: remainingSpareKv=0.090, kvTrigger=0.200, kvSafe=false, remainingSpareQueue=5.0, queueTrigger=3.0%!(EXTRA bool=true)"}
{"level":"DEBUG","ts":"2025-11-28T21:24:23.531Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, totalReplicas=3, nonSaturated=3, avgSpareKv=0.227, avgSpareQueue=5.0, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"INFO","ts":"2025-11-28T21:24:23.531Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, totalReplicas=3, nonSaturated=3, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"DEBUG","ts":"2025-11-28T21:24:23.531Z","msg":"Capacity targets: no scaling needed (avgSpareKv=0.227, avgSpareQueue=5.0, all variants stable)"}
{"level":"DEBUG","ts":"2025-11-28T21:24:23.531Z","msg":"Capacity targets calculated: modelID=unsloth/Meta-Llama-3.1-8B, targets=map[ms-inference-scheduling-llm-d-modelservice-decode:3]"}
{"level":"INFO","ts":"2025-11-28T21:24:23.531Z","msg":"Capacity-only decisions made for model: unsloth/Meta-Llama-3.1-8B - decision count: 1"}
{"level":"INFO","ts":"2025-11-28T21:24:23.531Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-11-28T21:24:23.531Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=3→target=3"}
{"level":"DEBUG","ts":"2025-11-28T21:24:23.531Z","msg":"Found VA in map: variant=ms-inference-scheduling-llm-d-modelservice-decode, hasCurrentAlloc=true, accelerator=H100"}
{"level":"INFO","ts":"2025-11-28T21:24:23.531Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 3, desired-replicas: 3, accelerator: H100"}
{"level":"INFO","ts":"2025-11-28T21:24:23.531Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=3, accelerator=H100, capacityOnly=true"}
{"level":"INFO","ts":"2025-11-28T21:24:23.536Z","msg":"Applied capacity decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=3, target=3, reason=capacity-only mode: no-change"}
{"level":"INFO","ts":"2025-11-28T21:24:23.536Z","msg":"Reconciliation completed successfully: mode=capacity-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-11-28T21:24:23.536Z","msg":"Operating in CAPACITY-ONLY mode: reactive capacity-based scaling only"}
{"level":"INFO","ts":"2025-11-28T21:24:23.536Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-11-28T21:24:23.536Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-11-28T21:24:23.544Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=3, accelerator=H100, ttft=39.63ms, itl=19.81ms, cost=30.00, arrivalRate=2327.85"}
{"level":"INFO","ts":"2025-11-28T21:24:23.546Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9lshk5, queueLength=0"}
{"level":"INFO","ts":"2025-11-28T21:24:23.546Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9ww5mn, queueLength=0"}
{"level":"INFO","ts":"2025-11-28T21:24:23.546Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9l76md, queueLength=0"}
{"level":"DEBUG","ts":"2025-11-28T21:24:23.546Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=3"}
{"level":"INFO","ts":"2025-11-28T21:24:23.546Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9lshk5, usage=0.295 (29.5%)"}
{"level":"INFO","ts":"2025-11-28T21:24:23.546Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9ww5mn, usage=0.329 (32.9%)"}
{"level":"INFO","ts":"2025-11-28T21:24:23.546Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9l76md, usage=0.196 (19.6%)"}
{"level":"DEBUG","ts":"2025-11-28T21:24:23.546Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=3"}
{"level":"DEBUG","ts":"2025-11-28T21:24:23.546Z","msg":"Pod-to-variant matching successful: totalPods=3, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:3]"}
{"level":"DEBUG","ts":"2025-11-28T21:24:23.546Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=3"}
{"level":"DEBUG","ts":"2025-11-28T21:24:23.546Z","msg":"Collected capacity metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, metricsCount=3"}
{"level":"DEBUG","ts":"2025-11-28T21:24:23.546Z","msg":"Variant analysis initialized: variant=ms-inference-scheduling-llm-d-modelservice-decode, accelerator=H100, cost=10.00, replicaCount=3"}
{"level":"DEBUG","ts":"2025-11-28T21:24:23.546Z","msg":"Scale-down unsafe: insufficient headroom after redistribution: remainingSpareKv=0.090, kvTrigger=0.200, kvSafe=false, remainingSpareQueue=5.0, queueTrigger=3.0%!(EXTRA bool=true)"}
{"level":"DEBUG","ts":"2025-11-28T21:24:23.546Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, totalReplicas=3, nonSaturated=3, avgSpareKv=0.227, avgSpareQueue=5.0, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"INFO","ts":"2025-11-28T21:24:23.546Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, totalReplicas=3, nonSaturated=3, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"DEBUG","ts":"2025-11-28T21:24:23.546Z","msg":"Capacity targets: no scaling needed (avgSpareKv=0.227, avgSpareQueue=5.0, all variants stable)"}
{"level":"DEBUG","ts":"2025-11-28T21:24:23.546Z","msg":"Capacity targets calculated: modelID=unsloth/Meta-Llama-3.1-8B, targets=map[ms-inference-scheduling-llm-d-modelservice-decode:3]"}
{"level":"INFO","ts":"2025-11-28T21:24:23.546Z","msg":"Capacity-only decisions made for model: unsloth/Meta-Llama-3.1-8B - decision count: 1"}
{"level":"INFO","ts":"2025-11-28T21:24:23.546Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-11-28T21:24:23.546Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=3→target=3"}
{"level":"DEBUG","ts":"2025-11-28T21:24:23.546Z","msg":"Found VA in map: variant=ms-inference-scheduling-llm-d-modelservice-decode, hasCurrentAlloc=true, accelerator=H100"}
{"level":"INFO","ts":"2025-11-28T21:24:23.546Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 3, desired-replicas: 3, accelerator: H100"}
{"level":"INFO","ts":"2025-11-28T21:24:23.546Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=3, accelerator=H100, capacityOnly=true"}
{"level":"INFO","ts":"2025-11-28T21:24:23.550Z","msg":"Applied capacity decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=3, target=3, reason=capacity-only mode: no-change"}
{"level":"INFO","ts":"2025-11-28T21:24:23.550Z","msg":"Reconciliation completed successfully: mode=capacity-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-11-28T21:24:53.537Z","msg":"Operating in CAPACITY-ONLY mode: reactive capacity-based scaling only"}
{"level":"INFO","ts":"2025-11-28T21:24:53.537Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-11-28T21:24:53.537Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-11-28T21:24:53.550Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=3, accelerator=H100, ttft=34.77ms, itl=18.90ms, cost=30.00, arrivalRate=146.96"}
{"level":"INFO","ts":"2025-11-28T21:24:53.552Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9lshk5, queueLength=0"}
{"level":"INFO","ts":"2025-11-28T21:24:53.552Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9ww5mn, queueLength=0"}
{"level":"INFO","ts":"2025-11-28T21:24:53.552Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9l76md, queueLength=0"}
{"level":"DEBUG","ts":"2025-11-28T21:24:53.552Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=3"}
{"level":"INFO","ts":"2025-11-28T21:24:53.552Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9lshk5, usage=0.000 (0.0%)"}
{"level":"INFO","ts":"2025-11-28T21:24:53.552Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9ww5mn, usage=0.258 (25.8%)"}
{"level":"INFO","ts":"2025-11-28T21:24:53.552Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9l76md, usage=0.000 (0.0%)"}
{"level":"DEBUG","ts":"2025-11-28T21:24:53.552Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=3"}
{"level":"DEBUG","ts":"2025-11-28T21:24:53.552Z","msg":"Pod-to-variant matching successful: totalPods=3, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:3]"}
{"level":"DEBUG","ts":"2025-11-28T21:24:53.552Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=3"}
{"level":"DEBUG","ts":"2025-11-28T21:24:53.552Z","msg":"Collected capacity metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, metricsCount=3"}
{"level":"DEBUG","ts":"2025-11-28T21:24:53.552Z","msg":"Variant analysis initialized: variant=ms-inference-scheduling-llm-d-modelservice-decode, accelerator=H100, cost=10.00, replicaCount=3"}
{"level":"DEBUG","ts":"2025-11-28T21:24:53.552Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, totalReplicas=3, nonSaturated=3, avgSpareKv=0.414, avgSpareQueue=5.0, shouldScaleUp=false, scaleDownSafe=true"}
{"level":"INFO","ts":"2025-11-28T21:24:53.552Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, totalReplicas=3, nonSaturated=3, shouldScaleUp=false, scaleDownSafe=true"}
{"level":"INFO","ts":"2025-11-28T21:24:53.552Z","msg":"Capacity target: scale-down most expensive variant: variant=ms-inference-scheduling-llm-d-modelservice-decode, cost=10.00, currentReplicas=3, readyReplicas=3, target=2"}
{"level":"DEBUG","ts":"2025-11-28T21:24:53.552Z","msg":"Capacity targets calculated: modelID=unsloth/Meta-Llama-3.1-8B, targets=map[ms-inference-scheduling-llm-d-modelservice-decode:2]"}
{"level":"INFO","ts":"2025-11-28T21:24:53.552Z","msg":"Capacity-only decisions made for model: unsloth/Meta-Llama-3.1-8B - decision count: 1"}
{"level":"INFO","ts":"2025-11-28T21:24:53.552Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-11-28T21:24:53.552Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=scale-down, current=3→target=2"}
{"level":"DEBUG","ts":"2025-11-28T21:24:53.552Z","msg":"Found VA in map: variant=ms-inference-scheduling-llm-d-modelservice-decode, hasCurrentAlloc=true, accelerator=H100"}
{"level":"INFO","ts":"2025-11-28T21:24:53.552Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 3, desired-replicas: 2, accelerator: H100"}
{"level":"INFO","ts":"2025-11-28T21:24:53.552Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=2, accelerator=H100, capacityOnly=true"}
{"level":"INFO","ts":"2025-11-28T21:24:53.557Z","msg":"Applied capacity decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=scale-down, current=3, target=2, reason=capacity-only mode: scale-down"}
{"level":"INFO","ts":"2025-11-28T21:24:53.557Z","msg":"Reconciliation completed successfully: mode=capacity-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-11-28T21:24:53.557Z","msg":"Operating in CAPACITY-ONLY mode: reactive capacity-based scaling only"}
{"level":"INFO","ts":"2025-11-28T21:24:53.557Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-11-28T21:24:53.557Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-11-28T21:24:53.566Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=3, accelerator=H100, ttft=34.77ms, itl=18.90ms, cost=30.00, arrivalRate=146.96"}
{"level":"INFO","ts":"2025-11-28T21:24:53.567Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9lshk5, usage=0.000 (0.0%)"}
{"level":"INFO","ts":"2025-11-28T21:24:53.567Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9ww5mn, usage=0.258 (25.8%)"}
{"level":"INFO","ts":"2025-11-28T21:24:53.567Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9l76md, usage=0.000 (0.0%)"}
{"level":"DEBUG","ts":"2025-11-28T21:24:53.567Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=3"}
{"level":"INFO","ts":"2025-11-28T21:24:53.567Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9lshk5, queueLength=0"}
{"level":"INFO","ts":"2025-11-28T21:24:53.567Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9ww5mn, queueLength=0"}
{"level":"INFO","ts":"2025-11-28T21:24:53.567Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9l76md, queueLength=0"}
{"level":"DEBUG","ts":"2025-11-28T21:24:53.567Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=3"}
{"level":"DEBUG","ts":"2025-11-28T21:24:53.568Z","msg":"Pod-to-variant matching successful: totalPods=3, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:3]"}
{"level":"DEBUG","ts":"2025-11-28T21:24:53.568Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=3"}
{"level":"DEBUG","ts":"2025-11-28T21:24:53.568Z","msg":"Collected capacity metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, metricsCount=3"}
{"level":"DEBUG","ts":"2025-11-28T21:24:53.568Z","msg":"Variant analysis initialized: variant=ms-inference-scheduling-llm-d-modelservice-decode, accelerator=H100, cost=10.00, replicaCount=3"}
{"level":"DEBUG","ts":"2025-11-28T21:24:53.568Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, totalReplicas=3, nonSaturated=3, avgSpareKv=0.414, avgSpareQueue=5.0, shouldScaleUp=false, scaleDownSafe=true"}
{"level":"INFO","ts":"2025-11-28T21:24:53.568Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, totalReplicas=3, nonSaturated=3, shouldScaleUp=false, scaleDownSafe=true"}
{"level":"DEBUG","ts":"2025-11-28T21:24:53.568Z","msg":"Preserving desired replicas: variant=ms-inference-scheduling-llm-d-modelservice-decode, currentReplicas=3, readyReplicas=3, desired=2"}
{"level":"DEBUG","ts":"2025-11-28T21:24:53.568Z","msg":"Capacity targets calculated: modelID=unsloth/Meta-Llama-3.1-8B, targets=map[ms-inference-scheduling-llm-d-modelservice-decode:2]"}
{"level":"INFO","ts":"2025-11-28T21:24:53.568Z","msg":"Capacity-only decisions made for model: unsloth/Meta-Llama-3.1-8B - decision count: 1"}
{"level":"INFO","ts":"2025-11-28T21:24:53.568Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-11-28T21:24:53.568Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=scale-down, current=3→target=2"}
{"level":"DEBUG","ts":"2025-11-28T21:24:53.568Z","msg":"Found VA in map: variant=ms-inference-scheduling-llm-d-modelservice-decode, hasCurrentAlloc=true, accelerator=H100"}
{"level":"INFO","ts":"2025-11-28T21:24:53.568Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 3, desired-replicas: 2, accelerator: H100"}
{"level":"INFO","ts":"2025-11-28T21:24:53.568Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=2, accelerator=H100, capacityOnly=true"}
{"level":"INFO","ts":"2025-11-28T21:24:53.572Z","msg":"Applied capacity decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=scale-down, current=3, target=2, reason=capacity-only mode: scale-down"}
{"level":"INFO","ts":"2025-11-28T21:24:53.572Z","msg":"Reconciliation completed successfully: mode=capacity-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-11-28T21:25:23.558Z","msg":"Operating in CAPACITY-ONLY mode: reactive capacity-based scaling only"}
{"level":"INFO","ts":"2025-11-28T21:25:23.559Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-11-28T21:25:23.559Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-11-28T21:25:23.613Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=2, accelerator=H100, ttft=0.00ms, itl=0.00ms, cost=20.00, arrivalRate=0.00"}
{"level":"INFO","ts":"2025-11-28T21:25:23.616Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9lshk5, usage=0.000 (0.0%)"}
{"level":"INFO","ts":"2025-11-28T21:25:23.616Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9ww5mn, usage=0.000 (0.0%)"}
{"level":"INFO","ts":"2025-11-28T21:25:23.616Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9l76md, usage=0.000 (0.0%)"}
{"level":"DEBUG","ts":"2025-11-28T21:25:23.616Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=3"}
{"level":"INFO","ts":"2025-11-28T21:25:23.617Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9lshk5, queueLength=0"}
{"level":"INFO","ts":"2025-11-28T21:25:23.617Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9ww5mn, queueLength=0"}
{"level":"INFO","ts":"2025-11-28T21:25:23.617Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9l76md, queueLength=0"}
{"level":"DEBUG","ts":"2025-11-28T21:25:23.617Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=3"}
{"level":"DEBUG","ts":"2025-11-28T21:25:23.617Z","msg":"Pod-to-variant matching successful: totalPods=3, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:3]"}
{"level":"DEBUG","ts":"2025-11-28T21:25:23.617Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=3"}
{"level":"DEBUG","ts":"2025-11-28T21:25:23.617Z","msg":"Collected capacity metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, metricsCount=3"}
{"level":"DEBUG","ts":"2025-11-28T21:25:23.617Z","msg":"Variant analysis initialized: variant=ms-inference-scheduling-llm-d-modelservice-decode, accelerator=H100, cost=10.00, replicaCount=3"}
{"level":"DEBUG","ts":"2025-11-28T21:25:23.617Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, totalReplicas=3, nonSaturated=3, avgSpareKv=0.500, avgSpareQueue=5.0, shouldScaleUp=false, scaleDownSafe=true"}
{"level":"INFO","ts":"2025-11-28T21:25:23.617Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, totalReplicas=3, nonSaturated=3, shouldScaleUp=false, scaleDownSafe=true"}
{"level":"INFO","ts":"2025-11-28T21:25:23.617Z","msg":"Capacity target: scale-down most expensive variant: variant=ms-inference-scheduling-llm-d-modelservice-decode, cost=10.00, currentReplicas=2, readyReplicas=3, target=2"}
{"level":"DEBUG","ts":"2025-11-28T21:25:23.617Z","msg":"Capacity targets calculated: modelID=unsloth/Meta-Llama-3.1-8B, targets=map[ms-inference-scheduling-llm-d-modelservice-decode:2]"}
{"level":"INFO","ts":"2025-11-28T21:25:23.617Z","msg":"Capacity-only decisions made for model: unsloth/Meta-Llama-3.1-8B - decision count: 1"}
{"level":"INFO","ts":"2025-11-28T21:25:23.617Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-11-28T21:25:23.617Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=2→target=2"}
{"level":"DEBUG","ts":"2025-11-28T21:25:23.617Z","msg":"Found VA in map: variant=ms-inference-scheduling-llm-d-modelservice-decode, hasCurrentAlloc=true, accelerator=H100"}
{"level":"INFO","ts":"2025-11-28T21:25:23.617Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 2, desired-replicas: 2, accelerator: H100"}
{"level":"INFO","ts":"2025-11-28T21:25:23.617Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=2, accelerator=H100, capacityOnly=true"}
{"level":"INFO","ts":"2025-11-28T21:25:23.622Z","msg":"Applied capacity decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=2, target=2, reason=capacity-only mode: no-change"}
{"level":"INFO","ts":"2025-11-28T21:25:23.622Z","msg":"Reconciliation completed successfully: mode=capacity-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-11-28T21:25:23.622Z","msg":"Operating in CAPACITY-ONLY mode: reactive capacity-based scaling only"}
{"level":"INFO","ts":"2025-11-28T21:25:23.622Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-11-28T21:25:23.622Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-11-28T21:25:23.633Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=2, accelerator=H100, ttft=0.00ms, itl=0.00ms, cost=20.00, arrivalRate=0.00"}
{"level":"INFO","ts":"2025-11-28T21:25:23.634Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9lshk5, queueLength=0"}
{"level":"INFO","ts":"2025-11-28T21:25:23.634Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9ww5mn, queueLength=0"}
{"level":"INFO","ts":"2025-11-28T21:25:23.634Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9l76md, queueLength=0"}
{"level":"DEBUG","ts":"2025-11-28T21:25:23.634Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=3"}
{"level":"INFO","ts":"2025-11-28T21:25:23.634Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9lshk5, usage=0.000 (0.0%)"}
{"level":"INFO","ts":"2025-11-28T21:25:23.634Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9ww5mn, usage=0.000 (0.0%)"}
{"level":"INFO","ts":"2025-11-28T21:25:23.634Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9l76md, usage=0.000 (0.0%)"}
{"level":"DEBUG","ts":"2025-11-28T21:25:23.634Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=3"}
{"level":"DEBUG","ts":"2025-11-28T21:25:23.634Z","msg":"Pod-to-variant matching successful: totalPods=3, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:3]"}
{"level":"DEBUG","ts":"2025-11-28T21:25:23.634Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=3"}
{"level":"DEBUG","ts":"2025-11-28T21:25:23.634Z","msg":"Collected capacity metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, metricsCount=3"}
{"level":"DEBUG","ts":"2025-11-28T21:25:23.634Z","msg":"Variant analysis initialized: variant=ms-inference-scheduling-llm-d-modelservice-decode, accelerator=H100, cost=10.00, replicaCount=3"}
{"level":"DEBUG","ts":"2025-11-28T21:25:23.634Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, totalReplicas=3, nonSaturated=3, avgSpareKv=0.500, avgSpareQueue=5.0, shouldScaleUp=false, scaleDownSafe=true"}
{"level":"INFO","ts":"2025-11-28T21:25:23.634Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, totalReplicas=3, nonSaturated=3, shouldScaleUp=false, scaleDownSafe=true"}
{"level":"INFO","ts":"2025-11-28T21:25:23.634Z","msg":"Capacity target: scale-down most expensive variant: variant=ms-inference-scheduling-llm-d-modelservice-decode, cost=10.00, currentReplicas=2, readyReplicas=3, target=2"}
{"level":"DEBUG","ts":"2025-11-28T21:25:23.634Z","msg":"Capacity targets calculated: modelID=unsloth/Meta-Llama-3.1-8B, targets=map[ms-inference-scheduling-llm-d-modelservice-decode:2]"}
{"level":"INFO","ts":"2025-11-28T21:25:23.634Z","msg":"Capacity-only decisions made for model: unsloth/Meta-Llama-3.1-8B - decision count: 1"}
{"level":"INFO","ts":"2025-11-28T21:25:23.634Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-11-28T21:25:23.634Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=2→target=2"}
{"level":"DEBUG","ts":"2025-11-28T21:25:23.634Z","msg":"Found VA in map: variant=ms-inference-scheduling-llm-d-modelservice-decode, hasCurrentAlloc=true, accelerator=H100"}
{"level":"INFO","ts":"2025-11-28T21:25:23.635Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 2, desired-replicas: 2, accelerator: H100"}
{"level":"INFO","ts":"2025-11-28T21:25:23.635Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=2, accelerator=H100, capacityOnly=true"}
{"level":"INFO","ts":"2025-11-28T21:25:23.640Z","msg":"Applied capacity decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=2, target=2, reason=capacity-only mode: no-change"}
{"level":"INFO","ts":"2025-11-28T21:25:23.640Z","msg":"Reconciliation completed successfully: mode=capacity-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-11-28T21:25:53.634Z","msg":"Operating in CAPACITY-ONLY mode: reactive capacity-based scaling only"}
{"level":"INFO","ts":"2025-11-28T21:25:53.634Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-11-28T21:25:53.634Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-11-28T21:25:53.648Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=2, accelerator=H100, ttft=0.00ms, itl=0.00ms, cost=20.00, arrivalRate=0.00"}
{"level":"INFO","ts":"2025-11-28T21:25:53.650Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9lshk5, queueLength=0"}
{"level":"INFO","ts":"2025-11-28T21:25:53.650Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9ww5mn, queueLength=0"}
{"level":"INFO","ts":"2025-11-28T21:25:53.650Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9l76md, queueLength=0"}
{"level":"DEBUG","ts":"2025-11-28T21:25:53.650Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=3"}
{"level":"INFO","ts":"2025-11-28T21:25:53.650Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9lshk5, usage=0.000 (0.0%)"}
{"level":"INFO","ts":"2025-11-28T21:25:53.650Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9ww5mn, usage=0.000 (0.0%)"}
{"level":"INFO","ts":"2025-11-28T21:25:53.650Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9l76md, usage=0.000 (0.0%)"}
{"level":"DEBUG","ts":"2025-11-28T21:25:53.650Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=3"}
{"level":"DEBUG","ts":"2025-11-28T21:25:53.650Z","msg":"Pod-to-variant matching successful: totalPods=3, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:3]"}
{"level":"DEBUG","ts":"2025-11-28T21:25:53.650Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=3"}
{"level":"DEBUG","ts":"2025-11-28T21:25:53.650Z","msg":"Collected capacity metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, metricsCount=3"}
{"level":"DEBUG","ts":"2025-11-28T21:25:53.650Z","msg":"Variant analysis initialized: variant=ms-inference-scheduling-llm-d-modelservice-decode, accelerator=H100, cost=10.00, replicaCount=3"}
{"level":"DEBUG","ts":"2025-11-28T21:25:53.650Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, totalReplicas=3, nonSaturated=3, avgSpareKv=0.500, avgSpareQueue=5.0, shouldScaleUp=false, scaleDownSafe=true"}
{"level":"INFO","ts":"2025-11-28T21:25:53.650Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, totalReplicas=3, nonSaturated=3, shouldScaleUp=false, scaleDownSafe=true"}
{"level":"INFO","ts":"2025-11-28T21:25:53.650Z","msg":"Capacity target: scale-down most expensive variant: variant=ms-inference-scheduling-llm-d-modelservice-decode, cost=10.00, currentReplicas=2, readyReplicas=3, target=2"}
{"level":"DEBUG","ts":"2025-11-28T21:25:53.650Z","msg":"Capacity targets calculated: modelID=unsloth/Meta-Llama-3.1-8B, targets=map[ms-inference-scheduling-llm-d-modelservice-decode:2]"}
{"level":"INFO","ts":"2025-11-28T21:25:53.650Z","msg":"Capacity-only decisions made for model: unsloth/Meta-Llama-3.1-8B - decision count: 1"}
{"level":"INFO","ts":"2025-11-28T21:25:53.650Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-11-28T21:25:53.650Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=2→target=2"}
{"level":"DEBUG","ts":"2025-11-28T21:25:53.650Z","msg":"Found VA in map: variant=ms-inference-scheduling-llm-d-modelservice-decode, hasCurrentAlloc=true, accelerator=H100"}
{"level":"INFO","ts":"2025-11-28T21:25:53.650Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 2, desired-replicas: 2, accelerator: H100"}
{"level":"INFO","ts":"2025-11-28T21:25:53.650Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=2, accelerator=H100, capacityOnly=true"}
{"level":"INFO","ts":"2025-11-28T21:25:53.656Z","msg":"Applied capacity decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=2, target=2, reason=capacity-only mode: no-change"}
{"level":"INFO","ts":"2025-11-28T21:25:53.656Z","msg":"Reconciliation completed successfully: mode=capacity-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-11-28T21:25:53.656Z","msg":"Operating in CAPACITY-ONLY mode: reactive capacity-based scaling only"}
{"level":"INFO","ts":"2025-11-28T21:25:53.656Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-11-28T21:25:53.656Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-11-28T21:25:53.665Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=2, accelerator=H100, ttft=0.00ms, itl=0.00ms, cost=20.00, arrivalRate=0.00"}
{"level":"INFO","ts":"2025-11-28T21:25:53.666Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9lshk5, usage=0.000 (0.0%)"}
{"level":"INFO","ts":"2025-11-28T21:25:53.666Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9ww5mn, usage=0.000 (0.0%)"}
{"level":"INFO","ts":"2025-11-28T21:25:53.666Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9l76md, usage=0.000 (0.0%)"}
{"level":"DEBUG","ts":"2025-11-28T21:25:53.666Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=3"}
{"level":"INFO","ts":"2025-11-28T21:25:53.666Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9lshk5, queueLength=0"}
{"level":"INFO","ts":"2025-11-28T21:25:53.666Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9ww5mn, queueLength=0"}
{"level":"INFO","ts":"2025-11-28T21:25:53.666Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9l76md, queueLength=0"}
{"level":"DEBUG","ts":"2025-11-28T21:25:53.666Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=3"}
{"level":"DEBUG","ts":"2025-11-28T21:25:53.666Z","msg":"Pod-to-variant matching successful: totalPods=3, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:3]"}
{"level":"DEBUG","ts":"2025-11-28T21:25:53.666Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=3"}
{"level":"DEBUG","ts":"2025-11-28T21:25:53.666Z","msg":"Collected capacity metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, metricsCount=3"}
{"level":"DEBUG","ts":"2025-11-28T21:25:53.666Z","msg":"Variant analysis initialized: variant=ms-inference-scheduling-llm-d-modelservice-decode, accelerator=H100, cost=10.00, replicaCount=3"}
{"level":"DEBUG","ts":"2025-11-28T21:25:53.666Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, totalReplicas=3, nonSaturated=3, avgSpareKv=0.500, avgSpareQueue=5.0, shouldScaleUp=false, scaleDownSafe=true"}
{"level":"INFO","ts":"2025-11-28T21:25:53.666Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, totalReplicas=3, nonSaturated=3, shouldScaleUp=false, scaleDownSafe=true"}
{"level":"INFO","ts":"2025-11-28T21:25:53.666Z","msg":"Capacity target: scale-down most expensive variant: variant=ms-inference-scheduling-llm-d-modelservice-decode, cost=10.00, currentReplicas=2, readyReplicas=3, target=2"}
{"level":"DEBUG","ts":"2025-11-28T21:25:53.666Z","msg":"Capacity targets calculated: modelID=unsloth/Meta-Llama-3.1-8B, targets=map[ms-inference-scheduling-llm-d-modelservice-decode:2]"}
{"level":"INFO","ts":"2025-11-28T21:25:53.666Z","msg":"Capacity-only decisions made for model: unsloth/Meta-Llama-3.1-8B - decision count: 1"}
{"level":"INFO","ts":"2025-11-28T21:25:53.666Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-11-28T21:25:53.666Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=2→target=2"}
{"level":"DEBUG","ts":"2025-11-28T21:25:53.666Z","msg":"Found VA in map: variant=ms-inference-scheduling-llm-d-modelservice-decode, hasCurrentAlloc=true, accelerator=H100"}
{"level":"INFO","ts":"2025-11-28T21:25:53.666Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 2, desired-replicas: 2, accelerator: H100"}
{"level":"INFO","ts":"2025-11-28T21:25:53.666Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=2, accelerator=H100, capacityOnly=true"}
{"level":"INFO","ts":"2025-11-28T21:25:53.671Z","msg":"Applied capacity decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=2, target=2, reason=capacity-only mode: no-change"}
{"level":"INFO","ts":"2025-11-28T21:25:53.671Z","msg":"Reconciliation completed successfully: mode=capacity-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-11-28T21:26:23.656Z","msg":"Operating in CAPACITY-ONLY mode: reactive capacity-based scaling only"}
{"level":"INFO","ts":"2025-11-28T21:26:23.656Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-11-28T21:26:23.656Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-11-28T21:26:23.670Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=2, accelerator=H100, ttft=0.00ms, itl=0.00ms, cost=20.00, arrivalRate=0.00"}
{"level":"INFO","ts":"2025-11-28T21:26:23.671Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9lshk5, queueLength=0"}
{"level":"INFO","ts":"2025-11-28T21:26:23.671Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9l76md, queueLength=0"}
{"level":"DEBUG","ts":"2025-11-28T21:26:23.671Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=2"}
{"level":"INFO","ts":"2025-11-28T21:26:23.671Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9lshk5, usage=0.000 (0.0%)"}
{"level":"INFO","ts":"2025-11-28T21:26:23.671Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9l76md, usage=0.000 (0.0%)"}
{"level":"DEBUG","ts":"2025-11-28T21:26:23.671Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=2"}
{"level":"DEBUG","ts":"2025-11-28T21:26:23.672Z","msg":"Pod-to-variant matching successful: totalPods=2, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:2]"}
{"level":"DEBUG","ts":"2025-11-28T21:26:23.672Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=2"}
{"level":"DEBUG","ts":"2025-11-28T21:26:23.672Z","msg":"Collected capacity metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, metricsCount=2"}
{"level":"DEBUG","ts":"2025-11-28T21:26:23.672Z","msg":"Variant analysis initialized: variant=ms-inference-scheduling-llm-d-modelservice-decode, accelerator=H100, cost=10.00, replicaCount=2"}
{"level":"DEBUG","ts":"2025-11-28T21:26:23.672Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, totalReplicas=2, nonSaturated=2, avgSpareKv=0.500, avgSpareQueue=5.0, shouldScaleUp=false, scaleDownSafe=true"}
{"level":"INFO","ts":"2025-11-28T21:26:23.672Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, totalReplicas=2, nonSaturated=2, shouldScaleUp=false, scaleDownSafe=true"}
{"level":"INFO","ts":"2025-11-28T21:26:23.672Z","msg":"Capacity target: scale-down most expensive variant: variant=ms-inference-scheduling-llm-d-modelservice-decode, cost=10.00, currentReplicas=2, readyReplicas=2, target=1"}
{"level":"DEBUG","ts":"2025-11-28T21:26:23.672Z","msg":"Capacity targets calculated: modelID=unsloth/Meta-Llama-3.1-8B, targets=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"INFO","ts":"2025-11-28T21:26:23.672Z","msg":"Capacity-only decisions made for model: unsloth/Meta-Llama-3.1-8B - decision count: 1"}
{"level":"INFO","ts":"2025-11-28T21:26:23.672Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-11-28T21:26:23.672Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=scale-down, current=2→target=1"}
{"level":"DEBUG","ts":"2025-11-28T21:26:23.672Z","msg":"Found VA in map: variant=ms-inference-scheduling-llm-d-modelservice-decode, hasCurrentAlloc=true, accelerator=H100"}
{"level":"INFO","ts":"2025-11-28T21:26:23.672Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 2, desired-replicas: 1, accelerator: H100"}
{"level":"INFO","ts":"2025-11-28T21:26:23.672Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=1, accelerator=H100, capacityOnly=true"}
{"level":"INFO","ts":"2025-11-28T21:26:23.677Z","msg":"Applied capacity decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=scale-down, current=2, target=1, reason=capacity-only mode: scale-down"}
{"level":"INFO","ts":"2025-11-28T21:26:23.677Z","msg":"Reconciliation completed successfully: mode=capacity-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-11-28T21:26:23.677Z","msg":"Operating in CAPACITY-ONLY mode: reactive capacity-based scaling only"}
{"level":"INFO","ts":"2025-11-28T21:26:23.677Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-11-28T21:26:23.677Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-11-28T21:26:23.692Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=2, accelerator=H100, ttft=0.00ms, itl=0.00ms, cost=20.00, arrivalRate=0.00"}
{"level":"INFO","ts":"2025-11-28T21:26:23.693Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9lshk5, queueLength=0"}
{"level":"INFO","ts":"2025-11-28T21:26:23.693Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9l76md, queueLength=0"}
{"level":"DEBUG","ts":"2025-11-28T21:26:23.693Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=2"}
{"level":"INFO","ts":"2025-11-28T21:26:23.693Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9lshk5, usage=0.000 (0.0%)"}
{"level":"INFO","ts":"2025-11-28T21:26:23.693Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9l76md, usage=0.000 (0.0%)"}
{"level":"DEBUG","ts":"2025-11-28T21:26:23.693Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=2"}
{"level":"DEBUG","ts":"2025-11-28T21:26:23.694Z","msg":"Pod-to-variant matching successful: totalPods=2, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:2]"}
{"level":"DEBUG","ts":"2025-11-28T21:26:23.694Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=2"}
{"level":"DEBUG","ts":"2025-11-28T21:26:23.694Z","msg":"Collected capacity metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, metricsCount=2"}
{"level":"DEBUG","ts":"2025-11-28T21:26:23.694Z","msg":"Variant analysis initialized: variant=ms-inference-scheduling-llm-d-modelservice-decode, accelerator=H100, cost=10.00, replicaCount=2"}
{"level":"DEBUG","ts":"2025-11-28T21:26:23.694Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, totalReplicas=2, nonSaturated=2, avgSpareKv=0.500, avgSpareQueue=5.0, shouldScaleUp=false, scaleDownSafe=true"}
{"level":"INFO","ts":"2025-11-28T21:26:23.694Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, totalReplicas=2, nonSaturated=2, shouldScaleUp=false, scaleDownSafe=true"}
{"level":"INFO","ts":"2025-11-28T21:26:23.694Z","msg":"Capacity target: scale-down most expensive variant: variant=ms-inference-scheduling-llm-d-modelservice-decode, cost=10.00, currentReplicas=2, readyReplicas=2, target=1"}
{"level":"DEBUG","ts":"2025-11-28T21:26:23.694Z","msg":"Capacity targets calculated: modelID=unsloth/Meta-Llama-3.1-8B, targets=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"INFO","ts":"2025-11-28T21:26:23.694Z","msg":"Capacity-only decisions made for model: unsloth/Meta-Llama-3.1-8B - decision count: 1"}
{"level":"INFO","ts":"2025-11-28T21:26:23.694Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-11-28T21:26:23.694Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=scale-down, current=2→target=1"}
{"level":"DEBUG","ts":"2025-11-28T21:26:23.694Z","msg":"Found VA in map: variant=ms-inference-scheduling-llm-d-modelservice-decode, hasCurrentAlloc=true, accelerator=H100"}
{"level":"INFO","ts":"2025-11-28T21:26:23.694Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 2, desired-replicas: 1, accelerator: H100"}
{"level":"INFO","ts":"2025-11-28T21:26:23.694Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=1, accelerator=H100, capacityOnly=true"}
{"level":"INFO","ts":"2025-11-28T21:26:23.699Z","msg":"Applied capacity decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=scale-down, current=2, target=1, reason=capacity-only mode: scale-down"}
{"level":"INFO","ts":"2025-11-28T21:26:23.699Z","msg":"Reconciliation completed successfully: mode=capacity-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-11-28T21:26:53.677Z","msg":"Operating in CAPACITY-ONLY mode: reactive capacity-based scaling only"}
{"level":"INFO","ts":"2025-11-28T21:26:53.677Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-11-28T21:26:53.677Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-11-28T21:26:53.691Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=1, accelerator=H100, ttft=0.00ms, itl=0.00ms, cost=10.00, arrivalRate=0.00"}
{"level":"INFO","ts":"2025-11-28T21:26:53.693Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9lshk5, queueLength=0"}
{"level":"INFO","ts":"2025-11-28T21:26:53.693Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9l76md, queueLength=0"}
{"level":"DEBUG","ts":"2025-11-28T21:26:53.693Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=2"}
{"level":"INFO","ts":"2025-11-28T21:26:53.693Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9lshk5, usage=0.000 (0.0%)"}
{"level":"INFO","ts":"2025-11-28T21:26:53.693Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9l76md, usage=0.000 (0.0%)"}
{"level":"DEBUG","ts":"2025-11-28T21:26:53.693Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=2"}
{"level":"DEBUG","ts":"2025-11-28T21:26:53.693Z","msg":"Pod-to-variant matching successful: totalPods=2, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:2]"}
{"level":"DEBUG","ts":"2025-11-28T21:26:53.693Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=2"}
{"level":"DEBUG","ts":"2025-11-28T21:26:53.693Z","msg":"Collected capacity metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, metricsCount=2"}
{"level":"DEBUG","ts":"2025-11-28T21:26:53.693Z","msg":"Variant analysis initialized: variant=ms-inference-scheduling-llm-d-modelservice-decode, accelerator=H100, cost=10.00, replicaCount=2"}
{"level":"DEBUG","ts":"2025-11-28T21:26:53.693Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, totalReplicas=2, nonSaturated=2, avgSpareKv=0.500, avgSpareQueue=5.0, shouldScaleUp=false, scaleDownSafe=true"}
{"level":"INFO","ts":"2025-11-28T21:26:53.693Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, totalReplicas=2, nonSaturated=2, shouldScaleUp=false, scaleDownSafe=true"}
{"level":"INFO","ts":"2025-11-28T21:26:53.693Z","msg":"Capacity target: scale-down most expensive variant: variant=ms-inference-scheduling-llm-d-modelservice-decode, cost=10.00, currentReplicas=1, readyReplicas=2, target=1"}
{"level":"DEBUG","ts":"2025-11-28T21:26:53.693Z","msg":"Capacity targets calculated: modelID=unsloth/Meta-Llama-3.1-8B, targets=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"INFO","ts":"2025-11-28T21:26:53.693Z","msg":"Capacity-only decisions made for model: unsloth/Meta-Llama-3.1-8B - decision count: 1"}
{"level":"INFO","ts":"2025-11-28T21:26:53.693Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-11-28T21:26:53.693Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1→target=1"}
{"level":"DEBUG","ts":"2025-11-28T21:26:53.693Z","msg":"Found VA in map: variant=ms-inference-scheduling-llm-d-modelservice-decode, hasCurrentAlloc=true, accelerator=H100"}
{"level":"INFO","ts":"2025-11-28T21:26:53.693Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 1, desired-replicas: 1, accelerator: H100"}
{"level":"INFO","ts":"2025-11-28T21:26:53.693Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=1, accelerator=H100, capacityOnly=true"}
{"level":"INFO","ts":"2025-11-28T21:26:53.699Z","msg":"Applied capacity decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1, target=1, reason=capacity-only mode: no-change"}
{"level":"INFO","ts":"2025-11-28T21:26:53.699Z","msg":"Reconciliation completed successfully: mode=capacity-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-11-28T21:26:53.700Z","msg":"Operating in CAPACITY-ONLY mode: reactive capacity-based scaling only"}
{"level":"INFO","ts":"2025-11-28T21:26:53.700Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-11-28T21:26:53.700Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-11-28T21:26:53.708Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=1, accelerator=H100, ttft=0.00ms, itl=0.00ms, cost=10.00, arrivalRate=0.00"}
{"level":"INFO","ts":"2025-11-28T21:26:53.710Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9lshk5, usage=0.000 (0.0%)"}
{"level":"INFO","ts":"2025-11-28T21:26:53.710Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9l76md, usage=0.000 (0.0%)"}
{"level":"DEBUG","ts":"2025-11-28T21:26:53.710Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=2"}
{"level":"INFO","ts":"2025-11-28T21:26:53.710Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9lshk5, queueLength=0"}
{"level":"INFO","ts":"2025-11-28T21:26:53.710Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9l76md, queueLength=0"}
{"level":"DEBUG","ts":"2025-11-28T21:26:53.710Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=2"}
{"level":"DEBUG","ts":"2025-11-28T21:26:53.710Z","msg":"Pod-to-variant matching successful: totalPods=2, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:2]"}
{"level":"DEBUG","ts":"2025-11-28T21:26:53.710Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=2"}
{"level":"DEBUG","ts":"2025-11-28T21:26:53.710Z","msg":"Collected capacity metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, metricsCount=2"}
{"level":"DEBUG","ts":"2025-11-28T21:26:53.710Z","msg":"Variant analysis initialized: variant=ms-inference-scheduling-llm-d-modelservice-decode, accelerator=H100, cost=10.00, replicaCount=2"}
{"level":"DEBUG","ts":"2025-11-28T21:26:53.710Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, totalReplicas=2, nonSaturated=2, avgSpareKv=0.500, avgSpareQueue=5.0, shouldScaleUp=false, scaleDownSafe=true"}
{"level":"INFO","ts":"2025-11-28T21:26:53.710Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, totalReplicas=2, nonSaturated=2, shouldScaleUp=false, scaleDownSafe=true"}
{"level":"INFO","ts":"2025-11-28T21:26:53.710Z","msg":"Capacity target: scale-down most expensive variant: variant=ms-inference-scheduling-llm-d-modelservice-decode, cost=10.00, currentReplicas=1, readyReplicas=2, target=1"}
{"level":"DEBUG","ts":"2025-11-28T21:26:53.710Z","msg":"Capacity targets calculated: modelID=unsloth/Meta-Llama-3.1-8B, targets=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"INFO","ts":"2025-11-28T21:26:53.710Z","msg":"Capacity-only decisions made for model: unsloth/Meta-Llama-3.1-8B - decision count: 1"}
{"level":"INFO","ts":"2025-11-28T21:26:53.710Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-11-28T21:26:53.710Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1→target=1"}
{"level":"DEBUG","ts":"2025-11-28T21:26:53.710Z","msg":"Found VA in map: variant=ms-inference-scheduling-llm-d-modelservice-decode, hasCurrentAlloc=true, accelerator=H100"}
{"level":"INFO","ts":"2025-11-28T21:26:53.710Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 1, desired-replicas: 1, accelerator: H100"}
{"level":"INFO","ts":"2025-11-28T21:26:53.710Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=1, accelerator=H100, capacityOnly=true"}
{"level":"INFO","ts":"2025-11-28T21:26:53.714Z","msg":"Applied capacity decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1, target=1, reason=capacity-only mode: no-change"}
{"level":"INFO","ts":"2025-11-28T21:26:53.714Z","msg":"Reconciliation completed successfully: mode=capacity-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-11-28T21:27:23.699Z","msg":"Operating in CAPACITY-ONLY mode: reactive capacity-based scaling only"}
{"level":"INFO","ts":"2025-11-28T21:27:23.699Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-11-28T21:27:23.699Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-11-28T21:27:23.724Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=1, accelerator=H100, ttft=0.00ms, itl=0.00ms, cost=10.00, arrivalRate=0.00"}
{"level":"INFO","ts":"2025-11-28T21:27:23.726Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9l76md, queueLength=0"}
{"level":"DEBUG","ts":"2025-11-28T21:27:23.726Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"INFO","ts":"2025-11-28T21:27:23.726Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9l76md, usage=0.000 (0.0%)"}
{"level":"DEBUG","ts":"2025-11-28T21:27:23.726Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"DEBUG","ts":"2025-11-28T21:27:23.726Z","msg":"Pod-to-variant matching successful: totalPods=1, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"DEBUG","ts":"2025-11-28T21:27:23.726Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=1"}
{"level":"DEBUG","ts":"2025-11-28T21:27:23.726Z","msg":"Collected capacity metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, metricsCount=1"}
{"level":"DEBUG","ts":"2025-11-28T21:27:23.726Z","msg":"Variant analysis initialized: variant=ms-inference-scheduling-llm-d-modelservice-decode, accelerator=H100, cost=10.00, replicaCount=1"}
{"level":"DEBUG","ts":"2025-11-28T21:27:23.726Z","msg":"Scale-down unsafe: insufficient non-saturated replicas: nonSaturated=1, required=2"}
{"level":"DEBUG","ts":"2025-11-28T21:27:23.726Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, totalReplicas=1, nonSaturated=1, avgSpareKv=0.500, avgSpareQueue=5.0, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"INFO","ts":"2025-11-28T21:27:23.726Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, totalReplicas=1, nonSaturated=1, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"DEBUG","ts":"2025-11-28T21:27:23.726Z","msg":"Capacity targets: no scaling needed (avgSpareKv=0.500, avgSpareQueue=5.0, all variants stable)"}
{"level":"DEBUG","ts":"2025-11-28T21:27:23.726Z","msg":"Capacity targets calculated: modelID=unsloth/Meta-Llama-3.1-8B, targets=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"INFO","ts":"2025-11-28T21:27:23.726Z","msg":"Capacity-only decisions made for model: unsloth/Meta-Llama-3.1-8B - decision count: 1"}
{"level":"INFO","ts":"2025-11-28T21:27:23.726Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-11-28T21:27:23.726Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1→target=1"}
{"level":"DEBUG","ts":"2025-11-28T21:27:23.726Z","msg":"Found VA in map: variant=ms-inference-scheduling-llm-d-modelservice-decode, hasCurrentAlloc=true, accelerator=H100"}
{"level":"INFO","ts":"2025-11-28T21:27:23.726Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 1, desired-replicas: 1, accelerator: H100"}
{"level":"INFO","ts":"2025-11-28T21:27:23.726Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=1, accelerator=H100, capacityOnly=true"}
{"level":"INFO","ts":"2025-11-28T21:27:23.733Z","msg":"Applied capacity decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1, target=1, reason=capacity-only mode: no-change"}
{"level":"INFO","ts":"2025-11-28T21:27:23.733Z","msg":"Reconciliation completed successfully: mode=capacity-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-11-28T21:27:23.733Z","msg":"Operating in CAPACITY-ONLY mode: reactive capacity-based scaling only"}
{"level":"INFO","ts":"2025-11-28T21:27:23.733Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-11-28T21:27:23.733Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-11-28T21:27:23.742Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=1, accelerator=H100, ttft=0.00ms, itl=0.00ms, cost=10.00, arrivalRate=0.00"}
{"level":"INFO","ts":"2025-11-28T21:27:23.743Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9l76md, usage=0.000 (0.0%)"}
{"level":"DEBUG","ts":"2025-11-28T21:27:23.743Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"INFO","ts":"2025-11-28T21:27:23.743Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9l76md, queueLength=0"}
{"level":"DEBUG","ts":"2025-11-28T21:27:23.744Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"DEBUG","ts":"2025-11-28T21:27:23.744Z","msg":"Pod-to-variant matching successful: totalPods=1, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"DEBUG","ts":"2025-11-28T21:27:23.744Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=1"}
{"level":"DEBUG","ts":"2025-11-28T21:27:23.744Z","msg":"Collected capacity metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, metricsCount=1"}
{"level":"DEBUG","ts":"2025-11-28T21:27:23.744Z","msg":"Variant analysis initialized: variant=ms-inference-scheduling-llm-d-modelservice-decode, accelerator=H100, cost=10.00, replicaCount=1"}
{"level":"DEBUG","ts":"2025-11-28T21:27:23.744Z","msg":"Scale-down unsafe: insufficient non-saturated replicas: nonSaturated=1, required=2"}
{"level":"DEBUG","ts":"2025-11-28T21:27:23.744Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, totalReplicas=1, nonSaturated=1, avgSpareKv=0.500, avgSpareQueue=5.0, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"INFO","ts":"2025-11-28T21:27:23.744Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, totalReplicas=1, nonSaturated=1, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"DEBUG","ts":"2025-11-28T21:27:23.744Z","msg":"Capacity targets: no scaling needed (avgSpareKv=0.500, avgSpareQueue=5.0, all variants stable)"}
{"level":"DEBUG","ts":"2025-11-28T21:27:23.744Z","msg":"Capacity targets calculated: modelID=unsloth/Meta-Llama-3.1-8B, targets=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"INFO","ts":"2025-11-28T21:27:23.744Z","msg":"Capacity-only decisions made for model: unsloth/Meta-Llama-3.1-8B - decision count: 1"}
{"level":"INFO","ts":"2025-11-28T21:27:23.744Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-11-28T21:27:23.744Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1→target=1"}
{"level":"DEBUG","ts":"2025-11-28T21:27:23.744Z","msg":"Found VA in map: variant=ms-inference-scheduling-llm-d-modelservice-decode, hasCurrentAlloc=true, accelerator=H100"}
{"level":"INFO","ts":"2025-11-28T21:27:23.744Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 1, desired-replicas: 1, accelerator: H100"}
{"level":"INFO","ts":"2025-11-28T21:27:23.744Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=1, accelerator=H100, capacityOnly=true"}
{"level":"INFO","ts":"2025-11-28T21:27:23.749Z","msg":"Applied capacity decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1, target=1, reason=capacity-only mode: no-change"}
{"level":"INFO","ts":"2025-11-28T21:27:23.749Z","msg":"Reconciliation completed successfully: mode=capacity-only, modelsProcessed=1, decisionsApplied=1"}
