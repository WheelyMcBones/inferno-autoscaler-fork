{"level":"INFO","ts":"2025-11-27T23:14:29.315Z","msg":"Operating in CAPACITY-ONLY mode: reactive capacity-based scaling only"}
{"level":"INFO","ts":"2025-11-27T23:14:29.315Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-11-27T23:14:29.315Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-11-27T23:14:29.331Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=1, accelerator=H100, ttft=0.00ms, itl=0.00ms, cost=10.00, arrivalRate=%!f(string=0.)"}
{"level":"INFO","ts":"2025-11-27T23:14:29.332Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9zt6lq, queueLength=0"}
{"level":"DEBUG","ts":"2025-11-27T23:14:29.332Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"INFO","ts":"2025-11-27T23:14:29.332Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9zt6lq, usage=0.000 (0.0%)"}
{"level":"DEBUG","ts":"2025-11-27T23:14:29.332Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"DEBUG","ts":"2025-11-27T23:14:29.332Z","msg":"Pod-to-variant matching successful: totalPods=1, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"DEBUG","ts":"2025-11-27T23:14:29.332Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=1"}
{"level":"DEBUG","ts":"2025-11-27T23:14:29.332Z","msg":"Collected capacity metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, metricsCount=1"}
{"level":"DEBUG","ts":"2025-11-27T23:14:29.332Z","msg":"Variant analysis initialized: variant=ms-inference-scheduling-llm-d-modelservice-decode, accelerator=H100, cost=10.00, replicaCount=1"}
{"level":"DEBUG","ts":"2025-11-27T23:14:29.332Z","msg":"Scale-down unsafe: insufficient non-saturated replicas: nonSaturated=1, required=2"}
{"level":"DEBUG","ts":"2025-11-27T23:14:29.332Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, totalReplicas=1, nonSaturated=1, avgSpareKv=0.500, avgSpareQueue=5.0, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"INFO","ts":"2025-11-27T23:14:29.332Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, totalReplicas=1, nonSaturated=1, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"DEBUG","ts":"2025-11-27T23:14:29.333Z","msg":"Capacity targets: no scaling needed (avgSpareKv=0.500, avgSpareQueue=5.0, all variants stable)"}
{"level":"DEBUG","ts":"2025-11-27T23:14:29.333Z","msg":"Capacity targets calculated: modelID=unsloth/Meta-Llama-3.1-8B, targets=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"INFO","ts":"2025-11-27T23:14:29.333Z","msg":"Capacity-only decisions made for model: unsloth/Meta-Llama-3.1-8B - decision count: 1"}
{"level":"INFO","ts":"2025-11-27T23:14:29.333Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-11-27T23:14:29.333Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1→target=1"}
{"level":"DEBUG","ts":"2025-11-27T23:14:29.333Z","msg":"Found VA in map: variant=ms-inference-scheduling-llm-d-modelservice-decode, hasCurrentAlloc=true, accelerator=H100"}
{"level":"INFO","ts":"2025-11-27T23:14:29.333Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 1, desired-replicas: 1, accelerator: H100"}
{"level":"INFO","ts":"2025-11-27T23:14:29.333Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=1, accelerator=H100, capacityOnly=true"}
{"level":"INFO","ts":"2025-11-27T23:14:29.338Z","msg":"Applied capacity decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1, target=1, reason=capacity-only mode: no-change"}
{"level":"INFO","ts":"2025-11-27T23:14:29.338Z","msg":"Reconciliation completed successfully: mode=capacity-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-11-27T23:14:29.353Z","msg":"Operating in CAPACITY-ONLY mode: reactive capacity-based scaling only"}
{"level":"INFO","ts":"2025-11-27T23:14:29.353Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-11-27T23:14:29.353Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-11-27T23:14:29.363Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=1, accelerator=H100, ttft=0.00ms, itl=0.00ms, cost=10.00, arrivalRate=%!f(string=0.)"}
{"level":"INFO","ts":"2025-11-27T23:14:29.364Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9zt6lq, usage=0.000 (0.0%)"}
{"level":"DEBUG","ts":"2025-11-27T23:14:29.364Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"INFO","ts":"2025-11-27T23:14:29.364Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9zt6lq, queueLength=0"}
{"level":"DEBUG","ts":"2025-11-27T23:14:29.364Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"DEBUG","ts":"2025-11-27T23:14:29.364Z","msg":"Pod-to-variant matching successful: totalPods=1, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"DEBUG","ts":"2025-11-27T23:14:29.364Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=1"}
{"level":"DEBUG","ts":"2025-11-27T23:14:29.364Z","msg":"Collected capacity metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, metricsCount=1"}
{"level":"DEBUG","ts":"2025-11-27T23:14:29.364Z","msg":"Variant analysis initialized: variant=ms-inference-scheduling-llm-d-modelservice-decode, accelerator=H100, cost=10.00, replicaCount=1"}
{"level":"DEBUG","ts":"2025-11-27T23:14:29.364Z","msg":"Scale-down unsafe: insufficient non-saturated replicas: nonSaturated=1, required=2"}
{"level":"DEBUG","ts":"2025-11-27T23:14:29.364Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, totalReplicas=1, nonSaturated=1, avgSpareKv=0.500, avgSpareQueue=5.0, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"INFO","ts":"2025-11-27T23:14:29.364Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, totalReplicas=1, nonSaturated=1, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"DEBUG","ts":"2025-11-27T23:14:29.364Z","msg":"Capacity targets: no scaling needed (avgSpareKv=0.500, avgSpareQueue=5.0, all variants stable)"}
{"level":"DEBUG","ts":"2025-11-27T23:14:29.364Z","msg":"Capacity targets calculated: modelID=unsloth/Meta-Llama-3.1-8B, targets=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"INFO","ts":"2025-11-27T23:14:29.364Z","msg":"Capacity-only decisions made for model: unsloth/Meta-Llama-3.1-8B - decision count: 1"}
{"level":"INFO","ts":"2025-11-27T23:14:29.364Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-11-27T23:14:29.364Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1→target=1"}
{"level":"DEBUG","ts":"2025-11-27T23:14:29.364Z","msg":"Found VA in map: variant=ms-inference-scheduling-llm-d-modelservice-decode, hasCurrentAlloc=true, accelerator=H100"}
{"level":"INFO","ts":"2025-11-27T23:14:29.364Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 1, desired-replicas: 1, accelerator: H100"}
{"level":"INFO","ts":"2025-11-27T23:14:29.364Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=1, accelerator=H100, capacityOnly=true"}
{"level":"INFO","ts":"2025-11-27T23:14:29.369Z","msg":"Applied capacity decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1, target=1, reason=capacity-only mode: no-change"}
{"level":"INFO","ts":"2025-11-27T23:14:29.369Z","msg":"Reconciliation completed successfully: mode=capacity-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-11-27T23:14:59.338Z","msg":"Operating in CAPACITY-ONLY mode: reactive capacity-based scaling only"}
{"level":"INFO","ts":"2025-11-27T23:14:59.338Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-11-27T23:14:59.338Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-11-27T23:14:59.351Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=1, accelerator=H100, ttft=0.00ms, itl=0.00ms, cost=10.00, arrivalRate=%!f(string=0.)"}
{"level":"INFO","ts":"2025-11-27T23:14:59.352Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9zt6lq, usage=0.000 (0.0%)"}
{"level":"DEBUG","ts":"2025-11-27T23:14:59.352Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"INFO","ts":"2025-11-27T23:14:59.352Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9zt6lq, queueLength=0"}
{"level":"DEBUG","ts":"2025-11-27T23:14:59.352Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"DEBUG","ts":"2025-11-27T23:14:59.352Z","msg":"Pod-to-variant matching successful: totalPods=1, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"DEBUG","ts":"2025-11-27T23:14:59.352Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=1"}
{"level":"DEBUG","ts":"2025-11-27T23:14:59.352Z","msg":"Collected capacity metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, metricsCount=1"}
{"level":"DEBUG","ts":"2025-11-27T23:14:59.352Z","msg":"Variant analysis initialized: variant=ms-inference-scheduling-llm-d-modelservice-decode, accelerator=H100, cost=10.00, replicaCount=1"}
{"level":"DEBUG","ts":"2025-11-27T23:14:59.352Z","msg":"Scale-down unsafe: insufficient non-saturated replicas: nonSaturated=1, required=2"}
{"level":"DEBUG","ts":"2025-11-27T23:14:59.352Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, totalReplicas=1, nonSaturated=1, avgSpareKv=0.500, avgSpareQueue=5.0, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"INFO","ts":"2025-11-27T23:14:59.352Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, totalReplicas=1, nonSaturated=1, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"DEBUG","ts":"2025-11-27T23:14:59.352Z","msg":"Capacity targets: no scaling needed (avgSpareKv=0.500, avgSpareQueue=5.0, all variants stable)"}
{"level":"DEBUG","ts":"2025-11-27T23:14:59.352Z","msg":"Capacity targets calculated: modelID=unsloth/Meta-Llama-3.1-8B, targets=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"INFO","ts":"2025-11-27T23:14:59.352Z","msg":"Capacity-only decisions made for model: unsloth/Meta-Llama-3.1-8B - decision count: 1"}
{"level":"INFO","ts":"2025-11-27T23:14:59.352Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-11-27T23:14:59.352Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1→target=1"}
{"level":"DEBUG","ts":"2025-11-27T23:14:59.352Z","msg":"Found VA in map: variant=ms-inference-scheduling-llm-d-modelservice-decode, hasCurrentAlloc=true, accelerator=H100"}
{"level":"INFO","ts":"2025-11-27T23:14:59.352Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 1, desired-replicas: 1, accelerator: H100"}
{"level":"INFO","ts":"2025-11-27T23:14:59.352Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=1, accelerator=H100, capacityOnly=true"}
{"level":"INFO","ts":"2025-11-27T23:14:59.357Z","msg":"Applied capacity decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1, target=1, reason=capacity-only mode: no-change"}
{"level":"INFO","ts":"2025-11-27T23:14:59.357Z","msg":"Reconciliation completed successfully: mode=capacity-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-11-27T23:14:59.370Z","msg":"Operating in CAPACITY-ONLY mode: reactive capacity-based scaling only"}
{"level":"INFO","ts":"2025-11-27T23:14:59.370Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-11-27T23:14:59.370Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-11-27T23:14:59.381Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=1, accelerator=H100, ttft=0.00ms, itl=0.00ms, cost=10.00, arrivalRate=%!f(string=0.)"}
{"level":"INFO","ts":"2025-11-27T23:14:59.382Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9zt6lq, usage=0.000 (0.0%)"}
{"level":"DEBUG","ts":"2025-11-27T23:14:59.382Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"INFO","ts":"2025-11-27T23:14:59.383Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9zt6lq, queueLength=0"}
{"level":"DEBUG","ts":"2025-11-27T23:14:59.383Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"DEBUG","ts":"2025-11-27T23:14:59.383Z","msg":"Pod-to-variant matching successful: totalPods=1, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"DEBUG","ts":"2025-11-27T23:14:59.383Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=1"}
{"level":"DEBUG","ts":"2025-11-27T23:14:59.383Z","msg":"Collected capacity metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, metricsCount=1"}
{"level":"DEBUG","ts":"2025-11-27T23:14:59.383Z","msg":"Variant analysis initialized: variant=ms-inference-scheduling-llm-d-modelservice-decode, accelerator=H100, cost=10.00, replicaCount=1"}
{"level":"DEBUG","ts":"2025-11-27T23:14:59.383Z","msg":"Scale-down unsafe: insufficient non-saturated replicas: nonSaturated=1, required=2"}
{"level":"DEBUG","ts":"2025-11-27T23:14:59.383Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, totalReplicas=1, nonSaturated=1, avgSpareKv=0.500, avgSpareQueue=5.0, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"INFO","ts":"2025-11-27T23:14:59.383Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, totalReplicas=1, nonSaturated=1, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"DEBUG","ts":"2025-11-27T23:14:59.383Z","msg":"Capacity targets: no scaling needed (avgSpareKv=0.500, avgSpareQueue=5.0, all variants stable)"}
{"level":"DEBUG","ts":"2025-11-27T23:14:59.383Z","msg":"Capacity targets calculated: modelID=unsloth/Meta-Llama-3.1-8B, targets=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"INFO","ts":"2025-11-27T23:14:59.383Z","msg":"Capacity-only decisions made for model: unsloth/Meta-Llama-3.1-8B - decision count: 1"}
{"level":"INFO","ts":"2025-11-27T23:14:59.383Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-11-27T23:14:59.383Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1→target=1"}
{"level":"DEBUG","ts":"2025-11-27T23:14:59.383Z","msg":"Found VA in map: variant=ms-inference-scheduling-llm-d-modelservice-decode, hasCurrentAlloc=true, accelerator=H100"}
{"level":"INFO","ts":"2025-11-27T23:14:59.383Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 1, desired-replicas: 1, accelerator: H100"}
{"level":"INFO","ts":"2025-11-27T23:14:59.383Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=1, accelerator=H100, capacityOnly=true"}
{"level":"INFO","ts":"2025-11-27T23:14:59.388Z","msg":"Applied capacity decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1, target=1, reason=capacity-only mode: no-change"}
{"level":"INFO","ts":"2025-11-27T23:14:59.388Z","msg":"Reconciliation completed successfully: mode=capacity-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-11-27T23:15:29.357Z","msg":"Operating in CAPACITY-ONLY mode: reactive capacity-based scaling only"}
{"level":"INFO","ts":"2025-11-27T23:15:29.357Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-11-27T23:15:29.357Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-11-27T23:15:29.378Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=1, accelerator=H100, ttft=0.00ms, itl=0.00ms, cost=10.00, arrivalRate=%!f(string=0.)"}
{"level":"INFO","ts":"2025-11-27T23:15:29.380Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9zt6lq, queueLength=0"}
{"level":"DEBUG","ts":"2025-11-27T23:15:29.380Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"INFO","ts":"2025-11-27T23:15:29.380Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9zt6lq, usage=0.000 (0.0%)"}
{"level":"DEBUG","ts":"2025-11-27T23:15:29.380Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"DEBUG","ts":"2025-11-27T23:15:29.380Z","msg":"Pod-to-variant matching successful: totalPods=1, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"DEBUG","ts":"2025-11-27T23:15:29.380Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=1"}
{"level":"DEBUG","ts":"2025-11-27T23:15:29.380Z","msg":"Collected capacity metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, metricsCount=1"}
{"level":"DEBUG","ts":"2025-11-27T23:15:29.380Z","msg":"Variant analysis initialized: variant=ms-inference-scheduling-llm-d-modelservice-decode, accelerator=H100, cost=10.00, replicaCount=1"}
{"level":"DEBUG","ts":"2025-11-27T23:15:29.380Z","msg":"Scale-down unsafe: insufficient non-saturated replicas: nonSaturated=1, required=2"}
{"level":"DEBUG","ts":"2025-11-27T23:15:29.380Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, totalReplicas=1, nonSaturated=1, avgSpareKv=0.500, avgSpareQueue=5.0, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"INFO","ts":"2025-11-27T23:15:29.380Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, totalReplicas=1, nonSaturated=1, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"DEBUG","ts":"2025-11-27T23:15:29.380Z","msg":"Capacity targets: no scaling needed (avgSpareKv=0.500, avgSpareQueue=5.0, all variants stable)"}
{"level":"DEBUG","ts":"2025-11-27T23:15:29.380Z","msg":"Capacity targets calculated: modelID=unsloth/Meta-Llama-3.1-8B, targets=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"INFO","ts":"2025-11-27T23:15:29.380Z","msg":"Capacity-only decisions made for model: unsloth/Meta-Llama-3.1-8B - decision count: 1"}
{"level":"INFO","ts":"2025-11-27T23:15:29.380Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-11-27T23:15:29.380Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1→target=1"}
{"level":"DEBUG","ts":"2025-11-27T23:15:29.380Z","msg":"Found VA in map: variant=ms-inference-scheduling-llm-d-modelservice-decode, hasCurrentAlloc=true, accelerator=H100"}
{"level":"INFO","ts":"2025-11-27T23:15:29.380Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 1, desired-replicas: 1, accelerator: H100"}
{"level":"INFO","ts":"2025-11-27T23:15:29.380Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=1, accelerator=H100, capacityOnly=true"}
{"level":"INFO","ts":"2025-11-27T23:15:29.385Z","msg":"Applied capacity decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1, target=1, reason=capacity-only mode: no-change"}
{"level":"INFO","ts":"2025-11-27T23:15:29.385Z","msg":"Reconciliation completed successfully: mode=capacity-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-11-27T23:15:29.389Z","msg":"Operating in CAPACITY-ONLY mode: reactive capacity-based scaling only"}
{"level":"INFO","ts":"2025-11-27T23:15:29.389Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-11-27T23:15:29.389Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-11-27T23:15:29.397Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=1, accelerator=H100, ttft=0.00ms, itl=0.00ms, cost=10.00, arrivalRate=%!f(string=0.)"}
{"level":"INFO","ts":"2025-11-27T23:15:29.398Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9zt6lq, queueLength=0"}
{"level":"DEBUG","ts":"2025-11-27T23:15:29.398Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"INFO","ts":"2025-11-27T23:15:29.399Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9zt6lq, usage=0.000 (0.0%)"}
{"level":"DEBUG","ts":"2025-11-27T23:15:29.399Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"DEBUG","ts":"2025-11-27T23:15:29.399Z","msg":"Pod-to-variant matching successful: totalPods=1, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"DEBUG","ts":"2025-11-27T23:15:29.399Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=1"}
{"level":"DEBUG","ts":"2025-11-27T23:15:29.399Z","msg":"Collected capacity metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, metricsCount=1"}
{"level":"DEBUG","ts":"2025-11-27T23:15:29.399Z","msg":"Variant analysis initialized: variant=ms-inference-scheduling-llm-d-modelservice-decode, accelerator=H100, cost=10.00, replicaCount=1"}
{"level":"DEBUG","ts":"2025-11-27T23:15:29.399Z","msg":"Scale-down unsafe: insufficient non-saturated replicas: nonSaturated=1, required=2"}
{"level":"DEBUG","ts":"2025-11-27T23:15:29.399Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, totalReplicas=1, nonSaturated=1, avgSpareKv=0.500, avgSpareQueue=5.0, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"INFO","ts":"2025-11-27T23:15:29.399Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, totalReplicas=1, nonSaturated=1, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"DEBUG","ts":"2025-11-27T23:15:29.399Z","msg":"Capacity targets: no scaling needed (avgSpareKv=0.500, avgSpareQueue=5.0, all variants stable)"}
{"level":"DEBUG","ts":"2025-11-27T23:15:29.399Z","msg":"Capacity targets calculated: modelID=unsloth/Meta-Llama-3.1-8B, targets=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"INFO","ts":"2025-11-27T23:15:29.399Z","msg":"Capacity-only decisions made for model: unsloth/Meta-Llama-3.1-8B - decision count: 1"}
{"level":"INFO","ts":"2025-11-27T23:15:29.399Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-11-27T23:15:29.399Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1→target=1"}
{"level":"DEBUG","ts":"2025-11-27T23:15:29.399Z","msg":"Found VA in map: variant=ms-inference-scheduling-llm-d-modelservice-decode, hasCurrentAlloc=true, accelerator=H100"}
{"level":"INFO","ts":"2025-11-27T23:15:29.399Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 1, desired-replicas: 1, accelerator: H100"}
{"level":"INFO","ts":"2025-11-27T23:15:29.399Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=1, accelerator=H100, capacityOnly=true"}
{"level":"INFO","ts":"2025-11-27T23:15:29.403Z","msg":"Applied capacity decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1, target=1, reason=capacity-only mode: no-change"}
{"level":"INFO","ts":"2025-11-27T23:15:29.403Z","msg":"Reconciliation completed successfully: mode=capacity-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-11-27T23:15:59.385Z","msg":"Operating in CAPACITY-ONLY mode: reactive capacity-based scaling only"}
{"level":"INFO","ts":"2025-11-27T23:15:59.387Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-11-27T23:15:59.387Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-11-27T23:15:59.578Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=1, accelerator=H100, ttft=0.00ms, itl=0.00ms, cost=10.00, arrivalRate=%!f(string=0.)"}
{"level":"INFO","ts":"2025-11-27T23:15:59.580Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9zt6lq, usage=0.000 (0.0%)"}
{"level":"DEBUG","ts":"2025-11-27T23:15:59.580Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"INFO","ts":"2025-11-27T23:15:59.580Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9zt6lq, queueLength=0"}
{"level":"DEBUG","ts":"2025-11-27T23:15:59.580Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"DEBUG","ts":"2025-11-27T23:15:59.580Z","msg":"Pod-to-variant matching successful: totalPods=1, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"DEBUG","ts":"2025-11-27T23:15:59.580Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=1"}
{"level":"DEBUG","ts":"2025-11-27T23:15:59.580Z","msg":"Collected capacity metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, metricsCount=1"}
{"level":"DEBUG","ts":"2025-11-27T23:15:59.580Z","msg":"Variant analysis initialized: variant=ms-inference-scheduling-llm-d-modelservice-decode, accelerator=H100, cost=10.00, replicaCount=1"}
{"level":"DEBUG","ts":"2025-11-27T23:15:59.580Z","msg":"Scale-down unsafe: insufficient non-saturated replicas: nonSaturated=1, required=2"}
{"level":"DEBUG","ts":"2025-11-27T23:15:59.580Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, totalReplicas=1, nonSaturated=1, avgSpareKv=0.500, avgSpareQueue=5.0, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"INFO","ts":"2025-11-27T23:15:59.580Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, totalReplicas=1, nonSaturated=1, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"DEBUG","ts":"2025-11-27T23:15:59.580Z","msg":"Capacity targets: no scaling needed (avgSpareKv=0.500, avgSpareQueue=5.0, all variants stable)"}
{"level":"DEBUG","ts":"2025-11-27T23:15:59.580Z","msg":"Capacity targets calculated: modelID=unsloth/Meta-Llama-3.1-8B, targets=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"INFO","ts":"2025-11-27T23:15:59.580Z","msg":"Capacity-only decisions made for model: unsloth/Meta-Llama-3.1-8B - decision count: 1"}
{"level":"INFO","ts":"2025-11-27T23:15:59.580Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-11-27T23:15:59.580Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1→target=1"}
{"level":"DEBUG","ts":"2025-11-27T23:15:59.580Z","msg":"Found VA in map: variant=ms-inference-scheduling-llm-d-modelservice-decode, hasCurrentAlloc=true, accelerator=H100"}
{"level":"INFO","ts":"2025-11-27T23:15:59.580Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 1, desired-replicas: 1, accelerator: H100"}
{"level":"INFO","ts":"2025-11-27T23:15:59.580Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=1, accelerator=H100, capacityOnly=true"}
{"level":"INFO","ts":"2025-11-27T23:15:59.585Z","msg":"Applied capacity decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1, target=1, reason=capacity-only mode: no-change"}
{"level":"INFO","ts":"2025-11-27T23:15:59.585Z","msg":"Reconciliation completed successfully: mode=capacity-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-11-27T23:15:59.585Z","msg":"Operating in CAPACITY-ONLY mode: reactive capacity-based scaling only"}
{"level":"INFO","ts":"2025-11-27T23:15:59.585Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-11-27T23:15:59.585Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-11-27T23:15:59.594Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=1, accelerator=H100, ttft=0.00ms, itl=0.00ms, cost=10.00, arrivalRate=%!f(string=0.)"}
{"level":"INFO","ts":"2025-11-27T23:15:59.596Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9zt6lq, usage=0.000 (0.0%)"}
{"level":"INFO","ts":"2025-11-27T23:15:59.596Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9zt6lq, queueLength=0"}
{"level":"DEBUG","ts":"2025-11-27T23:15:59.596Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"DEBUG","ts":"2025-11-27T23:15:59.596Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"DEBUG","ts":"2025-11-27T23:15:59.596Z","msg":"Pod-to-variant matching successful: totalPods=1, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"DEBUG","ts":"2025-11-27T23:15:59.596Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=1"}
{"level":"DEBUG","ts":"2025-11-27T23:15:59.596Z","msg":"Collected capacity metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, metricsCount=1"}
{"level":"DEBUG","ts":"2025-11-27T23:15:59.596Z","msg":"Variant analysis initialized: variant=ms-inference-scheduling-llm-d-modelservice-decode, accelerator=H100, cost=10.00, replicaCount=1"}
{"level":"DEBUG","ts":"2025-11-27T23:15:59.596Z","msg":"Scale-down unsafe: insufficient non-saturated replicas: nonSaturated=1, required=2"}
{"level":"DEBUG","ts":"2025-11-27T23:15:59.596Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, totalReplicas=1, nonSaturated=1, avgSpareKv=0.500, avgSpareQueue=5.0, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"INFO","ts":"2025-11-27T23:15:59.596Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, totalReplicas=1, nonSaturated=1, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"DEBUG","ts":"2025-11-27T23:15:59.596Z","msg":"Capacity targets: no scaling needed (avgSpareKv=0.500, avgSpareQueue=5.0, all variants stable)"}
{"level":"DEBUG","ts":"2025-11-27T23:15:59.596Z","msg":"Capacity targets calculated: modelID=unsloth/Meta-Llama-3.1-8B, targets=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"INFO","ts":"2025-11-27T23:15:59.596Z","msg":"Capacity-only decisions made for model: unsloth/Meta-Llama-3.1-8B - decision count: 1"}
{"level":"INFO","ts":"2025-11-27T23:15:59.596Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-11-27T23:15:59.596Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1→target=1"}
{"level":"DEBUG","ts":"2025-11-27T23:15:59.596Z","msg":"Found VA in map: variant=ms-inference-scheduling-llm-d-modelservice-decode, hasCurrentAlloc=true, accelerator=H100"}
{"level":"INFO","ts":"2025-11-27T23:15:59.596Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 1, desired-replicas: 1, accelerator: H100"}
{"level":"INFO","ts":"2025-11-27T23:15:59.596Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=1, accelerator=H100, capacityOnly=true"}
{"level":"INFO","ts":"2025-11-27T23:15:59.602Z","msg":"Applied capacity decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1, target=1, reason=capacity-only mode: no-change"}
{"level":"INFO","ts":"2025-11-27T23:15:59.603Z","msg":"Reconciliation completed successfully: mode=capacity-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-11-27T23:16:29.585Z","msg":"Operating in CAPACITY-ONLY mode: reactive capacity-based scaling only"}
{"level":"INFO","ts":"2025-11-27T23:16:29.585Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-11-27T23:16:29.585Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-11-27T23:16:29.610Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=1, accelerator=H100, ttft=21.19ms, itl=10.33ms, cost=10.00, arrivalRate=%!f(string=89)"}
{"level":"INFO","ts":"2025-11-27T23:16:29.611Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9zt6lq, usage=0.055 (5.5%)"}
{"level":"DEBUG","ts":"2025-11-27T23:16:29.611Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"INFO","ts":"2025-11-27T23:16:29.611Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9zt6lq, queueLength=0"}
{"level":"DEBUG","ts":"2025-11-27T23:16:29.611Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"DEBUG","ts":"2025-11-27T23:16:29.612Z","msg":"Pod-to-variant matching successful: totalPods=1, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"DEBUG","ts":"2025-11-27T23:16:29.612Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=1"}
{"level":"DEBUG","ts":"2025-11-27T23:16:29.612Z","msg":"Collected capacity metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, metricsCount=1"}
{"level":"DEBUG","ts":"2025-11-27T23:16:29.612Z","msg":"Variant analysis initialized: variant=ms-inference-scheduling-llm-d-modelservice-decode, accelerator=H100, cost=10.00, replicaCount=1"}
{"level":"DEBUG","ts":"2025-11-27T23:16:29.612Z","msg":"Scale-down unsafe: insufficient non-saturated replicas: nonSaturated=1, required=2"}
{"level":"DEBUG","ts":"2025-11-27T23:16:29.612Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, totalReplicas=1, nonSaturated=1, avgSpareKv=0.445, avgSpareQueue=5.0, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"INFO","ts":"2025-11-27T23:16:29.612Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, totalReplicas=1, nonSaturated=1, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"DEBUG","ts":"2025-11-27T23:16:29.612Z","msg":"Capacity targets: no scaling needed (avgSpareKv=0.445, avgSpareQueue=5.0, all variants stable)"}
{"level":"DEBUG","ts":"2025-11-27T23:16:29.612Z","msg":"Capacity targets calculated: modelID=unsloth/Meta-Llama-3.1-8B, targets=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"INFO","ts":"2025-11-27T23:16:29.612Z","msg":"Capacity-only decisions made for model: unsloth/Meta-Llama-3.1-8B - decision count: 1"}
{"level":"INFO","ts":"2025-11-27T23:16:29.612Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-11-27T23:16:29.612Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1→target=1"}
{"level":"DEBUG","ts":"2025-11-27T23:16:29.612Z","msg":"Found VA in map: variant=ms-inference-scheduling-llm-d-modelservice-decode, hasCurrentAlloc=true, accelerator=H100"}
{"level":"INFO","ts":"2025-11-27T23:16:29.612Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 1, desired-replicas: 1, accelerator: H100"}
{"level":"INFO","ts":"2025-11-27T23:16:29.612Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=1, accelerator=H100, capacityOnly=true"}
{"level":"INFO","ts":"2025-11-27T23:16:29.617Z","msg":"Applied capacity decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1, target=1, reason=capacity-only mode: no-change"}
{"level":"INFO","ts":"2025-11-27T23:16:29.617Z","msg":"Reconciliation completed successfully: mode=capacity-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-11-27T23:16:29.617Z","msg":"Operating in CAPACITY-ONLY mode: reactive capacity-based scaling only"}
{"level":"INFO","ts":"2025-11-27T23:16:29.617Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-11-27T23:16:29.617Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-11-27T23:16:29.626Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=1, accelerator=H100, ttft=21.19ms, itl=10.33ms, cost=10.00, arrivalRate=%!f(string=89)"}
{"level":"INFO","ts":"2025-11-27T23:16:29.628Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9zt6lq, queueLength=0"}
{"level":"DEBUG","ts":"2025-11-27T23:16:29.628Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"INFO","ts":"2025-11-27T23:16:29.628Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9zt6lq, usage=0.055 (5.5%)"}
{"level":"DEBUG","ts":"2025-11-27T23:16:29.628Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"DEBUG","ts":"2025-11-27T23:16:29.628Z","msg":"Pod-to-variant matching successful: totalPods=1, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"DEBUG","ts":"2025-11-27T23:16:29.628Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=1"}
{"level":"DEBUG","ts":"2025-11-27T23:16:29.628Z","msg":"Collected capacity metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, metricsCount=1"}
{"level":"DEBUG","ts":"2025-11-27T23:16:29.628Z","msg":"Variant analysis initialized: variant=ms-inference-scheduling-llm-d-modelservice-decode, accelerator=H100, cost=10.00, replicaCount=1"}
{"level":"DEBUG","ts":"2025-11-27T23:16:29.628Z","msg":"Scale-down unsafe: insufficient non-saturated replicas: nonSaturated=1, required=2"}
{"level":"DEBUG","ts":"2025-11-27T23:16:29.628Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, totalReplicas=1, nonSaturated=1, avgSpareKv=0.445, avgSpareQueue=5.0, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"INFO","ts":"2025-11-27T23:16:29.628Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, totalReplicas=1, nonSaturated=1, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"DEBUG","ts":"2025-11-27T23:16:29.628Z","msg":"Capacity targets: no scaling needed (avgSpareKv=0.445, avgSpareQueue=5.0, all variants stable)"}
{"level":"DEBUG","ts":"2025-11-27T23:16:29.628Z","msg":"Capacity targets calculated: modelID=unsloth/Meta-Llama-3.1-8B, targets=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"INFO","ts":"2025-11-27T23:16:29.628Z","msg":"Capacity-only decisions made for model: unsloth/Meta-Llama-3.1-8B - decision count: 1"}
{"level":"INFO","ts":"2025-11-27T23:16:29.628Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-11-27T23:16:29.628Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1→target=1"}
{"level":"DEBUG","ts":"2025-11-27T23:16:29.628Z","msg":"Found VA in map: variant=ms-inference-scheduling-llm-d-modelservice-decode, hasCurrentAlloc=true, accelerator=H100"}
{"level":"INFO","ts":"2025-11-27T23:16:29.628Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 1, desired-replicas: 1, accelerator: H100"}
{"level":"INFO","ts":"2025-11-27T23:16:29.628Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=1, accelerator=H100, capacityOnly=true"}
{"level":"INFO","ts":"2025-11-27T23:16:29.635Z","msg":"Applied capacity decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1, target=1, reason=capacity-only mode: no-change"}
{"level":"INFO","ts":"2025-11-27T23:16:29.635Z","msg":"Reconciliation completed successfully: mode=capacity-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-11-27T23:16:59.618Z","msg":"Operating in CAPACITY-ONLY mode: reactive capacity-based scaling only"}
{"level":"INFO","ts":"2025-11-27T23:16:59.618Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-11-27T23:16:59.618Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-11-27T23:16:59.636Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=1, accelerator=H100, ttft=23.35ms, itl=11.63ms, cost=10.00, arrivalRate=%!f(string=46)"}
{"level":"INFO","ts":"2025-11-27T23:16:59.637Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9zt6lq, usage=0.074 (7.4%)"}
{"level":"DEBUG","ts":"2025-11-27T23:16:59.637Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"INFO","ts":"2025-11-27T23:16:59.637Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9zt6lq, queueLength=0"}
{"level":"DEBUG","ts":"2025-11-27T23:16:59.637Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"DEBUG","ts":"2025-11-27T23:16:59.637Z","msg":"Pod-to-variant matching successful: totalPods=1, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"DEBUG","ts":"2025-11-27T23:16:59.637Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=1"}
{"level":"DEBUG","ts":"2025-11-27T23:16:59.637Z","msg":"Collected capacity metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, metricsCount=1"}
{"level":"DEBUG","ts":"2025-11-27T23:16:59.637Z","msg":"Variant analysis initialized: variant=ms-inference-scheduling-llm-d-modelservice-decode, accelerator=H100, cost=10.00, replicaCount=1"}
{"level":"DEBUG","ts":"2025-11-27T23:16:59.637Z","msg":"Scale-down unsafe: insufficient non-saturated replicas: nonSaturated=1, required=2"}
{"level":"DEBUG","ts":"2025-11-27T23:16:59.637Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, totalReplicas=1, nonSaturated=1, avgSpareKv=0.426, avgSpareQueue=5.0, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"INFO","ts":"2025-11-27T23:16:59.637Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, totalReplicas=1, nonSaturated=1, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"DEBUG","ts":"2025-11-27T23:16:59.638Z","msg":"Capacity targets: no scaling needed (avgSpareKv=0.426, avgSpareQueue=5.0, all variants stable)"}
{"level":"DEBUG","ts":"2025-11-27T23:16:59.638Z","msg":"Capacity targets calculated: modelID=unsloth/Meta-Llama-3.1-8B, targets=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"INFO","ts":"2025-11-27T23:16:59.638Z","msg":"Capacity-only decisions made for model: unsloth/Meta-Llama-3.1-8B - decision count: 1"}
{"level":"INFO","ts":"2025-11-27T23:16:59.638Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-11-27T23:16:59.638Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1→target=1"}
{"level":"DEBUG","ts":"2025-11-27T23:16:59.638Z","msg":"Found VA in map: variant=ms-inference-scheduling-llm-d-modelservice-decode, hasCurrentAlloc=true, accelerator=H100"}
{"level":"INFO","ts":"2025-11-27T23:16:59.638Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 1, desired-replicas: 1, accelerator: H100"}
{"level":"INFO","ts":"2025-11-27T23:16:59.638Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=1, accelerator=H100, capacityOnly=true"}
{"level":"INFO","ts":"2025-11-27T23:16:59.642Z","msg":"Applied capacity decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1, target=1, reason=capacity-only mode: no-change"}
{"level":"INFO","ts":"2025-11-27T23:16:59.642Z","msg":"Reconciliation completed successfully: mode=capacity-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-11-27T23:16:59.642Z","msg":"Operating in CAPACITY-ONLY mode: reactive capacity-based scaling only"}
{"level":"INFO","ts":"2025-11-27T23:16:59.642Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-11-27T23:16:59.642Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-11-27T23:16:59.650Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=1, accelerator=H100, ttft=23.35ms, itl=11.63ms, cost=10.00, arrivalRate=%!f(string=46)"}
{"level":"INFO","ts":"2025-11-27T23:16:59.652Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9zt6lq, usage=0.074 (7.4%)"}
{"level":"DEBUG","ts":"2025-11-27T23:16:59.652Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"INFO","ts":"2025-11-27T23:16:59.652Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9zt6lq, queueLength=0"}
{"level":"DEBUG","ts":"2025-11-27T23:16:59.652Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"DEBUG","ts":"2025-11-27T23:16:59.652Z","msg":"Pod-to-variant matching successful: totalPods=1, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"DEBUG","ts":"2025-11-27T23:16:59.652Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=1"}
{"level":"DEBUG","ts":"2025-11-27T23:16:59.652Z","msg":"Collected capacity metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, metricsCount=1"}
{"level":"DEBUG","ts":"2025-11-27T23:16:59.652Z","msg":"Variant analysis initialized: variant=ms-inference-scheduling-llm-d-modelservice-decode, accelerator=H100, cost=10.00, replicaCount=1"}
{"level":"DEBUG","ts":"2025-11-27T23:16:59.652Z","msg":"Scale-down unsafe: insufficient non-saturated replicas: nonSaturated=1, required=2"}
{"level":"DEBUG","ts":"2025-11-27T23:16:59.652Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, totalReplicas=1, nonSaturated=1, avgSpareKv=0.426, avgSpareQueue=5.0, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"INFO","ts":"2025-11-27T23:16:59.652Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, totalReplicas=1, nonSaturated=1, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"DEBUG","ts":"2025-11-27T23:16:59.652Z","msg":"Capacity targets: no scaling needed (avgSpareKv=0.426, avgSpareQueue=5.0, all variants stable)"}
{"level":"DEBUG","ts":"2025-11-27T23:16:59.652Z","msg":"Capacity targets calculated: modelID=unsloth/Meta-Llama-3.1-8B, targets=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"INFO","ts":"2025-11-27T23:16:59.652Z","msg":"Capacity-only decisions made for model: unsloth/Meta-Llama-3.1-8B - decision count: 1"}
{"level":"INFO","ts":"2025-11-27T23:16:59.652Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-11-27T23:16:59.652Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1→target=1"}
{"level":"DEBUG","ts":"2025-11-27T23:16:59.652Z","msg":"Found VA in map: variant=ms-inference-scheduling-llm-d-modelservice-decode, hasCurrentAlloc=true, accelerator=H100"}
{"level":"INFO","ts":"2025-11-27T23:16:59.652Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 1, desired-replicas: 1, accelerator: H100"}
{"level":"INFO","ts":"2025-11-27T23:16:59.652Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=1, accelerator=H100, capacityOnly=true"}
{"level":"INFO","ts":"2025-11-27T23:16:59.657Z","msg":"Applied capacity decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1, target=1, reason=capacity-only mode: no-change"}
{"level":"INFO","ts":"2025-11-27T23:16:59.657Z","msg":"Reconciliation completed successfully: mode=capacity-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-11-27T23:17:29.643Z","msg":"Operating in CAPACITY-ONLY mode: reactive capacity-based scaling only"}
{"level":"INFO","ts":"2025-11-27T23:17:29.643Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-11-27T23:17:29.643Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-11-27T23:17:29.656Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=1, accelerator=H100, ttft=25.26ms, itl=13.19ms, cost=10.00, arrivalRate=%!f(string=72)"}
{"level":"INFO","ts":"2025-11-27T23:17:29.661Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9zt6lq, usage=0.211 (21.1%)"}
{"level":"DEBUG","ts":"2025-11-27T23:17:29.661Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"INFO","ts":"2025-11-27T23:17:29.661Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9zt6lq, queueLength=0"}
{"level":"DEBUG","ts":"2025-11-27T23:17:29.661Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"DEBUG","ts":"2025-11-27T23:17:29.661Z","msg":"Pod-to-variant matching successful: totalPods=1, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"DEBUG","ts":"2025-11-27T23:17:29.661Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=1"}
{"level":"DEBUG","ts":"2025-11-27T23:17:29.661Z","msg":"Collected capacity metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, metricsCount=1"}
{"level":"DEBUG","ts":"2025-11-27T23:17:29.661Z","msg":"Variant analysis initialized: variant=ms-inference-scheduling-llm-d-modelservice-decode, accelerator=H100, cost=10.00, replicaCount=1"}
{"level":"DEBUG","ts":"2025-11-27T23:17:29.661Z","msg":"Scale-down unsafe: insufficient non-saturated replicas: nonSaturated=1, required=2"}
{"level":"DEBUG","ts":"2025-11-27T23:17:29.661Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, totalReplicas=1, nonSaturated=1, avgSpareKv=0.289, avgSpareQueue=5.0, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"INFO","ts":"2025-11-27T23:17:29.661Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, totalReplicas=1, nonSaturated=1, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"DEBUG","ts":"2025-11-27T23:17:29.661Z","msg":"Capacity targets: no scaling needed (avgSpareKv=0.289, avgSpareQueue=5.0, all variants stable)"}
{"level":"DEBUG","ts":"2025-11-27T23:17:29.661Z","msg":"Capacity targets calculated: modelID=unsloth/Meta-Llama-3.1-8B, targets=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"INFO","ts":"2025-11-27T23:17:29.661Z","msg":"Capacity-only decisions made for model: unsloth/Meta-Llama-3.1-8B - decision count: 1"}
{"level":"INFO","ts":"2025-11-27T23:17:29.661Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-11-27T23:17:29.661Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1→target=1"}
{"level":"DEBUG","ts":"2025-11-27T23:17:29.661Z","msg":"Found VA in map: variant=ms-inference-scheduling-llm-d-modelservice-decode, hasCurrentAlloc=true, accelerator=H100"}
{"level":"INFO","ts":"2025-11-27T23:17:29.661Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 1, desired-replicas: 1, accelerator: H100"}
{"level":"INFO","ts":"2025-11-27T23:17:29.661Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=1, accelerator=H100, capacityOnly=true"}
{"level":"INFO","ts":"2025-11-27T23:17:29.666Z","msg":"Applied capacity decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1, target=1, reason=capacity-only mode: no-change"}
{"level":"INFO","ts":"2025-11-27T23:17:29.666Z","msg":"Reconciliation completed successfully: mode=capacity-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-11-27T23:17:29.666Z","msg":"Operating in CAPACITY-ONLY mode: reactive capacity-based scaling only"}
{"level":"INFO","ts":"2025-11-27T23:17:29.666Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-11-27T23:17:29.666Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-11-27T23:17:29.677Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=1, accelerator=H100, ttft=25.26ms, itl=13.19ms, cost=10.00, arrivalRate=%!f(string=72)"}
{"level":"INFO","ts":"2025-11-27T23:17:29.679Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9zt6lq, queueLength=0"}
{"level":"DEBUG","ts":"2025-11-27T23:17:29.679Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"INFO","ts":"2025-11-27T23:17:29.679Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9zt6lq, usage=0.211 (21.1%)"}
{"level":"DEBUG","ts":"2025-11-27T23:17:29.679Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"DEBUG","ts":"2025-11-27T23:17:29.679Z","msg":"Pod-to-variant matching successful: totalPods=1, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"DEBUG","ts":"2025-11-27T23:17:29.679Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=1"}
{"level":"DEBUG","ts":"2025-11-27T23:17:29.679Z","msg":"Collected capacity metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, metricsCount=1"}
{"level":"DEBUG","ts":"2025-11-27T23:17:29.679Z","msg":"Variant analysis initialized: variant=ms-inference-scheduling-llm-d-modelservice-decode, accelerator=H100, cost=10.00, replicaCount=1"}
{"level":"DEBUG","ts":"2025-11-27T23:17:29.679Z","msg":"Scale-down unsafe: insufficient non-saturated replicas: nonSaturated=1, required=2"}
{"level":"DEBUG","ts":"2025-11-27T23:17:29.679Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, totalReplicas=1, nonSaturated=1, avgSpareKv=0.289, avgSpareQueue=5.0, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"INFO","ts":"2025-11-27T23:17:29.679Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, totalReplicas=1, nonSaturated=1, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"DEBUG","ts":"2025-11-27T23:17:29.679Z","msg":"Capacity targets: no scaling needed (avgSpareKv=0.289, avgSpareQueue=5.0, all variants stable)"}
{"level":"DEBUG","ts":"2025-11-27T23:17:29.679Z","msg":"Capacity targets calculated: modelID=unsloth/Meta-Llama-3.1-8B, targets=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"INFO","ts":"2025-11-27T23:17:29.679Z","msg":"Capacity-only decisions made for model: unsloth/Meta-Llama-3.1-8B - decision count: 1"}
{"level":"INFO","ts":"2025-11-27T23:17:29.679Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-11-27T23:17:29.679Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1→target=1"}
{"level":"DEBUG","ts":"2025-11-27T23:17:29.679Z","msg":"Found VA in map: variant=ms-inference-scheduling-llm-d-modelservice-decode, hasCurrentAlloc=true, accelerator=H100"}
{"level":"INFO","ts":"2025-11-27T23:17:29.679Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 1, desired-replicas: 1, accelerator: H100"}
{"level":"INFO","ts":"2025-11-27T23:17:29.679Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=1, accelerator=H100, capacityOnly=true"}
{"level":"INFO","ts":"2025-11-27T23:17:29.683Z","msg":"Applied capacity decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1, target=1, reason=capacity-only mode: no-change"}
{"level":"INFO","ts":"2025-11-27T23:17:29.683Z","msg":"Reconciliation completed successfully: mode=capacity-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-11-27T23:17:59.666Z","msg":"Operating in CAPACITY-ONLY mode: reactive capacity-based scaling only"}
{"level":"INFO","ts":"2025-11-27T23:17:59.666Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-11-27T23:17:59.666Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-11-27T23:17:59.685Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=1, accelerator=H100, ttft=36.74ms, itl=21.32ms, cost=10.00, arrivalRate=%!f(string=11)"}
{"level":"INFO","ts":"2025-11-27T23:17:59.687Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9zt6lq, queueLength=0"}
{"level":"DEBUG","ts":"2025-11-27T23:17:59.687Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"INFO","ts":"2025-11-27T23:17:59.687Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9zt6lq, usage=0.377 (37.7%)"}
{"level":"DEBUG","ts":"2025-11-27T23:17:59.687Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"DEBUG","ts":"2025-11-27T23:17:59.687Z","msg":"Pod-to-variant matching successful: totalPods=1, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"DEBUG","ts":"2025-11-27T23:17:59.687Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=1"}
{"level":"DEBUG","ts":"2025-11-27T23:17:59.687Z","msg":"Collected capacity metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, metricsCount=1"}
{"level":"DEBUG","ts":"2025-11-27T23:17:59.687Z","msg":"Variant analysis initialized: variant=ms-inference-scheduling-llm-d-modelservice-decode, accelerator=H100, cost=10.00, replicaCount=1"}
{"level":"DEBUG","ts":"2025-11-27T23:17:59.687Z","msg":"Scale-down unsafe: insufficient non-saturated replicas: nonSaturated=1, required=2"}
{"level":"DEBUG","ts":"2025-11-27T23:17:59.687Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, totalReplicas=1, nonSaturated=1, avgSpareKv=0.123, avgSpareQueue=5.0, shouldScaleUp=true, scaleDownSafe=false"}
{"level":"INFO","ts":"2025-11-27T23:17:59.687Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, totalReplicas=1, nonSaturated=1, shouldScaleUp=true, scaleDownSafe=false"}
{"level":"INFO","ts":"2025-11-27T23:17:59.687Z","msg":"Capacity target: scale-up cheapest variant: variant=ms-inference-scheduling-llm-d-modelservice-decode, cost=10.00, currentReplicas=1, readyReplicas=1, target=2, reason=KV spare capacity low (0.123 < 0.200)"}
{"level":"DEBUG","ts":"2025-11-27T23:17:59.687Z","msg":"Capacity targets calculated: modelID=unsloth/Meta-Llama-3.1-8B, targets=map[ms-inference-scheduling-llm-d-modelservice-decode:2]"}
{"level":"INFO","ts":"2025-11-27T23:17:59.687Z","msg":"Capacity-only decisions made for model: unsloth/Meta-Llama-3.1-8B - decision count: 1"}
{"level":"INFO","ts":"2025-11-27T23:17:59.687Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-11-27T23:17:59.687Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=scale-up, current=1→target=2"}
{"level":"DEBUG","ts":"2025-11-27T23:17:59.687Z","msg":"Found VA in map: variant=ms-inference-scheduling-llm-d-modelservice-decode, hasCurrentAlloc=true, accelerator=H100"}
{"level":"INFO","ts":"2025-11-27T23:17:59.687Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 1, desired-replicas: 2, accelerator: H100"}
{"level":"INFO","ts":"2025-11-27T23:17:59.687Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=2, accelerator=H100, capacityOnly=true"}
{"level":"INFO","ts":"2025-11-27T23:17:59.776Z","msg":"Applied capacity decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=scale-up, current=1, target=2, reason=capacity-only mode: scale-up"}
{"level":"INFO","ts":"2025-11-27T23:17:59.776Z","msg":"Reconciliation completed successfully: mode=capacity-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-11-27T23:17:59.776Z","msg":"Operating in CAPACITY-ONLY mode: reactive capacity-based scaling only"}
{"level":"INFO","ts":"2025-11-27T23:17:59.776Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-11-27T23:17:59.776Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-11-27T23:17:59.786Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=1, accelerator=H100, ttft=36.74ms, itl=21.32ms, cost=10.00, arrivalRate=%!f(string=11)"}
{"level":"INFO","ts":"2025-11-27T23:17:59.787Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9zt6lq, queueLength=0"}
{"level":"DEBUG","ts":"2025-11-27T23:17:59.787Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"INFO","ts":"2025-11-27T23:17:59.787Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9zt6lq, usage=0.377 (37.7%)"}
{"level":"DEBUG","ts":"2025-11-27T23:17:59.787Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"DEBUG","ts":"2025-11-27T23:17:59.788Z","msg":"Pod-to-variant matching successful: totalPods=1, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"DEBUG","ts":"2025-11-27T23:17:59.788Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=1"}
{"level":"DEBUG","ts":"2025-11-27T23:17:59.788Z","msg":"Collected capacity metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, metricsCount=1"}
{"level":"DEBUG","ts":"2025-11-27T23:17:59.788Z","msg":"Variant analysis initialized: variant=ms-inference-scheduling-llm-d-modelservice-decode, accelerator=H100, cost=10.00, replicaCount=1"}
{"level":"DEBUG","ts":"2025-11-27T23:17:59.788Z","msg":"Scale-down unsafe: insufficient non-saturated replicas: nonSaturated=1, required=2"}
{"level":"DEBUG","ts":"2025-11-27T23:17:59.788Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, totalReplicas=1, nonSaturated=1, avgSpareKv=0.123, avgSpareQueue=5.0, shouldScaleUp=true, scaleDownSafe=false"}
{"level":"INFO","ts":"2025-11-27T23:17:59.788Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, totalReplicas=1, nonSaturated=1, shouldScaleUp=true, scaleDownSafe=false"}
{"level":"DEBUG","ts":"2025-11-27T23:17:59.788Z","msg":"Preserving desired replicas: variant=ms-inference-scheduling-llm-d-modelservice-decode, currentReplicas=1, readyReplicas=1, desired=2"}
{"level":"DEBUG","ts":"2025-11-27T23:17:59.788Z","msg":"Capacity targets calculated: modelID=unsloth/Meta-Llama-3.1-8B, targets=map[ms-inference-scheduling-llm-d-modelservice-decode:2]"}
{"level":"INFO","ts":"2025-11-27T23:17:59.788Z","msg":"Capacity-only decisions made for model: unsloth/Meta-Llama-3.1-8B - decision count: 1"}
{"level":"INFO","ts":"2025-11-27T23:17:59.788Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-11-27T23:17:59.788Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=scale-up, current=1→target=2"}
{"level":"DEBUG","ts":"2025-11-27T23:17:59.788Z","msg":"Found VA in map: variant=ms-inference-scheduling-llm-d-modelservice-decode, hasCurrentAlloc=true, accelerator=H100"}
{"level":"INFO","ts":"2025-11-27T23:17:59.788Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 1, desired-replicas: 2, accelerator: H100"}
{"level":"INFO","ts":"2025-11-27T23:17:59.788Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=2, accelerator=H100, capacityOnly=true"}
{"level":"INFO","ts":"2025-11-27T23:17:59.792Z","msg":"Applied capacity decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=scale-up, current=1, target=2, reason=capacity-only mode: scale-up"}
{"level":"INFO","ts":"2025-11-27T23:17:59.792Z","msg":"Reconciliation completed successfully: mode=capacity-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-11-27T23:18:29.776Z","msg":"Operating in CAPACITY-ONLY mode: reactive capacity-based scaling only"}
{"level":"INFO","ts":"2025-11-27T23:18:29.776Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-11-27T23:18:29.776Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-11-27T23:18:29.810Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=2, accelerator=H100, ttft=47.51ms, itl=27.06ms, cost=20.00, arrivalRate=%!f(string=13)"}
{"level":"INFO","ts":"2025-11-27T23:18:29.812Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9zt6lq, queueLength=0"}
{"level":"DEBUG","ts":"2025-11-27T23:18:29.812Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"INFO","ts":"2025-11-27T23:18:29.812Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9zt6lq, usage=0.397 (39.7%)"}
{"level":"DEBUG","ts":"2025-11-27T23:18:29.812Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"DEBUG","ts":"2025-11-27T23:18:29.812Z","msg":"Pod-to-variant matching successful: totalPods=1, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"DEBUG","ts":"2025-11-27T23:18:29.812Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=1"}
{"level":"DEBUG","ts":"2025-11-27T23:18:29.812Z","msg":"Collected capacity metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, metricsCount=1"}
{"level":"DEBUG","ts":"2025-11-27T23:18:29.812Z","msg":"Variant analysis initialized: variant=ms-inference-scheduling-llm-d-modelservice-decode, accelerator=H100, cost=10.00, replicaCount=1"}
{"level":"DEBUG","ts":"2025-11-27T23:18:29.812Z","msg":"Scale-down unsafe: insufficient non-saturated replicas: nonSaturated=1, required=2"}
{"level":"DEBUG","ts":"2025-11-27T23:18:29.812Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, totalReplicas=1, nonSaturated=1, avgSpareKv=0.103, avgSpareQueue=5.0, shouldScaleUp=true, scaleDownSafe=false"}
{"level":"INFO","ts":"2025-11-27T23:18:29.812Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, totalReplicas=1, nonSaturated=1, shouldScaleUp=true, scaleDownSafe=false"}
{"level":"INFO","ts":"2025-11-27T23:18:29.812Z","msg":"Capacity target: scale-up cheapest variant: variant=ms-inference-scheduling-llm-d-modelservice-decode, cost=10.00, currentReplicas=2, readyReplicas=1, target=2, reason=KV spare capacity low (0.103 < 0.200)"}
{"level":"DEBUG","ts":"2025-11-27T23:18:29.812Z","msg":"Capacity targets calculated: modelID=unsloth/Meta-Llama-3.1-8B, targets=map[ms-inference-scheduling-llm-d-modelservice-decode:2]"}
{"level":"INFO","ts":"2025-11-27T23:18:29.812Z","msg":"Capacity-only decisions made for model: unsloth/Meta-Llama-3.1-8B - decision count: 1"}
{"level":"INFO","ts":"2025-11-27T23:18:29.812Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-11-27T23:18:29.812Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=2→target=2"}
{"level":"DEBUG","ts":"2025-11-27T23:18:29.812Z","msg":"Found VA in map: variant=ms-inference-scheduling-llm-d-modelservice-decode, hasCurrentAlloc=true, accelerator=H100"}
{"level":"INFO","ts":"2025-11-27T23:18:29.812Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 2, desired-replicas: 2, accelerator: H100"}
{"level":"INFO","ts":"2025-11-27T23:18:29.812Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=2, accelerator=H100, capacityOnly=true"}
{"level":"INFO","ts":"2025-11-27T23:18:29.817Z","msg":"Applied capacity decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=2, target=2, reason=capacity-only mode: no-change"}
{"level":"INFO","ts":"2025-11-27T23:18:29.817Z","msg":"Reconciliation completed successfully: mode=capacity-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-11-27T23:18:29.817Z","msg":"Operating in CAPACITY-ONLY mode: reactive capacity-based scaling only"}
{"level":"INFO","ts":"2025-11-27T23:18:29.817Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-11-27T23:18:29.817Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-11-27T23:18:29.826Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=2, accelerator=H100, ttft=47.51ms, itl=27.06ms, cost=20.00, arrivalRate=%!f(string=13)"}
{"level":"INFO","ts":"2025-11-27T23:18:29.828Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9zt6lq, usage=0.397 (39.7%)"}
{"level":"DEBUG","ts":"2025-11-27T23:18:29.828Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"INFO","ts":"2025-11-27T23:18:29.828Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9zt6lq, queueLength=0"}
{"level":"DEBUG","ts":"2025-11-27T23:18:29.828Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"DEBUG","ts":"2025-11-27T23:18:29.828Z","msg":"Pod-to-variant matching successful: totalPods=1, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"DEBUG","ts":"2025-11-27T23:18:29.828Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=1"}
{"level":"DEBUG","ts":"2025-11-27T23:18:29.828Z","msg":"Collected capacity metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, metricsCount=1"}
{"level":"DEBUG","ts":"2025-11-27T23:18:29.828Z","msg":"Variant analysis initialized: variant=ms-inference-scheduling-llm-d-modelservice-decode, accelerator=H100, cost=10.00, replicaCount=1"}
{"level":"DEBUG","ts":"2025-11-27T23:18:29.828Z","msg":"Scale-down unsafe: insufficient non-saturated replicas: nonSaturated=1, required=2"}
{"level":"DEBUG","ts":"2025-11-27T23:18:29.828Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, totalReplicas=1, nonSaturated=1, avgSpareKv=0.103, avgSpareQueue=5.0, shouldScaleUp=true, scaleDownSafe=false"}
{"level":"INFO","ts":"2025-11-27T23:18:29.828Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, totalReplicas=1, nonSaturated=1, shouldScaleUp=true, scaleDownSafe=false"}
{"level":"INFO","ts":"2025-11-27T23:18:29.828Z","msg":"Capacity target: scale-up cheapest variant: variant=ms-inference-scheduling-llm-d-modelservice-decode, cost=10.00, currentReplicas=2, readyReplicas=1, target=2, reason=KV spare capacity low (0.103 < 0.200)"}
{"level":"DEBUG","ts":"2025-11-27T23:18:29.828Z","msg":"Capacity targets calculated: modelID=unsloth/Meta-Llama-3.1-8B, targets=map[ms-inference-scheduling-llm-d-modelservice-decode:2]"}
{"level":"INFO","ts":"2025-11-27T23:18:29.828Z","msg":"Capacity-only decisions made for model: unsloth/Meta-Llama-3.1-8B - decision count: 1"}
{"level":"INFO","ts":"2025-11-27T23:18:29.828Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-11-27T23:18:29.828Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=2→target=2"}
{"level":"DEBUG","ts":"2025-11-27T23:18:29.828Z","msg":"Found VA in map: variant=ms-inference-scheduling-llm-d-modelservice-decode, hasCurrentAlloc=true, accelerator=H100"}
{"level":"INFO","ts":"2025-11-27T23:18:29.828Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 2, desired-replicas: 2, accelerator: H100"}
{"level":"INFO","ts":"2025-11-27T23:18:29.828Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=2, accelerator=H100, capacityOnly=true"}
{"level":"INFO","ts":"2025-11-27T23:18:29.833Z","msg":"Applied capacity decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=2, target=2, reason=capacity-only mode: no-change"}
{"level":"INFO","ts":"2025-11-27T23:18:29.833Z","msg":"Reconciliation completed successfully: mode=capacity-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-11-27T23:18:59.819Z","msg":"Operating in CAPACITY-ONLY mode: reactive capacity-based scaling only"}
{"level":"INFO","ts":"2025-11-27T23:18:59.819Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-11-27T23:18:59.819Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-11-27T23:18:59.833Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=2, accelerator=H100, ttft=53.29ms, itl=31.55ms, cost=20.00, arrivalRate=%!f(string=14)"}
{"level":"INFO","ts":"2025-11-27T23:18:59.835Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9zt6lq, usage=0.559 (55.9%)"}
{"level":"DEBUG","ts":"2025-11-27T23:18:59.835Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"INFO","ts":"2025-11-27T23:18:59.835Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9zt6lq, queueLength=0"}
{"level":"DEBUG","ts":"2025-11-27T23:18:59.835Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"DEBUG","ts":"2025-11-27T23:18:59.835Z","msg":"Pod-to-variant matching successful: totalPods=1, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"DEBUG","ts":"2025-11-27T23:18:59.835Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=1"}
{"level":"DEBUG","ts":"2025-11-27T23:18:59.835Z","msg":"Collected capacity metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, metricsCount=1"}
{"level":"DEBUG","ts":"2025-11-27T23:18:59.835Z","msg":"Variant analysis initialized: variant=ms-inference-scheduling-llm-d-modelservice-decode, accelerator=H100, cost=10.00, replicaCount=1"}
{"level":"DEBUG","ts":"2025-11-27T23:18:59.835Z","msg":"Scale-down unsafe: insufficient non-saturated replicas: nonSaturated=0, required=2"}
{"level":"DEBUG","ts":"2025-11-27T23:18:59.835Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, totalReplicas=1, nonSaturated=0, avgSpareKv=0.000, avgSpareQueue=0.0, shouldScaleUp=true, scaleDownSafe=false"}
{"level":"INFO","ts":"2025-11-27T23:18:59.835Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, totalReplicas=1, nonSaturated=0, shouldScaleUp=true, scaleDownSafe=false"}
{"level":"INFO","ts":"2025-11-27T23:18:59.835Z","msg":"Capacity target: scale-up cheapest variant: variant=ms-inference-scheduling-llm-d-modelservice-decode, cost=10.00, currentReplicas=2, readyReplicas=1, target=2, reason=both KV spare (0.000 < 0.200) and queue spare (0.0 < 3.0)"}
{"level":"DEBUG","ts":"2025-11-27T23:18:59.835Z","msg":"Capacity targets calculated: modelID=unsloth/Meta-Llama-3.1-8B, targets=map[ms-inference-scheduling-llm-d-modelservice-decode:2]"}
{"level":"INFO","ts":"2025-11-27T23:18:59.835Z","msg":"Capacity-only decisions made for model: unsloth/Meta-Llama-3.1-8B - decision count: 1"}
{"level":"INFO","ts":"2025-11-27T23:18:59.835Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-11-27T23:18:59.835Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=2→target=2"}
{"level":"DEBUG","ts":"2025-11-27T23:18:59.835Z","msg":"Found VA in map: variant=ms-inference-scheduling-llm-d-modelservice-decode, hasCurrentAlloc=true, accelerator=H100"}
{"level":"INFO","ts":"2025-11-27T23:18:59.835Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 2, desired-replicas: 2, accelerator: H100"}
{"level":"INFO","ts":"2025-11-27T23:18:59.835Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=2, accelerator=H100, capacityOnly=true"}
{"level":"INFO","ts":"2025-11-27T23:18:59.841Z","msg":"Applied capacity decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=2, target=2, reason=capacity-only mode: no-change"}
{"level":"INFO","ts":"2025-11-27T23:18:59.841Z","msg":"Reconciliation completed successfully: mode=capacity-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-11-27T23:18:59.841Z","msg":"Operating in CAPACITY-ONLY mode: reactive capacity-based scaling only"}
{"level":"INFO","ts":"2025-11-27T23:18:59.841Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-11-27T23:18:59.841Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-11-27T23:18:59.849Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=2, accelerator=H100, ttft=53.29ms, itl=31.55ms, cost=20.00, arrivalRate=%!f(string=14)"}
{"level":"INFO","ts":"2025-11-27T23:18:59.851Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9zt6lq, queueLength=0"}
{"level":"DEBUG","ts":"2025-11-27T23:18:59.851Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"INFO","ts":"2025-11-27T23:18:59.851Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9zt6lq, usage=0.559 (55.9%)"}
{"level":"DEBUG","ts":"2025-11-27T23:18:59.851Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"DEBUG","ts":"2025-11-27T23:18:59.851Z","msg":"Pod-to-variant matching successful: totalPods=1, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"DEBUG","ts":"2025-11-27T23:18:59.851Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=1"}
{"level":"DEBUG","ts":"2025-11-27T23:18:59.851Z","msg":"Collected capacity metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, metricsCount=1"}
{"level":"DEBUG","ts":"2025-11-27T23:18:59.851Z","msg":"Variant analysis initialized: variant=ms-inference-scheduling-llm-d-modelservice-decode, accelerator=H100, cost=10.00, replicaCount=1"}
{"level":"DEBUG","ts":"2025-11-27T23:18:59.851Z","msg":"Scale-down unsafe: insufficient non-saturated replicas: nonSaturated=0, required=2"}
{"level":"DEBUG","ts":"2025-11-27T23:18:59.851Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, totalReplicas=1, nonSaturated=0, avgSpareKv=0.000, avgSpareQueue=0.0, shouldScaleUp=true, scaleDownSafe=false"}
{"level":"INFO","ts":"2025-11-27T23:18:59.851Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, totalReplicas=1, nonSaturated=0, shouldScaleUp=true, scaleDownSafe=false"}
{"level":"INFO","ts":"2025-11-27T23:18:59.851Z","msg":"Capacity target: scale-up cheapest variant: variant=ms-inference-scheduling-llm-d-modelservice-decode, cost=10.00, currentReplicas=2, readyReplicas=1, target=2, reason=both KV spare (0.000 < 0.200) and queue spare (0.0 < 3.0)"}
{"level":"DEBUG","ts":"2025-11-27T23:18:59.851Z","msg":"Capacity targets calculated: modelID=unsloth/Meta-Llama-3.1-8B, targets=map[ms-inference-scheduling-llm-d-modelservice-decode:2]"}
{"level":"INFO","ts":"2025-11-27T23:18:59.851Z","msg":"Capacity-only decisions made for model: unsloth/Meta-Llama-3.1-8B - decision count: 1"}
{"level":"INFO","ts":"2025-11-27T23:18:59.851Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-11-27T23:18:59.851Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=2→target=2"}
{"level":"DEBUG","ts":"2025-11-27T23:18:59.851Z","msg":"Found VA in map: variant=ms-inference-scheduling-llm-d-modelservice-decode, hasCurrentAlloc=true, accelerator=H100"}
{"level":"INFO","ts":"2025-11-27T23:18:59.851Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 2, desired-replicas: 2, accelerator: H100"}
{"level":"INFO","ts":"2025-11-27T23:18:59.851Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=2, accelerator=H100, capacityOnly=true"}
{"level":"INFO","ts":"2025-11-27T23:18:59.855Z","msg":"Applied capacity decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=2, target=2, reason=capacity-only mode: no-change"}
{"level":"INFO","ts":"2025-11-27T23:18:59.855Z","msg":"Reconciliation completed successfully: mode=capacity-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-11-27T23:19:29.842Z","msg":"Operating in CAPACITY-ONLY mode: reactive capacity-based scaling only"}
{"level":"INFO","ts":"2025-11-27T23:19:29.842Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-11-27T23:19:29.842Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-11-27T23:19:29.888Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=2, accelerator=H100, ttft=58.81ms, itl=35.18ms, cost=20.00, arrivalRate=%!f(string=13)"}
{"level":"INFO","ts":"2025-11-27T23:19:29.890Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9zt6lq, queueLength=0"}
{"level":"DEBUG","ts":"2025-11-27T23:19:29.890Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"INFO","ts":"2025-11-27T23:19:29.890Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9zt6lq, usage=0.559 (55.9%)"}
{"level":"DEBUG","ts":"2025-11-27T23:19:29.890Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"DEBUG","ts":"2025-11-27T23:19:29.890Z","msg":"Pod-to-variant matching successful: totalPods=1, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"DEBUG","ts":"2025-11-27T23:19:29.890Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=1"}
{"level":"DEBUG","ts":"2025-11-27T23:19:29.890Z","msg":"Collected capacity metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, metricsCount=1"}
{"level":"DEBUG","ts":"2025-11-27T23:19:29.890Z","msg":"Variant analysis initialized: variant=ms-inference-scheduling-llm-d-modelservice-decode, accelerator=H100, cost=10.00, replicaCount=1"}
{"level":"DEBUG","ts":"2025-11-27T23:19:29.890Z","msg":"Scale-down unsafe: insufficient non-saturated replicas: nonSaturated=0, required=2"}
{"level":"DEBUG","ts":"2025-11-27T23:19:29.890Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, totalReplicas=1, nonSaturated=0, avgSpareKv=0.000, avgSpareQueue=0.0, shouldScaleUp=true, scaleDownSafe=false"}
{"level":"INFO","ts":"2025-11-27T23:19:29.890Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, totalReplicas=1, nonSaturated=0, shouldScaleUp=true, scaleDownSafe=false"}
{"level":"INFO","ts":"2025-11-27T23:19:29.890Z","msg":"Capacity target: scale-up cheapest variant: variant=ms-inference-scheduling-llm-d-modelservice-decode, cost=10.00, currentReplicas=2, readyReplicas=1, target=2, reason=both KV spare (0.000 < 0.200) and queue spare (0.0 < 3.0)"}
{"level":"DEBUG","ts":"2025-11-27T23:19:29.890Z","msg":"Capacity targets calculated: modelID=unsloth/Meta-Llama-3.1-8B, targets=map[ms-inference-scheduling-llm-d-modelservice-decode:2]"}
{"level":"INFO","ts":"2025-11-27T23:19:29.890Z","msg":"Capacity-only decisions made for model: unsloth/Meta-Llama-3.1-8B - decision count: 1"}
{"level":"INFO","ts":"2025-11-27T23:19:29.890Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-11-27T23:19:29.890Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=2→target=2"}
{"level":"DEBUG","ts":"2025-11-27T23:19:29.890Z","msg":"Found VA in map: variant=ms-inference-scheduling-llm-d-modelservice-decode, hasCurrentAlloc=true, accelerator=H100"}
{"level":"INFO","ts":"2025-11-27T23:19:29.890Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 2, desired-replicas: 2, accelerator: H100"}
{"level":"INFO","ts":"2025-11-27T23:19:29.890Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=2, accelerator=H100, capacityOnly=true"}
{"level":"INFO","ts":"2025-11-27T23:19:29.895Z","msg":"Applied capacity decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=2, target=2, reason=capacity-only mode: no-change"}
{"level":"INFO","ts":"2025-11-27T23:19:29.895Z","msg":"Reconciliation completed successfully: mode=capacity-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-11-27T23:19:29.895Z","msg":"Operating in CAPACITY-ONLY mode: reactive capacity-based scaling only"}
{"level":"INFO","ts":"2025-11-27T23:19:29.895Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-11-27T23:19:29.895Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-11-27T23:19:29.904Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=2, accelerator=H100, ttft=58.81ms, itl=35.18ms, cost=20.00, arrivalRate=%!f(string=13)"}
{"level":"INFO","ts":"2025-11-27T23:19:29.906Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9zt6lq, queueLength=0"}
{"level":"DEBUG","ts":"2025-11-27T23:19:29.906Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"INFO","ts":"2025-11-27T23:19:29.906Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9zt6lq, usage=0.559 (55.9%)"}
{"level":"DEBUG","ts":"2025-11-27T23:19:29.906Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"DEBUG","ts":"2025-11-27T23:19:29.906Z","msg":"Pod-to-variant matching successful: totalPods=1, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"DEBUG","ts":"2025-11-27T23:19:29.906Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=1"}
{"level":"DEBUG","ts":"2025-11-27T23:19:29.906Z","msg":"Collected capacity metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, metricsCount=1"}
{"level":"DEBUG","ts":"2025-11-27T23:19:29.906Z","msg":"Variant analysis initialized: variant=ms-inference-scheduling-llm-d-modelservice-decode, accelerator=H100, cost=10.00, replicaCount=1"}
{"level":"DEBUG","ts":"2025-11-27T23:19:29.906Z","msg":"Scale-down unsafe: insufficient non-saturated replicas: nonSaturated=0, required=2"}
{"level":"DEBUG","ts":"2025-11-27T23:19:29.906Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, totalReplicas=1, nonSaturated=0, avgSpareKv=0.000, avgSpareQueue=0.0, shouldScaleUp=true, scaleDownSafe=false"}
{"level":"INFO","ts":"2025-11-27T23:19:29.906Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, totalReplicas=1, nonSaturated=0, shouldScaleUp=true, scaleDownSafe=false"}
{"level":"INFO","ts":"2025-11-27T23:19:29.906Z","msg":"Capacity target: scale-up cheapest variant: variant=ms-inference-scheduling-llm-d-modelservice-decode, cost=10.00, currentReplicas=2, readyReplicas=1, target=2, reason=both KV spare (0.000 < 0.200) and queue spare (0.0 < 3.0)"}
{"level":"DEBUG","ts":"2025-11-27T23:19:29.906Z","msg":"Capacity targets calculated: modelID=unsloth/Meta-Llama-3.1-8B, targets=map[ms-inference-scheduling-llm-d-modelservice-decode:2]"}
{"level":"INFO","ts":"2025-11-27T23:19:29.906Z","msg":"Capacity-only decisions made for model: unsloth/Meta-Llama-3.1-8B - decision count: 1"}
{"level":"INFO","ts":"2025-11-27T23:19:29.906Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-11-27T23:19:29.906Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=2→target=2"}
{"level":"DEBUG","ts":"2025-11-27T23:19:29.906Z","msg":"Found VA in map: variant=ms-inference-scheduling-llm-d-modelservice-decode, hasCurrentAlloc=true, accelerator=H100"}
{"level":"INFO","ts":"2025-11-27T23:19:29.906Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 2, desired-replicas: 2, accelerator: H100"}
{"level":"INFO","ts":"2025-11-27T23:19:29.906Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=2, accelerator=H100, capacityOnly=true"}
{"level":"INFO","ts":"2025-11-27T23:19:29.910Z","msg":"Applied capacity decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=2, target=2, reason=capacity-only mode: no-change"}
{"level":"INFO","ts":"2025-11-27T23:19:29.910Z","msg":"Reconciliation completed successfully: mode=capacity-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-11-27T23:19:59.897Z","msg":"Operating in CAPACITY-ONLY mode: reactive capacity-based scaling only"}
{"level":"INFO","ts":"2025-11-27T23:19:59.897Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-11-27T23:19:59.897Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-11-27T23:19:59.922Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=2, accelerator=H100, ttft=59.91ms, itl=35.36ms, cost=20.00, arrivalRate=%!f(string=14)"}
{"level":"INFO","ts":"2025-11-27T23:19:59.978Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9zt6lq, queueLength=0"}
{"level":"INFO","ts":"2025-11-27T23:19:59.978Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9q8qn9, queueLength=0"}
{"level":"DEBUG","ts":"2025-11-27T23:19:59.978Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=2"}
{"level":"INFO","ts":"2025-11-27T23:19:59.982Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9zt6lq, usage=0.533 (53.3%)"}
{"level":"INFO","ts":"2025-11-27T23:19:59.982Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9q8qn9, usage=0.033 (3.3%)"}
{"level":"DEBUG","ts":"2025-11-27T23:19:59.982Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=2"}
{"level":"DEBUG","ts":"2025-11-27T23:19:59.983Z","msg":"Pod-to-variant matching successful: totalPods=2, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:2]"}
{"level":"DEBUG","ts":"2025-11-27T23:19:59.983Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=2"}
{"level":"DEBUG","ts":"2025-11-27T23:19:59.983Z","msg":"Collected capacity metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, metricsCount=2"}
{"level":"DEBUG","ts":"2025-11-27T23:19:59.983Z","msg":"Variant analysis initialized: variant=ms-inference-scheduling-llm-d-modelservice-decode, accelerator=H100, cost=10.00, replicaCount=2"}
{"level":"DEBUG","ts":"2025-11-27T23:19:59.983Z","msg":"Scale-down unsafe: insufficient non-saturated replicas: nonSaturated=1, required=2"}
{"level":"DEBUG","ts":"2025-11-27T23:19:59.983Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, totalReplicas=2, nonSaturated=1, avgSpareKv=0.467, avgSpareQueue=5.0, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"INFO","ts":"2025-11-27T23:19:59.983Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, totalReplicas=2, nonSaturated=1, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"DEBUG","ts":"2025-11-27T23:19:59.983Z","msg":"Capacity targets: no scaling needed (avgSpareKv=0.467, avgSpareQueue=5.0, all variants stable)"}
{"level":"DEBUG","ts":"2025-11-27T23:19:59.983Z","msg":"Capacity targets calculated: modelID=unsloth/Meta-Llama-3.1-8B, targets=map[ms-inference-scheduling-llm-d-modelservice-decode:2]"}
{"level":"INFO","ts":"2025-11-27T23:19:59.983Z","msg":"Capacity-only decisions made for model: unsloth/Meta-Llama-3.1-8B - decision count: 1"}
{"level":"INFO","ts":"2025-11-27T23:19:59.983Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-11-27T23:19:59.983Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=2→target=2"}
{"level":"DEBUG","ts":"2025-11-27T23:19:59.983Z","msg":"Found VA in map: variant=ms-inference-scheduling-llm-d-modelservice-decode, hasCurrentAlloc=true, accelerator=H100"}
{"level":"INFO","ts":"2025-11-27T23:19:59.983Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 2, desired-replicas: 2, accelerator: H100"}
{"level":"INFO","ts":"2025-11-27T23:19:59.983Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=2, accelerator=H100, capacityOnly=true"}
{"level":"INFO","ts":"2025-11-27T23:20:00.076Z","msg":"Applied capacity decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=2, target=2, reason=capacity-only mode: no-change"}
{"level":"INFO","ts":"2025-11-27T23:20:00.076Z","msg":"Reconciliation completed successfully: mode=capacity-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-11-27T23:20:00.077Z","msg":"Operating in CAPACITY-ONLY mode: reactive capacity-based scaling only"}
{"level":"INFO","ts":"2025-11-27T23:20:00.077Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-11-27T23:20:00.077Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-11-27T23:20:00.091Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=2, accelerator=H100, ttft=59.91ms, itl=35.36ms, cost=20.00, arrivalRate=%!f(string=14)"}
{"level":"INFO","ts":"2025-11-27T23:20:00.093Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9zt6lq, queueLength=0"}
{"level":"INFO","ts":"2025-11-27T23:20:00.093Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9q8qn9, queueLength=0"}
{"level":"DEBUG","ts":"2025-11-27T23:20:00.093Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=2"}
{"level":"INFO","ts":"2025-11-27T23:20:00.093Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9zt6lq, usage=0.533 (53.3%)"}
{"level":"INFO","ts":"2025-11-27T23:20:00.093Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9q8qn9, usage=0.033 (3.3%)"}
{"level":"DEBUG","ts":"2025-11-27T23:20:00.093Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=2"}
{"level":"DEBUG","ts":"2025-11-27T23:20:00.093Z","msg":"Pod-to-variant matching successful: totalPods=2, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:2]"}
{"level":"DEBUG","ts":"2025-11-27T23:20:00.093Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=2"}
{"level":"DEBUG","ts":"2025-11-27T23:20:00.093Z","msg":"Collected capacity metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, metricsCount=2"}
{"level":"DEBUG","ts":"2025-11-27T23:20:00.093Z","msg":"Variant analysis initialized: variant=ms-inference-scheduling-llm-d-modelservice-decode, accelerator=H100, cost=10.00, replicaCount=2"}
{"level":"DEBUG","ts":"2025-11-27T23:20:00.093Z","msg":"Scale-down unsafe: insufficient non-saturated replicas: nonSaturated=1, required=2"}
{"level":"DEBUG","ts":"2025-11-27T23:20:00.093Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, totalReplicas=2, nonSaturated=1, avgSpareKv=0.467, avgSpareQueue=5.0, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"INFO","ts":"2025-11-27T23:20:00.093Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, totalReplicas=2, nonSaturated=1, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"DEBUG","ts":"2025-11-27T23:20:00.094Z","msg":"Capacity targets: no scaling needed (avgSpareKv=0.467, avgSpareQueue=5.0, all variants stable)"}
{"level":"DEBUG","ts":"2025-11-27T23:20:00.094Z","msg":"Capacity targets calculated: modelID=unsloth/Meta-Llama-3.1-8B, targets=map[ms-inference-scheduling-llm-d-modelservice-decode:2]"}
{"level":"INFO","ts":"2025-11-27T23:20:00.094Z","msg":"Capacity-only decisions made for model: unsloth/Meta-Llama-3.1-8B - decision count: 1"}
{"level":"INFO","ts":"2025-11-27T23:20:00.094Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-11-27T23:20:00.094Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=2→target=2"}
{"level":"DEBUG","ts":"2025-11-27T23:20:00.094Z","msg":"Found VA in map: variant=ms-inference-scheduling-llm-d-modelservice-decode, hasCurrentAlloc=true, accelerator=H100"}
{"level":"INFO","ts":"2025-11-27T23:20:00.094Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 2, desired-replicas: 2, accelerator: H100"}
{"level":"INFO","ts":"2025-11-27T23:20:00.094Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=2, accelerator=H100, capacityOnly=true"}
{"level":"INFO","ts":"2025-11-27T23:20:00.098Z","msg":"Applied capacity decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=2, target=2, reason=capacity-only mode: no-change"}
{"level":"INFO","ts":"2025-11-27T23:20:00.098Z","msg":"Reconciliation completed successfully: mode=capacity-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-11-27T23:20:30.077Z","msg":"Operating in CAPACITY-ONLY mode: reactive capacity-based scaling only"}
{"level":"INFO","ts":"2025-11-27T23:20:30.077Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-11-27T23:20:30.077Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-11-27T23:20:30.099Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=2, accelerator=H100, ttft=46.92ms, itl=28.03ms, cost=20.00, arrivalRate=%!f(string=18)"}
{"level":"INFO","ts":"2025-11-27T23:20:30.100Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9zt6lq, queueLength=0"}
{"level":"INFO","ts":"2025-11-27T23:20:30.100Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9q8qn9, queueLength=0"}
{"level":"DEBUG","ts":"2025-11-27T23:20:30.100Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=2"}
{"level":"INFO","ts":"2025-11-27T23:20:30.100Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9zt6lq, usage=0.549 (54.9%)"}
{"level":"INFO","ts":"2025-11-27T23:20:30.100Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9q8qn9, usage=0.107 (10.7%)"}
{"level":"DEBUG","ts":"2025-11-27T23:20:30.100Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=2"}
{"level":"DEBUG","ts":"2025-11-27T23:20:30.100Z","msg":"Pod-to-variant matching successful: totalPods=2, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:2]"}
{"level":"DEBUG","ts":"2025-11-27T23:20:30.100Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=2"}
{"level":"DEBUG","ts":"2025-11-27T23:20:30.100Z","msg":"Collected capacity metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, metricsCount=2"}
{"level":"DEBUG","ts":"2025-11-27T23:20:30.100Z","msg":"Variant analysis initialized: variant=ms-inference-scheduling-llm-d-modelservice-decode, accelerator=H100, cost=10.00, replicaCount=2"}
{"level":"DEBUG","ts":"2025-11-27T23:20:30.100Z","msg":"Scale-down unsafe: insufficient non-saturated replicas: nonSaturated=1, required=2"}
{"level":"DEBUG","ts":"2025-11-27T23:20:30.100Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, totalReplicas=2, nonSaturated=1, avgSpareKv=0.393, avgSpareQueue=5.0, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"INFO","ts":"2025-11-27T23:20:30.100Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, totalReplicas=2, nonSaturated=1, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"DEBUG","ts":"2025-11-27T23:20:30.100Z","msg":"Capacity targets: no scaling needed (avgSpareKv=0.393, avgSpareQueue=5.0, all variants stable)"}
{"level":"DEBUG","ts":"2025-11-27T23:20:30.100Z","msg":"Capacity targets calculated: modelID=unsloth/Meta-Llama-3.1-8B, targets=map[ms-inference-scheduling-llm-d-modelservice-decode:2]"}
{"level":"INFO","ts":"2025-11-27T23:20:30.100Z","msg":"Capacity-only decisions made for model: unsloth/Meta-Llama-3.1-8B - decision count: 1"}
{"level":"INFO","ts":"2025-11-27T23:20:30.100Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-11-27T23:20:30.100Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=2→target=2"}
{"level":"DEBUG","ts":"2025-11-27T23:20:30.100Z","msg":"Found VA in map: variant=ms-inference-scheduling-llm-d-modelservice-decode, hasCurrentAlloc=true, accelerator=H100"}
{"level":"INFO","ts":"2025-11-27T23:20:30.100Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 2, desired-replicas: 2, accelerator: H100"}
{"level":"INFO","ts":"2025-11-27T23:20:30.100Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=2, accelerator=H100, capacityOnly=true"}
{"level":"INFO","ts":"2025-11-27T23:20:30.105Z","msg":"Applied capacity decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=2, target=2, reason=capacity-only mode: no-change"}
{"level":"INFO","ts":"2025-11-27T23:20:30.105Z","msg":"Reconciliation completed successfully: mode=capacity-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-11-27T23:20:30.105Z","msg":"Operating in CAPACITY-ONLY mode: reactive capacity-based scaling only"}
{"level":"INFO","ts":"2025-11-27T23:20:30.105Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-11-27T23:20:30.105Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-11-27T23:20:30.114Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=2, accelerator=H100, ttft=46.91ms, itl=28.03ms, cost=20.00, arrivalRate=%!f(string=18)"}
{"level":"INFO","ts":"2025-11-27T23:20:30.115Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9zt6lq, queueLength=0"}
{"level":"INFO","ts":"2025-11-27T23:20:30.115Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9q8qn9, queueLength=0"}
{"level":"DEBUG","ts":"2025-11-27T23:20:30.115Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=2"}
{"level":"INFO","ts":"2025-11-27T23:20:30.115Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9zt6lq, usage=0.549 (54.9%)"}
{"level":"INFO","ts":"2025-11-27T23:20:30.115Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9q8qn9, usage=0.107 (10.7%)"}
{"level":"DEBUG","ts":"2025-11-27T23:20:30.115Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=2"}
{"level":"DEBUG","ts":"2025-11-27T23:20:30.116Z","msg":"Pod-to-variant matching successful: totalPods=2, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:2]"}
{"level":"DEBUG","ts":"2025-11-27T23:20:30.116Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=2"}
{"level":"DEBUG","ts":"2025-11-27T23:20:30.116Z","msg":"Collected capacity metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, metricsCount=2"}
{"level":"DEBUG","ts":"2025-11-27T23:20:30.116Z","msg":"Variant analysis initialized: variant=ms-inference-scheduling-llm-d-modelservice-decode, accelerator=H100, cost=10.00, replicaCount=2"}
{"level":"DEBUG","ts":"2025-11-27T23:20:30.116Z","msg":"Scale-down unsafe: insufficient non-saturated replicas: nonSaturated=1, required=2"}
{"level":"DEBUG","ts":"2025-11-27T23:20:30.116Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, totalReplicas=2, nonSaturated=1, avgSpareKv=0.393, avgSpareQueue=5.0, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"INFO","ts":"2025-11-27T23:20:30.116Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, totalReplicas=2, nonSaturated=1, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"DEBUG","ts":"2025-11-27T23:20:30.116Z","msg":"Capacity targets: no scaling needed (avgSpareKv=0.393, avgSpareQueue=5.0, all variants stable)"}
{"level":"DEBUG","ts":"2025-11-27T23:20:30.116Z","msg":"Capacity targets calculated: modelID=unsloth/Meta-Llama-3.1-8B, targets=map[ms-inference-scheduling-llm-d-modelservice-decode:2]"}
{"level":"INFO","ts":"2025-11-27T23:20:30.116Z","msg":"Capacity-only decisions made for model: unsloth/Meta-Llama-3.1-8B - decision count: 1"}
{"level":"INFO","ts":"2025-11-27T23:20:30.116Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-11-27T23:20:30.116Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=2→target=2"}
{"level":"DEBUG","ts":"2025-11-27T23:20:30.116Z","msg":"Found VA in map: variant=ms-inference-scheduling-llm-d-modelservice-decode, hasCurrentAlloc=true, accelerator=H100"}
{"level":"INFO","ts":"2025-11-27T23:20:30.116Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 2, desired-replicas: 2, accelerator: H100"}
{"level":"INFO","ts":"2025-11-27T23:20:30.116Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=2, accelerator=H100, capacityOnly=true"}
{"level":"INFO","ts":"2025-11-27T23:20:30.120Z","msg":"Applied capacity decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=2, target=2, reason=capacity-only mode: no-change"}
{"level":"INFO","ts":"2025-11-27T23:20:30.120Z","msg":"Reconciliation completed successfully: mode=capacity-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-11-27T23:21:00.106Z","msg":"Operating in CAPACITY-ONLY mode: reactive capacity-based scaling only"}
{"level":"INFO","ts":"2025-11-27T23:21:00.106Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-11-27T23:21:00.106Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-11-27T23:21:00.119Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=2, accelerator=H100, ttft=38.98ms, itl=22.87ms, cost=20.00, arrivalRate=%!f(string=24)"}
{"level":"INFO","ts":"2025-11-27T23:21:00.120Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9zt6lq, queueLength=0"}
{"level":"INFO","ts":"2025-11-27T23:21:00.120Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9q8qn9, queueLength=0"}
{"level":"DEBUG","ts":"2025-11-27T23:21:00.120Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=2"}
{"level":"INFO","ts":"2025-11-27T23:21:00.120Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9zt6lq, usage=0.475 (47.5%)"}
{"level":"INFO","ts":"2025-11-27T23:21:00.120Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9q8qn9, usage=0.141 (14.1%)"}
{"level":"DEBUG","ts":"2025-11-27T23:21:00.120Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=2"}
{"level":"DEBUG","ts":"2025-11-27T23:21:00.120Z","msg":"Pod-to-variant matching successful: totalPods=2, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:2]"}
{"level":"DEBUG","ts":"2025-11-27T23:21:00.120Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=2"}
{"level":"DEBUG","ts":"2025-11-27T23:21:00.120Z","msg":"Collected capacity metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, metricsCount=2"}
{"level":"DEBUG","ts":"2025-11-27T23:21:00.120Z","msg":"Variant analysis initialized: variant=ms-inference-scheduling-llm-d-modelservice-decode, accelerator=H100, cost=10.00, replicaCount=2"}
{"level":"DEBUG","ts":"2025-11-27T23:21:00.120Z","msg":"Scale-down unsafe: insufficient headroom after redistribution: remainingSpareKv=-0.115, kvTrigger=0.200, kvSafe=false, remainingSpareQueue=5.0, queueTrigger=3.0%!(EXTRA bool=true)"}
{"level":"DEBUG","ts":"2025-11-27T23:21:00.120Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, totalReplicas=2, nonSaturated=2, avgSpareKv=0.192, avgSpareQueue=5.0, shouldScaleUp=true, scaleDownSafe=false"}
{"level":"INFO","ts":"2025-11-27T23:21:00.120Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, totalReplicas=2, nonSaturated=2, shouldScaleUp=true, scaleDownSafe=false"}
{"level":"INFO","ts":"2025-11-27T23:21:00.120Z","msg":"Capacity target: scale-up cheapest variant: variant=ms-inference-scheduling-llm-d-modelservice-decode, cost=10.00, currentReplicas=2, readyReplicas=2, target=3, reason=KV spare capacity low (0.192 < 0.200)"}
{"level":"DEBUG","ts":"2025-11-27T23:21:00.120Z","msg":"Capacity targets calculated: modelID=unsloth/Meta-Llama-3.1-8B, targets=map[ms-inference-scheduling-llm-d-modelservice-decode:3]"}
{"level":"INFO","ts":"2025-11-27T23:21:00.120Z","msg":"Capacity-only decisions made for model: unsloth/Meta-Llama-3.1-8B - decision count: 1"}
{"level":"INFO","ts":"2025-11-27T23:21:00.120Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-11-27T23:21:00.120Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=scale-up, current=2→target=3"}
{"level":"DEBUG","ts":"2025-11-27T23:21:00.120Z","msg":"Found VA in map: variant=ms-inference-scheduling-llm-d-modelservice-decode, hasCurrentAlloc=true, accelerator=H100"}
{"level":"INFO","ts":"2025-11-27T23:21:00.120Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 2, desired-replicas: 3, accelerator: H100"}
{"level":"INFO","ts":"2025-11-27T23:21:00.120Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=3, accelerator=H100, capacityOnly=true"}
{"level":"INFO","ts":"2025-11-27T23:21:00.127Z","msg":"Applied capacity decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=scale-up, current=2, target=3, reason=capacity-only mode: scale-up"}
{"level":"INFO","ts":"2025-11-27T23:21:00.127Z","msg":"Reconciliation completed successfully: mode=capacity-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-11-27T23:21:00.127Z","msg":"Operating in CAPACITY-ONLY mode: reactive capacity-based scaling only"}
{"level":"INFO","ts":"2025-11-27T23:21:00.127Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-11-27T23:21:00.127Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-11-27T23:21:00.167Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=2, accelerator=H100, ttft=38.98ms, itl=22.87ms, cost=20.00, arrivalRate=%!f(string=24)"}
{"level":"INFO","ts":"2025-11-27T23:21:00.169Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9zt6lq, queueLength=0"}
{"level":"INFO","ts":"2025-11-27T23:21:00.169Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9q8qn9, queueLength=0"}
{"level":"DEBUG","ts":"2025-11-27T23:21:00.169Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=2"}
{"level":"INFO","ts":"2025-11-27T23:21:00.169Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9zt6lq, usage=0.475 (47.5%)"}
{"level":"INFO","ts":"2025-11-27T23:21:00.169Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9q8qn9, usage=0.141 (14.1%)"}
{"level":"DEBUG","ts":"2025-11-27T23:21:00.169Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=2"}
{"level":"DEBUG","ts":"2025-11-27T23:21:00.169Z","msg":"Pod-to-variant matching successful: totalPods=2, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:2]"}
{"level":"DEBUG","ts":"2025-11-27T23:21:00.169Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=2"}
{"level":"DEBUG","ts":"2025-11-27T23:21:00.169Z","msg":"Collected capacity metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, metricsCount=2"}
{"level":"DEBUG","ts":"2025-11-27T23:21:00.169Z","msg":"Variant analysis initialized: variant=ms-inference-scheduling-llm-d-modelservice-decode, accelerator=H100, cost=10.00, replicaCount=2"}
{"level":"DEBUG","ts":"2025-11-27T23:21:00.169Z","msg":"Scale-down unsafe: insufficient headroom after redistribution: remainingSpareKv=-0.115, kvTrigger=0.200, kvSafe=false, remainingSpareQueue=5.0, queueTrigger=3.0%!(EXTRA bool=true)"}
{"level":"DEBUG","ts":"2025-11-27T23:21:00.169Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, totalReplicas=2, nonSaturated=2, avgSpareKv=0.192, avgSpareQueue=5.0, shouldScaleUp=true, scaleDownSafe=false"}
{"level":"INFO","ts":"2025-11-27T23:21:00.169Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, totalReplicas=2, nonSaturated=2, shouldScaleUp=true, scaleDownSafe=false"}
{"level":"INFO","ts":"2025-11-27T23:21:00.169Z","msg":"Capacity target: scale-up cheapest variant: variant=ms-inference-scheduling-llm-d-modelservice-decode, cost=10.00, currentReplicas=2, readyReplicas=2, target=3, reason=KV spare capacity low (0.192 < 0.200)"}
{"level":"DEBUG","ts":"2025-11-27T23:21:00.169Z","msg":"Capacity targets calculated: modelID=unsloth/Meta-Llama-3.1-8B, targets=map[ms-inference-scheduling-llm-d-modelservice-decode:3]"}
{"level":"INFO","ts":"2025-11-27T23:21:00.169Z","msg":"Capacity-only decisions made for model: unsloth/Meta-Llama-3.1-8B - decision count: 1"}
{"level":"INFO","ts":"2025-11-27T23:21:00.169Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-11-27T23:21:00.169Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=scale-up, current=2→target=3"}
{"level":"DEBUG","ts":"2025-11-27T23:21:00.169Z","msg":"Found VA in map: variant=ms-inference-scheduling-llm-d-modelservice-decode, hasCurrentAlloc=true, accelerator=H100"}
{"level":"INFO","ts":"2025-11-27T23:21:00.169Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 2, desired-replicas: 3, accelerator: H100"}
{"level":"INFO","ts":"2025-11-27T23:21:00.169Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=3, accelerator=H100, capacityOnly=true"}
{"level":"INFO","ts":"2025-11-27T23:21:00.180Z","msg":"Applied capacity decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=scale-up, current=2, target=3, reason=capacity-only mode: scale-up"}
{"level":"INFO","ts":"2025-11-27T23:21:00.180Z","msg":"Reconciliation completed successfully: mode=capacity-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-11-27T23:21:30.128Z","msg":"Operating in CAPACITY-ONLY mode: reactive capacity-based scaling only"}
{"level":"INFO","ts":"2025-11-27T23:21:30.128Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-11-27T23:21:30.128Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-11-27T23:21:30.147Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=3, accelerator=H100, ttft=35.20ms, itl=19.90ms, cost=30.00, arrivalRate=%!f(string=23)"}
{"level":"INFO","ts":"2025-11-27T23:21:30.149Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9zt6lq, queueLength=0"}
{"level":"INFO","ts":"2025-11-27T23:21:30.149Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9q8qn9, queueLength=0"}
{"level":"DEBUG","ts":"2025-11-27T23:21:30.149Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=2"}
{"level":"INFO","ts":"2025-11-27T23:21:30.149Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9zt6lq, usage=0.474 (47.4%)"}
{"level":"INFO","ts":"2025-11-27T23:21:30.149Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9q8qn9, usage=0.150 (15.0%)"}
{"level":"DEBUG","ts":"2025-11-27T23:21:30.149Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=2"}
{"level":"DEBUG","ts":"2025-11-27T23:21:30.149Z","msg":"Pod-to-variant matching successful: totalPods=2, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:2]"}
{"level":"DEBUG","ts":"2025-11-27T23:21:30.149Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=2"}
{"level":"DEBUG","ts":"2025-11-27T23:21:30.149Z","msg":"Collected capacity metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, metricsCount=2"}
{"level":"DEBUG","ts":"2025-11-27T23:21:30.149Z","msg":"Variant analysis initialized: variant=ms-inference-scheduling-llm-d-modelservice-decode, accelerator=H100, cost=10.00, replicaCount=2"}
{"level":"DEBUG","ts":"2025-11-27T23:21:30.149Z","msg":"Scale-down unsafe: insufficient headroom after redistribution: remainingSpareKv=-0.124, kvTrigger=0.200, kvSafe=false, remainingSpareQueue=5.0, queueTrigger=3.0%!(EXTRA bool=true)"}
{"level":"DEBUG","ts":"2025-11-27T23:21:30.149Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, totalReplicas=2, nonSaturated=2, avgSpareKv=0.188, avgSpareQueue=5.0, shouldScaleUp=true, scaleDownSafe=false"}
{"level":"INFO","ts":"2025-11-27T23:21:30.149Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, totalReplicas=2, nonSaturated=2, shouldScaleUp=true, scaleDownSafe=false"}
{"level":"INFO","ts":"2025-11-27T23:21:30.149Z","msg":"Capacity target: scale-up cheapest variant: variant=ms-inference-scheduling-llm-d-modelservice-decode, cost=10.00, currentReplicas=3, readyReplicas=2, target=3, reason=KV spare capacity low (0.188 < 0.200)"}
{"level":"DEBUG","ts":"2025-11-27T23:21:30.149Z","msg":"Capacity targets calculated: modelID=unsloth/Meta-Llama-3.1-8B, targets=map[ms-inference-scheduling-llm-d-modelservice-decode:3]"}
{"level":"INFO","ts":"2025-11-27T23:21:30.149Z","msg":"Capacity-only decisions made for model: unsloth/Meta-Llama-3.1-8B - decision count: 1"}
{"level":"INFO","ts":"2025-11-27T23:21:30.149Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-11-27T23:21:30.149Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=3→target=3"}
{"level":"DEBUG","ts":"2025-11-27T23:21:30.149Z","msg":"Found VA in map: variant=ms-inference-scheduling-llm-d-modelservice-decode, hasCurrentAlloc=true, accelerator=H100"}
{"level":"INFO","ts":"2025-11-27T23:21:30.149Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 3, desired-replicas: 3, accelerator: H100"}
{"level":"INFO","ts":"2025-11-27T23:21:30.149Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=3, accelerator=H100, capacityOnly=true"}
{"level":"INFO","ts":"2025-11-27T23:21:30.154Z","msg":"Applied capacity decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=3, target=3, reason=capacity-only mode: no-change"}
{"level":"INFO","ts":"2025-11-27T23:21:30.154Z","msg":"Reconciliation completed successfully: mode=capacity-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-11-27T23:21:30.181Z","msg":"Operating in CAPACITY-ONLY mode: reactive capacity-based scaling only"}
{"level":"INFO","ts":"2025-11-27T23:21:30.181Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-11-27T23:21:30.181Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-11-27T23:21:30.192Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=3, accelerator=H100, ttft=35.20ms, itl=19.90ms, cost=30.00, arrivalRate=%!f(string=23)"}
{"level":"INFO","ts":"2025-11-27T23:21:30.193Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9zt6lq, queueLength=0"}
{"level":"INFO","ts":"2025-11-27T23:21:30.193Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9q8qn9, queueLength=0"}
{"level":"DEBUG","ts":"2025-11-27T23:21:30.193Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=2"}
{"level":"INFO","ts":"2025-11-27T23:21:30.199Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9zt6lq, usage=0.474 (47.4%)"}
{"level":"INFO","ts":"2025-11-27T23:21:30.199Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9q8qn9, usage=0.150 (15.0%)"}
{"level":"DEBUG","ts":"2025-11-27T23:21:30.199Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=2"}
{"level":"DEBUG","ts":"2025-11-27T23:21:30.199Z","msg":"Pod-to-variant matching successful: totalPods=2, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:2]"}
{"level":"DEBUG","ts":"2025-11-27T23:21:30.199Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=2"}
{"level":"DEBUG","ts":"2025-11-27T23:21:30.199Z","msg":"Collected capacity metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, metricsCount=2"}
{"level":"DEBUG","ts":"2025-11-27T23:21:30.199Z","msg":"Variant analysis initialized: variant=ms-inference-scheduling-llm-d-modelservice-decode, accelerator=H100, cost=10.00, replicaCount=2"}
{"level":"DEBUG","ts":"2025-11-27T23:21:30.199Z","msg":"Scale-down unsafe: insufficient headroom after redistribution: remainingSpareKv=-0.124, kvTrigger=0.200, kvSafe=false, remainingSpareQueue=5.0, queueTrigger=3.0%!(EXTRA bool=true)"}
{"level":"DEBUG","ts":"2025-11-27T23:21:30.199Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, totalReplicas=2, nonSaturated=2, avgSpareKv=0.188, avgSpareQueue=5.0, shouldScaleUp=true, scaleDownSafe=false"}
{"level":"INFO","ts":"2025-11-27T23:21:30.199Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, totalReplicas=2, nonSaturated=2, shouldScaleUp=true, scaleDownSafe=false"}
{"level":"INFO","ts":"2025-11-27T23:21:30.199Z","msg":"Capacity target: scale-up cheapest variant: variant=ms-inference-scheduling-llm-d-modelservice-decode, cost=10.00, currentReplicas=3, readyReplicas=2, target=3, reason=KV spare capacity low (0.188 < 0.200)"}
{"level":"DEBUG","ts":"2025-11-27T23:21:30.199Z","msg":"Capacity targets calculated: modelID=unsloth/Meta-Llama-3.1-8B, targets=map[ms-inference-scheduling-llm-d-modelservice-decode:3]"}
{"level":"INFO","ts":"2025-11-27T23:21:30.199Z","msg":"Capacity-only decisions made for model: unsloth/Meta-Llama-3.1-8B - decision count: 1"}
{"level":"INFO","ts":"2025-11-27T23:21:30.199Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-11-27T23:21:30.199Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=3→target=3"}
{"level":"DEBUG","ts":"2025-11-27T23:21:30.199Z","msg":"Found VA in map: variant=ms-inference-scheduling-llm-d-modelservice-decode, hasCurrentAlloc=true, accelerator=H100"}
{"level":"INFO","ts":"2025-11-27T23:21:30.199Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 3, desired-replicas: 3, accelerator: H100"}
{"level":"INFO","ts":"2025-11-27T23:21:30.199Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=3, accelerator=H100, capacityOnly=true"}
{"level":"INFO","ts":"2025-11-27T23:21:30.204Z","msg":"Applied capacity decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=3, target=3, reason=capacity-only mode: no-change"}
{"level":"INFO","ts":"2025-11-27T23:21:30.204Z","msg":"Reconciliation completed successfully: mode=capacity-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-11-27T23:22:00.155Z","msg":"Operating in CAPACITY-ONLY mode: reactive capacity-based scaling only"}
{"level":"INFO","ts":"2025-11-27T23:22:00.155Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-11-27T23:22:00.155Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-11-27T23:22:00.180Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=3, accelerator=H100, ttft=30.73ms, itl=16.55ms, cost=30.00, arrivalRate=%!f(string=21)"}
{"level":"INFO","ts":"2025-11-27T23:22:00.182Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9zt6lq, usage=0.356 (35.6%)"}
{"level":"INFO","ts":"2025-11-27T23:22:00.182Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9q8qn9, usage=0.152 (15.2%)"}
{"level":"DEBUG","ts":"2025-11-27T23:22:00.182Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=2"}
{"level":"INFO","ts":"2025-11-27T23:22:00.182Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9zt6lq, queueLength=0"}
{"level":"INFO","ts":"2025-11-27T23:22:00.182Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9q8qn9, queueLength=0"}
{"level":"DEBUG","ts":"2025-11-27T23:22:00.182Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=2"}
{"level":"DEBUG","ts":"2025-11-27T23:22:00.182Z","msg":"Pod-to-variant matching successful: totalPods=2, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:2]"}
{"level":"DEBUG","ts":"2025-11-27T23:22:00.182Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=2"}
{"level":"DEBUG","ts":"2025-11-27T23:22:00.182Z","msg":"Collected capacity metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, metricsCount=2"}
{"level":"DEBUG","ts":"2025-11-27T23:22:00.182Z","msg":"Variant analysis initialized: variant=ms-inference-scheduling-llm-d-modelservice-decode, accelerator=H100, cost=10.00, replicaCount=2"}
{"level":"DEBUG","ts":"2025-11-27T23:22:00.182Z","msg":"Scale-down unsafe: insufficient headroom after redistribution: remainingSpareKv=-0.009, kvTrigger=0.200, kvSafe=false, remainingSpareQueue=5.0, queueTrigger=3.0%!(EXTRA bool=true)"}
{"level":"DEBUG","ts":"2025-11-27T23:22:00.182Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, totalReplicas=2, nonSaturated=2, avgSpareKv=0.246, avgSpareQueue=5.0, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"INFO","ts":"2025-11-27T23:22:00.182Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, totalReplicas=2, nonSaturated=2, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"DEBUG","ts":"2025-11-27T23:22:00.182Z","msg":"Capacity targets: no scaling needed (avgSpareKv=0.246, avgSpareQueue=5.0, all variants stable)"}
{"level":"DEBUG","ts":"2025-11-27T23:22:00.182Z","msg":"Capacity targets calculated: modelID=unsloth/Meta-Llama-3.1-8B, targets=map[ms-inference-scheduling-llm-d-modelservice-decode:2]"}
{"level":"INFO","ts":"2025-11-27T23:22:00.182Z","msg":"Capacity-only decisions made for model: unsloth/Meta-Llama-3.1-8B - decision count: 1"}
{"level":"INFO","ts":"2025-11-27T23:22:00.182Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-11-27T23:22:00.182Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=scale-down, current=3→target=2"}
{"level":"DEBUG","ts":"2025-11-27T23:22:00.182Z","msg":"Found VA in map: variant=ms-inference-scheduling-llm-d-modelservice-decode, hasCurrentAlloc=true, accelerator=H100"}
{"level":"INFO","ts":"2025-11-27T23:22:00.182Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 3, desired-replicas: 2, accelerator: H100"}
{"level":"INFO","ts":"2025-11-27T23:22:00.182Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=2, accelerator=H100, capacityOnly=true"}
{"level":"INFO","ts":"2025-11-27T23:22:00.187Z","msg":"Applied capacity decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=scale-down, current=3, target=2, reason=capacity-only mode: scale-down"}
{"level":"INFO","ts":"2025-11-27T23:22:00.187Z","msg":"Reconciliation completed successfully: mode=capacity-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-11-27T23:22:00.205Z","msg":"Operating in CAPACITY-ONLY mode: reactive capacity-based scaling only"}
{"level":"INFO","ts":"2025-11-27T23:22:00.205Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-11-27T23:22:00.205Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-11-27T23:22:00.218Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=3, accelerator=H100, ttft=30.73ms, itl=16.55ms, cost=30.00, arrivalRate=%!f(string=21)"}
{"level":"INFO","ts":"2025-11-27T23:22:00.220Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9zt6lq, queueLength=0"}
{"level":"INFO","ts":"2025-11-27T23:22:00.220Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9q8qn9, queueLength=0"}
{"level":"DEBUG","ts":"2025-11-27T23:22:00.220Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=2"}
{"level":"INFO","ts":"2025-11-27T23:22:00.220Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9zt6lq, usage=0.356 (35.6%)"}
{"level":"INFO","ts":"2025-11-27T23:22:00.220Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9q8qn9, usage=0.152 (15.2%)"}
{"level":"DEBUG","ts":"2025-11-27T23:22:00.220Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=2"}
{"level":"DEBUG","ts":"2025-11-27T23:22:00.220Z","msg":"Pod-to-variant matching successful: totalPods=2, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:2]"}
{"level":"DEBUG","ts":"2025-11-27T23:22:00.220Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=2"}
{"level":"DEBUG","ts":"2025-11-27T23:22:00.220Z","msg":"Collected capacity metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, metricsCount=2"}
{"level":"DEBUG","ts":"2025-11-27T23:22:00.220Z","msg":"Variant analysis initialized: variant=ms-inference-scheduling-llm-d-modelservice-decode, accelerator=H100, cost=10.00, replicaCount=2"}
{"level":"DEBUG","ts":"2025-11-27T23:22:00.220Z","msg":"Scale-down unsafe: insufficient headroom after redistribution: remainingSpareKv=-0.009, kvTrigger=0.200, kvSafe=false, remainingSpareQueue=5.0, queueTrigger=3.0%!(EXTRA bool=true)"}
{"level":"DEBUG","ts":"2025-11-27T23:22:00.220Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, totalReplicas=2, nonSaturated=2, avgSpareKv=0.246, avgSpareQueue=5.0, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"INFO","ts":"2025-11-27T23:22:00.220Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, totalReplicas=2, nonSaturated=2, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"DEBUG","ts":"2025-11-27T23:22:00.220Z","msg":"Preserving desired replicas: variant=ms-inference-scheduling-llm-d-modelservice-decode, currentReplicas=3, readyReplicas=2, desired=2"}
{"level":"DEBUG","ts":"2025-11-27T23:22:00.220Z","msg":"Capacity targets: no scaling needed (avgSpareKv=0.246, avgSpareQueue=5.0, all variants stable)"}
{"level":"DEBUG","ts":"2025-11-27T23:22:00.220Z","msg":"Capacity targets calculated: modelID=unsloth/Meta-Llama-3.1-8B, targets=map[ms-inference-scheduling-llm-d-modelservice-decode:2]"}
{"level":"INFO","ts":"2025-11-27T23:22:00.220Z","msg":"Capacity-only decisions made for model: unsloth/Meta-Llama-3.1-8B - decision count: 1"}
{"level":"INFO","ts":"2025-11-27T23:22:00.220Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-11-27T23:22:00.220Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=scale-down, current=3→target=2"}
{"level":"DEBUG","ts":"2025-11-27T23:22:00.220Z","msg":"Found VA in map: variant=ms-inference-scheduling-llm-d-modelservice-decode, hasCurrentAlloc=true, accelerator=H100"}
{"level":"INFO","ts":"2025-11-27T23:22:00.220Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 3, desired-replicas: 2, accelerator: H100"}
{"level":"INFO","ts":"2025-11-27T23:22:00.220Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=2, accelerator=H100, capacityOnly=true"}
{"level":"INFO","ts":"2025-11-27T23:22:00.275Z","msg":"Applied capacity decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=scale-down, current=3, target=2, reason=capacity-only mode: scale-down"}
{"level":"INFO","ts":"2025-11-27T23:22:00.275Z","msg":"Reconciliation completed successfully: mode=capacity-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-11-27T23:22:30.188Z","msg":"Operating in CAPACITY-ONLY mode: reactive capacity-based scaling only"}
{"level":"INFO","ts":"2025-11-27T23:22:30.188Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-11-27T23:22:30.188Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-11-27T23:22:30.204Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=2, accelerator=H100, ttft=27.68ms, itl=14.53ms, cost=20.00, arrivalRate=%!f(string=18)"}
{"level":"INFO","ts":"2025-11-27T23:22:30.206Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9zt6lq, queueLength=0"}
{"level":"INFO","ts":"2025-11-27T23:22:30.206Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9q8qn9, queueLength=0"}
{"level":"DEBUG","ts":"2025-11-27T23:22:30.206Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=2"}
{"level":"INFO","ts":"2025-11-27T23:22:30.206Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9zt6lq, usage=0.203 (20.3%)"}
{"level":"INFO","ts":"2025-11-27T23:22:30.206Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9q8qn9, usage=0.123 (12.3%)"}
{"level":"DEBUG","ts":"2025-11-27T23:22:30.206Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=2"}
{"level":"DEBUG","ts":"2025-11-27T23:22:30.206Z","msg":"Pod-to-variant matching successful: totalPods=2, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:2]"}
{"level":"DEBUG","ts":"2025-11-27T23:22:30.206Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=2"}
{"level":"DEBUG","ts":"2025-11-27T23:22:30.206Z","msg":"Collected capacity metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, metricsCount=2"}
{"level":"DEBUG","ts":"2025-11-27T23:22:30.206Z","msg":"Variant analysis initialized: variant=ms-inference-scheduling-llm-d-modelservice-decode, accelerator=H100, cost=10.00, replicaCount=2"}
{"level":"DEBUG","ts":"2025-11-27T23:22:30.206Z","msg":"Scale-down unsafe: insufficient headroom after redistribution: remainingSpareKv=0.173, kvTrigger=0.200, kvSafe=false, remainingSpareQueue=5.0, queueTrigger=3.0%!(EXTRA bool=true)"}
{"level":"DEBUG","ts":"2025-11-27T23:22:30.206Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, totalReplicas=2, nonSaturated=2, avgSpareKv=0.337, avgSpareQueue=5.0, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"INFO","ts":"2025-11-27T23:22:30.206Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, totalReplicas=2, nonSaturated=2, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"DEBUG","ts":"2025-11-27T23:22:30.206Z","msg":"Capacity targets: no scaling needed (avgSpareKv=0.337, avgSpareQueue=5.0, all variants stable)"}
{"level":"DEBUG","ts":"2025-11-27T23:22:30.206Z","msg":"Capacity targets calculated: modelID=unsloth/Meta-Llama-3.1-8B, targets=map[ms-inference-scheduling-llm-d-modelservice-decode:2]"}
{"level":"INFO","ts":"2025-11-27T23:22:30.206Z","msg":"Capacity-only decisions made for model: unsloth/Meta-Llama-3.1-8B - decision count: 1"}
{"level":"INFO","ts":"2025-11-27T23:22:30.206Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-11-27T23:22:30.206Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=2→target=2"}
{"level":"DEBUG","ts":"2025-11-27T23:22:30.206Z","msg":"Found VA in map: variant=ms-inference-scheduling-llm-d-modelservice-decode, hasCurrentAlloc=true, accelerator=H100"}
{"level":"INFO","ts":"2025-11-27T23:22:30.206Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 2, desired-replicas: 2, accelerator: H100"}
{"level":"INFO","ts":"2025-11-27T23:22:30.206Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=2, accelerator=H100, capacityOnly=true"}
{"level":"INFO","ts":"2025-11-27T23:22:30.212Z","msg":"Applied capacity decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=2, target=2, reason=capacity-only mode: no-change"}
{"level":"INFO","ts":"2025-11-27T23:22:30.212Z","msg":"Reconciliation completed successfully: mode=capacity-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-11-27T23:22:30.276Z","msg":"Operating in CAPACITY-ONLY mode: reactive capacity-based scaling only"}
{"level":"INFO","ts":"2025-11-27T23:22:30.276Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-11-27T23:22:30.276Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-11-27T23:22:30.285Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=2, accelerator=H100, ttft=27.68ms, itl=14.53ms, cost=20.00, arrivalRate=%!f(string=18)"}
{"level":"INFO","ts":"2025-11-27T23:22:30.287Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9zt6lq, queueLength=0"}
{"level":"INFO","ts":"2025-11-27T23:22:30.287Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9q8qn9, queueLength=0"}
{"level":"DEBUG","ts":"2025-11-27T23:22:30.287Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=2"}
{"level":"INFO","ts":"2025-11-27T23:22:30.287Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9zt6lq, usage=0.203 (20.3%)"}
{"level":"INFO","ts":"2025-11-27T23:22:30.287Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9q8qn9, usage=0.123 (12.3%)"}
{"level":"DEBUG","ts":"2025-11-27T23:22:30.287Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=2"}
{"level":"DEBUG","ts":"2025-11-27T23:22:30.287Z","msg":"Pod-to-variant matching successful: totalPods=2, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:2]"}
{"level":"DEBUG","ts":"2025-11-27T23:22:30.287Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=2"}
{"level":"DEBUG","ts":"2025-11-27T23:22:30.287Z","msg":"Collected capacity metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, metricsCount=2"}
{"level":"DEBUG","ts":"2025-11-27T23:22:30.287Z","msg":"Variant analysis initialized: variant=ms-inference-scheduling-llm-d-modelservice-decode, accelerator=H100, cost=10.00, replicaCount=2"}
{"level":"DEBUG","ts":"2025-11-27T23:22:30.287Z","msg":"Scale-down unsafe: insufficient headroom after redistribution: remainingSpareKv=0.173, kvTrigger=0.200, kvSafe=false, remainingSpareQueue=5.0, queueTrigger=3.0%!(EXTRA bool=true)"}
{"level":"DEBUG","ts":"2025-11-27T23:22:30.287Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, totalReplicas=2, nonSaturated=2, avgSpareKv=0.337, avgSpareQueue=5.0, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"INFO","ts":"2025-11-27T23:22:30.287Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, totalReplicas=2, nonSaturated=2, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"DEBUG","ts":"2025-11-27T23:22:30.287Z","msg":"Capacity targets: no scaling needed (avgSpareKv=0.337, avgSpareQueue=5.0, all variants stable)"}
{"level":"DEBUG","ts":"2025-11-27T23:22:30.287Z","msg":"Capacity targets calculated: modelID=unsloth/Meta-Llama-3.1-8B, targets=map[ms-inference-scheduling-llm-d-modelservice-decode:2]"}
{"level":"INFO","ts":"2025-11-27T23:22:30.287Z","msg":"Capacity-only decisions made for model: unsloth/Meta-Llama-3.1-8B - decision count: 1"}
{"level":"INFO","ts":"2025-11-27T23:22:30.287Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-11-27T23:22:30.287Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=2→target=2"}
{"level":"DEBUG","ts":"2025-11-27T23:22:30.287Z","msg":"Found VA in map: variant=ms-inference-scheduling-llm-d-modelservice-decode, hasCurrentAlloc=true, accelerator=H100"}
{"level":"INFO","ts":"2025-11-27T23:22:30.287Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 2, desired-replicas: 2, accelerator: H100"}
{"level":"INFO","ts":"2025-11-27T23:22:30.287Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=2, accelerator=H100, capacityOnly=true"}
{"level":"INFO","ts":"2025-11-27T23:22:30.291Z","msg":"Applied capacity decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=2, target=2, reason=capacity-only mode: no-change"}
{"level":"INFO","ts":"2025-11-27T23:22:30.291Z","msg":"Reconciliation completed successfully: mode=capacity-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-11-27T23:23:00.213Z","msg":"Operating in CAPACITY-ONLY mode: reactive capacity-based scaling only"}
{"level":"INFO","ts":"2025-11-27T23:23:00.213Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-11-27T23:23:00.213Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-11-27T23:23:00.230Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=2, accelerator=H100, ttft=25.18ms, itl=13.17ms, cost=20.00, arrivalRate=%!f(string=16)"}
{"level":"INFO","ts":"2025-11-27T23:23:00.231Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9zt6lq, queueLength=0"}
{"level":"INFO","ts":"2025-11-27T23:23:00.231Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9q8qn9, queueLength=0"}
{"level":"DEBUG","ts":"2025-11-27T23:23:00.231Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=2"}
{"level":"INFO","ts":"2025-11-27T23:23:00.231Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9zt6lq, usage=0.192 (19.2%)"}
{"level":"INFO","ts":"2025-11-27T23:23:00.231Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9q8qn9, usage=0.102 (10.2%)"}
{"level":"DEBUG","ts":"2025-11-27T23:23:00.231Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=2"}
{"level":"DEBUG","ts":"2025-11-27T23:23:00.232Z","msg":"Pod-to-variant matching successful: totalPods=2, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:2]"}
{"level":"DEBUG","ts":"2025-11-27T23:23:00.232Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=2"}
{"level":"DEBUG","ts":"2025-11-27T23:23:00.232Z","msg":"Collected capacity metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, metricsCount=2"}
{"level":"DEBUG","ts":"2025-11-27T23:23:00.232Z","msg":"Variant analysis initialized: variant=ms-inference-scheduling-llm-d-modelservice-decode, accelerator=H100, cost=10.00, replicaCount=2"}
{"level":"DEBUG","ts":"2025-11-27T23:23:00.232Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, totalReplicas=2, nonSaturated=2, avgSpareKv=0.353, avgSpareQueue=5.0, shouldScaleUp=false, scaleDownSafe=true"}
{"level":"INFO","ts":"2025-11-27T23:23:00.232Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, totalReplicas=2, nonSaturated=2, shouldScaleUp=false, scaleDownSafe=true"}
{"level":"INFO","ts":"2025-11-27T23:23:00.232Z","msg":"Capacity target: scale-down most expensive variant: variant=ms-inference-scheduling-llm-d-modelservice-decode, cost=10.00, currentReplicas=2, readyReplicas=2, target=1"}
{"level":"DEBUG","ts":"2025-11-27T23:23:00.232Z","msg":"Capacity targets calculated: modelID=unsloth/Meta-Llama-3.1-8B, targets=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"INFO","ts":"2025-11-27T23:23:00.232Z","msg":"Capacity-only decisions made for model: unsloth/Meta-Llama-3.1-8B - decision count: 1"}
{"level":"INFO","ts":"2025-11-27T23:23:00.232Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-11-27T23:23:00.232Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=scale-down, current=2→target=1"}
{"level":"DEBUG","ts":"2025-11-27T23:23:00.232Z","msg":"Found VA in map: variant=ms-inference-scheduling-llm-d-modelservice-decode, hasCurrentAlloc=true, accelerator=H100"}
{"level":"INFO","ts":"2025-11-27T23:23:00.232Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 2, desired-replicas: 1, accelerator: H100"}
{"level":"INFO","ts":"2025-11-27T23:23:00.232Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=1, accelerator=H100, capacityOnly=true"}
{"level":"INFO","ts":"2025-11-27T23:23:00.238Z","msg":"Applied capacity decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=scale-down, current=2, target=1, reason=capacity-only mode: scale-down"}
{"level":"INFO","ts":"2025-11-27T23:23:00.238Z","msg":"Reconciliation completed successfully: mode=capacity-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-11-27T23:23:00.292Z","msg":"Operating in CAPACITY-ONLY mode: reactive capacity-based scaling only"}
{"level":"INFO","ts":"2025-11-27T23:23:00.292Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-11-27T23:23:00.292Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-11-27T23:23:00.303Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=2, accelerator=H100, ttft=25.18ms, itl=13.17ms, cost=20.00, arrivalRate=%!f(string=16)"}
{"level":"INFO","ts":"2025-11-27T23:23:00.304Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9zt6lq, queueLength=0"}
{"level":"INFO","ts":"2025-11-27T23:23:00.304Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9q8qn9, queueLength=0"}
{"level":"DEBUG","ts":"2025-11-27T23:23:00.304Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=2"}
{"level":"INFO","ts":"2025-11-27T23:23:00.304Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9zt6lq, usage=0.192 (19.2%)"}
{"level":"INFO","ts":"2025-11-27T23:23:00.304Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9q8qn9, usage=0.102 (10.2%)"}
{"level":"DEBUG","ts":"2025-11-27T23:23:00.304Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=2"}
{"level":"DEBUG","ts":"2025-11-27T23:23:00.305Z","msg":"Pod-to-variant matching successful: totalPods=2, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:2]"}
{"level":"DEBUG","ts":"2025-11-27T23:23:00.305Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=2"}
{"level":"DEBUG","ts":"2025-11-27T23:23:00.305Z","msg":"Collected capacity metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, metricsCount=2"}
{"level":"DEBUG","ts":"2025-11-27T23:23:00.305Z","msg":"Variant analysis initialized: variant=ms-inference-scheduling-llm-d-modelservice-decode, accelerator=H100, cost=10.00, replicaCount=2"}
{"level":"DEBUG","ts":"2025-11-27T23:23:00.305Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, totalReplicas=2, nonSaturated=2, avgSpareKv=0.353, avgSpareQueue=5.0, shouldScaleUp=false, scaleDownSafe=true"}
{"level":"INFO","ts":"2025-11-27T23:23:00.305Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, totalReplicas=2, nonSaturated=2, shouldScaleUp=false, scaleDownSafe=true"}
{"level":"DEBUG","ts":"2025-11-27T23:23:00.305Z","msg":"Preserving desired replicas: variant=ms-inference-scheduling-llm-d-modelservice-decode, currentReplicas=2, readyReplicas=2, desired=1"}
{"level":"DEBUG","ts":"2025-11-27T23:23:00.305Z","msg":"Capacity targets calculated: modelID=unsloth/Meta-Llama-3.1-8B, targets=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"INFO","ts":"2025-11-27T23:23:00.305Z","msg":"Capacity-only decisions made for model: unsloth/Meta-Llama-3.1-8B - decision count: 1"}
{"level":"INFO","ts":"2025-11-27T23:23:00.305Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-11-27T23:23:00.305Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=scale-down, current=2→target=1"}
{"level":"DEBUG","ts":"2025-11-27T23:23:00.305Z","msg":"Found VA in map: variant=ms-inference-scheduling-llm-d-modelservice-decode, hasCurrentAlloc=true, accelerator=H100"}
{"level":"INFO","ts":"2025-11-27T23:23:00.305Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 2, desired-replicas: 1, accelerator: H100"}
{"level":"INFO","ts":"2025-11-27T23:23:00.305Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=1, accelerator=H100, capacityOnly=true"}
{"level":"INFO","ts":"2025-11-27T23:23:00.309Z","msg":"Applied capacity decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=scale-down, current=2, target=1, reason=capacity-only mode: scale-down"}
{"level":"INFO","ts":"2025-11-27T23:23:00.309Z","msg":"Reconciliation completed successfully: mode=capacity-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-11-27T23:23:30.239Z","msg":"Operating in CAPACITY-ONLY mode: reactive capacity-based scaling only"}
{"level":"INFO","ts":"2025-11-27T23:23:30.239Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-11-27T23:23:30.239Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-11-27T23:23:30.255Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=1, accelerator=H100, ttft=23.23ms, itl=11.96ms, cost=10.00, arrivalRate=%!f(string=14)"}
{"level":"INFO","ts":"2025-11-27T23:23:30.256Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9zt6lq, queueLength=0"}
{"level":"INFO","ts":"2025-11-27T23:23:30.256Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9q8qn9, queueLength=0"}
{"level":"DEBUG","ts":"2025-11-27T23:23:30.256Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=2"}
{"level":"INFO","ts":"2025-11-27T23:23:30.256Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9zt6lq, usage=0.113 (11.3%)"}
{"level":"INFO","ts":"2025-11-27T23:23:30.256Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9q8qn9, usage=0.102 (10.2%)"}
{"level":"DEBUG","ts":"2025-11-27T23:23:30.256Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=2"}
{"level":"DEBUG","ts":"2025-11-27T23:23:30.257Z","msg":"Pod-to-variant matching successful: totalPods=2, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:2]"}
{"level":"DEBUG","ts":"2025-11-27T23:23:30.257Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=2"}
{"level":"DEBUG","ts":"2025-11-27T23:23:30.257Z","msg":"Collected capacity metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, metricsCount=2"}
{"level":"DEBUG","ts":"2025-11-27T23:23:30.257Z","msg":"Variant analysis initialized: variant=ms-inference-scheduling-llm-d-modelservice-decode, accelerator=H100, cost=10.00, replicaCount=2"}
{"level":"DEBUG","ts":"2025-11-27T23:23:30.257Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, totalReplicas=2, nonSaturated=2, avgSpareKv=0.392, avgSpareQueue=5.0, shouldScaleUp=false, scaleDownSafe=true"}
{"level":"INFO","ts":"2025-11-27T23:23:30.257Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, totalReplicas=2, nonSaturated=2, shouldScaleUp=false, scaleDownSafe=true"}
{"level":"INFO","ts":"2025-11-27T23:23:30.257Z","msg":"Capacity target: scale-down most expensive variant: variant=ms-inference-scheduling-llm-d-modelservice-decode, cost=10.00, currentReplicas=1, readyReplicas=2, target=1"}
{"level":"DEBUG","ts":"2025-11-27T23:23:30.257Z","msg":"Capacity targets calculated: modelID=unsloth/Meta-Llama-3.1-8B, targets=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"INFO","ts":"2025-11-27T23:23:30.257Z","msg":"Capacity-only decisions made for model: unsloth/Meta-Llama-3.1-8B - decision count: 1"}
{"level":"INFO","ts":"2025-11-27T23:23:30.257Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-11-27T23:23:30.257Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1→target=1"}
{"level":"DEBUG","ts":"2025-11-27T23:23:30.257Z","msg":"Found VA in map: variant=ms-inference-scheduling-llm-d-modelservice-decode, hasCurrentAlloc=true, accelerator=H100"}
{"level":"INFO","ts":"2025-11-27T23:23:30.257Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 1, desired-replicas: 1, accelerator: H100"}
{"level":"INFO","ts":"2025-11-27T23:23:30.257Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=1, accelerator=H100, capacityOnly=true"}
{"level":"INFO","ts":"2025-11-27T23:23:30.261Z","msg":"Applied capacity decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1, target=1, reason=capacity-only mode: no-change"}
{"level":"INFO","ts":"2025-11-27T23:23:30.261Z","msg":"Reconciliation completed successfully: mode=capacity-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-11-27T23:23:30.310Z","msg":"Operating in CAPACITY-ONLY mode: reactive capacity-based scaling only"}
{"level":"INFO","ts":"2025-11-27T23:23:30.310Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-11-27T23:23:30.310Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-11-27T23:23:30.320Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=1, accelerator=H100, ttft=23.23ms, itl=11.96ms, cost=10.00, arrivalRate=%!f(string=14)"}
{"level":"INFO","ts":"2025-11-27T23:23:30.321Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9zt6lq, queueLength=0"}
{"level":"INFO","ts":"2025-11-27T23:23:30.321Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9q8qn9, queueLength=0"}
{"level":"DEBUG","ts":"2025-11-27T23:23:30.321Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=2"}
{"level":"INFO","ts":"2025-11-27T23:23:30.321Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9zt6lq, usage=0.113 (11.3%)"}
{"level":"INFO","ts":"2025-11-27T23:23:30.321Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9q8qn9, usage=0.102 (10.2%)"}
{"level":"DEBUG","ts":"2025-11-27T23:23:30.321Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=2"}
{"level":"DEBUG","ts":"2025-11-27T23:23:30.321Z","msg":"Pod-to-variant matching successful: totalPods=2, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:2]"}
{"level":"DEBUG","ts":"2025-11-27T23:23:30.321Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=2"}
{"level":"DEBUG","ts":"2025-11-27T23:23:30.321Z","msg":"Collected capacity metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, metricsCount=2"}
{"level":"DEBUG","ts":"2025-11-27T23:23:30.321Z","msg":"Variant analysis initialized: variant=ms-inference-scheduling-llm-d-modelservice-decode, accelerator=H100, cost=10.00, replicaCount=2"}
{"level":"DEBUG","ts":"2025-11-27T23:23:30.321Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, totalReplicas=2, nonSaturated=2, avgSpareKv=0.392, avgSpareQueue=5.0, shouldScaleUp=false, scaleDownSafe=true"}
{"level":"INFO","ts":"2025-11-27T23:23:30.321Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, totalReplicas=2, nonSaturated=2, shouldScaleUp=false, scaleDownSafe=true"}
{"level":"INFO","ts":"2025-11-27T23:23:30.321Z","msg":"Capacity target: scale-down most expensive variant: variant=ms-inference-scheduling-llm-d-modelservice-decode, cost=10.00, currentReplicas=1, readyReplicas=2, target=1"}
{"level":"DEBUG","ts":"2025-11-27T23:23:30.321Z","msg":"Capacity targets calculated: modelID=unsloth/Meta-Llama-3.1-8B, targets=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"INFO","ts":"2025-11-27T23:23:30.321Z","msg":"Capacity-only decisions made for model: unsloth/Meta-Llama-3.1-8B - decision count: 1"}
{"level":"INFO","ts":"2025-11-27T23:23:30.321Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-11-27T23:23:30.321Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1→target=1"}
{"level":"DEBUG","ts":"2025-11-27T23:23:30.321Z","msg":"Found VA in map: variant=ms-inference-scheduling-llm-d-modelservice-decode, hasCurrentAlloc=true, accelerator=H100"}
{"level":"INFO","ts":"2025-11-27T23:23:30.321Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 1, desired-replicas: 1, accelerator: H100"}
{"level":"INFO","ts":"2025-11-27T23:23:30.321Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=1, accelerator=H100, capacityOnly=true"}
{"level":"INFO","ts":"2025-11-27T23:23:30.325Z","msg":"Applied capacity decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1, target=1, reason=capacity-only mode: no-change"}
{"level":"INFO","ts":"2025-11-27T23:23:30.325Z","msg":"Reconciliation completed successfully: mode=capacity-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-11-27T23:24:00.264Z","msg":"Operating in CAPACITY-ONLY mode: reactive capacity-based scaling only"}
{"level":"INFO","ts":"2025-11-27T23:24:00.264Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-11-27T23:24:00.264Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-11-27T23:24:00.276Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=1, accelerator=H100, ttft=23.18ms, itl=11.92ms, cost=10.00, arrivalRate=%!f(string=66)"}
{"level":"INFO","ts":"2025-11-27T23:24:00.278Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9zt6lq, queueLength=0"}
{"level":"INFO","ts":"2025-11-27T23:24:00.278Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9q8qn9, queueLength=0"}
{"level":"DEBUG","ts":"2025-11-27T23:24:00.278Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=2"}
{"level":"INFO","ts":"2025-11-27T23:24:00.278Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9zt6lq, usage=0.115 (11.5%)"}
{"level":"INFO","ts":"2025-11-27T23:24:00.278Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9q8qn9, usage=0.059 (5.9%)"}
{"level":"DEBUG","ts":"2025-11-27T23:24:00.278Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=2"}
{"level":"DEBUG","ts":"2025-11-27T23:24:00.278Z","msg":"Pod-to-variant matching successful: totalPods=2, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:2]"}
{"level":"DEBUG","ts":"2025-11-27T23:24:00.278Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=2"}
{"level":"DEBUG","ts":"2025-11-27T23:24:00.278Z","msg":"Collected capacity metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, metricsCount=2"}
{"level":"DEBUG","ts":"2025-11-27T23:24:00.278Z","msg":"Variant analysis initialized: variant=ms-inference-scheduling-llm-d-modelservice-decode, accelerator=H100, cost=10.00, replicaCount=2"}
{"level":"DEBUG","ts":"2025-11-27T23:24:00.278Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, totalReplicas=2, nonSaturated=2, avgSpareKv=0.413, avgSpareQueue=5.0, shouldScaleUp=false, scaleDownSafe=true"}
{"level":"INFO","ts":"2025-11-27T23:24:00.278Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, totalReplicas=2, nonSaturated=2, shouldScaleUp=false, scaleDownSafe=true"}
{"level":"INFO","ts":"2025-11-27T23:24:00.278Z","msg":"Capacity target: scale-down most expensive variant: variant=ms-inference-scheduling-llm-d-modelservice-decode, cost=10.00, currentReplicas=1, readyReplicas=2, target=1"}
{"level":"DEBUG","ts":"2025-11-27T23:24:00.278Z","msg":"Capacity targets calculated: modelID=unsloth/Meta-Llama-3.1-8B, targets=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"INFO","ts":"2025-11-27T23:24:00.278Z","msg":"Capacity-only decisions made for model: unsloth/Meta-Llama-3.1-8B - decision count: 1"}
{"level":"INFO","ts":"2025-11-27T23:24:00.278Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-11-27T23:24:00.278Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1→target=1"}
{"level":"DEBUG","ts":"2025-11-27T23:24:00.278Z","msg":"Found VA in map: variant=ms-inference-scheduling-llm-d-modelservice-decode, hasCurrentAlloc=true, accelerator=H100"}
{"level":"INFO","ts":"2025-11-27T23:24:00.375Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 1, desired-replicas: 1, accelerator: H100"}
{"level":"INFO","ts":"2025-11-27T23:24:00.375Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=1, accelerator=H100, capacityOnly=true"}
{"level":"INFO","ts":"2025-11-27T23:24:00.379Z","msg":"Applied capacity decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1, target=1, reason=capacity-only mode: no-change"}
{"level":"INFO","ts":"2025-11-27T23:24:00.379Z","msg":"Reconciliation completed successfully: mode=capacity-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-11-27T23:24:00.379Z","msg":"Operating in CAPACITY-ONLY mode: reactive capacity-based scaling only"}
{"level":"INFO","ts":"2025-11-27T23:24:00.379Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-11-27T23:24:00.379Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-11-27T23:24:00.389Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=1, accelerator=H100, ttft=23.18ms, itl=11.92ms, cost=10.00, arrivalRate=%!f(string=66)"}
{"level":"INFO","ts":"2025-11-27T23:24:00.391Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9zt6lq, usage=0.115 (11.5%)"}
{"level":"INFO","ts":"2025-11-27T23:24:00.391Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9zt6lq, queueLength=0"}
{"level":"INFO","ts":"2025-11-27T23:24:00.391Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9q8qn9, queueLength=0"}
{"level":"DEBUG","ts":"2025-11-27T23:24:00.391Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=2"}
{"level":"INFO","ts":"2025-11-27T23:24:00.391Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9q8qn9, usage=0.059 (5.9%)"}
{"level":"DEBUG","ts":"2025-11-27T23:24:00.391Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=2"}
{"level":"DEBUG","ts":"2025-11-27T23:24:00.391Z","msg":"Pod-to-variant matching successful: totalPods=2, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:2]"}
{"level":"DEBUG","ts":"2025-11-27T23:24:00.391Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=2"}
{"level":"DEBUG","ts":"2025-11-27T23:24:00.391Z","msg":"Collected capacity metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, metricsCount=2"}
{"level":"DEBUG","ts":"2025-11-27T23:24:00.391Z","msg":"Variant analysis initialized: variant=ms-inference-scheduling-llm-d-modelservice-decode, accelerator=H100, cost=10.00, replicaCount=2"}
{"level":"DEBUG","ts":"2025-11-27T23:24:00.391Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, totalReplicas=2, nonSaturated=2, avgSpareKv=0.413, avgSpareQueue=5.0, shouldScaleUp=false, scaleDownSafe=true"}
{"level":"INFO","ts":"2025-11-27T23:24:00.391Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, totalReplicas=2, nonSaturated=2, shouldScaleUp=false, scaleDownSafe=true"}
{"level":"INFO","ts":"2025-11-27T23:24:00.391Z","msg":"Capacity target: scale-down most expensive variant: variant=ms-inference-scheduling-llm-d-modelservice-decode, cost=10.00, currentReplicas=1, readyReplicas=2, target=1"}
{"level":"DEBUG","ts":"2025-11-27T23:24:00.391Z","msg":"Capacity targets calculated: modelID=unsloth/Meta-Llama-3.1-8B, targets=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"INFO","ts":"2025-11-27T23:24:00.391Z","msg":"Capacity-only decisions made for model: unsloth/Meta-Llama-3.1-8B - decision count: 1"}
{"level":"INFO","ts":"2025-11-27T23:24:00.391Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-11-27T23:24:00.391Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1→target=1"}
{"level":"DEBUG","ts":"2025-11-27T23:24:00.391Z","msg":"Found VA in map: variant=ms-inference-scheduling-llm-d-modelservice-decode, hasCurrentAlloc=true, accelerator=H100"}
{"level":"INFO","ts":"2025-11-27T23:24:00.391Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 1, desired-replicas: 1, accelerator: H100"}
{"level":"INFO","ts":"2025-11-27T23:24:00.391Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=1, accelerator=H100, capacityOnly=true"}
{"level":"INFO","ts":"2025-11-27T23:24:00.395Z","msg":"Applied capacity decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1, target=1, reason=capacity-only mode: no-change"}
{"level":"INFO","ts":"2025-11-27T23:24:00.395Z","msg":"Reconciliation completed successfully: mode=capacity-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-11-27T23:24:30.381Z","msg":"Operating in CAPACITY-ONLY mode: reactive capacity-based scaling only"}
{"level":"INFO","ts":"2025-11-27T23:24:30.381Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-11-27T23:24:30.381Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-11-27T23:24:30.396Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=1, accelerator=H100, ttft=23.79ms, itl=11.72ms, cost=10.00, arrivalRate=%!f(string=66)"}
{"level":"INFO","ts":"2025-11-27T23:24:30.397Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9zt6lq, usage=0.087 (8.7%)"}
{"level":"DEBUG","ts":"2025-11-27T23:24:30.397Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"INFO","ts":"2025-11-27T23:24:30.397Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9zt6lq, queueLength=0"}
{"level":"DEBUG","ts":"2025-11-27T23:24:30.397Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"DEBUG","ts":"2025-11-27T23:24:30.397Z","msg":"Pod-to-variant matching successful: totalPods=1, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"DEBUG","ts":"2025-11-27T23:24:30.397Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=1"}
{"level":"DEBUG","ts":"2025-11-27T23:24:30.397Z","msg":"Collected capacity metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, metricsCount=1"}
{"level":"DEBUG","ts":"2025-11-27T23:24:30.397Z","msg":"Variant analysis initialized: variant=ms-inference-scheduling-llm-d-modelservice-decode, accelerator=H100, cost=10.00, replicaCount=1"}
{"level":"DEBUG","ts":"2025-11-27T23:24:30.397Z","msg":"Scale-down unsafe: insufficient non-saturated replicas: nonSaturated=1, required=2"}
{"level":"DEBUG","ts":"2025-11-27T23:24:30.397Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, totalReplicas=1, nonSaturated=1, avgSpareKv=0.413, avgSpareQueue=5.0, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"INFO","ts":"2025-11-27T23:24:30.397Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, totalReplicas=1, nonSaturated=1, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"DEBUG","ts":"2025-11-27T23:24:30.397Z","msg":"Capacity targets: no scaling needed (avgSpareKv=0.413, avgSpareQueue=5.0, all variants stable)"}
{"level":"DEBUG","ts":"2025-11-27T23:24:30.398Z","msg":"Capacity targets calculated: modelID=unsloth/Meta-Llama-3.1-8B, targets=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"INFO","ts":"2025-11-27T23:24:30.398Z","msg":"Capacity-only decisions made for model: unsloth/Meta-Llama-3.1-8B - decision count: 1"}
{"level":"INFO","ts":"2025-11-27T23:24:30.398Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-11-27T23:24:30.398Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1→target=1"}
{"level":"DEBUG","ts":"2025-11-27T23:24:30.398Z","msg":"Found VA in map: variant=ms-inference-scheduling-llm-d-modelservice-decode, hasCurrentAlloc=true, accelerator=H100"}
{"level":"INFO","ts":"2025-11-27T23:24:30.398Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 1, desired-replicas: 1, accelerator: H100"}
{"level":"INFO","ts":"2025-11-27T23:24:30.398Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=1, accelerator=H100, capacityOnly=true"}
{"level":"INFO","ts":"2025-11-27T23:24:30.403Z","msg":"Applied capacity decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1, target=1, reason=capacity-only mode: no-change"}
{"level":"INFO","ts":"2025-11-27T23:24:30.403Z","msg":"Reconciliation completed successfully: mode=capacity-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-11-27T23:24:30.403Z","msg":"Operating in CAPACITY-ONLY mode: reactive capacity-based scaling only"}
{"level":"INFO","ts":"2025-11-27T23:24:30.403Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-11-27T23:24:30.403Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-11-27T23:24:30.411Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=1, accelerator=H100, ttft=23.79ms, itl=11.72ms, cost=10.00, arrivalRate=%!f(string=66)"}
{"level":"INFO","ts":"2025-11-27T23:24:30.412Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9zt6lq, queueLength=0"}
{"level":"DEBUG","ts":"2025-11-27T23:24:30.412Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"INFO","ts":"2025-11-27T23:24:30.412Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9zt6lq, usage=0.087 (8.7%)"}
{"level":"DEBUG","ts":"2025-11-27T23:24:30.412Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"DEBUG","ts":"2025-11-27T23:24:30.412Z","msg":"Pod-to-variant matching successful: totalPods=1, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"DEBUG","ts":"2025-11-27T23:24:30.412Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=1"}
{"level":"DEBUG","ts":"2025-11-27T23:24:30.412Z","msg":"Collected capacity metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, metricsCount=1"}
{"level":"DEBUG","ts":"2025-11-27T23:24:30.412Z","msg":"Variant analysis initialized: variant=ms-inference-scheduling-llm-d-modelservice-decode, accelerator=H100, cost=10.00, replicaCount=1"}
{"level":"DEBUG","ts":"2025-11-27T23:24:30.412Z","msg":"Scale-down unsafe: insufficient non-saturated replicas: nonSaturated=1, required=2"}
{"level":"DEBUG","ts":"2025-11-27T23:24:30.412Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, totalReplicas=1, nonSaturated=1, avgSpareKv=0.413, avgSpareQueue=5.0, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"INFO","ts":"2025-11-27T23:24:30.412Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, totalReplicas=1, nonSaturated=1, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"DEBUG","ts":"2025-11-27T23:24:30.412Z","msg":"Capacity targets: no scaling needed (avgSpareKv=0.413, avgSpareQueue=5.0, all variants stable)"}
{"level":"DEBUG","ts":"2025-11-27T23:24:30.412Z","msg":"Capacity targets calculated: modelID=unsloth/Meta-Llama-3.1-8B, targets=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"INFO","ts":"2025-11-27T23:24:30.412Z","msg":"Capacity-only decisions made for model: unsloth/Meta-Llama-3.1-8B - decision count: 1"}
{"level":"INFO","ts":"2025-11-27T23:24:30.412Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-11-27T23:24:30.412Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1→target=1"}
{"level":"DEBUG","ts":"2025-11-27T23:24:30.412Z","msg":"Found VA in map: variant=ms-inference-scheduling-llm-d-modelservice-decode, hasCurrentAlloc=true, accelerator=H100"}
{"level":"INFO","ts":"2025-11-27T23:24:30.412Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 1, desired-replicas: 1, accelerator: H100"}
{"level":"INFO","ts":"2025-11-27T23:24:30.412Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=1, accelerator=H100, capacityOnly=true"}
{"level":"INFO","ts":"2025-11-27T23:24:30.416Z","msg":"Applied capacity decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1, target=1, reason=capacity-only mode: no-change"}
{"level":"INFO","ts":"2025-11-27T23:24:30.416Z","msg":"Reconciliation completed successfully: mode=capacity-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-11-27T23:25:00.403Z","msg":"Operating in CAPACITY-ONLY mode: reactive capacity-based scaling only"}
{"level":"INFO","ts":"2025-11-27T23:25:00.403Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-11-27T23:25:00.403Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-11-27T23:25:00.416Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=1, accelerator=H100, ttft=23.40ms, itl=11.42ms, cost=10.00, arrivalRate=%!f(string=31)"}
{"level":"INFO","ts":"2025-11-27T23:25:00.417Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9zt6lq, queueLength=0"}
{"level":"DEBUG","ts":"2025-11-27T23:25:00.417Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"INFO","ts":"2025-11-27T23:25:00.418Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9zt6lq, usage=0.085 (8.5%)"}
{"level":"DEBUG","ts":"2025-11-27T23:25:00.418Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"DEBUG","ts":"2025-11-27T23:25:00.418Z","msg":"Pod-to-variant matching successful: totalPods=1, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"DEBUG","ts":"2025-11-27T23:25:00.418Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=1"}
{"level":"DEBUG","ts":"2025-11-27T23:25:00.418Z","msg":"Collected capacity metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, metricsCount=1"}
{"level":"DEBUG","ts":"2025-11-27T23:25:00.418Z","msg":"Variant analysis initialized: variant=ms-inference-scheduling-llm-d-modelservice-decode, accelerator=H100, cost=10.00, replicaCount=1"}
{"level":"DEBUG","ts":"2025-11-27T23:25:00.418Z","msg":"Scale-down unsafe: insufficient non-saturated replicas: nonSaturated=1, required=2"}
{"level":"DEBUG","ts":"2025-11-27T23:25:00.418Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, totalReplicas=1, nonSaturated=1, avgSpareKv=0.415, avgSpareQueue=5.0, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"INFO","ts":"2025-11-27T23:25:00.418Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, totalReplicas=1, nonSaturated=1, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"DEBUG","ts":"2025-11-27T23:25:00.418Z","msg":"Capacity targets: no scaling needed (avgSpareKv=0.415, avgSpareQueue=5.0, all variants stable)"}
{"level":"DEBUG","ts":"2025-11-27T23:25:00.418Z","msg":"Capacity targets calculated: modelID=unsloth/Meta-Llama-3.1-8B, targets=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"INFO","ts":"2025-11-27T23:25:00.418Z","msg":"Capacity-only decisions made for model: unsloth/Meta-Llama-3.1-8B - decision count: 1"}
{"level":"INFO","ts":"2025-11-27T23:25:00.418Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-11-27T23:25:00.418Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1→target=1"}
{"level":"DEBUG","ts":"2025-11-27T23:25:00.418Z","msg":"Found VA in map: variant=ms-inference-scheduling-llm-d-modelservice-decode, hasCurrentAlloc=true, accelerator=H100"}
{"level":"INFO","ts":"2025-11-27T23:25:00.418Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 1, desired-replicas: 1, accelerator: H100"}
{"level":"INFO","ts":"2025-11-27T23:25:00.418Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=1, accelerator=H100, capacityOnly=true"}
{"level":"INFO","ts":"2025-11-27T23:25:00.422Z","msg":"Applied capacity decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1, target=1, reason=capacity-only mode: no-change"}
{"level":"INFO","ts":"2025-11-27T23:25:00.422Z","msg":"Reconciliation completed successfully: mode=capacity-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-11-27T23:25:00.422Z","msg":"Operating in CAPACITY-ONLY mode: reactive capacity-based scaling only"}
{"level":"INFO","ts":"2025-11-27T23:25:00.422Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-11-27T23:25:00.422Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-11-27T23:25:00.431Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=1, accelerator=H100, ttft=23.40ms, itl=11.42ms, cost=10.00, arrivalRate=%!f(string=31)"}
{"level":"INFO","ts":"2025-11-27T23:25:00.432Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9zt6lq, queueLength=0"}
{"level":"DEBUG","ts":"2025-11-27T23:25:00.432Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"INFO","ts":"2025-11-27T23:25:00.432Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9zt6lq, usage=0.085 (8.5%)"}
{"level":"DEBUG","ts":"2025-11-27T23:25:00.432Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"DEBUG","ts":"2025-11-27T23:25:00.432Z","msg":"Pod-to-variant matching successful: totalPods=1, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"DEBUG","ts":"2025-11-27T23:25:00.432Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=1"}
{"level":"DEBUG","ts":"2025-11-27T23:25:00.432Z","msg":"Collected capacity metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, metricsCount=1"}
{"level":"DEBUG","ts":"2025-11-27T23:25:00.432Z","msg":"Variant analysis initialized: variant=ms-inference-scheduling-llm-d-modelservice-decode, accelerator=H100, cost=10.00, replicaCount=1"}
{"level":"DEBUG","ts":"2025-11-27T23:25:00.432Z","msg":"Scale-down unsafe: insufficient non-saturated replicas: nonSaturated=1, required=2"}
{"level":"DEBUG","ts":"2025-11-27T23:25:00.432Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, totalReplicas=1, nonSaturated=1, avgSpareKv=0.415, avgSpareQueue=5.0, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"INFO","ts":"2025-11-27T23:25:00.432Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, totalReplicas=1, nonSaturated=1, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"DEBUG","ts":"2025-11-27T23:25:00.432Z","msg":"Capacity targets: no scaling needed (avgSpareKv=0.415, avgSpareQueue=5.0, all variants stable)"}
{"level":"DEBUG","ts":"2025-11-27T23:25:00.432Z","msg":"Capacity targets calculated: modelID=unsloth/Meta-Llama-3.1-8B, targets=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"INFO","ts":"2025-11-27T23:25:00.432Z","msg":"Capacity-only decisions made for model: unsloth/Meta-Llama-3.1-8B - decision count: 1"}
{"level":"INFO","ts":"2025-11-27T23:25:00.432Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-11-27T23:25:00.432Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1→target=1"}
{"level":"DEBUG","ts":"2025-11-27T23:25:00.432Z","msg":"Found VA in map: variant=ms-inference-scheduling-llm-d-modelservice-decode, hasCurrentAlloc=true, accelerator=H100"}
{"level":"INFO","ts":"2025-11-27T23:25:00.432Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 1, desired-replicas: 1, accelerator: H100"}
{"level":"INFO","ts":"2025-11-27T23:25:00.432Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=1, accelerator=H100, capacityOnly=true"}
{"level":"INFO","ts":"2025-11-27T23:25:00.436Z","msg":"Applied capacity decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1, target=1, reason=capacity-only mode: no-change"}
{"level":"INFO","ts":"2025-11-27T23:25:00.436Z","msg":"Reconciliation completed successfully: mode=capacity-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-11-27T23:25:30.423Z","msg":"Operating in CAPACITY-ONLY mode: reactive capacity-based scaling only"}
{"level":"INFO","ts":"2025-11-27T23:25:30.423Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-11-27T23:25:30.423Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-11-27T23:25:30.436Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=1, accelerator=H100, ttft=0.00ms, itl=0.00ms, cost=10.00, arrivalRate=%!f(string=0.)"}
{"level":"INFO","ts":"2025-11-27T23:25:30.437Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9zt6lq, queueLength=0"}
{"level":"DEBUG","ts":"2025-11-27T23:25:30.437Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"INFO","ts":"2025-11-27T23:25:30.437Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9zt6lq, usage=0.000 (0.0%)"}
{"level":"DEBUG","ts":"2025-11-27T23:25:30.437Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"DEBUG","ts":"2025-11-27T23:25:30.437Z","msg":"Pod-to-variant matching successful: totalPods=1, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"DEBUG","ts":"2025-11-27T23:25:30.437Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=1"}
{"level":"DEBUG","ts":"2025-11-27T23:25:30.437Z","msg":"Collected capacity metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, metricsCount=1"}
{"level":"DEBUG","ts":"2025-11-27T23:25:30.437Z","msg":"Variant analysis initialized: variant=ms-inference-scheduling-llm-d-modelservice-decode, accelerator=H100, cost=10.00, replicaCount=1"}
{"level":"DEBUG","ts":"2025-11-27T23:25:30.437Z","msg":"Scale-down unsafe: insufficient non-saturated replicas: nonSaturated=1, required=2"}
{"level":"DEBUG","ts":"2025-11-27T23:25:30.437Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, totalReplicas=1, nonSaturated=1, avgSpareKv=0.500, avgSpareQueue=5.0, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"INFO","ts":"2025-11-27T23:25:30.437Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, totalReplicas=1, nonSaturated=1, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"DEBUG","ts":"2025-11-27T23:25:30.437Z","msg":"Capacity targets: no scaling needed (avgSpareKv=0.500, avgSpareQueue=5.0, all variants stable)"}
{"level":"DEBUG","ts":"2025-11-27T23:25:30.437Z","msg":"Capacity targets calculated: modelID=unsloth/Meta-Llama-3.1-8B, targets=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"INFO","ts":"2025-11-27T23:25:30.437Z","msg":"Capacity-only decisions made for model: unsloth/Meta-Llama-3.1-8B - decision count: 1"}
{"level":"INFO","ts":"2025-11-27T23:25:30.437Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-11-27T23:25:30.437Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1→target=1"}
{"level":"DEBUG","ts":"2025-11-27T23:25:30.437Z","msg":"Found VA in map: variant=ms-inference-scheduling-llm-d-modelservice-decode, hasCurrentAlloc=true, accelerator=H100"}
{"level":"INFO","ts":"2025-11-27T23:25:30.437Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 1, desired-replicas: 1, accelerator: H100"}
{"level":"INFO","ts":"2025-11-27T23:25:30.437Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=1, accelerator=H100, capacityOnly=true"}
{"level":"INFO","ts":"2025-11-27T23:25:30.450Z","msg":"Applied capacity decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1, target=1, reason=capacity-only mode: no-change"}
{"level":"INFO","ts":"2025-11-27T23:25:30.450Z","msg":"Reconciliation completed successfully: mode=capacity-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-11-27T23:25:30.450Z","msg":"Operating in CAPACITY-ONLY mode: reactive capacity-based scaling only"}
{"level":"INFO","ts":"2025-11-27T23:25:30.450Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-11-27T23:25:30.450Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-11-27T23:25:30.459Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=1, accelerator=H100, ttft=0.00ms, itl=0.00ms, cost=10.00, arrivalRate=%!f(string=0.)"}
{"level":"INFO","ts":"2025-11-27T23:25:30.461Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9zt6lq, queueLength=0"}
{"level":"DEBUG","ts":"2025-11-27T23:25:30.461Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"INFO","ts":"2025-11-27T23:25:30.461Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9zt6lq, usage=0.000 (0.0%)"}
{"level":"DEBUG","ts":"2025-11-27T23:25:30.461Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"DEBUG","ts":"2025-11-27T23:25:30.461Z","msg":"Pod-to-variant matching successful: totalPods=1, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"DEBUG","ts":"2025-11-27T23:25:30.461Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=1"}
{"level":"DEBUG","ts":"2025-11-27T23:25:30.461Z","msg":"Collected capacity metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, metricsCount=1"}
{"level":"DEBUG","ts":"2025-11-27T23:25:30.461Z","msg":"Variant analysis initialized: variant=ms-inference-scheduling-llm-d-modelservice-decode, accelerator=H100, cost=10.00, replicaCount=1"}
{"level":"DEBUG","ts":"2025-11-27T23:25:30.461Z","msg":"Scale-down unsafe: insufficient non-saturated replicas: nonSaturated=1, required=2"}
{"level":"DEBUG","ts":"2025-11-27T23:25:30.461Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, totalReplicas=1, nonSaturated=1, avgSpareKv=0.500, avgSpareQueue=5.0, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"INFO","ts":"2025-11-27T23:25:30.461Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, totalReplicas=1, nonSaturated=1, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"DEBUG","ts":"2025-11-27T23:25:30.461Z","msg":"Capacity targets: no scaling needed (avgSpareKv=0.500, avgSpareQueue=5.0, all variants stable)"}
{"level":"DEBUG","ts":"2025-11-27T23:25:30.461Z","msg":"Capacity targets calculated: modelID=unsloth/Meta-Llama-3.1-8B, targets=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"INFO","ts":"2025-11-27T23:25:30.461Z","msg":"Capacity-only decisions made for model: unsloth/Meta-Llama-3.1-8B - decision count: 1"}
{"level":"INFO","ts":"2025-11-27T23:25:30.461Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-11-27T23:25:30.461Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1→target=1"}
{"level":"DEBUG","ts":"2025-11-27T23:25:30.461Z","msg":"Found VA in map: variant=ms-inference-scheduling-llm-d-modelservice-decode, hasCurrentAlloc=true, accelerator=H100"}
{"level":"INFO","ts":"2025-11-27T23:25:30.461Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 1, desired-replicas: 1, accelerator: H100"}
{"level":"INFO","ts":"2025-11-27T23:25:30.461Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=1, accelerator=H100, capacityOnly=true"}
{"level":"INFO","ts":"2025-11-27T23:25:30.468Z","msg":"Applied capacity decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1, target=1, reason=capacity-only mode: no-change"}
{"level":"INFO","ts":"2025-11-27T23:25:30.468Z","msg":"Reconciliation completed successfully: mode=capacity-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-11-27T23:26:00.452Z","msg":"Operating in CAPACITY-ONLY mode: reactive capacity-based scaling only"}
{"level":"INFO","ts":"2025-11-27T23:26:00.452Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-11-27T23:26:00.452Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-11-27T23:26:00.559Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=1, accelerator=H100, ttft=0.00ms, itl=0.00ms, cost=10.00, arrivalRate=%!f(string=0.)"}
{"level":"INFO","ts":"2025-11-27T23:26:00.576Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9zt6lq, usage=0.000 (0.0%)"}
{"level":"DEBUG","ts":"2025-11-27T23:26:00.576Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"INFO","ts":"2025-11-27T23:26:00.584Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9zt6lq, queueLength=0"}
{"level":"DEBUG","ts":"2025-11-27T23:26:00.584Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"DEBUG","ts":"2025-11-27T23:26:00.584Z","msg":"Pod-to-variant matching successful: totalPods=1, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"DEBUG","ts":"2025-11-27T23:26:00.584Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=1"}
{"level":"DEBUG","ts":"2025-11-27T23:26:00.584Z","msg":"Collected capacity metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, metricsCount=1"}
{"level":"DEBUG","ts":"2025-11-27T23:26:00.584Z","msg":"Variant analysis initialized: variant=ms-inference-scheduling-llm-d-modelservice-decode, accelerator=H100, cost=10.00, replicaCount=1"}
{"level":"DEBUG","ts":"2025-11-27T23:26:00.584Z","msg":"Scale-down unsafe: insufficient non-saturated replicas: nonSaturated=1, required=2"}
{"level":"DEBUG","ts":"2025-11-27T23:26:00.584Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, totalReplicas=1, nonSaturated=1, avgSpareKv=0.500, avgSpareQueue=5.0, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"INFO","ts":"2025-11-27T23:26:00.584Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, totalReplicas=1, nonSaturated=1, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"DEBUG","ts":"2025-11-27T23:26:00.584Z","msg":"Capacity targets: no scaling needed (avgSpareKv=0.500, avgSpareQueue=5.0, all variants stable)"}
{"level":"DEBUG","ts":"2025-11-27T23:26:00.584Z","msg":"Capacity targets calculated: modelID=unsloth/Meta-Llama-3.1-8B, targets=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"INFO","ts":"2025-11-27T23:26:00.584Z","msg":"Capacity-only decisions made for model: unsloth/Meta-Llama-3.1-8B - decision count: 1"}
{"level":"INFO","ts":"2025-11-27T23:26:00.584Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-11-27T23:26:00.584Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1→target=1"}
{"level":"DEBUG","ts":"2025-11-27T23:26:00.584Z","msg":"Found VA in map: variant=ms-inference-scheduling-llm-d-modelservice-decode, hasCurrentAlloc=true, accelerator=H100"}
{"level":"INFO","ts":"2025-11-27T23:26:00.584Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 1, desired-replicas: 1, accelerator: H100"}
{"level":"INFO","ts":"2025-11-27T23:26:00.584Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=1, accelerator=H100, capacityOnly=true"}
{"level":"INFO","ts":"2025-11-27T23:26:00.589Z","msg":"Applied capacity decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1, target=1, reason=capacity-only mode: no-change"}
{"level":"INFO","ts":"2025-11-27T23:26:00.589Z","msg":"Reconciliation completed successfully: mode=capacity-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-11-27T23:26:00.589Z","msg":"Operating in CAPACITY-ONLY mode: reactive capacity-based scaling only"}
{"level":"INFO","ts":"2025-11-27T23:26:00.589Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-11-27T23:26:00.589Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-11-27T23:26:00.712Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=1, accelerator=H100, ttft=0.00ms, itl=0.00ms, cost=10.00, arrivalRate=%!f(string=0.)"}
{"level":"INFO","ts":"2025-11-27T23:26:00.741Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9zt6lq, usage=0.000 (0.0%)"}
{"level":"DEBUG","ts":"2025-11-27T23:26:00.741Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"INFO","ts":"2025-11-27T23:26:00.741Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9zt6lq, queueLength=0"}
{"level":"DEBUG","ts":"2025-11-27T23:26:00.741Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"DEBUG","ts":"2025-11-27T23:26:00.741Z","msg":"Pod-to-variant matching successful: totalPods=1, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"DEBUG","ts":"2025-11-27T23:26:00.741Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=1"}
{"level":"DEBUG","ts":"2025-11-27T23:26:00.741Z","msg":"Collected capacity metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, metricsCount=1"}
{"level":"DEBUG","ts":"2025-11-27T23:26:00.741Z","msg":"Variant analysis initialized: variant=ms-inference-scheduling-llm-d-modelservice-decode, accelerator=H100, cost=10.00, replicaCount=1"}
{"level":"DEBUG","ts":"2025-11-27T23:26:00.741Z","msg":"Scale-down unsafe: insufficient non-saturated replicas: nonSaturated=1, required=2"}
{"level":"DEBUG","ts":"2025-11-27T23:26:00.741Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, totalReplicas=1, nonSaturated=1, avgSpareKv=0.500, avgSpareQueue=5.0, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"INFO","ts":"2025-11-27T23:26:00.741Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, totalReplicas=1, nonSaturated=1, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"DEBUG","ts":"2025-11-27T23:26:00.741Z","msg":"Capacity targets: no scaling needed (avgSpareKv=0.500, avgSpareQueue=5.0, all variants stable)"}
{"level":"DEBUG","ts":"2025-11-27T23:26:00.741Z","msg":"Capacity targets calculated: modelID=unsloth/Meta-Llama-3.1-8B, targets=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"INFO","ts":"2025-11-27T23:26:00.741Z","msg":"Capacity-only decisions made for model: unsloth/Meta-Llama-3.1-8B - decision count: 1"}
{"level":"INFO","ts":"2025-11-27T23:26:00.741Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-11-27T23:26:00.741Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1→target=1"}
{"level":"DEBUG","ts":"2025-11-27T23:26:00.741Z","msg":"Found VA in map: variant=ms-inference-scheduling-llm-d-modelservice-decode, hasCurrentAlloc=true, accelerator=H100"}
{"level":"INFO","ts":"2025-11-27T23:26:00.742Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 1, desired-replicas: 1, accelerator: H100"}
{"level":"INFO","ts":"2025-11-27T23:26:00.742Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=1, accelerator=H100, capacityOnly=true"}
{"level":"INFO","ts":"2025-11-27T23:26:00.746Z","msg":"Applied capacity decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1, target=1, reason=capacity-only mode: no-change"}
{"level":"INFO","ts":"2025-11-27T23:26:00.746Z","msg":"Reconciliation completed successfully: mode=capacity-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-11-27T23:26:30.591Z","msg":"Operating in CAPACITY-ONLY mode: reactive capacity-based scaling only"}
{"level":"INFO","ts":"2025-11-27T23:26:30.591Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-11-27T23:26:30.591Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-11-27T23:26:30.612Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=1, accelerator=H100, ttft=0.00ms, itl=0.00ms, cost=10.00, arrivalRate=%!f(string=0.)"}
{"level":"INFO","ts":"2025-11-27T23:26:30.614Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9zt6lq, usage=0.000 (0.0%)"}
{"level":"DEBUG","ts":"2025-11-27T23:26:30.614Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"INFO","ts":"2025-11-27T23:26:30.614Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9zt6lq, queueLength=0"}
{"level":"DEBUG","ts":"2025-11-27T23:26:30.614Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"DEBUG","ts":"2025-11-27T23:26:30.614Z","msg":"Pod-to-variant matching successful: totalPods=1, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"DEBUG","ts":"2025-11-27T23:26:30.614Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=1"}
{"level":"DEBUG","ts":"2025-11-27T23:26:30.614Z","msg":"Collected capacity metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, metricsCount=1"}
{"level":"DEBUG","ts":"2025-11-27T23:26:30.614Z","msg":"Variant analysis initialized: variant=ms-inference-scheduling-llm-d-modelservice-decode, accelerator=H100, cost=10.00, replicaCount=1"}
{"level":"DEBUG","ts":"2025-11-27T23:26:30.614Z","msg":"Scale-down unsafe: insufficient non-saturated replicas: nonSaturated=1, required=2"}
{"level":"DEBUG","ts":"2025-11-27T23:26:30.614Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, totalReplicas=1, nonSaturated=1, avgSpareKv=0.500, avgSpareQueue=5.0, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"INFO","ts":"2025-11-27T23:26:30.614Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, totalReplicas=1, nonSaturated=1, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"DEBUG","ts":"2025-11-27T23:26:30.614Z","msg":"Capacity targets: no scaling needed (avgSpareKv=0.500, avgSpareQueue=5.0, all variants stable)"}
{"level":"DEBUG","ts":"2025-11-27T23:26:30.614Z","msg":"Capacity targets calculated: modelID=unsloth/Meta-Llama-3.1-8B, targets=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"INFO","ts":"2025-11-27T23:26:30.614Z","msg":"Capacity-only decisions made for model: unsloth/Meta-Llama-3.1-8B - decision count: 1"}
{"level":"INFO","ts":"2025-11-27T23:26:30.614Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-11-27T23:26:30.614Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1→target=1"}
{"level":"DEBUG","ts":"2025-11-27T23:26:30.614Z","msg":"Found VA in map: variant=ms-inference-scheduling-llm-d-modelservice-decode, hasCurrentAlloc=true, accelerator=H100"}
{"level":"INFO","ts":"2025-11-27T23:26:30.614Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 1, desired-replicas: 1, accelerator: H100"}
{"level":"INFO","ts":"2025-11-27T23:26:30.614Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=1, accelerator=H100, capacityOnly=true"}
{"level":"INFO","ts":"2025-11-27T23:26:30.619Z","msg":"Applied capacity decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1, target=1, reason=capacity-only mode: no-change"}
{"level":"INFO","ts":"2025-11-27T23:26:30.619Z","msg":"Reconciliation completed successfully: mode=capacity-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-11-27T23:26:30.747Z","msg":"Operating in CAPACITY-ONLY mode: reactive capacity-based scaling only"}
{"level":"INFO","ts":"2025-11-27T23:26:30.747Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-11-27T23:26:30.747Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-11-27T23:26:30.757Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=1, accelerator=H100, ttft=0.00ms, itl=0.00ms, cost=10.00, arrivalRate=%!f(string=0.)"}
{"level":"INFO","ts":"2025-11-27T23:26:30.758Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9zt6lq, usage=0.000 (0.0%)"}
{"level":"DEBUG","ts":"2025-11-27T23:26:30.758Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"INFO","ts":"2025-11-27T23:26:30.758Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9zt6lq, queueLength=0"}
{"level":"DEBUG","ts":"2025-11-27T23:26:30.758Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"DEBUG","ts":"2025-11-27T23:26:30.758Z","msg":"Pod-to-variant matching successful: totalPods=1, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"DEBUG","ts":"2025-11-27T23:26:30.758Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=1"}
{"level":"DEBUG","ts":"2025-11-27T23:26:30.758Z","msg":"Collected capacity metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, metricsCount=1"}
{"level":"DEBUG","ts":"2025-11-27T23:26:30.758Z","msg":"Variant analysis initialized: variant=ms-inference-scheduling-llm-d-modelservice-decode, accelerator=H100, cost=10.00, replicaCount=1"}
{"level":"DEBUG","ts":"2025-11-27T23:26:30.758Z","msg":"Scale-down unsafe: insufficient non-saturated replicas: nonSaturated=1, required=2"}
{"level":"DEBUG","ts":"2025-11-27T23:26:30.758Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, totalReplicas=1, nonSaturated=1, avgSpareKv=0.500, avgSpareQueue=5.0, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"INFO","ts":"2025-11-27T23:26:30.758Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, totalReplicas=1, nonSaturated=1, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"DEBUG","ts":"2025-11-27T23:26:30.758Z","msg":"Capacity targets: no scaling needed (avgSpareKv=0.500, avgSpareQueue=5.0, all variants stable)"}
{"level":"DEBUG","ts":"2025-11-27T23:26:30.758Z","msg":"Capacity targets calculated: modelID=unsloth/Meta-Llama-3.1-8B, targets=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"INFO","ts":"2025-11-27T23:26:30.758Z","msg":"Capacity-only decisions made for model: unsloth/Meta-Llama-3.1-8B - decision count: 1"}
{"level":"INFO","ts":"2025-11-27T23:26:30.758Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-11-27T23:26:30.758Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1→target=1"}
{"level":"DEBUG","ts":"2025-11-27T23:26:30.758Z","msg":"Found VA in map: variant=ms-inference-scheduling-llm-d-modelservice-decode, hasCurrentAlloc=true, accelerator=H100"}
{"level":"INFO","ts":"2025-11-27T23:26:30.758Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 1, desired-replicas: 1, accelerator: H100"}
{"level":"INFO","ts":"2025-11-27T23:26:30.758Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=1, accelerator=H100, capacityOnly=true"}
{"level":"INFO","ts":"2025-11-27T23:26:30.762Z","msg":"Applied capacity decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1, target=1, reason=capacity-only mode: no-change"}
{"level":"INFO","ts":"2025-11-27T23:26:30.762Z","msg":"Reconciliation completed successfully: mode=capacity-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-11-27T23:27:00.620Z","msg":"Operating in CAPACITY-ONLY mode: reactive capacity-based scaling only"}
{"level":"INFO","ts":"2025-11-27T23:27:00.620Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-11-27T23:27:00.620Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-11-27T23:27:00.633Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=1, accelerator=H100, ttft=0.00ms, itl=0.00ms, cost=10.00, arrivalRate=%!f(string=0.)"}
{"level":"INFO","ts":"2025-11-27T23:27:00.635Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9zt6lq, usage=0.000 (0.0%)"}
{"level":"DEBUG","ts":"2025-11-27T23:27:00.635Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"INFO","ts":"2025-11-27T23:27:00.635Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9zt6lq, queueLength=0"}
{"level":"DEBUG","ts":"2025-11-27T23:27:00.635Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"DEBUG","ts":"2025-11-27T23:27:00.635Z","msg":"Pod-to-variant matching successful: totalPods=1, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"DEBUG","ts":"2025-11-27T23:27:00.635Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=1"}
{"level":"DEBUG","ts":"2025-11-27T23:27:00.635Z","msg":"Collected capacity metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, metricsCount=1"}
{"level":"DEBUG","ts":"2025-11-27T23:27:00.635Z","msg":"Variant analysis initialized: variant=ms-inference-scheduling-llm-d-modelservice-decode, accelerator=H100, cost=10.00, replicaCount=1"}
{"level":"DEBUG","ts":"2025-11-27T23:27:00.635Z","msg":"Scale-down unsafe: insufficient non-saturated replicas: nonSaturated=1, required=2"}
{"level":"DEBUG","ts":"2025-11-27T23:27:00.635Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, totalReplicas=1, nonSaturated=1, avgSpareKv=0.500, avgSpareQueue=5.0, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"INFO","ts":"2025-11-27T23:27:00.635Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, totalReplicas=1, nonSaturated=1, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"DEBUG","ts":"2025-11-27T23:27:00.635Z","msg":"Capacity targets: no scaling needed (avgSpareKv=0.500, avgSpareQueue=5.0, all variants stable)"}
{"level":"DEBUG","ts":"2025-11-27T23:27:00.635Z","msg":"Capacity targets calculated: modelID=unsloth/Meta-Llama-3.1-8B, targets=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"INFO","ts":"2025-11-27T23:27:00.635Z","msg":"Capacity-only decisions made for model: unsloth/Meta-Llama-3.1-8B - decision count: 1"}
{"level":"INFO","ts":"2025-11-27T23:27:00.635Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-11-27T23:27:00.635Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1→target=1"}
{"level":"DEBUG","ts":"2025-11-27T23:27:00.635Z","msg":"Found VA in map: variant=ms-inference-scheduling-llm-d-modelservice-decode, hasCurrentAlloc=true, accelerator=H100"}
{"level":"INFO","ts":"2025-11-27T23:27:00.635Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 1, desired-replicas: 1, accelerator: H100"}
{"level":"INFO","ts":"2025-11-27T23:27:00.635Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=1, accelerator=H100, capacityOnly=true"}
{"level":"INFO","ts":"2025-11-27T23:27:00.640Z","msg":"Applied capacity decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1, target=1, reason=capacity-only mode: no-change"}
{"level":"INFO","ts":"2025-11-27T23:27:00.640Z","msg":"Reconciliation completed successfully: mode=capacity-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-11-27T23:27:00.763Z","msg":"Operating in CAPACITY-ONLY mode: reactive capacity-based scaling only"}
{"level":"INFO","ts":"2025-11-27T23:27:00.763Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-11-27T23:27:00.763Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-11-27T23:27:00.774Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=1, accelerator=H100, ttft=0.00ms, itl=0.00ms, cost=10.00, arrivalRate=%!f(string=0.)"}
{"level":"INFO","ts":"2025-11-27T23:27:00.776Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9zt6lq, queueLength=0"}
{"level":"DEBUG","ts":"2025-11-27T23:27:00.776Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"INFO","ts":"2025-11-27T23:27:00.776Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9zt6lq, usage=0.000 (0.0%)"}
{"level":"DEBUG","ts":"2025-11-27T23:27:00.776Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"DEBUG","ts":"2025-11-27T23:27:00.776Z","msg":"Pod-to-variant matching successful: totalPods=1, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"DEBUG","ts":"2025-11-27T23:27:00.776Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=1"}
{"level":"DEBUG","ts":"2025-11-27T23:27:00.776Z","msg":"Collected capacity metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, metricsCount=1"}
{"level":"DEBUG","ts":"2025-11-27T23:27:00.776Z","msg":"Variant analysis initialized: variant=ms-inference-scheduling-llm-d-modelservice-decode, accelerator=H100, cost=10.00, replicaCount=1"}
{"level":"DEBUG","ts":"2025-11-27T23:27:00.776Z","msg":"Scale-down unsafe: insufficient non-saturated replicas: nonSaturated=1, required=2"}
{"level":"DEBUG","ts":"2025-11-27T23:27:00.776Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, totalReplicas=1, nonSaturated=1, avgSpareKv=0.500, avgSpareQueue=5.0, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"INFO","ts":"2025-11-27T23:27:00.776Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, totalReplicas=1, nonSaturated=1, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"DEBUG","ts":"2025-11-27T23:27:00.776Z","msg":"Capacity targets: no scaling needed (avgSpareKv=0.500, avgSpareQueue=5.0, all variants stable)"}
{"level":"DEBUG","ts":"2025-11-27T23:27:00.776Z","msg":"Capacity targets calculated: modelID=unsloth/Meta-Llama-3.1-8B, targets=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"INFO","ts":"2025-11-27T23:27:00.776Z","msg":"Capacity-only decisions made for model: unsloth/Meta-Llama-3.1-8B - decision count: 1"}
{"level":"INFO","ts":"2025-11-27T23:27:00.776Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-11-27T23:27:00.776Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1→target=1"}
{"level":"DEBUG","ts":"2025-11-27T23:27:00.776Z","msg":"Found VA in map: variant=ms-inference-scheduling-llm-d-modelservice-decode, hasCurrentAlloc=true, accelerator=H100"}
{"level":"INFO","ts":"2025-11-27T23:27:00.776Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 1, desired-replicas: 1, accelerator: H100"}
{"level":"INFO","ts":"2025-11-27T23:27:00.776Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=1, accelerator=H100, capacityOnly=true"}
{"level":"INFO","ts":"2025-11-27T23:27:00.780Z","msg":"Applied capacity decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1, target=1, reason=capacity-only mode: no-change"}
{"level":"INFO","ts":"2025-11-27T23:27:00.780Z","msg":"Reconciliation completed successfully: mode=capacity-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-11-27T23:27:30.641Z","msg":"Operating in CAPACITY-ONLY mode: reactive capacity-based scaling only"}
{"level":"INFO","ts":"2025-11-27T23:27:30.641Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-11-27T23:27:30.641Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-11-27T23:27:30.654Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=1, accelerator=H100, ttft=0.00ms, itl=0.00ms, cost=10.00, arrivalRate=%!f(string=0.)"}
{"level":"INFO","ts":"2025-11-27T23:27:30.655Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9zt6lq, queueLength=0"}
{"level":"DEBUG","ts":"2025-11-27T23:27:30.655Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"INFO","ts":"2025-11-27T23:27:30.655Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9zt6lq, usage=0.000 (0.0%)"}
{"level":"DEBUG","ts":"2025-11-27T23:27:30.655Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"DEBUG","ts":"2025-11-27T23:27:30.655Z","msg":"Pod-to-variant matching successful: totalPods=1, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"DEBUG","ts":"2025-11-27T23:27:30.655Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=1"}
{"level":"DEBUG","ts":"2025-11-27T23:27:30.655Z","msg":"Collected capacity metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, metricsCount=1"}
{"level":"DEBUG","ts":"2025-11-27T23:27:30.655Z","msg":"Variant analysis initialized: variant=ms-inference-scheduling-llm-d-modelservice-decode, accelerator=H100, cost=10.00, replicaCount=1"}
{"level":"DEBUG","ts":"2025-11-27T23:27:30.655Z","msg":"Scale-down unsafe: insufficient non-saturated replicas: nonSaturated=1, required=2"}
{"level":"DEBUG","ts":"2025-11-27T23:27:30.655Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, totalReplicas=1, nonSaturated=1, avgSpareKv=0.500, avgSpareQueue=5.0, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"INFO","ts":"2025-11-27T23:27:30.655Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, totalReplicas=1, nonSaturated=1, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"DEBUG","ts":"2025-11-27T23:27:30.655Z","msg":"Capacity targets: no scaling needed (avgSpareKv=0.500, avgSpareQueue=5.0, all variants stable)"}
{"level":"DEBUG","ts":"2025-11-27T23:27:30.655Z","msg":"Capacity targets calculated: modelID=unsloth/Meta-Llama-3.1-8B, targets=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"INFO","ts":"2025-11-27T23:27:30.655Z","msg":"Capacity-only decisions made for model: unsloth/Meta-Llama-3.1-8B - decision count: 1"}
{"level":"INFO","ts":"2025-11-27T23:27:30.655Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-11-27T23:27:30.656Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1→target=1"}
{"level":"DEBUG","ts":"2025-11-27T23:27:30.656Z","msg":"Found VA in map: variant=ms-inference-scheduling-llm-d-modelservice-decode, hasCurrentAlloc=true, accelerator=H100"}
{"level":"INFO","ts":"2025-11-27T23:27:30.656Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 1, desired-replicas: 1, accelerator: H100"}
{"level":"INFO","ts":"2025-11-27T23:27:30.656Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=1, accelerator=H100, capacityOnly=true"}
{"level":"INFO","ts":"2025-11-27T23:27:30.660Z","msg":"Applied capacity decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1, target=1, reason=capacity-only mode: no-change"}
{"level":"INFO","ts":"2025-11-27T23:27:30.660Z","msg":"Reconciliation completed successfully: mode=capacity-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-11-27T23:27:30.780Z","msg":"Operating in CAPACITY-ONLY mode: reactive capacity-based scaling only"}
{"level":"INFO","ts":"2025-11-27T23:27:30.781Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-11-27T23:27:30.781Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-11-27T23:27:30.809Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=1, accelerator=H100, ttft=0.00ms, itl=0.00ms, cost=10.00, arrivalRate=%!f(string=0.)"}
{"level":"INFO","ts":"2025-11-27T23:27:30.811Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9zt6lq, queueLength=0"}
{"level":"DEBUG","ts":"2025-11-27T23:27:30.811Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"INFO","ts":"2025-11-27T23:27:30.811Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9zt6lq, usage=0.000 (0.0%)"}
{"level":"DEBUG","ts":"2025-11-27T23:27:30.811Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"DEBUG","ts":"2025-11-27T23:27:30.811Z","msg":"Pod-to-variant matching successful: totalPods=1, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"DEBUG","ts":"2025-11-27T23:27:30.811Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=1"}
{"level":"DEBUG","ts":"2025-11-27T23:27:30.811Z","msg":"Collected capacity metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, metricsCount=1"}
{"level":"DEBUG","ts":"2025-11-27T23:27:30.811Z","msg":"Variant analysis initialized: variant=ms-inference-scheduling-llm-d-modelservice-decode, accelerator=H100, cost=10.00, replicaCount=1"}
{"level":"DEBUG","ts":"2025-11-27T23:27:30.811Z","msg":"Scale-down unsafe: insufficient non-saturated replicas: nonSaturated=1, required=2"}
{"level":"DEBUG","ts":"2025-11-27T23:27:30.811Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, totalReplicas=1, nonSaturated=1, avgSpareKv=0.500, avgSpareQueue=5.0, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"INFO","ts":"2025-11-27T23:27:30.811Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, totalReplicas=1, nonSaturated=1, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"DEBUG","ts":"2025-11-27T23:27:30.811Z","msg":"Capacity targets: no scaling needed (avgSpareKv=0.500, avgSpareQueue=5.0, all variants stable)"}
{"level":"DEBUG","ts":"2025-11-27T23:27:30.811Z","msg":"Capacity targets calculated: modelID=unsloth/Meta-Llama-3.1-8B, targets=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"INFO","ts":"2025-11-27T23:27:30.811Z","msg":"Capacity-only decisions made for model: unsloth/Meta-Llama-3.1-8B - decision count: 1"}
{"level":"INFO","ts":"2025-11-27T23:27:30.811Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-11-27T23:27:30.811Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1→target=1"}
{"level":"DEBUG","ts":"2025-11-27T23:27:30.811Z","msg":"Found VA in map: variant=ms-inference-scheduling-llm-d-modelservice-decode, hasCurrentAlloc=true, accelerator=H100"}
{"level":"INFO","ts":"2025-11-27T23:27:30.811Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 1, desired-replicas: 1, accelerator: H100"}
{"level":"INFO","ts":"2025-11-27T23:27:30.811Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=1, accelerator=H100, capacityOnly=true"}
{"level":"INFO","ts":"2025-11-27T23:27:30.816Z","msg":"Applied capacity decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1, target=1, reason=capacity-only mode: no-change"}
{"level":"INFO","ts":"2025-11-27T23:27:30.816Z","msg":"Reconciliation completed successfully: mode=capacity-only, modelsProcessed=1, decisionsApplied=1"}
