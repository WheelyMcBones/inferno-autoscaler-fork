{"level":"INFO","ts":"2025-12-09T05:14:25.966Z","msg":"Zap logger initialized"}
{"level":"INFO","ts":"2025-12-09T05:14:25.968Z","msg":"Creating metrics emitter instance"}
{"level":"INFO","ts":"2025-12-09T05:14:25.968Z","msg":"Metrics emitter created successfully"}
{"level":"INFO","ts":"2025-12-09T05:14:25.968Z","msg":"Using Prometheus configuration from environment variables: address=https://thanos-querier.openshift-monitoring.svc.cluster.local:9091"}
{"level":"INFO","ts":"2025-12-09T05:14:25.968Z","msg":"Initializing Prometheus client -> address: https://thanos-querier.openshift-monitoring.svc.cluster.local:9091, tls_enabled: true"}
{"level":"INFO","ts":"2025-12-09T05:14:25.968Z","msg":"CA certificate loaded successfullypath/etc/ssl/certs/prometheus-ca.crt"}
{"level":"INFO","ts":"2025-12-09T05:14:25.968Z","msg":"TLS configuration applied to Prometheus HTTPS transport"}
{"level":"INFO","ts":"2025-12-09T05:14:25.968Z","msg":"Bearer token loaded from filepath/var/run/secrets/kubernetes.io/serviceaccount/token"}
{"level":"INFO","ts":"2025-12-09T05:14:26.042Z","msg":"Prometheus API validation successful with queryqueryup"}
{"level":"INFO","ts":"2025-12-09T05:14:26.042Z","msg":"Prometheus client and API wrapper initialized and validated successfully"}
{"level":"INFO","ts":"2025-12-09T05:14:26.042Z","msg":"Starting manager"}
{"level":"INFO","ts":"2025-12-09T05:14:26.042Z","msg":"Registering custom metrics with Prometheus registry"}
{"level":"info","ts":"2025-12-09T05:14:26Z","logger":"controller-runtime.metrics","msg":"Starting metrics server"}
{"level":"INFO","ts":"2025-12-09T05:14:26.043Z","msg":"disabling http/2"}
{"level":"info","ts":"2025-12-09T05:14:26Z","msg":"starting server","name":"health probe","addr":"[::]:8081"}
I1209 05:14:26.043455       1 leaderelection.go:257] attempting to acquire leader lease workload-variant-autoscaler-system/72dd1cf1.llm-d.ai...
I1209 05:14:26.054039       1 leaderelection.go:271] successfully acquired lease workload-variant-autoscaler-system/72dd1cf1.llm-d.ai
{"level":"info","ts":"2025-12-09T05:14:26Z","msg":"Starting EventSource","controller":"variantAutoscaling","controllerGroup":"llmd.ai","controllerKind":"VariantAutoscaling","source":"kind source: *v1.ConfigMap"}
{"level":"info","ts":"2025-12-09T05:14:26Z","msg":"Starting EventSource","controller":"variantAutoscaling","controllerGroup":"llmd.ai","controllerKind":"VariantAutoscaling","source":"kind source: *v1.ServiceMonitor"}
{"level":"info","ts":"2025-12-09T05:14:26Z","msg":"Starting EventSource","controller":"variantAutoscaling","controllerGroup":"llmd.ai","controllerKind":"VariantAutoscaling","source":"kind source: *v1alpha1.VariantAutoscaling"}
{"level":"info","ts":"2025-12-09T05:14:27Z","logger":"controller-runtime.metrics","msg":"Serving metrics server","bindAddress":":8443","secure":true}
{"level":"info","ts":"2025-12-09T05:14:28Z","msg":"Starting Controller","controller":"variantAutoscaling","controllerGroup":"llmd.ai","controllerKind":"VariantAutoscaling"}
{"level":"info","ts":"2025-12-09T05:14:28Z","msg":"Starting workers","controller":"variantAutoscaling","controllerGroup":"llmd.ai","controllerKind":"VariantAutoscaling","worker count":1}
{"level":"INFO","ts":"2025-12-09T05:14:28.043Z","msg":"Reconciling VariantAutoscaling: name=ms-inference-scheduling-llm-d-modelservice-decode, namespace=llm-d-inference-scheduler, modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-09T05:14:28.043Z","msg":"saturation scaling config cache updated: entries=1, has_default=true"}
{"level":"INFO","ts":"2025-12-09T05:14:28.043Z","msg":"saturation scaling configuration loaded successfully"}
{"level":"INFO","ts":"2025-12-09T05:14:28.043Z","msg":"Operating in MODEL-ONLY mode: model-based optimization only"}
{"level":"INFO","ts":"2025-12-09T05:14:28.043Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-12-09T05:14:28.043Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-12-09T05:14:28.046Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-666b5f4cq54t2, queueLength=0"}
{"level":"DEBUG","ts":"2025-12-09T05:14:28.046Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"INFO","ts":"2025-12-09T05:14:28.046Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-666b5f4cq54t2, usage=0.000 (0.0%)"}
{"level":"DEBUG","ts":"2025-12-09T05:14:28.046Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"DEBUG","ts":"2025-12-09T05:14:28.249Z","msg":"Pod-to-variant matching successful: totalPods=1, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"DEBUG","ts":"2025-12-09T05:14:28.249Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=1"}
{"level":"DEBUG","ts":"2025-12-09T05:14:28.249Z","msg":"Collected capacity metrics for observability in model-only mode: modelID=unsloth/Meta-Llama-3.1-8B, replicaCount=1"}
{"level":"DEBUG","ts":"2025-12-09T05:14:28.263Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=1, accelerator=H100, ttft=0.00ms, itl=0.00ms, cost=100.00, maxBatch=256, arrivalRate=0.00, avgInputTokens=0.00, avgOutputTokens=0.00"}
{"level":"DEBUG","ts":"2025-12-09T05:14:28.263Z","msg":"Experimental model tuner is enabled globally: tuning model performance parameters for active VAs"}
{"level":"DEBUG","ts":"2025-12-09T05:14:28.263Z","msg":"Using state vals from VA status to tune variant ms-inference-scheduling-llm-d-modelservice-decode: alpha= 8.902763, beta= 0.042113, gamma= 17.136303, delta= 0.000245"}
{"level":"DEBUG","ts":"2025-12-09T05:14:28.263Z","msg":"[Tuner Config] variant=ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler | Initial state: alpha=8.902763, beta=0.042113, gamma=17.136303, delta=0.000245 | SLO targets: TTFT=1000.00ms, ITL=10.00ms | Has covariance=true"}
{"level":"DEBUG","ts":"2025-12-09T05:14:28.263Z","msg":"Skipping tuning for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler due to insufficient load. Using fallback parameters."}
{"level":"INFO","ts":"2025-12-09T05:14:28.263Z","msg":"Using tuned parameters from status for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler"}
{"level":"DEBUG","ts":"2025-12-09T05:14:28.263Z","msg":"Updated SystemData for model=unsloth/Meta-Llama-3.1-8B, accelerator=H100: alpha=8.902763, beta=0.042113, gamma=17.136303, delta=0.000245"}
{"level":"DEBUG","ts":"2025-12-09T05:14:28.263Z","msg":"Optimization solution - system: Solution: \ns=ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler; c=Premium; m=unsloth/Meta-Llama-3.1-8B; rate=0; inTk=0; outTk=0; sol=1, sat=false, alloc={acc=H100; numRep=1; maxBatch=512; cost=100, val=0, itl=8.944877, ttft=17.136547, rho=0, maxRPM=645362.4}; slo-itl=10, slo-ttft=1000, slo-tps=0 \nAllocationByType: \nname=NVIDIA-H100-80GB-HBM3, count=1, limit=0, cost=100 \ntotalCost=100 \n"}
{"level":"DEBUG","ts":"2025-12-09T05:14:28.263Z","msg":"Setting accelerator name: Name=H100, allocationData={H100 1 512 100 8.944877 17.136547 {0 0 0}}"}
{"level":"INFO","ts":"2025-12-09T05:14:28.263Z","msg":"Model-based optimization completed for model: unsloth/Meta-Llama-3.1-8B - model-based targets: map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"WARN","ts":"2025-12-09T05:14:28.263Z","msg":"saturation analysis unavailable, using model-based targets only: modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-09T05:14:28.263Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-12-09T05:14:28.263Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1→target=1"}
{"level":"INFO","ts":"2025-12-09T05:14:28.263Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 1, desired-replicas: 1, accelerator: H100"}
{"level":"INFO","ts":"2025-12-09T05:14:28.263Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=1, accelerator=H100, SaturationOnly=false"}
{"level":"INFO","ts":"2025-12-09T05:14:28.270Z","msg":"Applied Saturation decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1, target=1, reason=model-based only (Saturation unavailable)"}
{"level":"INFO","ts":"2025-12-09T05:14:28.270Z","msg":"Reconciliation completed successfully: mode=model-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-12-09T05:14:28.270Z","msg":"VariantAutoscaling resource not found, may have been deleted: name=, namespace="}
{"level":"INFO","ts":"2025-12-09T05:15:28.270Z","msg":"Reconciling VariantAutoscaling: name=ms-inference-scheduling-llm-d-modelservice-decode, namespace=llm-d-inference-scheduler, modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-09T05:15:28.270Z","msg":"saturation scaling config cache updated: entries=1, has_default=true"}
{"level":"INFO","ts":"2025-12-09T05:15:28.270Z","msg":"saturation scaling configuration loaded successfully"}
{"level":"INFO","ts":"2025-12-09T05:15:28.270Z","msg":"Operating in MODEL-ONLY mode: model-based optimization only"}
{"level":"INFO","ts":"2025-12-09T05:15:28.270Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-12-09T05:15:28.270Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-12-09T05:15:28.275Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-666b5f4cq54t2, queueLength=0"}
{"level":"DEBUG","ts":"2025-12-09T05:15:28.275Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"INFO","ts":"2025-12-09T05:15:28.275Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-666b5f4cq54t2, usage=0.000 (0.0%)"}
{"level":"DEBUG","ts":"2025-12-09T05:15:28.275Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"DEBUG","ts":"2025-12-09T05:15:28.277Z","msg":"Pod-to-variant matching successful: totalPods=1, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"DEBUG","ts":"2025-12-09T05:15:28.277Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=1"}
{"level":"DEBUG","ts":"2025-12-09T05:15:28.277Z","msg":"Collected capacity metrics for observability in model-only mode: modelID=unsloth/Meta-Llama-3.1-8B, replicaCount=1"}
{"level":"DEBUG","ts":"2025-12-09T05:15:28.288Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=1, accelerator=H100, ttft=0.00ms, itl=0.00ms, cost=100.00, maxBatch=256, arrivalRate=0.00, avgInputTokens=0.00, avgOutputTokens=0.00"}
{"level":"DEBUG","ts":"2025-12-09T05:15:28.288Z","msg":"Experimental model tuner is enabled globally: tuning model performance parameters for active VAs"}
{"level":"DEBUG","ts":"2025-12-09T05:15:28.288Z","msg":"Using state vals from VA status to tune variant ms-inference-scheduling-llm-d-modelservice-decode: alpha= 8.902763, beta= 0.042113, gamma= 17.136303, delta= 0.000245"}
{"level":"DEBUG","ts":"2025-12-09T05:15:28.288Z","msg":"[Tuner Config] variant=ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler | Initial state: alpha=8.902763, beta=0.042113, gamma=17.136303, delta=0.000245 | SLO targets: TTFT=1000.00ms, ITL=10.00ms | Has covariance=true"}
{"level":"DEBUG","ts":"2025-12-09T05:15:28.288Z","msg":"Skipping tuning for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler due to insufficient load. Using fallback parameters."}
{"level":"INFO","ts":"2025-12-09T05:15:28.288Z","msg":"Using tuned parameters from status for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler"}
{"level":"DEBUG","ts":"2025-12-09T05:15:28.288Z","msg":"Updated SystemData for model=unsloth/Meta-Llama-3.1-8B, accelerator=H100: alpha=8.902763, beta=0.042113, gamma=17.136303, delta=0.000245"}
{"level":"DEBUG","ts":"2025-12-09T05:15:28.288Z","msg":"Optimization solution - system: Solution: \ns=ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler; c=Premium; m=unsloth/Meta-Llama-3.1-8B; rate=0; inTk=0; outTk=0; sol=1, sat=false, alloc={acc=H100; numRep=1; maxBatch=512; cost=100, val=0, itl=8.944877, ttft=17.136547, rho=0, maxRPM=645362.4}; slo-itl=10, slo-ttft=1000, slo-tps=0 \nAllocationByType: \nname=NVIDIA-H100-80GB-HBM3, count=1, limit=0, cost=100 \ntotalCost=100 \n"}
{"level":"DEBUG","ts":"2025-12-09T05:15:28.288Z","msg":"Setting accelerator name: Name=H100, allocationData={H100 1 512 100 8.944877 17.136547 {0 0 0}}"}
{"level":"INFO","ts":"2025-12-09T05:15:28.288Z","msg":"Model-based optimization completed for model: unsloth/Meta-Llama-3.1-8B - model-based targets: map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"WARN","ts":"2025-12-09T05:15:28.288Z","msg":"saturation analysis unavailable, using model-based targets only: modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-09T05:15:28.288Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-12-09T05:15:28.288Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1→target=1"}
{"level":"INFO","ts":"2025-12-09T05:15:28.288Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 1, desired-replicas: 1, accelerator: H100"}
{"level":"INFO","ts":"2025-12-09T05:15:28.288Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=1, accelerator=H100, SaturationOnly=false"}
{"level":"INFO","ts":"2025-12-09T05:15:28.293Z","msg":"Applied Saturation decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1, target=1, reason=model-based only (Saturation unavailable)"}
{"level":"INFO","ts":"2025-12-09T05:15:28.294Z","msg":"Reconciliation completed successfully: mode=model-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-12-09T05:16:28.294Z","msg":"Reconciling VariantAutoscaling: name=ms-inference-scheduling-llm-d-modelservice-decode, namespace=llm-d-inference-scheduler, modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-09T05:16:28.294Z","msg":"saturation scaling config cache updated: entries=1, has_default=true"}
{"level":"INFO","ts":"2025-12-09T05:16:28.294Z","msg":"saturation scaling configuration loaded successfully"}
{"level":"INFO","ts":"2025-12-09T05:16:28.294Z","msg":"Operating in MODEL-ONLY mode: model-based optimization only"}
{"level":"INFO","ts":"2025-12-09T05:16:28.294Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-12-09T05:16:28.294Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-12-09T05:16:28.304Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-666b5f4cq54t2, usage=0.075 (7.5%)"}
{"level":"DEBUG","ts":"2025-12-09T05:16:28.304Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"INFO","ts":"2025-12-09T05:16:28.313Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-666b5f4cq54t2, queueLength=0"}
{"level":"DEBUG","ts":"2025-12-09T05:16:28.313Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"DEBUG","ts":"2025-12-09T05:16:28.316Z","msg":"Pod-to-variant matching successful: totalPods=1, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"DEBUG","ts":"2025-12-09T05:16:28.316Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=1"}
{"level":"DEBUG","ts":"2025-12-09T05:16:28.316Z","msg":"Collected capacity metrics for observability in model-only mode: modelID=unsloth/Meta-Llama-3.1-8B, replicaCount=1"}
{"level":"DEBUG","ts":"2025-12-09T05:16:28.327Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=1, accelerator=H100, ttft=16.84ms, itl=9.63ms, cost=100.00, maxBatch=256, arrivalRate=286.43, avgInputTokens=257.78, avgOutputTokens=410.32"}
{"level":"DEBUG","ts":"2025-12-09T05:16:28.327Z","msg":"Experimental model tuner is enabled globally: tuning model performance parameters for active VAs"}
{"level":"DEBUG","ts":"2025-12-09T05:16:28.327Z","msg":"Using state vals from VA status to tune variant ms-inference-scheduling-llm-d-modelservice-decode: alpha= 8.902763, beta= 0.042113, gamma= 17.136303, delta= 0.000245"}
{"level":"DEBUG","ts":"2025-12-09T05:16:28.327Z","msg":"[Tuner Config] variant=ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler | Initial state: alpha=8.902763, beta=0.042113, gamma=17.136303, delta=0.000245 | SLO targets: TTFT=1000.00ms, ITL=10.00ms | Has covariance=true"}
{"level":"INFO","ts":"2025-12-09T05:16:28.327Z","msg":"Tuner validation succeeded for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler, server ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler - New NIS: 0.041811"}
{"level":"DEBUG","ts":"2025-12-09T05:16:28.327Z","msg":"Model tuner results: model=unsloth/Meta-Llama-3.1-8B, accelerator=H100, alpha=8.822747, beta=0.041546, gamma=17.084726, delta=0.000245, NIS=0.04"}
{"level":"DEBUG","ts":"2025-12-09T05:16:28.327Z","msg":"Updated tuner status for variant ms-inference-scheduling-llm-d-modelservice-decode: alpha=8.822747, beta=0.041546, gamma=17.084726, delta=0.000245, NIS=0.041811"}
{"level":"INFO","ts":"2025-12-09T05:16:28.327Z","msg":"Tuned performance parameters result: variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler - alpha: 8.822747, beta: 0.041546, gamma: 17.084726, delta: 0.000245"}
{"level":"DEBUG","ts":"2025-12-09T05:16:28.336Z","msg":"Optimization solution - system: Solution: \ns=ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler; c=Premium; m=unsloth/Meta-Llama-3.1-8B; rate=286.43; inTk=257; outTk=410; sol=1, sat=false, alloc={acc=H100; numRep=1; maxBatch=512; cost=100, val=0, itl=9.65079, ttft=18.339653, rho=0.03697402, maxRPM=399.17636}; slo-itl=10, slo-ttft=1000, slo-tps=0 \nAllocationByType: \nname=NVIDIA-H100-80GB-HBM3, count=1, limit=0, cost=100 \ntotalCost=100 \n"}
{"level":"DEBUG","ts":"2025-12-09T05:16:28.336Z","msg":"Setting accelerator name: Name=H100, allocationData={H100 1 512 100 9.65079 18.339653 {286.43 257 410}}"}
{"level":"INFO","ts":"2025-12-09T05:16:28.336Z","msg":"Model-based optimization completed for model: unsloth/Meta-Llama-3.1-8B - model-based targets: map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"WARN","ts":"2025-12-09T05:16:28.336Z","msg":"saturation analysis unavailable, using model-based targets only: modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-09T05:16:28.336Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-12-09T05:16:28.336Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1→target=1"}
{"level":"INFO","ts":"2025-12-09T05:16:28.336Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 1, desired-replicas: 1, accelerator: H100"}
{"level":"INFO","ts":"2025-12-09T05:16:28.336Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=1, accelerator=H100, SaturationOnly=false"}
{"level":"INFO","ts":"2025-12-09T05:16:28.342Z","msg":"Applied Saturation decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1, target=1, reason=model-based only (Saturation unavailable)"}
{"level":"INFO","ts":"2025-12-09T05:16:28.342Z","msg":"Reconciliation completed successfully: mode=model-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-12-09T05:17:28.342Z","msg":"Reconciling VariantAutoscaling: name=ms-inference-scheduling-llm-d-modelservice-decode, namespace=llm-d-inference-scheduler, modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-09T05:17:28.342Z","msg":"saturation scaling config cache updated: entries=1, has_default=true"}
{"level":"INFO","ts":"2025-12-09T05:17:28.342Z","msg":"saturation scaling configuration loaded successfully"}
{"level":"INFO","ts":"2025-12-09T05:17:28.342Z","msg":"Operating in MODEL-ONLY mode: model-based optimization only"}
{"level":"INFO","ts":"2025-12-09T05:17:28.342Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-12-09T05:17:28.342Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-12-09T05:17:28.347Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-666b5f4cq54t2, usage=0.145 (14.5%)"}
{"level":"DEBUG","ts":"2025-12-09T05:17:28.347Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"INFO","ts":"2025-12-09T05:17:28.347Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-666b5f4cq54t2, queueLength=0"}
{"level":"DEBUG","ts":"2025-12-09T05:17:28.347Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"DEBUG","ts":"2025-12-09T05:17:28.349Z","msg":"Pod-to-variant matching successful: totalPods=1, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"DEBUG","ts":"2025-12-09T05:17:28.349Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=1"}
{"level":"DEBUG","ts":"2025-12-09T05:17:28.349Z","msg":"Collected capacity metrics for observability in model-only mode: modelID=unsloth/Meta-Llama-3.1-8B, replicaCount=1"}
{"level":"DEBUG","ts":"2025-12-09T05:17:28.360Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=1, accelerator=H100, ttft=20.81ms, itl=11.72ms, cost=100.00, maxBatch=256, arrivalRate=701.41, avgInputTokens=247.87, avgOutputTokens=433.02"}
{"level":"DEBUG","ts":"2025-12-09T05:17:28.361Z","msg":"Experimental model tuner is enabled globally: tuning model performance parameters for active VAs"}
{"level":"DEBUG","ts":"2025-12-09T05:17:28.361Z","msg":"Using state vals from VA status to tune variant ms-inference-scheduling-llm-d-modelservice-decode: alpha= 8.822747, beta= 0.041546, gamma= 17.084726, delta= 0.000245"}
{"level":"DEBUG","ts":"2025-12-09T05:17:28.361Z","msg":"[Tuner Config] variant=ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler | Initial state: alpha=8.822747, beta=0.041546, gamma=17.084726, delta=0.000245 | SLO targets: TTFT=1000.00ms, ITL=10.00ms | Has covariance=true"}
{"level":"INFO","ts":"2025-12-09T05:17:28.361Z","msg":"Tuner validation succeeded for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler, server ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler - New NIS: 0.290051"}
{"level":"DEBUG","ts":"2025-12-09T05:17:28.361Z","msg":"Model tuner results: model=unsloth/Meta-Llama-3.1-8B, accelerator=H100, alpha=8.927133, beta=0.045890, gamma=17.089598, delta=0.000245, NIS=0.29"}
{"level":"DEBUG","ts":"2025-12-09T05:17:28.361Z","msg":"Updated tuner status for variant ms-inference-scheduling-llm-d-modelservice-decode: alpha=8.927133, beta=0.045890, gamma=17.089598, delta=0.000245, NIS=0.290051"}
{"level":"INFO","ts":"2025-12-09T05:17:28.361Z","msg":"Tuned performance parameters result: variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler - alpha: 8.927133, beta: 0.045890, gamma: 17.089598, delta: 0.000245"}
{"level":"DEBUG","ts":"2025-12-09T05:17:28.370Z","msg":"Optimization solution - system: Solution: \ns=ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler; c=Premium; m=unsloth/Meta-Llama-3.1-8B; rate=701.41; inTk=247; outTk=433; sol=1, sat=false, alloc={acc=H100; numRep=3; maxBatch=512; cost=300, val=200, itl=9.727737, ttft=18.14536, rho=0.03212153, maxRPM=309.49622}; slo-itl=10, slo-ttft=1000, slo-tps=0 \nAllocationByType: \nname=NVIDIA-H100-80GB-HBM3, count=3, limit=0, cost=300 \ntotalCost=300 \n"}
{"level":"DEBUG","ts":"2025-12-09T05:17:28.370Z","msg":"Setting accelerator name: Name=H100, allocationData={H100 3 512 300 9.727737 18.14536 {701.41 247 433}}"}
{"level":"INFO","ts":"2025-12-09T05:17:28.370Z","msg":"Model-based optimization completed for model: unsloth/Meta-Llama-3.1-8B - model-based targets: map[ms-inference-scheduling-llm-d-modelservice-decode:3]"}
{"level":"WARN","ts":"2025-12-09T05:17:28.370Z","msg":"saturation analysis unavailable, using model-based targets only: modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-09T05:17:28.370Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-12-09T05:17:28.370Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=scale-up, current=1→target=3"}
{"level":"INFO","ts":"2025-12-09T05:17:28.370Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 1, desired-replicas: 3, accelerator: H100"}
{"level":"INFO","ts":"2025-12-09T05:17:28.370Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=3, accelerator=H100, SaturationOnly=false"}
{"level":"INFO","ts":"2025-12-09T05:17:28.376Z","msg":"Applied Saturation decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=scale-up, current=1, target=3, reason=model-based only (Saturation unavailable)"}
{"level":"INFO","ts":"2025-12-09T05:17:28.376Z","msg":"Reconciliation completed successfully: mode=model-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-12-09T05:18:28.377Z","msg":"Reconciling VariantAutoscaling: name=ms-inference-scheduling-llm-d-modelservice-decode, namespace=llm-d-inference-scheduler, modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-09T05:18:28.377Z","msg":"saturation scaling config cache updated: entries=1, has_default=true"}
{"level":"INFO","ts":"2025-12-09T05:18:28.377Z","msg":"saturation scaling configuration loaded successfully"}
{"level":"INFO","ts":"2025-12-09T05:18:28.377Z","msg":"Operating in MODEL-ONLY mode: model-based optimization only"}
{"level":"INFO","ts":"2025-12-09T05:18:28.377Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-12-09T05:18:28.377Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-12-09T05:18:28.390Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-666b5f4cq54t2, usage=0.213 (21.3%)"}
{"level":"DEBUG","ts":"2025-12-09T05:18:28.390Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"INFO","ts":"2025-12-09T05:18:28.390Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-666b5f4cq54t2, queueLength=0"}
{"level":"DEBUG","ts":"2025-12-09T05:18:28.390Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"DEBUG","ts":"2025-12-09T05:18:28.394Z","msg":"Pod-to-variant matching successful: totalPods=1, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"DEBUG","ts":"2025-12-09T05:18:28.394Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=1"}
{"level":"DEBUG","ts":"2025-12-09T05:18:28.394Z","msg":"Collected capacity metrics for observability in model-only mode: modelID=unsloth/Meta-Llama-3.1-8B, replicaCount=1"}
{"level":"DEBUG","ts":"2025-12-09T05:18:28.419Z","msg":"Replica count mismatch for llm-d-inference-scheduler/unsloth/Meta-Llama-3.1-8B: spec=3, reporting_metrics=1"}
{"level":"DEBUG","ts":"2025-12-09T05:18:28.419Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=1, accelerator=H100, ttft=29.55ms, itl=16.22ms, cost=100.00, maxBatch=256, arrivalRate=1163.76, avgInputTokens=267.54, avgOutputTokens=403.60"}
{"level":"DEBUG","ts":"2025-12-09T05:18:28.419Z","msg":"Experimental model tuner is enabled globally: tuning model performance parameters for active VAs"}
{"level":"DEBUG","ts":"2025-12-09T05:18:28.419Z","msg":"Using state vals from VA status to tune variant ms-inference-scheduling-llm-d-modelservice-decode: alpha= 8.927133, beta= 0.045890, gamma= 17.089598, delta= 0.000245"}
{"level":"DEBUG","ts":"2025-12-09T05:18:28.419Z","msg":"[Tuner Config] variant=ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler | Initial state: alpha=8.927133, beta=0.045890, gamma=17.089598, delta=0.000245 | SLO targets: TTFT=1000.00ms, ITL=10.00ms | Has covariance=true"}
{"level":"INFO","ts":"2025-12-09T05:18:28.420Z","msg":"Tuner validation succeeded for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler, server ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler - New NIS: 3.442678"}
{"level":"DEBUG","ts":"2025-12-09T05:18:28.420Z","msg":"Model tuner results: model=unsloth/Meta-Llama-3.1-8B, accelerator=H100, alpha=8.915428, beta=0.058248, gamma=17.232052, delta=0.000245, NIS=3.44"}
{"level":"DEBUG","ts":"2025-12-09T05:18:28.420Z","msg":"Updated tuner status for variant ms-inference-scheduling-llm-d-modelservice-decode: alpha=8.915428, beta=0.058248, gamma=17.232052, delta=0.000245, NIS=3.442678"}
{"level":"INFO","ts":"2025-12-09T05:18:28.420Z","msg":"Tuned performance parameters result: variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler - alpha: 8.915428, beta: 0.058248, gamma: 17.232052, delta: 0.000245"}
{"level":"DEBUG","ts":"2025-12-09T05:18:28.429Z","msg":"Optimization solution - system: Solution: \ns=ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler; c=Premium; m=unsloth/Meta-Llama-3.1-8B; rate=1163.76; inTk=267; outTk=403; sol=1, sat=false, alloc={acc=H100; numRep=5; maxBatch=512; cost=500, val=400, itl=9.874776, ttft=18.309607, rho=0.0302151, maxRPM=261.7822}; slo-itl=10, slo-ttft=1000, slo-tps=0 \nAllocationByType: \nname=NVIDIA-H100-80GB-HBM3, count=5, limit=0, cost=500 \ntotalCost=500 \n"}
{"level":"DEBUG","ts":"2025-12-09T05:18:28.429Z","msg":"Setting accelerator name: Name=H100, allocationData={H100 5 512 500 9.874776 18.309607 {1163.76 267 403}}"}
{"level":"INFO","ts":"2025-12-09T05:18:28.429Z","msg":"Model-based optimization completed for model: unsloth/Meta-Llama-3.1-8B - model-based targets: map[ms-inference-scheduling-llm-d-modelservice-decode:5]"}
{"level":"WARN","ts":"2025-12-09T05:18:28.429Z","msg":"saturation analysis unavailable, using model-based targets only: modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-09T05:18:28.429Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-12-09T05:18:28.429Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=scale-up, current=1→target=5"}
{"level":"INFO","ts":"2025-12-09T05:18:28.429Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 1, desired-replicas: 5, accelerator: H100"}
{"level":"INFO","ts":"2025-12-09T05:18:28.429Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=5, accelerator=H100, SaturationOnly=false"}
{"level":"INFO","ts":"2025-12-09T05:18:28.436Z","msg":"Applied Saturation decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=scale-up, current=1, target=5, reason=model-based only (Saturation unavailable)"}
{"level":"INFO","ts":"2025-12-09T05:18:28.436Z","msg":"Reconciliation completed successfully: mode=model-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-12-09T05:19:28.437Z","msg":"Reconciling VariantAutoscaling: name=ms-inference-scheduling-llm-d-modelservice-decode, namespace=llm-d-inference-scheduler, modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-09T05:19:28.437Z","msg":"saturation scaling config cache updated: entries=1, has_default=true"}
{"level":"INFO","ts":"2025-12-09T05:19:28.437Z","msg":"saturation scaling configuration loaded successfully"}
{"level":"INFO","ts":"2025-12-09T05:19:28.437Z","msg":"Operating in MODEL-ONLY mode: model-based optimization only"}
{"level":"INFO","ts":"2025-12-09T05:19:28.437Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-12-09T05:19:28.437Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-12-09T05:19:28.446Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-666b5f4cq54t2, queueLength=0"}
{"level":"DEBUG","ts":"2025-12-09T05:19:28.446Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"INFO","ts":"2025-12-09T05:19:28.447Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-666b5f4cq54t2, usage=0.207 (20.7%)"}
{"level":"DEBUG","ts":"2025-12-09T05:19:28.447Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"DEBUG","ts":"2025-12-09T05:19:28.450Z","msg":"Pod-to-variant matching successful: totalPods=1, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"DEBUG","ts":"2025-12-09T05:19:28.450Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=1"}
{"level":"DEBUG","ts":"2025-12-09T05:19:28.450Z","msg":"Collected capacity metrics for observability in model-only mode: modelID=unsloth/Meta-Llama-3.1-8B, replicaCount=1"}
{"level":"DEBUG","ts":"2025-12-09T05:19:28.465Z","msg":"Replica count mismatch for llm-d-inference-scheduler/unsloth/Meta-Llama-3.1-8B: spec=5, reporting_metrics=1"}
{"level":"DEBUG","ts":"2025-12-09T05:19:28.465Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=1, accelerator=H100, ttft=22.25ms, itl=12.95ms, cost=100.00, maxBatch=256, arrivalRate=824.33, avgInputTokens=240.39, avgOutputTokens=489.84"}
{"level":"DEBUG","ts":"2025-12-09T05:19:28.465Z","msg":"Experimental model tuner is enabled globally: tuning model performance parameters for active VAs"}
{"level":"DEBUG","ts":"2025-12-09T05:19:28.465Z","msg":"Using state vals from VA status to tune variant ms-inference-scheduling-llm-d-modelservice-decode: alpha= 8.915428, beta= 0.058248, gamma= 17.232052, delta= 0.000245"}
{"level":"DEBUG","ts":"2025-12-09T05:19:28.465Z","msg":"[Tuner Config] variant=ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler | Initial state: alpha=8.915428, beta=0.058248, gamma=17.232052, delta=0.000245 | SLO targets: TTFT=1000.00ms, ITL=10.00ms | Has covariance=true"}
{"level":"INFO","ts":"2025-12-09T05:19:28.467Z","msg":"Tuner validation succeeded for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler, server ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler - New NIS: 3.581438"}
{"level":"DEBUG","ts":"2025-12-09T05:19:28.467Z","msg":"Model tuner results: model=unsloth/Meta-Llama-3.1-8B, accelerator=H100, alpha=8.131246, beta=0.055893, gamma=17.225965, delta=0.000245, NIS=3.58"}
{"level":"DEBUG","ts":"2025-12-09T05:19:28.467Z","msg":"Updated tuner status for variant ms-inference-scheduling-llm-d-modelservice-decode: alpha=8.131246, beta=0.055893, gamma=17.225965, delta=0.000245, NIS=3.581438"}
{"level":"INFO","ts":"2025-12-09T05:19:28.467Z","msg":"Tuned performance parameters result: variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler - alpha: 8.131246, beta: 0.055893, gamma: 17.225965, delta: 0.000245"}
{"level":"DEBUG","ts":"2025-12-09T05:19:28.470Z","msg":"Optimization solution - system: Solution: \ns=ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler; c=Premium; m=unsloth/Meta-Llama-3.1-8B; rate=824.33; inTk=240; outTk=489; sol=1, sat=false, alloc={acc=H100; numRep=3; maxBatch=512; cost=300, val=200, itl=9.361216, ttft=18.519846, rho=0.041026812, maxRPM=397.22305}; slo-itl=10, slo-ttft=1000, slo-tps=0 \nAllocationByType: \nname=NVIDIA-H100-80GB-HBM3, count=3, limit=0, cost=300 \ntotalCost=300 \n"}
{"level":"DEBUG","ts":"2025-12-09T05:19:28.470Z","msg":"Setting accelerator name: Name=H100, allocationData={H100 3 512 300 9.361216 18.519846 {824.33 240 489}}"}
{"level":"INFO","ts":"2025-12-09T05:19:28.470Z","msg":"Model-based optimization completed for model: unsloth/Meta-Llama-3.1-8B - model-based targets: map[ms-inference-scheduling-llm-d-modelservice-decode:3]"}
{"level":"WARN","ts":"2025-12-09T05:19:28.470Z","msg":"saturation analysis unavailable, using model-based targets only: modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-09T05:19:28.470Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-12-09T05:19:28.470Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=scale-up, current=1→target=3"}
{"level":"INFO","ts":"2025-12-09T05:19:28.470Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 1, desired-replicas: 3, accelerator: H100"}
{"level":"INFO","ts":"2025-12-09T05:19:28.470Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=3, accelerator=H100, SaturationOnly=false"}
{"level":"INFO","ts":"2025-12-09T05:19:28.476Z","msg":"Applied Saturation decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=scale-up, current=1, target=3, reason=model-based only (Saturation unavailable)"}
{"level":"INFO","ts":"2025-12-09T05:19:28.476Z","msg":"Reconciliation completed successfully: mode=model-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-12-09T05:20:28.477Z","msg":"Reconciling VariantAutoscaling: name=ms-inference-scheduling-llm-d-modelservice-decode, namespace=llm-d-inference-scheduler, modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-09T05:20:28.477Z","msg":"saturation scaling config cache updated: entries=1, has_default=true"}
{"level":"INFO","ts":"2025-12-09T05:20:28.477Z","msg":"saturation scaling configuration loaded successfully"}
{"level":"INFO","ts":"2025-12-09T05:20:28.477Z","msg":"Operating in MODEL-ONLY mode: model-based optimization only"}
{"level":"INFO","ts":"2025-12-09T05:20:28.477Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-12-09T05:20:28.477Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-12-09T05:20:28.485Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-666b5f4cq54t2, usage=0.064 (6.4%)"}
{"level":"INFO","ts":"2025-12-09T05:20:28.485Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-666b5f4c4jw6g, usage=0.012 (1.2%)"}
{"level":"INFO","ts":"2025-12-09T05:20:28.485Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-666b5f4cmhzlc, usage=0.056 (5.6%)"}
{"level":"DEBUG","ts":"2025-12-09T05:20:28.485Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=3"}
{"level":"INFO","ts":"2025-12-09T05:20:28.485Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-666b5f4cq54t2, queueLength=0"}
{"level":"INFO","ts":"2025-12-09T05:20:28.485Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-666b5f4c4jw6g, queueLength=0"}
{"level":"INFO","ts":"2025-12-09T05:20:28.485Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-666b5f4cmhzlc, queueLength=0"}
{"level":"DEBUG","ts":"2025-12-09T05:20:28.485Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=3"}
{"level":"DEBUG","ts":"2025-12-09T05:20:28.488Z","msg":"Pod-to-variant matching successful: totalPods=3, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:3]"}
{"level":"DEBUG","ts":"2025-12-09T05:20:28.488Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=3"}
{"level":"DEBUG","ts":"2025-12-09T05:20:28.488Z","msg":"Collected capacity metrics for observability in model-only mode: modelID=unsloth/Meta-Llama-3.1-8B, replicaCount=3"}
{"level":"DEBUG","ts":"2025-12-09T05:20:28.502Z","msg":"Replica count mismatch for llm-d-inference-scheduler/unsloth/Meta-Llama-3.1-8B: spec=3, reporting_metrics=2"}
{"level":"DEBUG","ts":"2025-12-09T05:20:28.502Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=2, accelerator=H100, ttft=19.23ms, itl=10.24ms, cost=200.00, maxBatch=256, arrivalRate=930.60, avgInputTokens=238.88, avgOutputTokens=405.31"}
{"level":"DEBUG","ts":"2025-12-09T05:20:28.502Z","msg":"Experimental model tuner is enabled globally: tuning model performance parameters for active VAs"}
{"level":"DEBUG","ts":"2025-12-09T05:20:28.502Z","msg":"Using state vals from VA status to tune variant ms-inference-scheduling-llm-d-modelservice-decode: alpha= 8.131246, beta= 0.055893, gamma= 17.225965, delta= 0.000245"}
{"level":"DEBUG","ts":"2025-12-09T05:20:28.502Z","msg":"[Tuner Config] variant=ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler | Initial state: alpha=8.131246, beta=0.055893, gamma=17.225965, delta=0.000245 | SLO targets: TTFT=1000.00ms, ITL=10.00ms | Has covariance=true"}
{"level":"INFO","ts":"2025-12-09T05:20:28.503Z","msg":"Tuner validation succeeded for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler, server ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler - New NIS: 0.141921"}
{"level":"DEBUG","ts":"2025-12-09T05:20:28.503Z","msg":"Model tuner results: model=unsloth/Meta-Llama-3.1-8B, accelerator=H100, alpha=8.408956, beta=0.054303, gamma=17.229261, delta=0.000245, NIS=0.14"}
{"level":"DEBUG","ts":"2025-12-09T05:20:28.503Z","msg":"Updated tuner status for variant ms-inference-scheduling-llm-d-modelservice-decode: alpha=8.408956, beta=0.054303, gamma=17.229261, delta=0.000245, NIS=0.141921"}
{"level":"INFO","ts":"2025-12-09T05:20:28.503Z","msg":"Tuned performance parameters result: variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler - alpha: 8.408956, beta: 0.054303, gamma: 17.229261, delta: 0.000245"}
{"level":"DEBUG","ts":"2025-12-09T05:20:28.513Z","msg":"Optimization solution - system: Solution: \ns=ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler; c=Premium; m=unsloth/Meta-Llama-3.1-8B; rate=930.6; inTk=238; outTk=405; sol=1, sat=false, alloc={acc=H100; numRep=3; maxBatch=512; cost=300, val=100, itl=9.5518265, ttft=18.456463, rho=0.039152592, maxRPM=418.3247}; slo-itl=10, slo-ttft=1000, slo-tps=0 \nAllocationByType: \nname=NVIDIA-H100-80GB-HBM3, count=3, limit=0, cost=300 \ntotalCost=300 \n"}
{"level":"DEBUG","ts":"2025-12-09T05:20:28.513Z","msg":"Setting accelerator name: Name=H100, allocationData={H100 3 512 300 9.5518265 18.456463 {930.6 238 405}}"}
{"level":"INFO","ts":"2025-12-09T05:20:28.513Z","msg":"Model-based optimization completed for model: unsloth/Meta-Llama-3.1-8B - model-based targets: map[ms-inference-scheduling-llm-d-modelservice-decode:3]"}
{"level":"WARN","ts":"2025-12-09T05:20:28.513Z","msg":"saturation analysis unavailable, using model-based targets only: modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-09T05:20:28.513Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-12-09T05:20:28.513Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=scale-up, current=2→target=3"}
{"level":"INFO","ts":"2025-12-09T05:20:28.513Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 2, desired-replicas: 3, accelerator: H100"}
{"level":"INFO","ts":"2025-12-09T05:20:28.513Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=3, accelerator=H100, SaturationOnly=false"}
{"level":"INFO","ts":"2025-12-09T05:20:28.520Z","msg":"Applied Saturation decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=scale-up, current=2, target=3, reason=model-based only (Saturation unavailable)"}
{"level":"INFO","ts":"2025-12-09T05:20:28.520Z","msg":"Reconciliation completed successfully: mode=model-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-12-09T05:21:28.520Z","msg":"Reconciling VariantAutoscaling: name=ms-inference-scheduling-llm-d-modelservice-decode, namespace=llm-d-inference-scheduler, modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-09T05:21:28.520Z","msg":"saturation scaling config cache updated: entries=1, has_default=true"}
{"level":"INFO","ts":"2025-12-09T05:21:28.520Z","msg":"saturation scaling configuration loaded successfully"}
{"level":"INFO","ts":"2025-12-09T05:21:28.520Z","msg":"Operating in MODEL-ONLY mode: model-based optimization only"}
{"level":"INFO","ts":"2025-12-09T05:21:28.520Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-12-09T05:21:28.520Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-12-09T05:21:28.525Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-666b5f4cq54t2, queueLength=0"}
{"level":"INFO","ts":"2025-12-09T05:21:28.525Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-666b5f4clhfr5, queueLength=0"}
{"level":"INFO","ts":"2025-12-09T05:21:28.525Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-666b5f4cmhzlc, queueLength=0"}
{"level":"DEBUG","ts":"2025-12-09T05:21:28.525Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=3"}
{"level":"INFO","ts":"2025-12-09T05:21:28.525Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-666b5f4cq54t2, usage=0.064 (6.4%)"}
{"level":"INFO","ts":"2025-12-09T05:21:28.525Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-666b5f4clhfr5, usage=0.066 (6.6%)"}
{"level":"INFO","ts":"2025-12-09T05:21:28.525Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-666b5f4cmhzlc, usage=0.058 (5.8%)"}
{"level":"DEBUG","ts":"2025-12-09T05:21:28.525Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=3"}
{"level":"DEBUG","ts":"2025-12-09T05:21:28.528Z","msg":"Pod-to-variant matching successful: totalPods=3, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:3]"}
{"level":"DEBUG","ts":"2025-12-09T05:21:28.528Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=3"}
{"level":"DEBUG","ts":"2025-12-09T05:21:28.528Z","msg":"Collected capacity metrics for observability in model-only mode: modelID=unsloth/Meta-Llama-3.1-8B, replicaCount=3"}
{"level":"DEBUG","ts":"2025-12-09T05:21:28.541Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=3, accelerator=H100, ttft=19.43ms, itl=10.26ms, cost=300.00, maxBatch=256, arrivalRate=1538.24, avgInputTokens=234.45, avgOutputTokens=438.27"}
{"level":"DEBUG","ts":"2025-12-09T05:21:28.541Z","msg":"Experimental model tuner is enabled globally: tuning model performance parameters for active VAs"}
{"level":"DEBUG","ts":"2025-12-09T05:21:28.541Z","msg":"Using state vals from VA status to tune variant ms-inference-scheduling-llm-d-modelservice-decode: alpha= 8.408956, beta= 0.054303, gamma= 17.229261, delta= 0.000245"}
{"level":"DEBUG","ts":"2025-12-09T05:21:28.541Z","msg":"[Tuner Config] variant=ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler | Initial state: alpha=8.408956, beta=0.054303, gamma=17.229261, delta=0.000245 | SLO targets: TTFT=1000.00ms, ITL=10.00ms | Has covariance=true"}
{"level":"INFO","ts":"2025-12-09T05:21:28.542Z","msg":"Tuner validation succeeded for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler, server ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler - New NIS: 0.320135"}
{"level":"DEBUG","ts":"2025-12-09T05:21:28.542Z","msg":"Model tuner results: model=unsloth/Meta-Llama-3.1-8B, accelerator=H100, alpha=8.175883, beta=0.053943, gamma=17.227049, delta=0.000245, NIS=0.32"}
{"level":"DEBUG","ts":"2025-12-09T05:21:28.542Z","msg":"Updated tuner status for variant ms-inference-scheduling-llm-d-modelservice-decode: alpha=8.175883, beta=0.053943, gamma=17.227049, delta=0.000245, NIS=0.320135"}
{"level":"INFO","ts":"2025-12-09T05:21:28.542Z","msg":"Tuned performance parameters result: variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler - alpha: 8.175883, beta: 0.053943, gamma: 17.227049, delta: 0.000245"}
{"level":"DEBUG","ts":"2025-12-09T05:21:28.546Z","msg":"Optimization solution - system: Solution: \ns=ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler; c=Premium; m=unsloth/Meta-Llama-3.1-8B; rate=1538.24; inTk=234; outTk=438; sol=1, sat=false, alloc={acc=H100; numRep=4; maxBatch=512; cost=400, val=100, itl=9.702216, ttft=18.849226, rho=0.053311605, maxRPM=448.5939}; slo-itl=10, slo-ttft=1000, slo-tps=0 \nAllocationByType: \nname=NVIDIA-H100-80GB-HBM3, count=4, limit=0, cost=400 \ntotalCost=400 \n"}
{"level":"DEBUG","ts":"2025-12-09T05:21:28.546Z","msg":"Setting accelerator name: Name=H100, allocationData={H100 4 512 400 9.702216 18.849226 {1538.24 234 438}}"}
{"level":"INFO","ts":"2025-12-09T05:21:28.546Z","msg":"Model-based optimization completed for model: unsloth/Meta-Llama-3.1-8B - model-based targets: map[ms-inference-scheduling-llm-d-modelservice-decode:4]"}
{"level":"WARN","ts":"2025-12-09T05:21:28.546Z","msg":"saturation analysis unavailable, using model-based targets only: modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-09T05:21:28.546Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-12-09T05:21:28.546Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=scale-up, current=3→target=4"}
{"level":"INFO","ts":"2025-12-09T05:21:28.546Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 3, desired-replicas: 4, accelerator: H100"}
{"level":"INFO","ts":"2025-12-09T05:21:28.546Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=4, accelerator=H100, SaturationOnly=false"}
{"level":"INFO","ts":"2025-12-09T05:21:28.552Z","msg":"Applied Saturation decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=scale-up, current=3, target=4, reason=model-based only (Saturation unavailable)"}
{"level":"INFO","ts":"2025-12-09T05:21:28.552Z","msg":"Reconciliation completed successfully: mode=model-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-12-09T05:22:28.553Z","msg":"Reconciling VariantAutoscaling: name=ms-inference-scheduling-llm-d-modelservice-decode, namespace=llm-d-inference-scheduler, modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-09T05:22:28.553Z","msg":"saturation scaling config cache updated: entries=1, has_default=true"}
{"level":"INFO","ts":"2025-12-09T05:22:28.553Z","msg":"saturation scaling configuration loaded successfully"}
{"level":"INFO","ts":"2025-12-09T05:22:28.553Z","msg":"Operating in MODEL-ONLY mode: model-based optimization only"}
{"level":"INFO","ts":"2025-12-09T05:22:28.553Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-12-09T05:22:28.553Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-12-09T05:22:28.563Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-666b5f4cq54t2, queueLength=0"}
{"level":"INFO","ts":"2025-12-09T05:22:28.563Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-666b5f4clhfr5, queueLength=0"}
{"level":"INFO","ts":"2025-12-09T05:22:28.563Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-666b5f4cmhzlc, queueLength=0"}
{"level":"DEBUG","ts":"2025-12-09T05:22:28.563Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=3"}
{"level":"INFO","ts":"2025-12-09T05:22:28.563Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-666b5f4cq54t2, usage=0.057 (5.7%)"}
{"level":"INFO","ts":"2025-12-09T05:22:28.563Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-666b5f4clhfr5, usage=0.000 (0.0%)"}
{"level":"INFO","ts":"2025-12-09T05:22:28.563Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-666b5f4cmhzlc, usage=0.011 (1.1%)"}
{"level":"DEBUG","ts":"2025-12-09T05:22:28.563Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=3"}
{"level":"DEBUG","ts":"2025-12-09T05:22:28.565Z","msg":"Pod-to-variant matching successful: totalPods=3, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:3]"}
{"level":"DEBUG","ts":"2025-12-09T05:22:28.565Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=3"}
{"level":"DEBUG","ts":"2025-12-09T05:22:28.565Z","msg":"Collected capacity metrics for observability in model-only mode: modelID=unsloth/Meta-Llama-3.1-8B, replicaCount=3"}
{"level":"DEBUG","ts":"2025-12-09T05:22:28.580Z","msg":"Replica count mismatch for llm-d-inference-scheduler/unsloth/Meta-Llama-3.1-8B: spec=4, reporting_metrics=3"}
{"level":"DEBUG","ts":"2025-12-09T05:22:28.580Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=3, accelerator=H100, ttft=19.47ms, itl=9.57ms, cost=300.00, maxBatch=256, arrivalRate=77.27, avgInputTokens=151.22, avgOutputTokens=835.62"}
{"level":"DEBUG","ts":"2025-12-09T05:22:28.580Z","msg":"Experimental model tuner is enabled globally: tuning model performance parameters for active VAs"}
{"level":"DEBUG","ts":"2025-12-09T05:22:28.580Z","msg":"Using state vals from VA status to tune variant ms-inference-scheduling-llm-d-modelservice-decode: alpha= 8.175883, beta= 0.053943, gamma= 17.227049, delta= 0.000245"}
{"level":"DEBUG","ts":"2025-12-09T05:22:28.580Z","msg":"[Tuner Config] variant=ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler | Initial state: alpha=8.175883, beta=0.053943, gamma=17.227049, delta=0.000245 | SLO targets: TTFT=1000.00ms, ITL=10.00ms | Has covariance=true"}
{"level":"INFO","ts":"2025-12-09T05:22:28.581Z","msg":"Tuner validation succeeded for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler, server ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler - New NIS: 4.183227"}
{"level":"DEBUG","ts":"2025-12-09T05:22:28.581Z","msg":"Model tuner results: model=unsloth/Meta-Llama-3.1-8B, accelerator=H100, alpha=9.126283, beta=0.048510, gamma=17.277304, delta=0.000245, NIS=4.18"}
{"level":"DEBUG","ts":"2025-12-09T05:22:28.581Z","msg":"Updated tuner status for variant ms-inference-scheduling-llm-d-modelservice-decode: alpha=9.126283, beta=0.048510, gamma=17.277304, delta=0.000245, NIS=4.183227"}
{"level":"INFO","ts":"2025-12-09T05:22:28.581Z","msg":"Tuned performance parameters result: variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler - alpha: 9.126283, beta: 0.048510, gamma: 17.277304, delta: 0.000245"}
{"level":"DEBUG","ts":"2025-12-09T05:22:28.590Z","msg":"Optimization solution - system: Solution: \ns=ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler; c=Premium; m=unsloth/Meta-Llama-3.1-8B; rate=77.27; inTk=151; outTk=835; sol=1, sat=false, alloc={acc=H100; numRep=1; maxBatch=512; cost=100, val=-200, itl=9.680258, ttft=17.699783, rho=0.020351376, maxRPM=122.1191}; slo-itl=10, slo-ttft=1000, slo-tps=0 \nAllocationByType: \nname=NVIDIA-H100-80GB-HBM3, count=1, limit=0, cost=100 \ntotalCost=100 \n"}
{"level":"DEBUG","ts":"2025-12-09T05:22:28.590Z","msg":"Setting accelerator name: Name=H100, allocationData={H100 1 512 100 9.680258 17.699783 {77.27 151 835}}"}
{"level":"INFO","ts":"2025-12-09T05:22:28.590Z","msg":"Model-based optimization completed for model: unsloth/Meta-Llama-3.1-8B - model-based targets: map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"WARN","ts":"2025-12-09T05:22:28.590Z","msg":"saturation analysis unavailable, using model-based targets only: modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-09T05:22:28.590Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-12-09T05:22:28.590Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=scale-down, current=3→target=1"}
{"level":"INFO","ts":"2025-12-09T05:22:28.590Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 3, desired-replicas: 1, accelerator: H100"}
{"level":"INFO","ts":"2025-12-09T05:22:28.590Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=1, accelerator=H100, SaturationOnly=false"}
{"level":"INFO","ts":"2025-12-09T05:22:28.596Z","msg":"Applied Saturation decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=scale-down, current=3, target=1, reason=model-based only (Saturation unavailable)"}
{"level":"INFO","ts":"2025-12-09T05:22:28.596Z","msg":"Reconciliation completed successfully: mode=model-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-12-09T05:23:28.597Z","msg":"Reconciling VariantAutoscaling: name=ms-inference-scheduling-llm-d-modelservice-decode, namespace=llm-d-inference-scheduler, modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-09T05:23:28.597Z","msg":"saturation scaling config cache updated: entries=1, has_default=true"}
{"level":"INFO","ts":"2025-12-09T05:23:28.597Z","msg":"saturation scaling configuration loaded successfully"}
{"level":"INFO","ts":"2025-12-09T05:23:28.597Z","msg":"Operating in MODEL-ONLY mode: model-based optimization only"}
{"level":"INFO","ts":"2025-12-09T05:23:28.597Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-12-09T05:23:28.597Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-12-09T05:23:28.601Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-666b5f4cq54t2, queueLength=0"}
{"level":"INFO","ts":"2025-12-09T05:23:28.601Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-666b5f4cmhzlc, queueLength=0"}
{"level":"DEBUG","ts":"2025-12-09T05:23:28.601Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=2"}
{"level":"INFO","ts":"2025-12-09T05:23:28.601Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-666b5f4cq54t2, usage=0.432 (43.2%)"}
{"level":"INFO","ts":"2025-12-09T05:23:28.601Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-666b5f4cmhzlc, usage=0.038 (3.8%)"}
{"level":"DEBUG","ts":"2025-12-09T05:23:28.601Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=2"}
{"level":"DEBUG","ts":"2025-12-09T05:23:28.604Z","msg":"Pod-to-variant matching successful: totalPods=2, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:2]"}
{"level":"DEBUG","ts":"2025-12-09T05:23:28.604Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=2"}
{"level":"DEBUG","ts":"2025-12-09T05:23:28.604Z","msg":"Collected capacity metrics for observability in model-only mode: modelID=unsloth/Meta-Llama-3.1-8B, replicaCount=2"}
{"level":"DEBUG","ts":"2025-12-09T05:23:28.616Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=1, accelerator=H100, ttft=34.87ms, itl=20.66ms, cost=100.00, maxBatch=256, arrivalRate=1117.52, avgInputTokens=273.46, avgOutputTokens=271.64"}
{"level":"DEBUG","ts":"2025-12-09T05:23:28.616Z","msg":"Experimental model tuner is enabled globally: tuning model performance parameters for active VAs"}
{"level":"DEBUG","ts":"2025-12-09T05:23:28.616Z","msg":"Using state vals from VA status to tune variant ms-inference-scheduling-llm-d-modelservice-decode: alpha= 9.126283, beta= 0.048510, gamma= 17.277304, delta= 0.000245"}
{"level":"DEBUG","ts":"2025-12-09T05:23:28.616Z","msg":"[Tuner Config] variant=ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler | Initial state: alpha=9.126283, beta=0.048510, gamma=17.277304, delta=0.000245 | SLO targets: TTFT=1000.00ms, ITL=10.00ms | Has covariance=true"}
{"level":"WARN","ts":"2025-12-09T05:23:28.616Z","msg":"Tuner validation failed (NIS=95.78), validation error: normalized innovation squared (NIS=95.78) exceeds threshold (7.38), rejecting update as outlier - returning previous state: alpha=9.126283, beta=0.048510, gamma=17.277304, delta=0.000245"}
{"level":"WARN","ts":"2025-12-09T05:23:28.616Z","msg":"Tuner NIS validation failed for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler, server ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler (NIS=95.78 exceeds threshold 7.38) - Keeping previous state: alpha=9.126283, beta=0.048510, gamma=17.277304, delta=0.000245"}
{"level":"INFO","ts":"2025-12-09T05:23:28.616Z","msg":"Keeping previous tuned parameters for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler due to NIS validation failure (NIS=95.782668)"}
{"level":"DEBUG","ts":"2025-12-09T05:23:28.616Z","msg":"Model tuner results: model=unsloth/Meta-Llama-3.1-8B, accelerator=H100, alpha=9.126283, beta=0.048510, gamma=17.277304, delta=0.000245, NIS=95.78"}
{"level":"DEBUG","ts":"2025-12-09T05:23:28.616Z","msg":"Updated tuner status for variant ms-inference-scheduling-llm-d-modelservice-decode: alpha=9.126283, beta=0.048510, gamma=17.277304, delta=0.000245, NIS=95.782668"}
{"level":"DEBUG","ts":"2025-12-09T05:23:28.626Z","msg":"Optimization solution - system: Solution: \ns=ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler; c=Premium; m=unsloth/Meta-Llama-3.1-8B; rate=1117.52; inTk=273; outTk=271; sol=1, sat=false, alloc={acc=H100; numRep=3; maxBatch=512; cost=300, val=200, itl=9.992946, ttft=18.472248, rho=0.03294074, maxRPM=375.4557}; slo-itl=10, slo-ttft=1000, slo-tps=0 \nAllocationByType: \nname=NVIDIA-H100-80GB-HBM3, count=3, limit=0, cost=300 \ntotalCost=300 \n"}
{"level":"DEBUG","ts":"2025-12-09T05:23:28.626Z","msg":"Setting accelerator name: Name=H100, allocationData={H100 3 512 300 9.992946 18.472248 {1117.52 273 271}}"}
{"level":"INFO","ts":"2025-12-09T05:23:28.626Z","msg":"Model-based optimization completed for model: unsloth/Meta-Llama-3.1-8B - model-based targets: map[ms-inference-scheduling-llm-d-modelservice-decode:3]"}
{"level":"WARN","ts":"2025-12-09T05:23:28.626Z","msg":"saturation analysis unavailable, using model-based targets only: modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-09T05:23:28.626Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-12-09T05:23:28.626Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=scale-up, current=1→target=3"}
{"level":"INFO","ts":"2025-12-09T05:23:28.626Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 1, desired-replicas: 3, accelerator: H100"}
{"level":"INFO","ts":"2025-12-09T05:23:28.626Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=3, accelerator=H100, SaturationOnly=false"}
{"level":"INFO","ts":"2025-12-09T05:23:28.631Z","msg":"Applied Saturation decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=scale-up, current=1, target=3, reason=model-based only (Saturation unavailable)"}
{"level":"INFO","ts":"2025-12-09T05:23:28.631Z","msg":"Reconciliation completed successfully: mode=model-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-12-09T05:24:28.632Z","msg":"Reconciling VariantAutoscaling: name=ms-inference-scheduling-llm-d-modelservice-decode, namespace=llm-d-inference-scheduler, modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-09T05:24:28.632Z","msg":"saturation scaling config cache updated: entries=1, has_default=true"}
{"level":"INFO","ts":"2025-12-09T05:24:28.632Z","msg":"saturation scaling configuration loaded successfully"}
{"level":"INFO","ts":"2025-12-09T05:24:28.632Z","msg":"Operating in MODEL-ONLY mode: model-based optimization only"}
{"level":"INFO","ts":"2025-12-09T05:24:28.632Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-12-09T05:24:28.632Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-12-09T05:24:28.640Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-666b5f4cq54t2, usage=0.428 (42.8%)"}
{"level":"DEBUG","ts":"2025-12-09T05:24:28.640Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"INFO","ts":"2025-12-09T05:24:28.641Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-666b5f4cq54t2, queueLength=0"}
{"level":"DEBUG","ts":"2025-12-09T05:24:28.641Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"DEBUG","ts":"2025-12-09T05:24:28.643Z","msg":"Pod-to-variant matching successful: totalPods=1, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"DEBUG","ts":"2025-12-09T05:24:28.643Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=1"}
{"level":"DEBUG","ts":"2025-12-09T05:24:28.643Z","msg":"Collected capacity metrics for observability in model-only mode: modelID=unsloth/Meta-Llama-3.1-8B, replicaCount=1"}
{"level":"DEBUG","ts":"2025-12-09T05:24:28.657Z","msg":"Replica count mismatch for llm-d-inference-scheduler/unsloth/Meta-Llama-3.1-8B: spec=3, reporting_metrics=1"}
{"level":"DEBUG","ts":"2025-12-09T05:24:28.657Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=1, accelerator=H100, ttft=36.00ms, itl=21.45ms, cost=100.00, maxBatch=256, arrivalRate=1075.80, avgInputTokens=205.57, avgOutputTokens=569.81"}
{"level":"DEBUG","ts":"2025-12-09T05:24:28.657Z","msg":"Experimental model tuner is enabled globally: tuning model performance parameters for active VAs"}
{"level":"DEBUG","ts":"2025-12-09T05:24:28.657Z","msg":"Using state vals from VA status to tune variant ms-inference-scheduling-llm-d-modelservice-decode: alpha= 9.126283, beta= 0.048510, gamma= 17.277304, delta= 0.000245"}
{"level":"DEBUG","ts":"2025-12-09T05:24:28.657Z","msg":"[Tuner Config] variant=ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler | Initial state: alpha=9.126283, beta=0.048510, gamma=17.277304, delta=0.000245 | SLO targets: TTFT=1000.00ms, ITL=10.00ms | Has covariance=true"}
{"level":"INFO","ts":"2025-12-09T05:24:28.657Z","msg":"Tuner validation succeeded for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler, server ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler - New NIS: 1.576127"}
{"level":"DEBUG","ts":"2025-12-09T05:24:28.657Z","msg":"Model tuner results: model=unsloth/Meta-Llama-3.1-8B, accelerator=H100, alpha=9.285566, beta=0.056436, gamma=17.483484, delta=0.000245, NIS=1.58"}
{"level":"DEBUG","ts":"2025-12-09T05:24:28.657Z","msg":"Updated tuner status for variant ms-inference-scheduling-llm-d-modelservice-decode: alpha=9.285566, beta=0.056436, gamma=17.483484, delta=0.000245, NIS=1.576127"}
{"level":"INFO","ts":"2025-12-09T05:24:28.657Z","msg":"Tuned performance parameters result: variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler - alpha: 9.285566, beta: 0.056436, gamma: 17.483484, delta: 0.000245"}
{"level":"DEBUG","ts":"2025-12-09T05:24:28.661Z","msg":"Optimization solution - system: Solution: \ns=ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler; c=Premium; m=unsloth/Meta-Llama-3.1-8B; rate=1075.8; inTk=205; outTk=569; sol=1, sat=false, alloc={acc=H100; numRep=9; maxBatch=512; cost=900, val=800, itl=9.98147, ttft=18.102957, rho=0.022130704, maxRPM=122.77091}; slo-itl=10, slo-ttft=1000, slo-tps=0 \nAllocationByType: \nname=NVIDIA-H100-80GB-HBM3, count=9, limit=0, cost=900 \ntotalCost=900 \n"}
{"level":"DEBUG","ts":"2025-12-09T05:24:28.661Z","msg":"Setting accelerator name: Name=H100, allocationData={H100 9 512 900 9.98147 18.102957 {1075.8 205 569}}"}
{"level":"INFO","ts":"2025-12-09T05:24:28.661Z","msg":"Model-based optimization completed for model: unsloth/Meta-Llama-3.1-8B - model-based targets: map[ms-inference-scheduling-llm-d-modelservice-decode:9]"}
{"level":"WARN","ts":"2025-12-09T05:24:28.661Z","msg":"saturation analysis unavailable, using model-based targets only: modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-09T05:24:28.661Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-12-09T05:24:28.661Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=scale-up, current=1→target=9"}
{"level":"INFO","ts":"2025-12-09T05:24:28.661Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 1, desired-replicas: 9, accelerator: H100"}
{"level":"INFO","ts":"2025-12-09T05:24:28.661Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=9, accelerator=H100, SaturationOnly=false"}
{"level":"INFO","ts":"2025-12-09T05:24:28.667Z","msg":"Applied Saturation decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=scale-up, current=1, target=9, reason=model-based only (Saturation unavailable)"}
{"level":"INFO","ts":"2025-12-09T05:24:28.667Z","msg":"Reconciliation completed successfully: mode=model-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-12-09T05:25:28.667Z","msg":"Reconciling VariantAutoscaling: name=ms-inference-scheduling-llm-d-modelservice-decode, namespace=llm-d-inference-scheduler, modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-09T05:25:28.667Z","msg":"saturation scaling config cache updated: entries=1, has_default=true"}
{"level":"INFO","ts":"2025-12-09T05:25:28.667Z","msg":"saturation scaling configuration loaded successfully"}
{"level":"INFO","ts":"2025-12-09T05:25:28.667Z","msg":"Operating in MODEL-ONLY mode: model-based optimization only"}
{"level":"INFO","ts":"2025-12-09T05:25:28.667Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-12-09T05:25:28.667Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-12-09T05:25:28.672Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-666b5f4cq54t2, queueLength=0"}
{"level":"DEBUG","ts":"2025-12-09T05:25:28.672Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"INFO","ts":"2025-12-09T05:25:28.672Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-666b5f4cq54t2, usage=0.056 (5.6%)"}
{"level":"DEBUG","ts":"2025-12-09T05:25:28.672Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"DEBUG","ts":"2025-12-09T05:25:28.675Z","msg":"Pod-to-variant matching successful: totalPods=1, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"DEBUG","ts":"2025-12-09T05:25:28.675Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=1"}
{"level":"DEBUG","ts":"2025-12-09T05:25:28.675Z","msg":"Collected capacity metrics for observability in model-only mode: modelID=unsloth/Meta-Llama-3.1-8B, replicaCount=1"}
{"level":"DEBUG","ts":"2025-12-09T05:25:28.690Z","msg":"Replica count mismatch for llm-d-inference-scheduler/unsloth/Meta-Llama-3.1-8B: spec=9, reporting_metrics=1"}
{"level":"DEBUG","ts":"2025-12-09T05:25:28.690Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=1, accelerator=H100, ttft=18.16ms, itl=9.25ms, cost=100.00, maxBatch=256, arrivalRate=320.26, avgInputTokens=280.02, avgOutputTokens=414.20"}
{"level":"DEBUG","ts":"2025-12-09T05:25:28.690Z","msg":"Experimental model tuner is enabled globally: tuning model performance parameters for active VAs"}
{"level":"DEBUG","ts":"2025-12-09T05:25:28.690Z","msg":"Using state vals from VA status to tune variant ms-inference-scheduling-llm-d-modelservice-decode: alpha= 9.285566, beta= 0.056436, gamma= 17.483484, delta= 0.000245"}
{"level":"DEBUG","ts":"2025-12-09T05:25:28.690Z","msg":"[Tuner Config] variant=ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler | Initial state: alpha=9.285566, beta=0.056436, gamma=17.483484, delta=0.000245 | SLO targets: TTFT=1000.00ms, ITL=10.00ms | Has covariance=true"}
{"level":"INFO","ts":"2025-12-09T05:25:28.691Z","msg":"Tuner validation succeeded for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler, server ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler - New NIS: 3.426015"}
{"level":"DEBUG","ts":"2025-12-09T05:25:28.691Z","msg":"Model tuner results: model=unsloth/Meta-Llama-3.1-8B, accelerator=H100, alpha=8.110758, beta=0.059000, gamma=17.443609, delta=0.000245, NIS=3.43"}
{"level":"DEBUG","ts":"2025-12-09T05:25:28.691Z","msg":"Updated tuner status for variant ms-inference-scheduling-llm-d-modelservice-decode: alpha=8.110758, beta=0.059000, gamma=17.443609, delta=0.000245, NIS=3.426015"}
{"level":"INFO","ts":"2025-12-09T05:25:28.691Z","msg":"Tuned performance parameters result: variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler - alpha: 8.110758, beta: 0.059000, gamma: 17.443609, delta: 0.000245"}
{"level":"DEBUG","ts":"2025-12-09T05:25:28.700Z","msg":"Optimization solution - system: Solution: \ns=ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler; c=Premium; m=unsloth/Meta-Llama-3.1-8B; rate=320.26; inTk=280; outTk=414; sol=1, sat=false, alloc={acc=H100; numRep=1; maxBatch=512; cost=100, val=0, itl=9.39805, ttft=18.940357, rho=0.040661506, maxRPM=448.539}; slo-itl=10, slo-ttft=1000, slo-tps=0 \nAllocationByType: \nname=NVIDIA-H100-80GB-HBM3, count=1, limit=0, cost=100 \ntotalCost=100 \n"}
{"level":"DEBUG","ts":"2025-12-09T05:25:28.700Z","msg":"Setting accelerator name: Name=H100, allocationData={H100 1 512 100 9.39805 18.940357 {320.26 280 414}}"}
{"level":"INFO","ts":"2025-12-09T05:25:28.700Z","msg":"Model-based optimization completed for model: unsloth/Meta-Llama-3.1-8B - model-based targets: map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"WARN","ts":"2025-12-09T05:25:28.700Z","msg":"saturation analysis unavailable, using model-based targets only: modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-09T05:25:28.700Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-12-09T05:25:28.700Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1→target=1"}
{"level":"INFO","ts":"2025-12-09T05:25:28.700Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 1, desired-replicas: 1, accelerator: H100"}
{"level":"INFO","ts":"2025-12-09T05:25:28.700Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=1, accelerator=H100, SaturationOnly=false"}
{"level":"INFO","ts":"2025-12-09T05:25:28.709Z","msg":"Applied Saturation decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1, target=1, reason=model-based only (Saturation unavailable)"}
{"level":"INFO","ts":"2025-12-09T05:25:28.709Z","msg":"Reconciliation completed successfully: mode=model-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-12-09T05:26:28.710Z","msg":"Reconciling VariantAutoscaling: name=ms-inference-scheduling-llm-d-modelservice-decode, namespace=llm-d-inference-scheduler, modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-09T05:26:28.710Z","msg":"saturation scaling config cache updated: entries=1, has_default=true"}
{"level":"INFO","ts":"2025-12-09T05:26:28.710Z","msg":"saturation scaling configuration loaded successfully"}
{"level":"INFO","ts":"2025-12-09T05:26:28.710Z","msg":"Operating in MODEL-ONLY mode: model-based optimization only"}
{"level":"INFO","ts":"2025-12-09T05:26:28.710Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-12-09T05:26:28.710Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-12-09T05:26:28.718Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-666b5f4cq54t2, usage=0.132 (13.2%)"}
{"level":"DEBUG","ts":"2025-12-09T05:26:28.718Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"INFO","ts":"2025-12-09T05:26:28.718Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-666b5f4cq54t2, queueLength=0"}
{"level":"DEBUG","ts":"2025-12-09T05:26:28.718Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"DEBUG","ts":"2025-12-09T05:26:28.720Z","msg":"Pod-to-variant matching successful: totalPods=1, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"DEBUG","ts":"2025-12-09T05:26:28.720Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=1"}
{"level":"DEBUG","ts":"2025-12-09T05:26:28.720Z","msg":"Collected capacity metrics for observability in model-only mode: modelID=unsloth/Meta-Llama-3.1-8B, replicaCount=1"}
{"level":"DEBUG","ts":"2025-12-09T05:26:28.733Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=1, accelerator=H100, ttft=21.54ms, itl=11.51ms, cost=100.00, maxBatch=256, arrivalRate=647.28, avgInputTokens=255.24, avgOutputTokens=396.62"}
{"level":"DEBUG","ts":"2025-12-09T05:26:28.733Z","msg":"Experimental model tuner is enabled globally: tuning model performance parameters for active VAs"}
{"level":"DEBUG","ts":"2025-12-09T05:26:28.733Z","msg":"Using state vals from VA status to tune variant ms-inference-scheduling-llm-d-modelservice-decode: alpha= 8.110758, beta= 0.059000, gamma= 17.443609, delta= 0.000245"}
{"level":"DEBUG","ts":"2025-12-09T05:26:28.733Z","msg":"[Tuner Config] variant=ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler | Initial state: alpha=8.110758, beta=0.059000, gamma=17.443609, delta=0.000245 | SLO targets: TTFT=1000.00ms, ITL=10.00ms | Has covariance=true"}
{"level":"INFO","ts":"2025-12-09T05:26:28.734Z","msg":"Tuner validation succeeded for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler, server ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler - New NIS: 0.703859"}
{"level":"DEBUG","ts":"2025-12-09T05:26:28.734Z","msg":"Model tuner results: model=unsloth/Meta-Llama-3.1-8B, accelerator=H100, alpha=8.437506, beta=0.059994, gamma=17.484880, delta=0.000245, NIS=0.70"}
{"level":"DEBUG","ts":"2025-12-09T05:26:28.734Z","msg":"Updated tuner status for variant ms-inference-scheduling-llm-d-modelservice-decode: alpha=8.437506, beta=0.059994, gamma=17.484880, delta=0.000245, NIS=0.703859"}
{"level":"INFO","ts":"2025-12-09T05:26:28.734Z","msg":"Tuned performance parameters result: variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler - alpha: 8.437506, beta: 0.059994, gamma: 17.484880, delta: 0.000245"}
{"level":"DEBUG","ts":"2025-12-09T05:26:28.743Z","msg":"Optimization solution - system: Solution: \ns=ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler; c=Premium; m=unsloth/Meta-Llama-3.1-8B; rate=647.28; inTk=255; outTk=396; sol=1, sat=false, alloc={acc=H100; numRep=2; maxBatch=512; cost=200, val=100, itl=9.749881, ttft=18.851551, rho=0.040771626, maxRPM=378.58517}; slo-itl=10, slo-ttft=1000, slo-tps=0 \nAllocationByType: \nname=NVIDIA-H100-80GB-HBM3, count=2, limit=0, cost=200 \ntotalCost=200 \n"}
{"level":"DEBUG","ts":"2025-12-09T05:26:28.743Z","msg":"Setting accelerator name: Name=H100, allocationData={H100 2 512 200 9.749881 18.851551 {647.28 255 396}}"}
{"level":"INFO","ts":"2025-12-09T05:26:28.743Z","msg":"Model-based optimization completed for model: unsloth/Meta-Llama-3.1-8B - model-based targets: map[ms-inference-scheduling-llm-d-modelservice-decode:2]"}
{"level":"WARN","ts":"2025-12-09T05:26:28.743Z","msg":"saturation analysis unavailable, using model-based targets only: modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-09T05:26:28.743Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-12-09T05:26:28.743Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=scale-up, current=1→target=2"}
{"level":"INFO","ts":"2025-12-09T05:26:28.743Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 1, desired-replicas: 2, accelerator: H100"}
{"level":"INFO","ts":"2025-12-09T05:26:28.743Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=2, accelerator=H100, SaturationOnly=false"}
{"level":"INFO","ts":"2025-12-09T05:26:28.749Z","msg":"Applied Saturation decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=scale-up, current=1, target=2, reason=model-based only (Saturation unavailable)"}
{"level":"INFO","ts":"2025-12-09T05:26:28.749Z","msg":"Reconciliation completed successfully: mode=model-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-12-09T05:27:28.750Z","msg":"Reconciling VariantAutoscaling: name=ms-inference-scheduling-llm-d-modelservice-decode, namespace=llm-d-inference-scheduler, modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-09T05:27:28.750Z","msg":"saturation scaling config cache updated: entries=1, has_default=true"}
{"level":"INFO","ts":"2025-12-09T05:27:28.750Z","msg":"saturation scaling configuration loaded successfully"}
{"level":"INFO","ts":"2025-12-09T05:27:28.750Z","msg":"Operating in MODEL-ONLY mode: model-based optimization only"}
{"level":"INFO","ts":"2025-12-09T05:27:28.750Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-12-09T05:27:28.750Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-12-09T05:27:28.756Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-666b5f4cq54t2, queueLength=0"}
{"level":"DEBUG","ts":"2025-12-09T05:27:28.756Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"INFO","ts":"2025-12-09T05:27:28.756Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-666b5f4cq54t2, usage=0.120 (12.0%)"}
{"level":"DEBUG","ts":"2025-12-09T05:27:28.756Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"DEBUG","ts":"2025-12-09T05:27:28.760Z","msg":"Pod-to-variant matching successful: totalPods=1, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"DEBUG","ts":"2025-12-09T05:27:28.760Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=1"}
{"level":"DEBUG","ts":"2025-12-09T05:27:28.760Z","msg":"Collected capacity metrics for observability in model-only mode: modelID=unsloth/Meta-Llama-3.1-8B, replicaCount=1"}
{"level":"DEBUG","ts":"2025-12-09T05:27:28.772Z","msg":"Replica count mismatch for llm-d-inference-scheduler/unsloth/Meta-Llama-3.1-8B: spec=2, reporting_metrics=1"}
{"level":"DEBUG","ts":"2025-12-09T05:27:28.772Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=1, accelerator=H100, ttft=22.85ms, itl=11.87ms, cost=100.00, maxBatch=256, arrivalRate=842.37, avgInputTokens=228.16, avgOutputTokens=446.76"}
{"level":"DEBUG","ts":"2025-12-09T05:27:28.772Z","msg":"Experimental model tuner is enabled globally: tuning model performance parameters for active VAs"}
{"level":"DEBUG","ts":"2025-12-09T05:27:28.772Z","msg":"Using state vals from VA status to tune variant ms-inference-scheduling-llm-d-modelservice-decode: alpha= 8.437506, beta= 0.059994, gamma= 17.484880, delta= 0.000245"}
{"level":"DEBUG","ts":"2025-12-09T05:27:28.772Z","msg":"[Tuner Config] variant=ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler | Initial state: alpha=8.437506, beta=0.059994, gamma=17.484880, delta=0.000245 | SLO targets: TTFT=1000.00ms, ITL=10.00ms | Has covariance=true"}
{"level":"INFO","ts":"2025-12-09T05:27:28.774Z","msg":"Tuner validation succeeded for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler, server ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler - New NIS: 3.562288"}
{"level":"DEBUG","ts":"2025-12-09T05:27:28.774Z","msg":"Model tuner results: model=unsloth/Meta-Llama-3.1-8B, accelerator=H100, alpha=7.880561, beta=0.054726, gamma=17.517153, delta=0.000245, NIS=3.56"}
{"level":"DEBUG","ts":"2025-12-09T05:27:28.774Z","msg":"Updated tuner status for variant ms-inference-scheduling-llm-d-modelservice-decode: alpha=7.880561, beta=0.054726, gamma=17.517153, delta=0.000245, NIS=3.562288"}
{"level":"INFO","ts":"2025-12-09T05:27:28.774Z","msg":"Tuned performance parameters result: variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler - alpha: 7.880561, beta: 0.054726, gamma: 17.517153, delta: 0.000245"}
{"level":"DEBUG","ts":"2025-12-09T05:27:28.783Z","msg":"Optimization solution - system: Solution: \ns=ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler; c=Premium; m=unsloth/Meta-Llama-3.1-8B; rate=842.37; inTk=228; outTk=446; sol=1, sat=false, alloc={acc=H100; numRep=2; maxBatch=512; cost=200, val=100, itl=9.580485, ttft=19.252285, rho=0.058715936, maxRPM=506.456}; slo-itl=10, slo-ttft=1000, slo-tps=0 \nAllocationByType: \nname=NVIDIA-H100-80GB-HBM3, count=2, limit=0, cost=200 \ntotalCost=200 \n"}
{"level":"DEBUG","ts":"2025-12-09T05:27:28.783Z","msg":"Setting accelerator name: Name=H100, allocationData={H100 2 512 200 9.580485 19.252285 {842.37 228 446}}"}
{"level":"INFO","ts":"2025-12-09T05:27:28.783Z","msg":"Model-based optimization completed for model: unsloth/Meta-Llama-3.1-8B - model-based targets: map[ms-inference-scheduling-llm-d-modelservice-decode:2]"}
{"level":"WARN","ts":"2025-12-09T05:27:28.783Z","msg":"saturation analysis unavailable, using model-based targets only: modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-09T05:27:28.783Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-12-09T05:27:28.783Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=scale-up, current=1→target=2"}
{"level":"INFO","ts":"2025-12-09T05:27:28.783Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 1, desired-replicas: 2, accelerator: H100"}
{"level":"INFO","ts":"2025-12-09T05:27:28.783Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=2, accelerator=H100, SaturationOnly=false"}
{"level":"INFO","ts":"2025-12-09T05:27:28.789Z","msg":"Applied Saturation decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=scale-up, current=1, target=2, reason=model-based only (Saturation unavailable)"}
{"level":"INFO","ts":"2025-12-09T05:27:28.789Z","msg":"Reconciliation completed successfully: mode=model-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-12-09T05:28:28.789Z","msg":"Reconciling VariantAutoscaling: name=ms-inference-scheduling-llm-d-modelservice-decode, namespace=llm-d-inference-scheduler, modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-09T05:28:28.789Z","msg":"saturation scaling config cache updated: entries=1, has_default=true"}
{"level":"INFO","ts":"2025-12-09T05:28:28.789Z","msg":"saturation scaling configuration loaded successfully"}
{"level":"INFO","ts":"2025-12-09T05:28:28.789Z","msg":"Operating in MODEL-ONLY mode: model-based optimization only"}
{"level":"INFO","ts":"2025-12-09T05:28:28.789Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-12-09T05:28:28.789Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-12-09T05:28:28.798Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-666b5f4cq54t2, usage=0.080 (8.0%)"}
{"level":"INFO","ts":"2025-12-09T05:28:28.798Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-666b5f4ccjt24, usage=0.012 (1.2%)"}
{"level":"DEBUG","ts":"2025-12-09T05:28:28.798Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=2"}
{"level":"INFO","ts":"2025-12-09T05:28:28.798Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-666b5f4cq54t2, queueLength=0"}
{"level":"INFO","ts":"2025-12-09T05:28:28.798Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-666b5f4ccjt24, queueLength=0"}
{"level":"DEBUG","ts":"2025-12-09T05:28:28.798Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=2"}
{"level":"DEBUG","ts":"2025-12-09T05:28:28.801Z","msg":"Pod-to-variant matching successful: totalPods=2, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:2]"}
{"level":"DEBUG","ts":"2025-12-09T05:28:28.801Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=2"}
{"level":"DEBUG","ts":"2025-12-09T05:28:28.801Z","msg":"Collected capacity metrics for observability in model-only mode: modelID=unsloth/Meta-Llama-3.1-8B, replicaCount=2"}
{"level":"DEBUG","ts":"2025-12-09T05:28:28.813Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=2, accelerator=H100, ttft=20.05ms, itl=10.51ms, cost=200.00, maxBatch=256, arrivalRate=631.50, avgInputTokens=222.96, avgOutputTokens=448.65"}
{"level":"DEBUG","ts":"2025-12-09T05:28:28.813Z","msg":"Experimental model tuner is enabled globally: tuning model performance parameters for active VAs"}
{"level":"DEBUG","ts":"2025-12-09T05:28:28.813Z","msg":"Using state vals from VA status to tune variant ms-inference-scheduling-llm-d-modelservice-decode: alpha= 7.880561, beta= 0.054726, gamma= 17.517153, delta= 0.000245"}
{"level":"DEBUG","ts":"2025-12-09T05:28:28.813Z","msg":"[Tuner Config] variant=ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler | Initial state: alpha=7.880561, beta=0.054726, gamma=17.517153, delta=0.000245 | SLO targets: TTFT=1000.00ms, ITL=10.00ms | Has covariance=true"}
{"level":"INFO","ts":"2025-12-09T05:28:28.814Z","msg":"Tuner validation succeeded for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler, server ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler - New NIS: 4.888104"}
{"level":"DEBUG","ts":"2025-12-09T05:28:28.814Z","msg":"Model tuner results: model=unsloth/Meta-Llama-3.1-8B, accelerator=H100, alpha=8.995097, beta=0.050435, gamma=17.536240, delta=0.000245, NIS=4.89"}
{"level":"DEBUG","ts":"2025-12-09T05:28:28.814Z","msg":"Updated tuner status for variant ms-inference-scheduling-llm-d-modelservice-decode: alpha=8.995097, beta=0.050435, gamma=17.536240, delta=0.000245, NIS=4.888104"}
{"level":"INFO","ts":"2025-12-09T05:28:28.814Z","msg":"Tuned performance parameters result: variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler - alpha: 8.995097, beta: 0.050435, gamma: 17.536240, delta: 0.000245"}
{"level":"DEBUG","ts":"2025-12-09T05:28:28.823Z","msg":"Optimization solution - system: Solution: \ns=ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler; c=Premium; m=unsloth/Meta-Llama-3.1-8B; rate=631.5; inTk=222; outTk=448; sol=1, sat=false, alloc={acc=H100; numRep=3; maxBatch=512; cost=300, val=100, itl=9.825969, ttft=18.432268, rho=0.030222645, maxRPM=252.96747}; slo-itl=10, slo-ttft=1000, slo-tps=0 \nAllocationByType: \nname=NVIDIA-H100-80GB-HBM3, count=3, limit=0, cost=300 \ntotalCost=300 \n"}
{"level":"DEBUG","ts":"2025-12-09T05:28:28.823Z","msg":"Setting accelerator name: Name=H100, allocationData={H100 3 512 300 9.825969 18.432268 {631.5 222 448}}"}
{"level":"INFO","ts":"2025-12-09T05:28:28.823Z","msg":"Model-based optimization completed for model: unsloth/Meta-Llama-3.1-8B - model-based targets: map[ms-inference-scheduling-llm-d-modelservice-decode:3]"}
{"level":"WARN","ts":"2025-12-09T05:28:28.823Z","msg":"saturation analysis unavailable, using model-based targets only: modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-09T05:28:28.823Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-12-09T05:28:28.823Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=scale-up, current=2→target=3"}
{"level":"INFO","ts":"2025-12-09T05:28:28.823Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 2, desired-replicas: 3, accelerator: H100"}
{"level":"INFO","ts":"2025-12-09T05:28:28.823Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=3, accelerator=H100, SaturationOnly=false"}
{"level":"INFO","ts":"2025-12-09T05:28:28.829Z","msg":"Applied Saturation decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=scale-up, current=2, target=3, reason=model-based only (Saturation unavailable)"}
{"level":"INFO","ts":"2025-12-09T05:28:28.829Z","msg":"Reconciliation completed successfully: mode=model-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-12-09T05:29:28.829Z","msg":"Reconciling VariantAutoscaling: name=ms-inference-scheduling-llm-d-modelservice-decode, namespace=llm-d-inference-scheduler, modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-09T05:29:28.830Z","msg":"saturation scaling config cache updated: entries=1, has_default=true"}
{"level":"INFO","ts":"2025-12-09T05:29:28.830Z","msg":"saturation scaling configuration loaded successfully"}
{"level":"INFO","ts":"2025-12-09T05:29:28.830Z","msg":"Operating in MODEL-ONLY mode: model-based optimization only"}
{"level":"INFO","ts":"2025-12-09T05:29:28.830Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-12-09T05:29:28.830Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-12-09T05:29:28.838Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-666b5f4cq54t2, usage=0.059 (5.9%)"}
{"level":"INFO","ts":"2025-12-09T05:29:28.838Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-666b5f4ccjt24, usage=0.051 (5.1%)"}
{"level":"DEBUG","ts":"2025-12-09T05:29:28.838Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=2"}
{"level":"INFO","ts":"2025-12-09T05:29:28.838Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-666b5f4cq54t2, queueLength=0"}
{"level":"INFO","ts":"2025-12-09T05:29:28.838Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-666b5f4ccjt24, queueLength=0"}
{"level":"DEBUG","ts":"2025-12-09T05:29:28.838Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=2"}
{"level":"DEBUG","ts":"2025-12-09T05:29:28.841Z","msg":"Pod-to-variant matching successful: totalPods=2, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:2]"}
{"level":"DEBUG","ts":"2025-12-09T05:29:28.841Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=2"}
{"level":"DEBUG","ts":"2025-12-09T05:29:28.841Z","msg":"Collected capacity metrics for observability in model-only mode: modelID=unsloth/Meta-Llama-3.1-8B, replicaCount=2"}
{"level":"DEBUG","ts":"2025-12-09T05:29:28.853Z","msg":"Replica count mismatch for llm-d-inference-scheduler/unsloth/Meta-Llama-3.1-8B: spec=3, reporting_metrics=2"}
{"level":"DEBUG","ts":"2025-12-09T05:29:28.853Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=2, accelerator=H100, ttft=18.30ms, itl=9.47ms, cost=200.00, maxBatch=256, arrivalRate=715.84, avgInputTokens=227.25, avgOutputTokens=457.60"}
{"level":"DEBUG","ts":"2025-12-09T05:29:28.853Z","msg":"Experimental model tuner is enabled globally: tuning model performance parameters for active VAs"}
{"level":"DEBUG","ts":"2025-12-09T05:29:28.853Z","msg":"Using state vals from VA status to tune variant ms-inference-scheduling-llm-d-modelservice-decode: alpha= 8.995097, beta= 0.050435, gamma= 17.536240, delta= 0.000245"}
{"level":"DEBUG","ts":"2025-12-09T05:29:28.853Z","msg":"[Tuner Config] variant=ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler | Initial state: alpha=8.995097, beta=0.050435, gamma=17.536240, delta=0.000245 | SLO targets: TTFT=1000.00ms, ITL=10.00ms | Has covariance=true"}
{"level":"INFO","ts":"2025-12-09T05:29:28.854Z","msg":"Tuner validation succeeded for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler, server ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler - New NIS: 2.613838"}
{"level":"DEBUG","ts":"2025-12-09T05:29:28.854Z","msg":"Model tuner results: model=unsloth/Meta-Llama-3.1-8B, accelerator=H100, alpha=8.275867, beta=0.049845, gamma=17.514790, delta=0.000245, NIS=2.61"}
{"level":"DEBUG","ts":"2025-12-09T05:29:28.854Z","msg":"Updated tuner status for variant ms-inference-scheduling-llm-d-modelservice-decode: alpha=8.275867, beta=0.049845, gamma=17.514790, delta=0.000245, NIS=2.613838"}
{"level":"INFO","ts":"2025-12-09T05:29:28.854Z","msg":"Tuned performance parameters result: variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler - alpha: 8.275867, beta: 0.049845, gamma: 17.514790, delta: 0.000245"}
{"level":"DEBUG","ts":"2025-12-09T05:29:28.862Z","msg":"Optimization solution - system: Solution: \ns=ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler; c=Premium; m=unsloth/Meta-Llama-3.1-8B; rate=715.84; inTk=227; outTk=457; sol=1, sat=false, alloc={acc=H100; numRep=2; maxBatch=512; cost=200, val=0, itl=9.638201, ttft=19.034807, rho=0.051428337, maxRPM=440.09634}; slo-itl=10, slo-ttft=1000, slo-tps=0 \nAllocationByType: \nname=NVIDIA-H100-80GB-HBM3, count=2, limit=0, cost=200 \ntotalCost=200 \n"}
{"level":"DEBUG","ts":"2025-12-09T05:29:28.862Z","msg":"Setting accelerator name: Name=H100, allocationData={H100 2 512 200 9.638201 19.034807 {715.84 227 457}}"}
{"level":"INFO","ts":"2025-12-09T05:29:28.862Z","msg":"Model-based optimization completed for model: unsloth/Meta-Llama-3.1-8B - model-based targets: map[ms-inference-scheduling-llm-d-modelservice-decode:2]"}
{"level":"WARN","ts":"2025-12-09T05:29:28.862Z","msg":"saturation analysis unavailable, using model-based targets only: modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-09T05:29:28.862Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-12-09T05:29:28.862Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=2→target=2"}
{"level":"INFO","ts":"2025-12-09T05:29:28.862Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 2, desired-replicas: 2, accelerator: H100"}
{"level":"INFO","ts":"2025-12-09T05:29:28.862Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=2, accelerator=H100, SaturationOnly=false"}
{"level":"INFO","ts":"2025-12-09T05:29:28.870Z","msg":"Applied Saturation decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=2, target=2, reason=model-based only (Saturation unavailable)"}
{"level":"INFO","ts":"2025-12-09T05:29:28.870Z","msg":"Reconciliation completed successfully: mode=model-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-12-09T05:30:28.870Z","msg":"Reconciling VariantAutoscaling: name=ms-inference-scheduling-llm-d-modelservice-decode, namespace=llm-d-inference-scheduler, modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-09T05:30:28.870Z","msg":"saturation scaling config cache updated: entries=1, has_default=true"}
{"level":"INFO","ts":"2025-12-09T05:30:28.870Z","msg":"saturation scaling configuration loaded successfully"}
{"level":"INFO","ts":"2025-12-09T05:30:28.870Z","msg":"Operating in MODEL-ONLY mode: model-based optimization only"}
{"level":"INFO","ts":"2025-12-09T05:30:28.870Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-12-09T05:30:28.870Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-12-09T05:30:28.878Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-666b5f4cq54t2, queueLength=0"}
{"level":"INFO","ts":"2025-12-09T05:30:28.878Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-666b5f4ccjt24, queueLength=0"}
{"level":"DEBUG","ts":"2025-12-09T05:30:28.878Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=2"}
{"level":"INFO","ts":"2025-12-09T05:30:28.879Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-666b5f4cq54t2, usage=0.047 (4.7%)"}
{"level":"INFO","ts":"2025-12-09T05:30:28.879Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-666b5f4ccjt24, usage=0.034 (3.4%)"}
{"level":"DEBUG","ts":"2025-12-09T05:30:28.879Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=2"}
{"level":"DEBUG","ts":"2025-12-09T05:30:28.882Z","msg":"Pod-to-variant matching successful: totalPods=2, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:2]"}
{"level":"DEBUG","ts":"2025-12-09T05:30:28.882Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=2"}
{"level":"DEBUG","ts":"2025-12-09T05:30:28.882Z","msg":"Collected capacity metrics for observability in model-only mode: modelID=unsloth/Meta-Llama-3.1-8B, replicaCount=2"}
{"level":"DEBUG","ts":"2025-12-09T05:30:28.906Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=2, accelerator=H100, ttft=18.24ms, itl=9.65ms, cost=200.00, maxBatch=256, arrivalRate=828.66, avgInputTokens=235.55, avgOutputTokens=429.66"}
{"level":"DEBUG","ts":"2025-12-09T05:30:28.906Z","msg":"Experimental model tuner is enabled globally: tuning model performance parameters for active VAs"}
{"level":"DEBUG","ts":"2025-12-09T05:30:28.906Z","msg":"Using state vals from VA status to tune variant ms-inference-scheduling-llm-d-modelservice-decode: alpha= 8.275867, beta= 0.049845, gamma= 17.514790, delta= 0.000245"}
{"level":"DEBUG","ts":"2025-12-09T05:30:28.906Z","msg":"[Tuner Config] variant=ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler | Initial state: alpha=8.275867, beta=0.049845, gamma=17.514790, delta=0.000245 | SLO targets: TTFT=1000.00ms, ITL=10.00ms | Has covariance=true"}
{"level":"INFO","ts":"2025-12-09T05:30:28.907Z","msg":"Tuner validation succeeded for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler, server ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler - New NIS: 0.042305"}
{"level":"DEBUG","ts":"2025-12-09T05:30:28.907Z","msg":"Model tuner results: model=unsloth/Meta-Llama-3.1-8B, accelerator=H100, alpha=8.193134, beta=0.049757, gamma=17.481758, delta=0.000245, NIS=0.04"}
{"level":"DEBUG","ts":"2025-12-09T05:30:28.907Z","msg":"Updated tuner status for variant ms-inference-scheduling-llm-d-modelservice-decode: alpha=8.193134, beta=0.049757, gamma=17.481758, delta=0.000245, NIS=0.042305"}
{"level":"INFO","ts":"2025-12-09T05:30:28.907Z","msg":"Tuned performance parameters result: variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler - alpha: 8.193134, beta: 0.049757, gamma: 17.481758, delta: 0.000245"}
{"level":"DEBUG","ts":"2025-12-09T05:30:28.916Z","msg":"Optimization solution - system: Solution: \ns=ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler; c=Premium; m=unsloth/Meta-Llama-3.1-8B; rate=828.66; inTk=235; outTk=429; sol=1, sat=false, alloc={acc=H100; numRep=2; maxBatch=512; cost=200, val=0, itl=9.671825, ttft=19.192768, rho=0.056090113, maxRPM=492.79907}; slo-itl=10, slo-ttft=1000, slo-tps=0 \nAllocationByType: \nname=NVIDIA-H100-80GB-HBM3, count=2, limit=0, cost=200 \ntotalCost=200 \n"}
{"level":"DEBUG","ts":"2025-12-09T05:30:28.916Z","msg":"Setting accelerator name: Name=H100, allocationData={H100 2 512 200 9.671825 19.192768 {828.66 235 429}}"}
{"level":"INFO","ts":"2025-12-09T05:30:28.916Z","msg":"Model-based optimization completed for model: unsloth/Meta-Llama-3.1-8B - model-based targets: map[ms-inference-scheduling-llm-d-modelservice-decode:2]"}
{"level":"WARN","ts":"2025-12-09T05:30:28.916Z","msg":"saturation analysis unavailable, using model-based targets only: modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-09T05:30:28.916Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-12-09T05:30:28.916Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=2→target=2"}
{"level":"INFO","ts":"2025-12-09T05:30:28.916Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 2, desired-replicas: 2, accelerator: H100"}
{"level":"INFO","ts":"2025-12-09T05:30:28.916Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=2, accelerator=H100, SaturationOnly=false"}
{"level":"INFO","ts":"2025-12-09T05:30:28.922Z","msg":"Applied Saturation decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=2, target=2, reason=model-based only (Saturation unavailable)"}
{"level":"INFO","ts":"2025-12-09T05:30:28.922Z","msg":"Reconciliation completed successfully: mode=model-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-12-09T05:31:28.923Z","msg":"Reconciling VariantAutoscaling: name=ms-inference-scheduling-llm-d-modelservice-decode, namespace=llm-d-inference-scheduler, modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-09T05:31:28.924Z","msg":"saturation scaling config cache updated: entries=1, has_default=true"}
{"level":"INFO","ts":"2025-12-09T05:31:28.924Z","msg":"saturation scaling configuration loaded successfully"}
{"level":"INFO","ts":"2025-12-09T05:31:28.924Z","msg":"Operating in MODEL-ONLY mode: model-based optimization only"}
{"level":"INFO","ts":"2025-12-09T05:31:28.924Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-12-09T05:31:28.924Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-12-09T05:31:28.927Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-666b5f4cq54t2, queueLength=0"}
{"level":"INFO","ts":"2025-12-09T05:31:28.927Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-666b5f4ccjt24, queueLength=0"}
{"level":"DEBUG","ts":"2025-12-09T05:31:28.927Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=2"}
{"level":"INFO","ts":"2025-12-09T05:31:28.927Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-666b5f4cq54t2, usage=0.071 (7.1%)"}
{"level":"INFO","ts":"2025-12-09T05:31:28.927Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-666b5f4ccjt24, usage=0.044 (4.4%)"}
{"level":"DEBUG","ts":"2025-12-09T05:31:28.927Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=2"}
{"level":"DEBUG","ts":"2025-12-09T05:31:28.931Z","msg":"Pod-to-variant matching successful: totalPods=2, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:2]"}
{"level":"DEBUG","ts":"2025-12-09T05:31:28.931Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=2"}
{"level":"DEBUG","ts":"2025-12-09T05:31:28.931Z","msg":"Collected capacity metrics for observability in model-only mode: modelID=unsloth/Meta-Llama-3.1-8B, replicaCount=2"}
{"level":"DEBUG","ts":"2025-12-09T05:31:28.944Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=2, accelerator=H100, ttft=18.06ms, itl=9.74ms, cost=200.00, maxBatch=256, arrivalRate=869.62, avgInputTokens=246.45, avgOutputTokens=437.72"}
{"level":"DEBUG","ts":"2025-12-09T05:31:28.944Z","msg":"Experimental model tuner is enabled globally: tuning model performance parameters for active VAs"}
{"level":"DEBUG","ts":"2025-12-09T05:31:28.944Z","msg":"Using state vals from VA status to tune variant ms-inference-scheduling-llm-d-modelservice-decode: alpha= 8.193134, beta= 0.049757, gamma= 17.481758, delta= 0.000245"}
{"level":"DEBUG","ts":"2025-12-09T05:31:28.944Z","msg":"[Tuner Config] variant=ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler | Initial state: alpha=8.193134, beta=0.049757, gamma=17.481758, delta=0.000245 | SLO targets: TTFT=1000.00ms, ITL=10.00ms | Has covariance=true"}
{"level":"INFO","ts":"2025-12-09T05:31:28.945Z","msg":"Tuner validation succeeded for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler, server ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler - New NIS: 0.009217"}
{"level":"DEBUG","ts":"2025-12-09T05:31:28.945Z","msg":"Model tuner results: model=unsloth/Meta-Llama-3.1-8B, accelerator=H100, alpha=8.160904, beta=0.049707, gamma=17.436871, delta=0.000245, NIS=0.01"}
{"level":"DEBUG","ts":"2025-12-09T05:31:28.945Z","msg":"Updated tuner status for variant ms-inference-scheduling-llm-d-modelservice-decode: alpha=8.160904, beta=0.049707, gamma=17.436871, delta=0.000245, NIS=0.009217"}
{"level":"INFO","ts":"2025-12-09T05:31:28.945Z","msg":"Tuned performance parameters result: variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler - alpha: 8.160904, beta: 0.049707, gamma: 17.436871, delta: 0.000245"}
{"level":"DEBUG","ts":"2025-12-09T05:31:28.949Z","msg":"Optimization solution - system: Solution: \ns=ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler; c=Premium; m=unsloth/Meta-Llama-3.1-8B; rate=869.62; inTk=246; outTk=437; sol=1, sat=false, alloc={acc=H100; numRep=2; maxBatch=512; cost=200, val=0, itl=9.74868, ttft=19.362022, rho=0.06043444, maxRPM=493.16516}; slo-itl=10, slo-ttft=1000, slo-tps=0 \nAllocationByType: \nname=NVIDIA-H100-80GB-HBM3, count=2, limit=0, cost=200 \ntotalCost=200 \n"}
{"level":"DEBUG","ts":"2025-12-09T05:31:28.949Z","msg":"Setting accelerator name: Name=H100, allocationData={H100 2 512 200 9.74868 19.362022 {869.62 246 437}}"}
{"level":"INFO","ts":"2025-12-09T05:31:28.949Z","msg":"Model-based optimization completed for model: unsloth/Meta-Llama-3.1-8B - model-based targets: map[ms-inference-scheduling-llm-d-modelservice-decode:2]"}
{"level":"WARN","ts":"2025-12-09T05:31:28.949Z","msg":"saturation analysis unavailable, using model-based targets only: modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-09T05:31:28.949Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-12-09T05:31:28.949Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=2→target=2"}
{"level":"INFO","ts":"2025-12-09T05:31:28.949Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 2, desired-replicas: 2, accelerator: H100"}
{"level":"INFO","ts":"2025-12-09T05:31:28.949Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=2, accelerator=H100, SaturationOnly=false"}
{"level":"INFO","ts":"2025-12-09T05:31:28.955Z","msg":"Applied Saturation decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=2, target=2, reason=model-based only (Saturation unavailable)"}
{"level":"INFO","ts":"2025-12-09T05:31:28.955Z","msg":"Reconciliation completed successfully: mode=model-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-12-09T05:32:28.956Z","msg":"Reconciling VariantAutoscaling: name=ms-inference-scheduling-llm-d-modelservice-decode, namespace=llm-d-inference-scheduler, modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-09T05:32:28.956Z","msg":"saturation scaling config cache updated: entries=1, has_default=true"}
{"level":"INFO","ts":"2025-12-09T05:32:28.956Z","msg":"saturation scaling configuration loaded successfully"}
{"level":"INFO","ts":"2025-12-09T05:32:28.956Z","msg":"Operating in MODEL-ONLY mode: model-based optimization only"}
{"level":"INFO","ts":"2025-12-09T05:32:28.956Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-12-09T05:32:28.956Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-12-09T05:32:28.964Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-666b5f4cq54t2, queueLength=0"}
{"level":"INFO","ts":"2025-12-09T05:32:28.964Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-666b5f4ccjt24, queueLength=0"}
{"level":"DEBUG","ts":"2025-12-09T05:32:28.964Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=2"}
{"level":"INFO","ts":"2025-12-09T05:32:28.965Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-666b5f4cq54t2, usage=0.050 (5.0%)"}
{"level":"INFO","ts":"2025-12-09T05:32:28.965Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-666b5f4ccjt24, usage=0.040 (4.0%)"}
{"level":"DEBUG","ts":"2025-12-09T05:32:28.965Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=2"}
{"level":"DEBUG","ts":"2025-12-09T05:32:28.967Z","msg":"Pod-to-variant matching successful: totalPods=2, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:2]"}
{"level":"DEBUG","ts":"2025-12-09T05:32:28.967Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=2"}
{"level":"DEBUG","ts":"2025-12-09T05:32:28.967Z","msg":"Collected capacity metrics for observability in model-only mode: modelID=unsloth/Meta-Llama-3.1-8B, replicaCount=2"}
{"level":"DEBUG","ts":"2025-12-09T05:32:28.982Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=2, accelerator=H100, ttft=17.53ms, itl=9.45ms, cost=200.00, maxBatch=256, arrivalRate=790.51, avgInputTokens=248.55, avgOutputTokens=448.58"}
{"level":"DEBUG","ts":"2025-12-09T05:32:28.982Z","msg":"Experimental model tuner is enabled globally: tuning model performance parameters for active VAs"}
{"level":"DEBUG","ts":"2025-12-09T05:32:28.982Z","msg":"Using state vals from VA status to tune variant ms-inference-scheduling-llm-d-modelservice-decode: alpha= 8.160904, beta= 0.049707, gamma= 17.436871, delta= 0.000245"}
{"level":"DEBUG","ts":"2025-12-09T05:32:28.982Z","msg":"[Tuner Config] variant=ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler | Initial state: alpha=8.160904, beta=0.049707, gamma=17.436871, delta=0.000245 | SLO targets: TTFT=1000.00ms, ITL=10.00ms | Has covariance=true"}
{"level":"INFO","ts":"2025-12-09T05:32:28.982Z","msg":"Tuner validation succeeded for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler, server ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler - New NIS: 0.091665"}
{"level":"DEBUG","ts":"2025-12-09T05:32:28.982Z","msg":"Model tuner results: model=unsloth/Meta-Llama-3.1-8B, accelerator=H100, alpha=8.040721, beta=0.049630, gamma=17.381395, delta=0.000245, NIS=0.09"}
{"level":"DEBUG","ts":"2025-12-09T05:32:28.982Z","msg":"Updated tuner status for variant ms-inference-scheduling-llm-d-modelservice-decode: alpha=8.040721, beta=0.049630, gamma=17.381395, delta=0.000245, NIS=0.091665"}
{"level":"INFO","ts":"2025-12-09T05:32:28.982Z","msg":"Tuned performance parameters result: variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler - alpha: 8.040721, beta: 0.049630, gamma: 17.381395, delta: 0.000245"}
{"level":"DEBUG","ts":"2025-12-09T05:32:28.992Z","msg":"Optimization solution - system: Solution: \ns=ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler; c=Premium; m=unsloth/Meta-Llama-3.1-8B; rate=790.51; inTk=248; outTk=448; sol=1, sat=false, alloc={acc=H100; numRep=2; maxBatch=512; cost=200, val=0, itl=9.482402, ttft=19.14636, rho=0.054782186, maxRPM=514.201}; slo-itl=10, slo-ttft=1000, slo-tps=0 \nAllocationByType: \nname=NVIDIA-H100-80GB-HBM3, count=2, limit=0, cost=200 \ntotalCost=200 \n"}
{"level":"DEBUG","ts":"2025-12-09T05:32:28.992Z","msg":"Setting accelerator name: Name=H100, allocationData={H100 2 512 200 9.482402 19.14636 {790.51 248 448}}"}
{"level":"INFO","ts":"2025-12-09T05:32:28.992Z","msg":"Model-based optimization completed for model: unsloth/Meta-Llama-3.1-8B - model-based targets: map[ms-inference-scheduling-llm-d-modelservice-decode:2]"}
{"level":"WARN","ts":"2025-12-09T05:32:28.992Z","msg":"saturation analysis unavailable, using model-based targets only: modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-09T05:32:28.992Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-12-09T05:32:28.992Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=2→target=2"}
{"level":"INFO","ts":"2025-12-09T05:32:28.992Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 2, desired-replicas: 2, accelerator: H100"}
{"level":"INFO","ts":"2025-12-09T05:32:28.992Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=2, accelerator=H100, SaturationOnly=false"}
{"level":"INFO","ts":"2025-12-09T05:32:28.997Z","msg":"Applied Saturation decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=2, target=2, reason=model-based only (Saturation unavailable)"}
{"level":"INFO","ts":"2025-12-09T05:32:28.997Z","msg":"Reconciliation completed successfully: mode=model-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-12-09T05:33:28.999Z","msg":"Reconciling VariantAutoscaling: name=ms-inference-scheduling-llm-d-modelservice-decode, namespace=llm-d-inference-scheduler, modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-09T05:33:28.999Z","msg":"saturation scaling config cache updated: entries=1, has_default=true"}
{"level":"INFO","ts":"2025-12-09T05:33:28.999Z","msg":"saturation scaling configuration loaded successfully"}
{"level":"INFO","ts":"2025-12-09T05:33:28.999Z","msg":"Operating in MODEL-ONLY mode: model-based optimization only"}
{"level":"INFO","ts":"2025-12-09T05:33:28.999Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-12-09T05:33:28.999Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-12-09T05:33:29.006Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-666b5f4cq54t2, queueLength=0"}
{"level":"INFO","ts":"2025-12-09T05:33:29.006Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-666b5f4ccjt24, queueLength=0"}
{"level":"DEBUG","ts":"2025-12-09T05:33:29.006Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=2"}
{"level":"INFO","ts":"2025-12-09T05:33:29.006Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-666b5f4cq54t2, usage=0.041 (4.1%)"}
{"level":"INFO","ts":"2025-12-09T05:33:29.006Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-666b5f4ccjt24, usage=0.041 (4.1%)"}
{"level":"DEBUG","ts":"2025-12-09T05:33:29.006Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=2"}
{"level":"DEBUG","ts":"2025-12-09T05:33:29.008Z","msg":"Pod-to-variant matching successful: totalPods=2, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:2]"}
{"level":"DEBUG","ts":"2025-12-09T05:33:29.008Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=2"}
{"level":"DEBUG","ts":"2025-12-09T05:33:29.008Z","msg":"Collected capacity metrics for observability in model-only mode: modelID=unsloth/Meta-Llama-3.1-8B, replicaCount=2"}
{"level":"DEBUG","ts":"2025-12-09T05:33:29.023Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=2, accelerator=H100, ttft=17.47ms, itl=9.43ms, cost=200.00, maxBatch=256, arrivalRate=775.19, avgInputTokens=246.25, avgOutputTokens=438.24"}
{"level":"DEBUG","ts":"2025-12-09T05:33:29.023Z","msg":"Experimental model tuner is enabled globally: tuning model performance parameters for active VAs"}
{"level":"DEBUG","ts":"2025-12-09T05:33:29.023Z","msg":"Using state vals from VA status to tune variant ms-inference-scheduling-llm-d-modelservice-decode: alpha= 8.040721, beta= 0.049630, gamma= 17.381395, delta= 0.000245"}
{"level":"DEBUG","ts":"2025-12-09T05:33:29.023Z","msg":"[Tuner Config] variant=ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler | Initial state: alpha=8.040721, beta=0.049630, gamma=17.381395, delta=0.000245 | SLO targets: TTFT=1000.00ms, ITL=10.00ms | Has covariance=true"}
{"level":"INFO","ts":"2025-12-09T05:33:29.023Z","msg":"Tuner validation succeeded for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler, server ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler - New NIS: 0.004325"}
{"level":"DEBUG","ts":"2025-12-09T05:33:29.023Z","msg":"Model tuner results: model=unsloth/Meta-Llama-3.1-8B, accelerator=H100, alpha=8.050577, beta=0.049635, gamma=17.328636, delta=0.000245, NIS=0.00"}
{"level":"DEBUG","ts":"2025-12-09T05:33:29.023Z","msg":"Updated tuner status for variant ms-inference-scheduling-llm-d-modelservice-decode: alpha=8.050577, beta=0.049635, gamma=17.328636, delta=0.000245, NIS=0.004325"}
{"level":"INFO","ts":"2025-12-09T05:33:29.023Z","msg":"Tuned performance parameters result: variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler - alpha: 8.050577, beta: 0.049635, gamma: 17.328636, delta: 0.000245"}
{"level":"DEBUG","ts":"2025-12-09T05:33:29.033Z","msg":"Optimization solution - system: Solution: \ns=ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler; c=Premium; m=unsloth/Meta-Llama-3.1-8B; rate=775.19; inTk=246; outTk=438; sol=1, sat=false, alloc={acc=H100; numRep=2; maxBatch=512; cost=200, val=0, itl=9.42725, ttft=19.000252, rho=0.052218184, maxRPM=523.154}; slo-itl=10, slo-ttft=1000, slo-tps=0 \nAllocationByType: \nname=NVIDIA-H100-80GB-HBM3, count=2, limit=0, cost=200 \ntotalCost=200 \n"}
{"level":"DEBUG","ts":"2025-12-09T05:33:29.033Z","msg":"Setting accelerator name: Name=H100, allocationData={H100 2 512 200 9.42725 19.000252 {775.19 246 438}}"}
{"level":"INFO","ts":"2025-12-09T05:33:29.033Z","msg":"Model-based optimization completed for model: unsloth/Meta-Llama-3.1-8B - model-based targets: map[ms-inference-scheduling-llm-d-modelservice-decode:2]"}
{"level":"WARN","ts":"2025-12-09T05:33:29.033Z","msg":"saturation analysis unavailable, using model-based targets only: modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-09T05:33:29.033Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-12-09T05:33:29.033Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=2→target=2"}
{"level":"INFO","ts":"2025-12-09T05:33:29.033Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 2, desired-replicas: 2, accelerator: H100"}
{"level":"INFO","ts":"2025-12-09T05:33:29.033Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=2, accelerator=H100, SaturationOnly=false"}
{"level":"INFO","ts":"2025-12-09T05:33:29.039Z","msg":"Applied Saturation decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=2, target=2, reason=model-based only (Saturation unavailable)"}
{"level":"INFO","ts":"2025-12-09T05:33:29.039Z","msg":"Reconciliation completed successfully: mode=model-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-12-09T05:34:29.040Z","msg":"Reconciling VariantAutoscaling: name=ms-inference-scheduling-llm-d-modelservice-decode, namespace=llm-d-inference-scheduler, modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-09T05:34:29.040Z","msg":"saturation scaling config cache updated: entries=1, has_default=true"}
{"level":"INFO","ts":"2025-12-09T05:34:29.040Z","msg":"saturation scaling configuration loaded successfully"}
{"level":"INFO","ts":"2025-12-09T05:34:29.040Z","msg":"Operating in MODEL-ONLY mode: model-based optimization only"}
{"level":"INFO","ts":"2025-12-09T05:34:29.040Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-12-09T05:34:29.040Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-12-09T05:34:29.049Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-666b5f4cq54t2, usage=0.045 (4.5%)"}
{"level":"INFO","ts":"2025-12-09T05:34:29.049Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-666b5f4ccjt24, usage=0.048 (4.8%)"}
{"level":"DEBUG","ts":"2025-12-09T05:34:29.049Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=2"}
{"level":"INFO","ts":"2025-12-09T05:34:29.049Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-666b5f4cq54t2, queueLength=0"}
{"level":"INFO","ts":"2025-12-09T05:34:29.049Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-666b5f4ccjt24, queueLength=0"}
{"level":"DEBUG","ts":"2025-12-09T05:34:29.049Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=2"}
{"level":"DEBUG","ts":"2025-12-09T05:34:29.052Z","msg":"Pod-to-variant matching successful: totalPods=2, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:2]"}
{"level":"DEBUG","ts":"2025-12-09T05:34:29.052Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=2"}
{"level":"DEBUG","ts":"2025-12-09T05:34:29.052Z","msg":"Collected capacity metrics for observability in model-only mode: modelID=unsloth/Meta-Llama-3.1-8B, replicaCount=2"}
{"level":"DEBUG","ts":"2025-12-09T05:34:29.064Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=2, accelerator=H100, ttft=17.58ms, itl=9.55ms, cost=200.00, maxBatch=256, arrivalRate=816.81, avgInputTokens=243.17, avgOutputTokens=419.60"}
{"level":"DEBUG","ts":"2025-12-09T05:34:29.064Z","msg":"Experimental model tuner is enabled globally: tuning model performance parameters for active VAs"}
{"level":"DEBUG","ts":"2025-12-09T05:34:29.064Z","msg":"Using state vals from VA status to tune variant ms-inference-scheduling-llm-d-modelservice-decode: alpha= 8.050577, beta= 0.049635, gamma= 17.328636, delta= 0.000245"}
{"level":"DEBUG","ts":"2025-12-09T05:34:29.064Z","msg":"[Tuner Config] variant=ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler | Initial state: alpha=8.050577, beta=0.049635, gamma=17.328636, delta=0.000245 | SLO targets: TTFT=1000.00ms, ITL=10.00ms | Has covariance=true"}
{"level":"INFO","ts":"2025-12-09T05:34:29.065Z","msg":"Tuner validation succeeded for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler, server ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler - New NIS: 0.038375"}
{"level":"DEBUG","ts":"2025-12-09T05:34:29.065Z","msg":"Model tuner results: model=unsloth/Meta-Llama-3.1-8B, accelerator=H100, alpha=8.125213, beta=0.049712, gamma=17.281113, delta=0.000245, NIS=0.04"}
{"level":"DEBUG","ts":"2025-12-09T05:34:29.065Z","msg":"Updated tuner status for variant ms-inference-scheduling-llm-d-modelservice-decode: alpha=8.125213, beta=0.049712, gamma=17.281113, delta=0.000245, NIS=0.038375"}
{"level":"INFO","ts":"2025-12-09T05:34:29.065Z","msg":"Tuned performance parameters result: variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler - alpha: 8.125213, beta: 0.049712, gamma: 17.281113, delta: 0.000245"}
{"level":"DEBUG","ts":"2025-12-09T05:34:29.074Z","msg":"Optimization solution - system: Solution: \ns=ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler; c=Premium; m=unsloth/Meta-Llama-3.1-8B; rate=816.81; inTk=243; outTk=419; sol=1, sat=false, alloc={acc=H100; numRep=2; maxBatch=512; cost=200, val=0, itl=9.529149, ttft=18.962463, rho=0.05320627, maxRPM=524.5328}; slo-itl=10, slo-ttft=1000, slo-tps=0 \nAllocationByType: \nname=NVIDIA-H100-80GB-HBM3, count=2, limit=0, cost=200 \ntotalCost=200 \n"}
{"level":"DEBUG","ts":"2025-12-09T05:34:29.074Z","msg":"Setting accelerator name: Name=H100, allocationData={H100 2 512 200 9.529149 18.962463 {816.81 243 419}}"}
{"level":"INFO","ts":"2025-12-09T05:34:29.074Z","msg":"Model-based optimization completed for model: unsloth/Meta-Llama-3.1-8B - model-based targets: map[ms-inference-scheduling-llm-d-modelservice-decode:2]"}
{"level":"WARN","ts":"2025-12-09T05:34:29.074Z","msg":"saturation analysis unavailable, using model-based targets only: modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-09T05:34:29.074Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-12-09T05:34:29.074Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=2→target=2"}
{"level":"INFO","ts":"2025-12-09T05:34:29.075Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 2, desired-replicas: 2, accelerator: H100"}
{"level":"INFO","ts":"2025-12-09T05:34:29.075Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=2, accelerator=H100, SaturationOnly=false"}
{"level":"INFO","ts":"2025-12-09T05:34:29.080Z","msg":"Applied Saturation decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=2, target=2, reason=model-based only (Saturation unavailable)"}
{"level":"INFO","ts":"2025-12-09T05:34:29.080Z","msg":"Reconciliation completed successfully: mode=model-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-12-09T05:35:29.081Z","msg":"Reconciling VariantAutoscaling: name=ms-inference-scheduling-llm-d-modelservice-decode, namespace=llm-d-inference-scheduler, modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-09T05:35:29.081Z","msg":"saturation scaling config cache updated: entries=1, has_default=true"}
{"level":"INFO","ts":"2025-12-09T05:35:29.081Z","msg":"saturation scaling configuration loaded successfully"}
{"level":"INFO","ts":"2025-12-09T05:35:29.081Z","msg":"Operating in MODEL-ONLY mode: model-based optimization only"}
{"level":"INFO","ts":"2025-12-09T05:35:29.081Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-12-09T05:35:29.081Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-12-09T05:35:29.085Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-666b5f4cq54t2, queueLength=0"}
{"level":"INFO","ts":"2025-12-09T05:35:29.085Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-666b5f4ccjt24, queueLength=0"}
{"level":"DEBUG","ts":"2025-12-09T05:35:29.085Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=2"}
{"level":"INFO","ts":"2025-12-09T05:35:29.085Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-666b5f4cq54t2, usage=0.045 (4.5%)"}
{"level":"INFO","ts":"2025-12-09T05:35:29.085Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-666b5f4ccjt24, usage=0.032 (3.2%)"}
{"level":"DEBUG","ts":"2025-12-09T05:35:29.085Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=2"}
{"level":"DEBUG","ts":"2025-12-09T05:35:29.088Z","msg":"Pod-to-variant matching successful: totalPods=2, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:2]"}
{"level":"DEBUG","ts":"2025-12-09T05:35:29.088Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=2"}
{"level":"DEBUG","ts":"2025-12-09T05:35:29.088Z","msg":"Collected capacity metrics for observability in model-only mode: modelID=unsloth/Meta-Llama-3.1-8B, replicaCount=2"}
{"level":"DEBUG","ts":"2025-12-09T05:35:29.101Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=2, accelerator=H100, ttft=17.64ms, itl=8.91ms, cost=200.00, maxBatch=256, arrivalRate=454.08, avgInputTokens=224.63, avgOutputTokens=477.48"}
{"level":"DEBUG","ts":"2025-12-09T05:35:29.101Z","msg":"Experimental model tuner is enabled globally: tuning model performance parameters for active VAs"}
{"level":"DEBUG","ts":"2025-12-09T05:35:29.101Z","msg":"Using state vals from VA status to tune variant ms-inference-scheduling-llm-d-modelservice-decode: alpha= 8.125213, beta= 0.049712, gamma= 17.281113, delta= 0.000245"}
{"level":"DEBUG","ts":"2025-12-09T05:35:29.101Z","msg":"[Tuner Config] variant=ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler | Initial state: alpha=8.125213, beta=0.049712, gamma=17.281113, delta=0.000245 | SLO targets: TTFT=1000.00ms, ITL=10.00ms | Has covariance=true"}
{"level":"INFO","ts":"2025-12-09T05:35:29.101Z","msg":"Tuner validation succeeded for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler, server ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler - New NIS: 0.016961"}
{"level":"DEBUG","ts":"2025-12-09T05:35:29.101Z","msg":"Model tuner results: model=unsloth/Meta-Llama-3.1-8B, accelerator=H100, alpha=8.070286, beta=0.049839, gamma=17.262262, delta=0.000245, NIS=0.02"}
{"level":"DEBUG","ts":"2025-12-09T05:35:29.101Z","msg":"Updated tuner status for variant ms-inference-scheduling-llm-d-modelservice-decode: alpha=8.070286, beta=0.049839, gamma=17.262262, delta=0.000245, NIS=0.016961"}
{"level":"INFO","ts":"2025-12-09T05:35:29.101Z","msg":"Tuned performance parameters result: variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler - alpha: 8.070286, beta: 0.049839, gamma: 17.262262, delta: 0.000245"}
{"level":"DEBUG","ts":"2025-12-09T05:35:29.110Z","msg":"Optimization solution - system: Solution: \ns=ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler; c=Premium; m=unsloth/Meta-Llama-3.1-8B; rate=454.08; inTk=224; outTk=477; sol=1, sat=false, alloc={acc=H100; numRep=1; maxBatch=512; cost=100, val=-100, itl=9.905876, ttft=19.28352, rho=0.06998154, maxRPM=473.5245}; slo-itl=10, slo-ttft=1000, slo-tps=0 \nAllocationByType: \nname=NVIDIA-H100-80GB-HBM3, count=1, limit=0, cost=100 \ntotalCost=100 \n"}
{"level":"DEBUG","ts":"2025-12-09T05:35:29.110Z","msg":"Setting accelerator name: Name=H100, allocationData={H100 1 512 100 9.905876 19.28352 {454.08 224 477}}"}
{"level":"INFO","ts":"2025-12-09T05:35:29.110Z","msg":"Model-based optimization completed for model: unsloth/Meta-Llama-3.1-8B - model-based targets: map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"WARN","ts":"2025-12-09T05:35:29.110Z","msg":"saturation analysis unavailable, using model-based targets only: modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-09T05:35:29.110Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-12-09T05:35:29.110Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=scale-down, current=2→target=1"}
{"level":"INFO","ts":"2025-12-09T05:35:29.110Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 2, desired-replicas: 1, accelerator: H100"}
{"level":"INFO","ts":"2025-12-09T05:35:29.110Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=1, accelerator=H100, SaturationOnly=false"}
{"level":"INFO","ts":"2025-12-09T05:35:29.116Z","msg":"Applied Saturation decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=scale-down, current=2, target=1, reason=model-based only (Saturation unavailable)"}
{"level":"INFO","ts":"2025-12-09T05:35:29.116Z","msg":"Reconciliation completed successfully: mode=model-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-12-09T05:36:29.118Z","msg":"Reconciling VariantAutoscaling: name=ms-inference-scheduling-llm-d-modelservice-decode, namespace=llm-d-inference-scheduler, modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-09T05:36:29.118Z","msg":"saturation scaling config cache updated: entries=1, has_default=true"}
{"level":"INFO","ts":"2025-12-09T05:36:29.118Z","msg":"saturation scaling configuration loaded successfully"}
{"level":"INFO","ts":"2025-12-09T05:36:29.118Z","msg":"Operating in MODEL-ONLY mode: model-based optimization only"}
{"level":"INFO","ts":"2025-12-09T05:36:29.118Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-12-09T05:36:29.118Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-12-09T05:36:29.125Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-666b5f4cq54t2, usage=0.053 (5.3%)"}
{"level":"INFO","ts":"2025-12-09T05:36:29.125Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-666b5f4cq54t2, queueLength=0"}
{"level":"INFO","ts":"2025-12-09T05:36:29.125Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-666b5f4ccjt24, queueLength=0"}
{"level":"DEBUG","ts":"2025-12-09T05:36:29.125Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=2"}
{"level":"INFO","ts":"2025-12-09T05:36:29.125Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-666b5f4ccjt24, usage=0.015 (1.5%)"}
{"level":"DEBUG","ts":"2025-12-09T05:36:29.125Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=2"}
{"level":"DEBUG","ts":"2025-12-09T05:36:29.127Z","msg":"Pod-to-variant matching successful: totalPods=2, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:2]"}
{"level":"DEBUG","ts":"2025-12-09T05:36:29.127Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=2"}
{"level":"DEBUG","ts":"2025-12-09T05:36:29.127Z","msg":"Collected capacity metrics for observability in model-only mode: modelID=unsloth/Meta-Llama-3.1-8B, replicaCount=2"}
{"level":"DEBUG","ts":"2025-12-09T05:36:29.140Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=1, accelerator=H100, ttft=19.09ms, itl=9.55ms, cost=100.00, maxBatch=256, arrivalRate=369.88, avgInputTokens=259.04, avgOutputTokens=440.89"}
{"level":"DEBUG","ts":"2025-12-09T05:36:29.140Z","msg":"Experimental model tuner is enabled globally: tuning model performance parameters for active VAs"}
{"level":"DEBUG","ts":"2025-12-09T05:36:29.140Z","msg":"Using state vals from VA status to tune variant ms-inference-scheduling-llm-d-modelservice-decode: alpha= 8.070286, beta= 0.049839, gamma= 17.262262, delta= 0.000245"}
{"level":"DEBUG","ts":"2025-12-09T05:36:29.140Z","msg":"[Tuner Config] variant=ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler | Initial state: alpha=8.070286, beta=0.049839, gamma=17.262262, delta=0.000245 | SLO targets: TTFT=1000.00ms, ITL=10.00ms | Has covariance=true"}
{"level":"INFO","ts":"2025-12-09T05:36:29.141Z","msg":"Tuner validation succeeded for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler, server ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler - New NIS: 0.070420"}
{"level":"DEBUG","ts":"2025-12-09T05:36:29.141Z","msg":"Model tuner results: model=unsloth/Meta-Llama-3.1-8B, accelerator=H100, alpha=8.171801, beta=0.050187, gamma=17.267000, delta=0.000245, NIS=0.07"}
{"level":"DEBUG","ts":"2025-12-09T05:36:29.141Z","msg":"Updated tuner status for variant ms-inference-scheduling-llm-d-modelservice-decode: alpha=8.171801, beta=0.050187, gamma=17.267000, delta=0.000245, NIS=0.070420"}
{"level":"INFO","ts":"2025-12-09T05:36:29.141Z","msg":"Tuned performance parameters result: variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler - alpha: 8.171801, beta: 0.050187, gamma: 17.267000, delta: 0.000245"}
{"level":"DEBUG","ts":"2025-12-09T05:36:29.144Z","msg":"Optimization solution - system: Solution: \ns=ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler; c=Premium; m=unsloth/Meta-Llama-3.1-8B; rate=369.88; inTk=259; outTk=440; sol=1, sat=false, alloc={acc=H100; numRep=1; maxBatch=512; cost=100, val=0, itl=9.520991, ttft=18.972895, rho=0.050553735, maxRPM=482.05884}; slo-itl=10, slo-ttft=1000, slo-tps=0 \nAllocationByType: \nname=NVIDIA-H100-80GB-HBM3, count=1, limit=0, cost=100 \ntotalCost=100 \n"}
{"level":"DEBUG","ts":"2025-12-09T05:36:29.144Z","msg":"Setting accelerator name: Name=H100, allocationData={H100 1 512 100 9.520991 18.972895 {369.88 259 440}}"}
{"level":"INFO","ts":"2025-12-09T05:36:29.144Z","msg":"Model-based optimization completed for model: unsloth/Meta-Llama-3.1-8B - model-based targets: map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"WARN","ts":"2025-12-09T05:36:29.144Z","msg":"saturation analysis unavailable, using model-based targets only: modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-09T05:36:29.144Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-12-09T05:36:29.144Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1→target=1"}
{"level":"INFO","ts":"2025-12-09T05:36:29.144Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 1, desired-replicas: 1, accelerator: H100"}
{"level":"INFO","ts":"2025-12-09T05:36:29.144Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=1, accelerator=H100, SaturationOnly=false"}
{"level":"INFO","ts":"2025-12-09T05:36:29.150Z","msg":"Applied Saturation decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1, target=1, reason=model-based only (Saturation unavailable)"}
{"level":"INFO","ts":"2025-12-09T05:36:29.150Z","msg":"Reconciliation completed successfully: mode=model-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-12-09T05:37:29.150Z","msg":"Reconciling VariantAutoscaling: name=ms-inference-scheduling-llm-d-modelservice-decode, namespace=llm-d-inference-scheduler, modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-09T05:37:29.151Z","msg":"saturation scaling config cache updated: entries=1, has_default=true"}
{"level":"INFO","ts":"2025-12-09T05:37:29.151Z","msg":"saturation scaling configuration loaded successfully"}
{"level":"INFO","ts":"2025-12-09T05:37:29.151Z","msg":"Operating in MODEL-ONLY mode: model-based optimization only"}
{"level":"INFO","ts":"2025-12-09T05:37:29.151Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-12-09T05:37:29.151Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-12-09T05:37:29.155Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-666b5f4cq54t2, usage=0.041 (4.1%)"}
{"level":"DEBUG","ts":"2025-12-09T05:37:29.155Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"INFO","ts":"2025-12-09T05:37:29.155Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-666b5f4cq54t2, queueLength=0"}
{"level":"DEBUG","ts":"2025-12-09T05:37:29.155Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"DEBUG","ts":"2025-12-09T05:37:29.157Z","msg":"Pod-to-variant matching successful: totalPods=1, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"DEBUG","ts":"2025-12-09T05:37:29.157Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=1"}
{"level":"DEBUG","ts":"2025-12-09T05:37:29.157Z","msg":"Collected capacity metrics for observability in model-only mode: modelID=unsloth/Meta-Llama-3.1-8B, replicaCount=1"}
{"level":"DEBUG","ts":"2025-12-09T05:37:29.170Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=1, accelerator=H100, ttft=19.23ms, itl=9.37ms, cost=100.00, maxBatch=256, arrivalRate=369.88, avgInputTokens=266.77, avgOutputTokens=410.24"}
{"level":"DEBUG","ts":"2025-12-09T05:37:29.170Z","msg":"Experimental model tuner is enabled globally: tuning model performance parameters for active VAs"}
{"level":"DEBUG","ts":"2025-12-09T05:37:29.170Z","msg":"Using state vals from VA status to tune variant ms-inference-scheduling-llm-d-modelservice-decode: alpha= 8.171801, beta= 0.050187, gamma= 17.267000, delta= 0.000245"}
{"level":"DEBUG","ts":"2025-12-09T05:37:29.170Z","msg":"[Tuner Config] variant=ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler | Initial state: alpha=8.171801, beta=0.050187, gamma=17.267000, delta=0.000245 | SLO targets: TTFT=1000.00ms, ITL=10.00ms | Has covariance=true"}
{"level":"INFO","ts":"2025-12-09T05:37:29.170Z","msg":"Tuner validation succeeded for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler, server ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler - New NIS: 0.007469"}
{"level":"DEBUG","ts":"2025-12-09T05:37:29.170Z","msg":"Model tuner results: model=unsloth/Meta-Llama-3.1-8B, accelerator=H100, alpha=8.137252, beta=0.050162, gamma=17.278568, delta=0.000245, NIS=0.01"}
{"level":"DEBUG","ts":"2025-12-09T05:37:29.170Z","msg":"Updated tuner status for variant ms-inference-scheduling-llm-d-modelservice-decode: alpha=8.137252, beta=0.050162, gamma=17.278568, delta=0.000245, NIS=0.007469"}
{"level":"INFO","ts":"2025-12-09T05:37:29.170Z","msg":"Tuned performance parameters result: variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler - alpha: 8.137252, beta: 0.050162, gamma: 17.278568, delta: 0.000245"}
{"level":"DEBUG","ts":"2025-12-09T05:37:29.179Z","msg":"Optimization solution - system: Solution: \ns=ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler; c=Premium; m=unsloth/Meta-Llama-3.1-8B; rate=369.88; inTk=266; outTk=410; sol=1, sat=false, alloc={acc=H100; numRep=1; maxBatch=512; cost=100, val=0, itl=9.379531, ttft=18.89254, rho=0.046417058, maxRPM=527.5585}; slo-itl=10, slo-ttft=1000, slo-tps=0 \nAllocationByType: \nname=NVIDIA-H100-80GB-HBM3, count=1, limit=0, cost=100 \ntotalCost=100 \n"}
{"level":"DEBUG","ts":"2025-12-09T05:37:29.179Z","msg":"Setting accelerator name: Name=H100, allocationData={H100 1 512 100 9.379531 18.89254 {369.88 266 410}}"}
{"level":"INFO","ts":"2025-12-09T05:37:29.179Z","msg":"Model-based optimization completed for model: unsloth/Meta-Llama-3.1-8B - model-based targets: map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"WARN","ts":"2025-12-09T05:37:29.179Z","msg":"saturation analysis unavailable, using model-based targets only: modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-09T05:37:29.179Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-12-09T05:37:29.179Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1→target=1"}
{"level":"INFO","ts":"2025-12-09T05:37:29.179Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 1, desired-replicas: 1, accelerator: H100"}
{"level":"INFO","ts":"2025-12-09T05:37:29.179Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=1, accelerator=H100, SaturationOnly=false"}
{"level":"INFO","ts":"2025-12-09T05:37:29.186Z","msg":"Applied Saturation decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1, target=1, reason=model-based only (Saturation unavailable)"}
{"level":"INFO","ts":"2025-12-09T05:37:29.186Z","msg":"Reconciliation completed successfully: mode=model-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-12-09T05:38:29.187Z","msg":"Reconciling VariantAutoscaling: name=ms-inference-scheduling-llm-d-modelservice-decode, namespace=llm-d-inference-scheduler, modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-09T05:38:29.187Z","msg":"saturation scaling config cache updated: entries=1, has_default=true"}
{"level":"INFO","ts":"2025-12-09T05:38:29.187Z","msg":"saturation scaling configuration loaded successfully"}
{"level":"INFO","ts":"2025-12-09T05:38:29.187Z","msg":"Operating in MODEL-ONLY mode: model-based optimization only"}
{"level":"INFO","ts":"2025-12-09T05:38:29.187Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-12-09T05:38:29.187Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-12-09T05:38:29.198Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-666b5f4cq54t2, queueLength=0"}
{"level":"DEBUG","ts":"2025-12-09T05:38:29.198Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"INFO","ts":"2025-12-09T05:38:29.198Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-666b5f4cq54t2, usage=0.047 (4.7%)"}
{"level":"DEBUG","ts":"2025-12-09T05:38:29.198Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"DEBUG","ts":"2025-12-09T05:38:29.200Z","msg":"Pod-to-variant matching successful: totalPods=1, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"DEBUG","ts":"2025-12-09T05:38:29.200Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=1"}
{"level":"DEBUG","ts":"2025-12-09T05:38:29.200Z","msg":"Collected capacity metrics for observability in model-only mode: modelID=unsloth/Meta-Llama-3.1-8B, replicaCount=1"}
{"level":"DEBUG","ts":"2025-12-09T05:38:29.212Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=1, accelerator=H100, ttft=19.46ms, itl=9.89ms, cost=100.00, maxBatch=256, arrivalRate=425.13, avgInputTokens=234.33, avgOutputTokens=490.01"}
{"level":"DEBUG","ts":"2025-12-09T05:38:29.212Z","msg":"Experimental model tuner is enabled globally: tuning model performance parameters for active VAs"}
{"level":"DEBUG","ts":"2025-12-09T05:38:29.212Z","msg":"Using state vals from VA status to tune variant ms-inference-scheduling-llm-d-modelservice-decode: alpha= 8.137252, beta= 0.050162, gamma= 17.278568, delta= 0.000245"}
{"level":"DEBUG","ts":"2025-12-09T05:38:29.212Z","msg":"[Tuner Config] variant=ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler | Initial state: alpha=8.137252, beta=0.050162, gamma=17.278568, delta=0.000245 | SLO targets: TTFT=1000.00ms, ITL=10.00ms | Has covariance=true"}
{"level":"INFO","ts":"2025-12-09T05:38:29.213Z","msg":"Tuner validation succeeded for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler, server ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler - New NIS: 0.002045"}
{"level":"DEBUG","ts":"2025-12-09T05:38:29.213Z","msg":"Model tuner results: model=unsloth/Meta-Llama-3.1-8B, accelerator=H100, alpha=8.121184, beta=0.050070, gamma=17.283598, delta=0.000245, NIS=0.00"}
{"level":"DEBUG","ts":"2025-12-09T05:38:29.213Z","msg":"Updated tuner status for variant ms-inference-scheduling-llm-d-modelservice-decode: alpha=8.121184, beta=0.050070, gamma=17.283598, delta=0.000245, NIS=0.002045"}
{"level":"INFO","ts":"2025-12-09T05:38:29.213Z","msg":"Tuned performance parameters result: variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler - alpha: 8.121184, beta: 0.050070, gamma: 17.283598, delta: 0.000245"}
{"level":"DEBUG","ts":"2025-12-09T05:38:29.222Z","msg":"Optimization solution - system: Solution: \ns=ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler; c=Premium; m=unsloth/Meta-Llama-3.1-8B; rate=425.13; inTk=234; outTk=490; sol=1, sat=false, alloc={acc=H100; numRep=1; maxBatch=512; cost=100, val=0, itl=9.89468, ttft=19.314232, rho=0.067226626, maxRPM=446.36795}; slo-itl=10, slo-ttft=1000, slo-tps=0 \nAllocationByType: \nname=NVIDIA-H100-80GB-HBM3, count=1, limit=0, cost=100 \ntotalCost=100 \n"}
{"level":"DEBUG","ts":"2025-12-09T05:38:29.222Z","msg":"Setting accelerator name: Name=H100, allocationData={H100 1 512 100 9.89468 19.314232 {425.13 234 490}}"}
{"level":"INFO","ts":"2025-12-09T05:38:29.222Z","msg":"Model-based optimization completed for model: unsloth/Meta-Llama-3.1-8B - model-based targets: map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"WARN","ts":"2025-12-09T05:38:29.222Z","msg":"saturation analysis unavailable, using model-based targets only: modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-09T05:38:29.222Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-12-09T05:38:29.222Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1→target=1"}
{"level":"INFO","ts":"2025-12-09T05:38:29.222Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 1, desired-replicas: 1, accelerator: H100"}
{"level":"INFO","ts":"2025-12-09T05:38:29.222Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=1, accelerator=H100, SaturationOnly=false"}
{"level":"INFO","ts":"2025-12-09T05:38:29.229Z","msg":"Applied Saturation decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1, target=1, reason=model-based only (Saturation unavailable)"}
{"level":"INFO","ts":"2025-12-09T05:38:29.229Z","msg":"Reconciliation completed successfully: mode=model-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-12-09T05:39:29.230Z","msg":"Reconciling VariantAutoscaling: name=ms-inference-scheduling-llm-d-modelservice-decode, namespace=llm-d-inference-scheduler, modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-09T05:39:29.230Z","msg":"saturation scaling config cache updated: entries=1, has_default=true"}
{"level":"INFO","ts":"2025-12-09T05:39:29.230Z","msg":"saturation scaling configuration loaded successfully"}
{"level":"INFO","ts":"2025-12-09T05:39:29.230Z","msg":"Operating in MODEL-ONLY mode: model-based optimization only"}
{"level":"INFO","ts":"2025-12-09T05:39:29.230Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-12-09T05:39:29.230Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-12-09T05:39:29.239Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-666b5f4cq54t2, usage=0.066 (6.6%)"}
{"level":"DEBUG","ts":"2025-12-09T05:39:29.239Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"INFO","ts":"2025-12-09T05:39:29.239Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-666b5f4cq54t2, queueLength=0"}
{"level":"DEBUG","ts":"2025-12-09T05:39:29.239Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"DEBUG","ts":"2025-12-09T05:39:29.241Z","msg":"Pod-to-variant matching successful: totalPods=1, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"DEBUG","ts":"2025-12-09T05:39:29.241Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=1"}
{"level":"DEBUG","ts":"2025-12-09T05:39:29.241Z","msg":"Collected capacity metrics for observability in model-only mode: modelID=unsloth/Meta-Llama-3.1-8B, replicaCount=1"}
{"level":"DEBUG","ts":"2025-12-09T05:39:29.253Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=1, accelerator=H100, ttft=19.57ms, itl=9.68ms, cost=100.00, maxBatch=256, arrivalRate=412.73, avgInputTokens=244.87, avgOutputTokens=469.75"}
{"level":"DEBUG","ts":"2025-12-09T05:39:29.253Z","msg":"Experimental model tuner is enabled globally: tuning model performance parameters for active VAs"}
{"level":"DEBUG","ts":"2025-12-09T05:39:29.253Z","msg":"Using state vals from VA status to tune variant ms-inference-scheduling-llm-d-modelservice-decode: alpha= 8.121184, beta= 0.050070, gamma= 17.283598, delta= 0.000245"}
{"level":"DEBUG","ts":"2025-12-09T05:39:29.253Z","msg":"[Tuner Config] variant=ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler | Initial state: alpha=8.121184, beta=0.050070, gamma=17.283598, delta=0.000245 | SLO targets: TTFT=1000.00ms, ITL=10.00ms | Has covariance=true"}
{"level":"INFO","ts":"2025-12-09T05:39:29.253Z","msg":"Tuner validation succeeded for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler, server ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler - New NIS: 0.013527"}
{"level":"DEBUG","ts":"2025-12-09T05:39:29.253Z","msg":"Model tuner results: model=unsloth/Meta-Llama-3.1-8B, accelerator=H100, alpha=8.074551, beta=0.050037, gamma=17.295340, delta=0.000245, NIS=0.01"}
{"level":"DEBUG","ts":"2025-12-09T05:39:29.253Z","msg":"Updated tuner status for variant ms-inference-scheduling-llm-d-modelservice-decode: alpha=8.074551, beta=0.050037, gamma=17.295340, delta=0.000245, NIS=0.013527"}
{"level":"INFO","ts":"2025-12-09T05:39:29.253Z","msg":"Tuned performance parameters result: variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler - alpha: 8.074551, beta: 0.050037, gamma: 17.295340, delta: 0.000245"}
{"level":"DEBUG","ts":"2025-12-09T05:39:29.263Z","msg":"Optimization solution - system: Solution: \ns=ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler; c=Premium; m=unsloth/Meta-Llama-3.1-8B; rate=412.73; inTk=244; outTk=469; sol=1, sat=false, alloc={acc=H100; numRep=1; maxBatch=512; cost=100, val=0, itl=9.692526, ttft=19.228352, rho=0.061201874, maxRPM=478.51382}; slo-itl=10, slo-ttft=1000, slo-tps=0 \nAllocationByType: \nname=NVIDIA-H100-80GB-HBM3, count=1, limit=0, cost=100 \ntotalCost=100 \n"}
{"level":"DEBUG","ts":"2025-12-09T05:39:29.263Z","msg":"Setting accelerator name: Name=H100, allocationData={H100 1 512 100 9.692526 19.228352 {412.73 244 469}}"}
{"level":"INFO","ts":"2025-12-09T05:39:29.263Z","msg":"Model-based optimization completed for model: unsloth/Meta-Llama-3.1-8B - model-based targets: map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"WARN","ts":"2025-12-09T05:39:29.263Z","msg":"saturation analysis unavailable, using model-based targets only: modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-09T05:39:29.263Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-12-09T05:39:29.263Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1→target=1"}
{"level":"INFO","ts":"2025-12-09T05:39:29.263Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 1, desired-replicas: 1, accelerator: H100"}
{"level":"INFO","ts":"2025-12-09T05:39:29.263Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=1, accelerator=H100, SaturationOnly=false"}
{"level":"INFO","ts":"2025-12-09T05:39:29.268Z","msg":"Applied Saturation decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1, target=1, reason=model-based only (Saturation unavailable)"}
{"level":"INFO","ts":"2025-12-09T05:39:29.268Z","msg":"Reconciliation completed successfully: mode=model-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-12-09T05:40:29.269Z","msg":"Reconciling VariantAutoscaling: name=ms-inference-scheduling-llm-d-modelservice-decode, namespace=llm-d-inference-scheduler, modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-09T05:40:29.269Z","msg":"saturation scaling config cache updated: entries=1, has_default=true"}
{"level":"INFO","ts":"2025-12-09T05:40:29.269Z","msg":"saturation scaling configuration loaded successfully"}
{"level":"INFO","ts":"2025-12-09T05:40:29.269Z","msg":"Operating in MODEL-ONLY mode: model-based optimization only"}
{"level":"INFO","ts":"2025-12-09T05:40:29.269Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-12-09T05:40:29.269Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-12-09T05:40:29.280Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-666b5f4cq54t2, queueLength=0"}
{"level":"DEBUG","ts":"2025-12-09T05:40:29.280Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"INFO","ts":"2025-12-09T05:40:29.280Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-666b5f4cq54t2, usage=0.047 (4.7%)"}
{"level":"DEBUG","ts":"2025-12-09T05:40:29.280Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"DEBUG","ts":"2025-12-09T05:40:29.282Z","msg":"Pod-to-variant matching successful: totalPods=1, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"DEBUG","ts":"2025-12-09T05:40:29.282Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=1"}
{"level":"DEBUG","ts":"2025-12-09T05:40:29.282Z","msg":"Collected capacity metrics for observability in model-only mode: modelID=unsloth/Meta-Llama-3.1-8B, replicaCount=1"}
{"level":"DEBUG","ts":"2025-12-09T05:40:29.296Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=1, accelerator=H100, ttft=19.36ms, itl=9.72ms, cost=100.00, maxBatch=256, arrivalRate=407.09, avgInputTokens=239.88, avgOutputTokens=452.93"}
{"level":"DEBUG","ts":"2025-12-09T05:40:29.296Z","msg":"Experimental model tuner is enabled globally: tuning model performance parameters for active VAs"}
{"level":"DEBUG","ts":"2025-12-09T05:40:29.296Z","msg":"Using state vals from VA status to tune variant ms-inference-scheduling-llm-d-modelservice-decode: alpha= 8.074551, beta= 0.050037, gamma= 17.295340, delta= 0.000245"}
{"level":"DEBUG","ts":"2025-12-09T05:40:29.296Z","msg":"[Tuner Config] variant=ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler | Initial state: alpha=8.074551, beta=0.050037, gamma=17.295340, delta=0.000245 | SLO targets: TTFT=1000.00ms, ITL=10.00ms | Has covariance=true"}
{"level":"INFO","ts":"2025-12-09T05:40:29.296Z","msg":"Tuner validation succeeded for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler, server ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler - New NIS: 0.040159"}
{"level":"DEBUG","ts":"2025-12-09T05:40:29.296Z","msg":"Model tuner results: model=unsloth/Meta-Llama-3.1-8B, accelerator=H100, alpha=8.156164, beta=0.050054, gamma=17.304031, delta=0.000245, NIS=0.04"}
{"level":"DEBUG","ts":"2025-12-09T05:40:29.296Z","msg":"Updated tuner status for variant ms-inference-scheduling-llm-d-modelservice-decode: alpha=8.156164, beta=0.050054, gamma=17.304031, delta=0.000245, NIS=0.040159"}
{"level":"INFO","ts":"2025-12-09T05:40:29.296Z","msg":"Tuned performance parameters result: variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler - alpha: 8.156164, beta: 0.050054, gamma: 17.304031, delta: 0.000245"}
{"level":"DEBUG","ts":"2025-12-09T05:40:29.306Z","msg":"Optimization solution - system: Solution: \ns=ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler; c=Premium; m=unsloth/Meta-Llama-3.1-8B; rate=407.09; inTk=239; outTk=452; sol=1, sat=false, alloc={acc=H100; numRep=1; maxBatch=512; cost=100, val=0, itl=9.698085, ttft=19.107847, rho=0.058213655, maxRPM=474.72226}; slo-itl=10, slo-ttft=1000, slo-tps=0 \nAllocationByType: \nname=NVIDIA-H100-80GB-HBM3, count=1, limit=0, cost=100 \ntotalCost=100 \n"}
{"level":"DEBUG","ts":"2025-12-09T05:40:29.306Z","msg":"Setting accelerator name: Name=H100, allocationData={H100 1 512 100 9.698085 19.107847 {407.09 239 452}}"}
{"level":"INFO","ts":"2025-12-09T05:40:29.306Z","msg":"Model-based optimization completed for model: unsloth/Meta-Llama-3.1-8B - model-based targets: map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"WARN","ts":"2025-12-09T05:40:29.306Z","msg":"saturation analysis unavailable, using model-based targets only: modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-09T05:40:29.306Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-12-09T05:40:29.306Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1→target=1"}
{"level":"INFO","ts":"2025-12-09T05:40:29.306Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 1, desired-replicas: 1, accelerator: H100"}
{"level":"INFO","ts":"2025-12-09T05:40:29.306Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=1, accelerator=H100, SaturationOnly=false"}
{"level":"INFO","ts":"2025-12-09T05:40:29.312Z","msg":"Applied Saturation decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1, target=1, reason=model-based only (Saturation unavailable)"}
{"level":"INFO","ts":"2025-12-09T05:40:29.312Z","msg":"Reconciliation completed successfully: mode=model-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-12-09T05:41:29.313Z","msg":"Reconciling VariantAutoscaling: name=ms-inference-scheduling-llm-d-modelservice-decode, namespace=llm-d-inference-scheduler, modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-09T05:41:29.313Z","msg":"saturation scaling config cache updated: entries=1, has_default=true"}
{"level":"INFO","ts":"2025-12-09T05:41:29.313Z","msg":"saturation scaling configuration loaded successfully"}
{"level":"INFO","ts":"2025-12-09T05:41:29.313Z","msg":"Operating in MODEL-ONLY mode: model-based optimization only"}
{"level":"INFO","ts":"2025-12-09T05:41:29.313Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-12-09T05:41:29.313Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-12-09T05:41:29.317Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-666b5f4cq54t2, usage=0.051 (5.1%)"}
{"level":"DEBUG","ts":"2025-12-09T05:41:29.317Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"INFO","ts":"2025-12-09T05:41:29.317Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-666b5f4cq54t2, queueLength=0"}
{"level":"DEBUG","ts":"2025-12-09T05:41:29.317Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"DEBUG","ts":"2025-12-09T05:41:29.319Z","msg":"Pod-to-variant matching successful: totalPods=1, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"DEBUG","ts":"2025-12-09T05:41:29.319Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=1"}
{"level":"DEBUG","ts":"2025-12-09T05:41:29.319Z","msg":"Collected capacity metrics for observability in model-only mode: modelID=unsloth/Meta-Llama-3.1-8B, replicaCount=1"}
{"level":"DEBUG","ts":"2025-12-09T05:41:29.332Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=1, accelerator=H100, ttft=18.98ms, itl=9.67ms, cost=100.00, maxBatch=256, arrivalRate=433.03, avgInputTokens=229.91, avgOutputTokens=474.04"}
{"level":"DEBUG","ts":"2025-12-09T05:41:29.332Z","msg":"Experimental model tuner is enabled globally: tuning model performance parameters for active VAs"}
{"level":"DEBUG","ts":"2025-12-09T05:41:29.332Z","msg":"Using state vals from VA status to tune variant ms-inference-scheduling-llm-d-modelservice-decode: alpha= 8.156164, beta= 0.050054, gamma= 17.304031, delta= 0.000245"}
{"level":"DEBUG","ts":"2025-12-09T05:41:29.332Z","msg":"[Tuner Config] variant=ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler | Initial state: alpha=8.156164, beta=0.050054, gamma=17.304031, delta=0.000245 | SLO targets: TTFT=1000.00ms, ITL=10.00ms | Has covariance=true"}
{"level":"INFO","ts":"2025-12-09T05:41:29.333Z","msg":"Tuner validation succeeded for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler, server ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler - New NIS: 0.148035"}
{"level":"DEBUG","ts":"2025-12-09T05:41:29.333Z","msg":"Model tuner results: model=unsloth/Meta-Llama-3.1-8B, accelerator=H100, alpha=8.010911, beta=0.049586, gamma=17.295677, delta=0.000245, NIS=0.15"}
{"level":"DEBUG","ts":"2025-12-09T05:41:29.333Z","msg":"Updated tuner status for variant ms-inference-scheduling-llm-d-modelservice-decode: alpha=8.010911, beta=0.049586, gamma=17.295677, delta=0.000245, NIS=0.148035"}
{"level":"INFO","ts":"2025-12-09T05:41:29.333Z","msg":"Tuned performance parameters result: variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler - alpha: 8.010911, beta: 0.049586, gamma: 17.295677, delta: 0.000245"}
{"level":"DEBUG","ts":"2025-12-09T05:41:29.343Z","msg":"Optimization solution - system: Solution: \ns=ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler; c=Premium; m=unsloth/Meta-Llama-3.1-8B; rate=433.03; inTk=229; outTk=474; sol=1, sat=false, alloc={acc=H100; numRep=1; maxBatch=512; cost=100, val=0, itl=9.7112055, ttft=19.219515, rho=0.065019615, maxRPM=494.12134}; slo-itl=10, slo-ttft=1000, slo-tps=0 \nAllocationByType: \nname=NVIDIA-H100-80GB-HBM3, count=1, limit=0, cost=100 \ntotalCost=100 \n"}
{"level":"DEBUG","ts":"2025-12-09T05:41:29.343Z","msg":"Setting accelerator name: Name=H100, allocationData={H100 1 512 100 9.7112055 19.219515 {433.03 229 474}}"}
{"level":"INFO","ts":"2025-12-09T05:41:29.343Z","msg":"Model-based optimization completed for model: unsloth/Meta-Llama-3.1-8B - model-based targets: map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"WARN","ts":"2025-12-09T05:41:29.343Z","msg":"saturation analysis unavailable, using model-based targets only: modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-09T05:41:29.343Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-12-09T05:41:29.343Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1→target=1"}
{"level":"INFO","ts":"2025-12-09T05:41:29.343Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 1, desired-replicas: 1, accelerator: H100"}
{"level":"INFO","ts":"2025-12-09T05:41:29.343Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=1, accelerator=H100, SaturationOnly=false"}
{"level":"INFO","ts":"2025-12-09T05:41:29.348Z","msg":"Applied Saturation decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1, target=1, reason=model-based only (Saturation unavailable)"}
{"level":"INFO","ts":"2025-12-09T05:41:29.348Z","msg":"Reconciliation completed successfully: mode=model-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-12-09T05:42:29.349Z","msg":"Reconciling VariantAutoscaling: name=ms-inference-scheduling-llm-d-modelservice-decode, namespace=llm-d-inference-scheduler, modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-09T05:42:29.349Z","msg":"saturation scaling config cache updated: entries=1, has_default=true"}
{"level":"INFO","ts":"2025-12-09T05:42:29.349Z","msg":"saturation scaling configuration loaded successfully"}
{"level":"INFO","ts":"2025-12-09T05:42:29.349Z","msg":"Operating in MODEL-ONLY mode: model-based optimization only"}
{"level":"INFO","ts":"2025-12-09T05:42:29.349Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-12-09T05:42:29.349Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-12-09T05:42:29.359Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-666b5f4cq54t2, usage=0.056 (5.6%)"}
{"level":"DEBUG","ts":"2025-12-09T05:42:29.359Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"INFO","ts":"2025-12-09T05:42:29.359Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-666b5f4cq54t2, queueLength=0"}
{"level":"DEBUG","ts":"2025-12-09T05:42:29.359Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"DEBUG","ts":"2025-12-09T05:42:29.361Z","msg":"Pod-to-variant matching successful: totalPods=1, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"DEBUG","ts":"2025-12-09T05:42:29.361Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=1"}
{"level":"DEBUG","ts":"2025-12-09T05:42:29.361Z","msg":"Collected capacity metrics for observability in model-only mode: modelID=unsloth/Meta-Llama-3.1-8B, replicaCount=1"}
{"level":"DEBUG","ts":"2025-12-09T05:42:29.373Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=1, accelerator=H100, ttft=19.35ms, itl=9.89ms, cost=100.00, maxBatch=256, arrivalRate=464.60, avgInputTokens=240.80, avgOutputTokens=451.55"}
{"level":"DEBUG","ts":"2025-12-09T05:42:29.373Z","msg":"Experimental model tuner is enabled globally: tuning model performance parameters for active VAs"}
{"level":"DEBUG","ts":"2025-12-09T05:42:29.373Z","msg":"Using state vals from VA status to tune variant ms-inference-scheduling-llm-d-modelservice-decode: alpha= 8.010911, beta= 0.049586, gamma= 17.295677, delta= 0.000245"}
{"level":"DEBUG","ts":"2025-12-09T05:42:29.373Z","msg":"[Tuner Config] variant=ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler | Initial state: alpha=8.010911, beta=0.049586, gamma=17.295677, delta=0.000245 | SLO targets: TTFT=1000.00ms, ITL=10.00ms | Has covariance=true"}
{"level":"INFO","ts":"2025-12-09T05:42:29.374Z","msg":"Tuner validation succeeded for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler, server ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler - New NIS: 0.051328"}
{"level":"DEBUG","ts":"2025-12-09T05:42:29.374Z","msg":"Model tuner results: model=unsloth/Meta-Llama-3.1-8B, accelerator=H100, alpha=8.098431, beta=0.049744, gamma=17.294716, delta=0.000245, NIS=0.05"}
{"level":"DEBUG","ts":"2025-12-09T05:42:29.374Z","msg":"Updated tuner status for variant ms-inference-scheduling-llm-d-modelservice-decode: alpha=8.098431, beta=0.049744, gamma=17.294716, delta=0.000245, NIS=0.051328"}
{"level":"INFO","ts":"2025-12-09T05:42:29.374Z","msg":"Tuned performance parameters result: variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler - alpha: 8.098431, beta: 0.049744, gamma: 17.294716, delta: 0.000245"}
{"level":"DEBUG","ts":"2025-12-09T05:42:29.383Z","msg":"Optimization solution - system: Solution: \ns=ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler; c=Premium; m=unsloth/Meta-Llama-3.1-8B; rate=464.6; inTk=240; outTk=451; sol=1, sat=false, alloc={acc=H100; numRep=1; maxBatch=512; cost=100, val=0, itl=9.865677, ttft=19.383709, rho=0.06743563, maxRPM=494.21838}; slo-itl=10, slo-ttft=1000, slo-tps=0 \nAllocationByType: \nname=NVIDIA-H100-80GB-HBM3, count=1, limit=0, cost=100 \ntotalCost=100 \n"}
{"level":"DEBUG","ts":"2025-12-09T05:42:29.383Z","msg":"Setting accelerator name: Name=H100, allocationData={H100 1 512 100 9.865677 19.383709 {464.6 240 451}}"}
{"level":"INFO","ts":"2025-12-09T05:42:29.383Z","msg":"Model-based optimization completed for model: unsloth/Meta-Llama-3.1-8B - model-based targets: map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"WARN","ts":"2025-12-09T05:42:29.383Z","msg":"saturation analysis unavailable, using model-based targets only: modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-09T05:42:29.383Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-12-09T05:42:29.383Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1→target=1"}
{"level":"INFO","ts":"2025-12-09T05:42:29.383Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 1, desired-replicas: 1, accelerator: H100"}
{"level":"INFO","ts":"2025-12-09T05:42:29.383Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=1, accelerator=H100, SaturationOnly=false"}
{"level":"INFO","ts":"2025-12-09T05:42:29.392Z","msg":"Applied Saturation decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1, target=1, reason=model-based only (Saturation unavailable)"}
{"level":"INFO","ts":"2025-12-09T05:42:29.392Z","msg":"Reconciliation completed successfully: mode=model-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-12-09T05:43:29.393Z","msg":"Reconciling VariantAutoscaling: name=ms-inference-scheduling-llm-d-modelservice-decode, namespace=llm-d-inference-scheduler, modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-09T05:43:29.393Z","msg":"saturation scaling config cache updated: entries=1, has_default=true"}
{"level":"INFO","ts":"2025-12-09T05:43:29.393Z","msg":"saturation scaling configuration loaded successfully"}
{"level":"INFO","ts":"2025-12-09T05:43:29.393Z","msg":"Operating in MODEL-ONLY mode: model-based optimization only"}
{"level":"INFO","ts":"2025-12-09T05:43:29.393Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-12-09T05:43:29.393Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-12-09T05:43:29.397Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-666b5f4cq54t2, queueLength=0"}
{"level":"DEBUG","ts":"2025-12-09T05:43:29.397Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"INFO","ts":"2025-12-09T05:43:29.397Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-666b5f4cq54t2, usage=0.051 (5.1%)"}
{"level":"DEBUG","ts":"2025-12-09T05:43:29.397Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"DEBUG","ts":"2025-12-09T05:43:29.399Z","msg":"Pod-to-variant matching successful: totalPods=1, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"DEBUG","ts":"2025-12-09T05:43:29.399Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=1"}
{"level":"DEBUG","ts":"2025-12-09T05:43:29.399Z","msg":"Collected capacity metrics for observability in model-only mode: modelID=unsloth/Meta-Llama-3.1-8B, replicaCount=1"}
{"level":"DEBUG","ts":"2025-12-09T05:43:29.411Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=1, accelerator=H100, ttft=19.12ms, itl=9.86ms, cost=100.00, maxBatch=256, arrivalRate=455.58, avgInputTokens=221.74, avgOutputTokens=481.16"}
{"level":"DEBUG","ts":"2025-12-09T05:43:29.411Z","msg":"Experimental model tuner is enabled globally: tuning model performance parameters for active VAs"}
{"level":"DEBUG","ts":"2025-12-09T05:43:29.411Z","msg":"Using state vals from VA status to tune variant ms-inference-scheduling-llm-d-modelservice-decode: alpha= 8.098431, beta= 0.049744, gamma= 17.294716, delta= 0.000245"}
{"level":"DEBUG","ts":"2025-12-09T05:43:29.411Z","msg":"[Tuner Config] variant=ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler | Initial state: alpha=8.098431, beta=0.049744, gamma=17.294716, delta=0.000245 | SLO targets: TTFT=1000.00ms, ITL=10.00ms | Has covariance=true"}
{"level":"INFO","ts":"2025-12-09T05:43:29.411Z","msg":"Tuner validation succeeded for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler, server ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler - New NIS: 0.027208"}
{"level":"DEBUG","ts":"2025-12-09T05:43:29.411Z","msg":"Model tuner results: model=unsloth/Meta-Llama-3.1-8B, accelerator=H100, alpha=8.036217, beta=0.049568, gamma=17.288605, delta=0.000245, NIS=0.03"}
{"level":"DEBUG","ts":"2025-12-09T05:43:29.411Z","msg":"Updated tuner status for variant ms-inference-scheduling-llm-d-modelservice-decode: alpha=8.036217, beta=0.049568, gamma=17.288605, delta=0.000245, NIS=0.027208"}
{"level":"INFO","ts":"2025-12-09T05:43:29.411Z","msg":"Tuned performance parameters result: variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler - alpha: 8.036217, beta: 0.049568, gamma: 17.288605, delta: 0.000245"}
{"level":"DEBUG","ts":"2025-12-09T05:43:29.421Z","msg":"Optimization solution - system: Solution: \ns=ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler; c=Premium; m=unsloth/Meta-Llama-3.1-8B; rate=455.58; inTk=221; outTk=481; sol=1, sat=false, alloc={acc=H100; numRep=1; maxBatch=512; cost=100, val=0, itl=9.877479, ttft=19.299892, rho=0.07059843, maxRPM=480.78204}; slo-itl=10, slo-ttft=1000, slo-tps=0 \nAllocationByType: \nname=NVIDIA-H100-80GB-HBM3, count=1, limit=0, cost=100 \ntotalCost=100 \n"}
{"level":"DEBUG","ts":"2025-12-09T05:43:29.421Z","msg":"Setting accelerator name: Name=H100, allocationData={H100 1 512 100 9.877479 19.299892 {455.58 221 481}}"}
{"level":"INFO","ts":"2025-12-09T05:43:29.421Z","msg":"Model-based optimization completed for model: unsloth/Meta-Llama-3.1-8B - model-based targets: map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"WARN","ts":"2025-12-09T05:43:29.421Z","msg":"saturation analysis unavailable, using model-based targets only: modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-09T05:43:29.421Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-12-09T05:43:29.421Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1→target=1"}
{"level":"INFO","ts":"2025-12-09T05:43:29.421Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 1, desired-replicas: 1, accelerator: H100"}
{"level":"INFO","ts":"2025-12-09T05:43:29.421Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=1, accelerator=H100, SaturationOnly=false"}
{"level":"INFO","ts":"2025-12-09T05:43:29.428Z","msg":"Applied Saturation decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1, target=1, reason=model-based only (Saturation unavailable)"}
{"level":"INFO","ts":"2025-12-09T05:43:29.428Z","msg":"Reconciliation completed successfully: mode=model-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-12-09T05:44:29.429Z","msg":"Reconciling VariantAutoscaling: name=ms-inference-scheduling-llm-d-modelservice-decode, namespace=llm-d-inference-scheduler, modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-09T05:44:29.429Z","msg":"saturation scaling config cache updated: entries=1, has_default=true"}
{"level":"INFO","ts":"2025-12-09T05:44:29.429Z","msg":"saturation scaling configuration loaded successfully"}
{"level":"INFO","ts":"2025-12-09T05:44:29.429Z","msg":"Operating in MODEL-ONLY mode: model-based optimization only"}
{"level":"INFO","ts":"2025-12-09T05:44:29.429Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-12-09T05:44:29.429Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-12-09T05:44:29.442Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-666b5f4cq54t2, queueLength=0"}
{"level":"DEBUG","ts":"2025-12-09T05:44:29.442Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"INFO","ts":"2025-12-09T05:44:29.442Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-666b5f4cq54t2, usage=0.037 (3.7%)"}
{"level":"DEBUG","ts":"2025-12-09T05:44:29.442Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"DEBUG","ts":"2025-12-09T05:44:29.444Z","msg":"Pod-to-variant matching successful: totalPods=1, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"DEBUG","ts":"2025-12-09T05:44:29.444Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=1"}
{"level":"DEBUG","ts":"2025-12-09T05:44:29.444Z","msg":"Collected capacity metrics for observability in model-only mode: modelID=unsloth/Meta-Llama-3.1-8B, replicaCount=1"}
{"level":"DEBUG","ts":"2025-12-09T05:44:29.456Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=1, accelerator=H100, ttft=18.87ms, itl=9.58ms, cost=100.00, maxBatch=256, arrivalRate=404.83, avgInputTokens=247.33, avgOutputTokens=454.86"}
{"level":"DEBUG","ts":"2025-12-09T05:44:29.456Z","msg":"Experimental model tuner is enabled globally: tuning model performance parameters for active VAs"}
{"level":"DEBUG","ts":"2025-12-09T05:44:29.456Z","msg":"Using state vals from VA status to tune variant ms-inference-scheduling-llm-d-modelservice-decode: alpha= 8.036217, beta= 0.049568, gamma= 17.288605, delta= 0.000245"}
{"level":"DEBUG","ts":"2025-12-09T05:44:29.456Z","msg":"[Tuner Config] variant=ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler | Initial state: alpha=8.036217, beta=0.049568, gamma=17.288605, delta=0.000245 | SLO targets: TTFT=1000.00ms, ITL=10.00ms | Has covariance=true"}
{"level":"INFO","ts":"2025-12-09T05:44:29.456Z","msg":"Tuner validation succeeded for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler, server ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler - New NIS: 0.005317"}
{"level":"DEBUG","ts":"2025-12-09T05:44:29.456Z","msg":"Model tuner results: model=unsloth/Meta-Llama-3.1-8B, accelerator=H100, alpha=8.068020, beta=0.049499, gamma=17.279982, delta=0.000245, NIS=0.01"}
{"level":"DEBUG","ts":"2025-12-09T05:44:29.456Z","msg":"Updated tuner status for variant ms-inference-scheduling-llm-d-modelservice-decode: alpha=8.068020, beta=0.049499, gamma=17.279982, delta=0.000245, NIS=0.005317"}
{"level":"INFO","ts":"2025-12-09T05:44:29.456Z","msg":"Tuned performance parameters result: variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler - alpha: 8.068020, beta: 0.049499, gamma: 17.279982, delta: 0.000245"}
{"level":"DEBUG","ts":"2025-12-09T05:44:29.465Z","msg":"Optimization solution - system: Solution: \ns=ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler; c=Premium; m=unsloth/Meta-Llama-3.1-8B; rate=404.83; inTk=247; outTk=454; sol=1, sat=false, alloc={acc=H100; numRep=1; maxBatch=512; cost=100, val=0, itl=9.572076, ttft=19.11877, rho=0.057393998, maxRPM=501.54425}; slo-itl=10, slo-ttft=1000, slo-tps=0 \nAllocationByType: \nname=NVIDIA-H100-80GB-HBM3, count=1, limit=0, cost=100 \ntotalCost=100 \n"}
{"level":"DEBUG","ts":"2025-12-09T05:44:29.465Z","msg":"Setting accelerator name: Name=H100, allocationData={H100 1 512 100 9.572076 19.11877 {404.83 247 454}}"}
{"level":"INFO","ts":"2025-12-09T05:44:29.465Z","msg":"Model-based optimization completed for model: unsloth/Meta-Llama-3.1-8B - model-based targets: map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"WARN","ts":"2025-12-09T05:44:29.465Z","msg":"saturation analysis unavailable, using model-based targets only: modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-09T05:44:29.465Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-12-09T05:44:29.465Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1→target=1"}
{"level":"INFO","ts":"2025-12-09T05:44:29.465Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 1, desired-replicas: 1, accelerator: H100"}
{"level":"INFO","ts":"2025-12-09T05:44:29.465Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=1, accelerator=H100, SaturationOnly=false"}
{"level":"INFO","ts":"2025-12-09T05:44:29.471Z","msg":"Applied Saturation decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1, target=1, reason=model-based only (Saturation unavailable)"}
{"level":"INFO","ts":"2025-12-09T05:44:29.471Z","msg":"Reconciliation completed successfully: mode=model-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-12-09T05:45:29.472Z","msg":"Reconciling VariantAutoscaling: name=ms-inference-scheduling-llm-d-modelservice-decode, namespace=llm-d-inference-scheduler, modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-09T05:45:29.472Z","msg":"saturation scaling config cache updated: entries=1, has_default=true"}
{"level":"INFO","ts":"2025-12-09T05:45:29.472Z","msg":"saturation scaling configuration loaded successfully"}
{"level":"INFO","ts":"2025-12-09T05:45:29.472Z","msg":"Operating in MODEL-ONLY mode: model-based optimization only"}
{"level":"INFO","ts":"2025-12-09T05:45:29.472Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-12-09T05:45:29.472Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-12-09T05:45:29.479Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-666b5f4cq54t2, queueLength=0"}
{"level":"DEBUG","ts":"2025-12-09T05:45:29.479Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"INFO","ts":"2025-12-09T05:45:29.489Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-666b5f4cq54t2, usage=0.047 (4.7%)"}
{"level":"DEBUG","ts":"2025-12-09T05:45:29.489Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"DEBUG","ts":"2025-12-09T05:45:29.491Z","msg":"Pod-to-variant matching successful: totalPods=1, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"DEBUG","ts":"2025-12-09T05:45:29.491Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=1"}
{"level":"DEBUG","ts":"2025-12-09T05:45:29.491Z","msg":"Collected capacity metrics for observability in model-only mode: modelID=unsloth/Meta-Llama-3.1-8B, replicaCount=1"}
{"level":"DEBUG","ts":"2025-12-09T05:45:29.503Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=1, accelerator=H100, ttft=18.87ms, itl=9.52ms, cost=100.00, maxBatch=256, arrivalRate=417.24, avgInputTokens=237.38, avgOutputTokens=440.07"}
{"level":"DEBUG","ts":"2025-12-09T05:45:29.503Z","msg":"Experimental model tuner is enabled globally: tuning model performance parameters for active VAs"}
{"level":"DEBUG","ts":"2025-12-09T05:45:29.503Z","msg":"Using state vals from VA status to tune variant ms-inference-scheduling-llm-d-modelservice-decode: alpha= 8.068020, beta= 0.049499, gamma= 17.279982, delta= 0.000245"}
{"level":"DEBUG","ts":"2025-12-09T05:45:29.503Z","msg":"[Tuner Config] variant=ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler | Initial state: alpha=8.068020, beta=0.049499, gamma=17.279982, delta=0.000245 | SLO targets: TTFT=1000.00ms, ITL=10.00ms | Has covariance=true"}
{"level":"INFO","ts":"2025-12-09T05:45:29.503Z","msg":"Tuner validation succeeded for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler, server ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler - New NIS: 0.007222"}
{"level":"DEBUG","ts":"2025-12-09T05:45:29.503Z","msg":"Model tuner results: model=unsloth/Meta-Llama-3.1-8B, accelerator=H100, alpha=8.033769, beta=0.049483, gamma=17.274466, delta=0.000245, NIS=0.01"}
{"level":"DEBUG","ts":"2025-12-09T05:45:29.503Z","msg":"Updated tuner status for variant ms-inference-scheduling-llm-d-modelservice-decode: alpha=8.033769, beta=0.049483, gamma=17.274466, delta=0.000245, NIS=0.007222"}
{"level":"INFO","ts":"2025-12-09T05:45:29.503Z","msg":"Tuned performance parameters result: variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler - alpha: 8.033769, beta: 0.049483, gamma: 17.274466, delta: 0.000245"}
{"level":"DEBUG","ts":"2025-12-09T05:45:29.514Z","msg":"Optimization solution - system: Solution: \ns=ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler; c=Premium; m=unsloth/Meta-Llama-3.1-8B; rate=417.24; inTk=237; outTk=440; sol=1, sat=false, alloc={acc=H100; numRep=1; maxBatch=512; cost=100, val=0, itl=9.529303, ttft=19.029387, rho=0.057077035, maxRPM=527.0665}; slo-itl=10, slo-ttft=1000, slo-tps=0 \nAllocationByType: \nname=NVIDIA-H100-80GB-HBM3, count=1, limit=0, cost=100 \ntotalCost=100 \n"}
{"level":"DEBUG","ts":"2025-12-09T05:45:29.514Z","msg":"Setting accelerator name: Name=H100, allocationData={H100 1 512 100 9.529303 19.029387 {417.24 237 440}}"}
{"level":"INFO","ts":"2025-12-09T05:45:29.514Z","msg":"Model-based optimization completed for model: unsloth/Meta-Llama-3.1-8B - model-based targets: map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"WARN","ts":"2025-12-09T05:45:29.514Z","msg":"saturation analysis unavailable, using model-based targets only: modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-09T05:45:29.514Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-12-09T05:45:29.514Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1→target=1"}
{"level":"INFO","ts":"2025-12-09T05:45:29.514Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 1, desired-replicas: 1, accelerator: H100"}
{"level":"INFO","ts":"2025-12-09T05:45:29.514Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=1, accelerator=H100, SaturationOnly=false"}
{"level":"INFO","ts":"2025-12-09T05:45:29.519Z","msg":"Applied Saturation decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1, target=1, reason=model-based only (Saturation unavailable)"}
{"level":"INFO","ts":"2025-12-09T05:45:29.519Z","msg":"Reconciliation completed successfully: mode=model-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-12-09T05:46:29.520Z","msg":"Reconciling VariantAutoscaling: name=ms-inference-scheduling-llm-d-modelservice-decode, namespace=llm-d-inference-scheduler, modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-09T05:46:29.521Z","msg":"saturation scaling config cache updated: entries=1, has_default=true"}
{"level":"INFO","ts":"2025-12-09T05:46:29.521Z","msg":"saturation scaling configuration loaded successfully"}
{"level":"INFO","ts":"2025-12-09T05:46:29.521Z","msg":"Operating in MODEL-ONLY mode: model-based optimization only"}
{"level":"INFO","ts":"2025-12-09T05:46:29.521Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-12-09T05:46:29.521Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-12-09T05:46:29.530Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-666b5f4cq54t2, queueLength=0"}
{"level":"DEBUG","ts":"2025-12-09T05:46:29.530Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"INFO","ts":"2025-12-09T05:46:29.540Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-666b5f4cq54t2, usage=0.034 (3.4%)"}
{"level":"DEBUG","ts":"2025-12-09T05:46:29.540Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"DEBUG","ts":"2025-12-09T05:46:29.543Z","msg":"Pod-to-variant matching successful: totalPods=1, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"DEBUG","ts":"2025-12-09T05:46:29.543Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=1"}
{"level":"DEBUG","ts":"2025-12-09T05:46:29.543Z","msg":"Collected capacity metrics for observability in model-only mode: modelID=unsloth/Meta-Llama-3.1-8B, replicaCount=1"}
{"level":"DEBUG","ts":"2025-12-09T05:46:29.558Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=1, accelerator=H100, ttft=17.83ms, itl=9.33ms, cost=100.00, maxBatch=256, arrivalRate=83.45, avgInputTokens=233.08, avgOutputTokens=493.28"}
{"level":"DEBUG","ts":"2025-12-09T05:46:29.558Z","msg":"Experimental model tuner is enabled globally: tuning model performance parameters for active VAs"}
{"level":"DEBUG","ts":"2025-12-09T05:46:29.558Z","msg":"Using state vals from VA status to tune variant ms-inference-scheduling-llm-d-modelservice-decode: alpha= 8.033769, beta= 0.049483, gamma= 17.274466, delta= 0.000245"}
{"level":"DEBUG","ts":"2025-12-09T05:46:29.558Z","msg":"[Tuner Config] variant=ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler | Initial state: alpha=8.033769, beta=0.049483, gamma=17.274466, delta=0.000245 | SLO targets: TTFT=1000.00ms, ITL=10.00ms | Has covariance=true"}
{"level":"INFO","ts":"2025-12-09T05:46:29.559Z","msg":"Tuner validation succeeded for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler, server ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler - New NIS: 2.659512"}
{"level":"DEBUG","ts":"2025-12-09T05:46:29.559Z","msg":"Model tuner results: model=unsloth/Meta-Llama-3.1-8B, accelerator=H100, alpha=8.841813, beta=0.041748, gamma=17.266930, delta=0.000245, NIS=2.66"}
{"level":"DEBUG","ts":"2025-12-09T05:46:29.559Z","msg":"Updated tuner status for variant ms-inference-scheduling-llm-d-modelservice-decode: alpha=8.841813, beta=0.041748, gamma=17.266930, delta=0.000245, NIS=2.659512"}
{"level":"INFO","ts":"2025-12-09T05:46:29.559Z","msg":"Tuned performance parameters result: variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler - alpha: 8.841813, beta: 0.041748, gamma: 17.266930, delta: 0.000245"}
{"level":"DEBUG","ts":"2025-12-09T05:46:29.568Z","msg":"Optimization solution - system: Solution: \ns=ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler; c=Premium; m=unsloth/Meta-Llama-3.1-8B; rate=83.45; inTk=233; outTk=493; sol=1, sat=false, alloc={acc=H100; numRep=1; maxBatch=512; cost=100, val=0, itl=9.145861, ttft=17.68268, rho=0.012271513, maxRPM=324.88385}; slo-itl=10, slo-ttft=1000, slo-tps=0 \nAllocationByType: \nname=NVIDIA-H100-80GB-HBM3, count=1, limit=0, cost=100 \ntotalCost=100 \n"}
{"level":"DEBUG","ts":"2025-12-09T05:46:29.568Z","msg":"Setting accelerator name: Name=H100, allocationData={H100 1 512 100 9.145861 17.68268 {83.45 233 493}}"}
{"level":"INFO","ts":"2025-12-09T05:46:29.568Z","msg":"Model-based optimization completed for model: unsloth/Meta-Llama-3.1-8B - model-based targets: map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"WARN","ts":"2025-12-09T05:46:29.568Z","msg":"saturation analysis unavailable, using model-based targets only: modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-09T05:46:29.568Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-12-09T05:46:29.568Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1→target=1"}
{"level":"INFO","ts":"2025-12-09T05:46:29.568Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 1, desired-replicas: 1, accelerator: H100"}
{"level":"INFO","ts":"2025-12-09T05:46:29.568Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=1, accelerator=H100, SaturationOnly=false"}
{"level":"INFO","ts":"2025-12-09T05:46:29.574Z","msg":"Applied Saturation decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1, target=1, reason=model-based only (Saturation unavailable)"}
{"level":"INFO","ts":"2025-12-09T05:46:29.574Z","msg":"Reconciliation completed successfully: mode=model-only, modelsProcessed=1, decisionsApplied=1"}
