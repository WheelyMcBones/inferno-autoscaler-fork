{"level":"INFO","ts":"2025-12-10T04:25:38.993Z","msg":"Zap logger initialized"}
{"level":"INFO","ts":"2025-12-10T04:25:39.070Z","msg":"Creating metrics emitter instance"}
{"level":"INFO","ts":"2025-12-10T04:25:39.070Z","msg":"Metrics emitter created successfully"}
{"level":"INFO","ts":"2025-12-10T04:25:39.070Z","msg":"Using Prometheus configuration from environment variables: address=https://thanos-querier.openshift-monitoring.svc.cluster.local:9091"}
{"level":"INFO","ts":"2025-12-10T04:25:39.070Z","msg":"Initializing Prometheus client -> address: https://thanos-querier.openshift-monitoring.svc.cluster.local:9091, tls_enabled: true"}
{"level":"INFO","ts":"2025-12-10T04:25:39.070Z","msg":"CA certificate loaded successfullypath/etc/ssl/certs/prometheus-ca.crt"}
{"level":"INFO","ts":"2025-12-10T04:25:39.070Z","msg":"TLS configuration applied to Prometheus HTTPS transport"}
{"level":"INFO","ts":"2025-12-10T04:25:39.070Z","msg":"Bearer token loaded from filepath/var/run/secrets/kubernetes.io/serviceaccount/token"}
{"level":"INFO","ts":"2025-12-10T04:25:39.150Z","msg":"Prometheus API validation successful with queryqueryup"}
{"level":"INFO","ts":"2025-12-10T04:25:39.150Z","msg":"Prometheus client and API wrapper initialized and validated successfully"}
{"level":"INFO","ts":"2025-12-10T04:25:39.150Z","msg":"Starting manager"}
{"level":"INFO","ts":"2025-12-10T04:25:39.150Z","msg":"Registering custom metrics with Prometheus registry"}
{"level":"info","ts":"2025-12-10T04:25:39Z","logger":"controller-runtime.metrics","msg":"Starting metrics server"}
{"level":"INFO","ts":"2025-12-10T04:25:39.150Z","msg":"disabling http/2"}
{"level":"info","ts":"2025-12-10T04:25:39Z","msg":"starting server","name":"health probe","addr":"[::]:8081"}
I1210 04:25:39.151000       1 leaderelection.go:257] attempting to acquire leader lease workload-variant-autoscaler-system/72dd1cf1.llm-d.ai...
{"level":"info","ts":"2025-12-10T04:25:39Z","logger":"controller-runtime.metrics","msg":"Serving metrics server","bindAddress":":8443","secure":true}
I1210 04:25:52.475821       1 leaderelection.go:271] successfully acquired lease workload-variant-autoscaler-system/72dd1cf1.llm-d.ai
{"level":"info","ts":"2025-12-10T04:25:52Z","msg":"Starting EventSource","controller":"variantAutoscaling","controllerGroup":"llmd.ai","controllerKind":"VariantAutoscaling","source":"kind source: *v1.ServiceMonitor"}
{"level":"info","ts":"2025-12-10T04:25:52Z","msg":"Starting EventSource","controller":"variantAutoscaling","controllerGroup":"llmd.ai","controllerKind":"VariantAutoscaling","source":"kind source: *v1.ConfigMap"}
{"level":"info","ts":"2025-12-10T04:25:52Z","msg":"Starting EventSource","controller":"variantAutoscaling","controllerGroup":"llmd.ai","controllerKind":"VariantAutoscaling","source":"kind source: *v1alpha1.VariantAutoscaling"}
{"level":"info","ts":"2025-12-10T04:25:53Z","msg":"Starting Controller","controller":"variantAutoscaling","controllerGroup":"llmd.ai","controllerKind":"VariantAutoscaling"}
{"level":"info","ts":"2025-12-10T04:25:53Z","msg":"Starting workers","controller":"variantAutoscaling","controllerGroup":"llmd.ai","controllerKind":"VariantAutoscaling","worker count":1}
{"level":"INFO","ts":"2025-12-10T04:25:53.989Z","msg":"Reconciling VariantAutoscaling: name=ms-inference-scheduling-llm-d-modelservice-decode, namespace=llm-d-inference-scheduler, modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-10T04:25:53.989Z","msg":"saturation scaling config cache updated: entries=1, has_default=true"}
{"level":"INFO","ts":"2025-12-10T04:25:53.989Z","msg":"saturation scaling configuration loaded successfully"}
{"level":"INFO","ts":"2025-12-10T04:25:53.989Z","msg":"Operating in MODEL-ONLY mode: model-based optimization only"}
{"level":"INFO","ts":"2025-12-10T04:25:53.989Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-12-10T04:25:53.989Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-12-10T04:25:53.995Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dgftqv, queueLength=0"}
{"level":"DEBUG","ts":"2025-12-10T04:25:53.995Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"INFO","ts":"2025-12-10T04:25:53.995Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dgftqv, usage=0.000 (0.0%)"}
{"level":"DEBUG","ts":"2025-12-10T04:25:53.995Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"DEBUG","ts":"2025-12-10T04:25:54.298Z","msg":"Pod-to-variant matching successful: totalPods=1, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"DEBUG","ts":"2025-12-10T04:25:54.298Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=1"}
{"level":"DEBUG","ts":"2025-12-10T04:25:54.298Z","msg":"Collected capacity metrics for observability in model-only mode: modelID=unsloth/Meta-Llama-3.1-8B, replicaCount=1"}
{"level":"DEBUG","ts":"2025-12-10T04:25:54.315Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=1, accelerator=H100, ttft=0.00ms, itl=0.00ms, cost=100.00, maxBatch=256, arrivalRate=0.00, avgInputTokens=0.00, avgOutputTokens=0.00"}
{"level":"DEBUG","ts":"2025-12-10T04:25:54.315Z","msg":"Experimental model tuner is enabled globally: tuning model performance parameters for active VAs"}
{"level":"DEBUG","ts":"2025-12-10T04:25:54.315Z","msg":"Using state vals from VA status to tune variant ms-inference-scheduling-llm-d-modelservice-decode: alpha= 7.062454, beta= 0.055659, gamma= 16.918692, delta= 0.000256"}
{"level":"DEBUG","ts":"2025-12-10T04:25:54.315Z","msg":"Skipping tuning for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler due to insufficient load. Using fallback parameters."}
{"level":"INFO","ts":"2025-12-10T04:25:54.315Z","msg":"Using tuned parameters from status for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler"}
{"level":"DEBUG","ts":"2025-12-10T04:25:54.315Z","msg":"Updated SystemData for model=unsloth/Meta-Llama-3.1-8B, accelerator=H100: alpha=7.062454, beta=0.055659, gamma=16.918692, delta=0.000256"}
{"level":"DEBUG","ts":"2025-12-10T04:25:54.315Z","msg":"Optimization solution - system: Solution: \ns=ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler; c=Premium; m=unsloth/Meta-Llama-3.1-8B; rate=0; inTk=0; outTk=0; sol=1, sat=false, alloc={acc=H100; numRep=1; maxBatch=512; cost=100, val=0, itl=7.118113, ttft=16.918947, rho=0, maxRPM=585379.25}; slo-itl=10, slo-ttft=1000, slo-tps=0 \nAllocationByType: \nname=NVIDIA-H100-80GB-HBM3, count=1, limit=0, cost=100 \ntotalCost=100 \n"}
{"level":"DEBUG","ts":"2025-12-10T04:25:54.315Z","msg":"Setting accelerator name: Name=H100, allocationData={H100 1 512 100 7.118113 16.918947 {0 0 0}}"}
{"level":"INFO","ts":"2025-12-10T04:25:54.315Z","msg":"Model-based optimization completed for model: unsloth/Meta-Llama-3.1-8B - model-based targets: map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"WARN","ts":"2025-12-10T04:25:54.315Z","msg":"saturation analysis unavailable, using model-based targets only: modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-10T04:25:54.315Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-12-10T04:25:54.315Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1→target=1"}
{"level":"INFO","ts":"2025-12-10T04:25:54.315Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 1, desired-replicas: 1, accelerator: H100"}
{"level":"INFO","ts":"2025-12-10T04:25:54.315Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=1, accelerator=H100, SaturationOnly=false"}
{"level":"INFO","ts":"2025-12-10T04:25:54.322Z","msg":"Applied Saturation decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1, target=1, reason=model-based only (Saturation unavailable)"}
{"level":"INFO","ts":"2025-12-10T04:25:54.322Z","msg":"Reconciliation completed successfully: mode=model-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-12-10T04:25:54.322Z","msg":"VariantAutoscaling resource not found, may have been deleted: name=, namespace="}
{"level":"INFO","ts":"2025-12-10T04:26:54.323Z","msg":"Reconciling VariantAutoscaling: name=ms-inference-scheduling-llm-d-modelservice-decode, namespace=llm-d-inference-scheduler, modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-10T04:26:54.323Z","msg":"saturation scaling config cache updated: entries=1, has_default=true"}
{"level":"INFO","ts":"2025-12-10T04:26:54.323Z","msg":"saturation scaling configuration loaded successfully"}
{"level":"INFO","ts":"2025-12-10T04:26:54.323Z","msg":"Operating in MODEL-ONLY mode: model-based optimization only"}
{"level":"INFO","ts":"2025-12-10T04:26:54.324Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-12-10T04:26:54.324Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-12-10T04:26:54.330Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dgftqv, usage=0.000 (0.0%)"}
{"level":"DEBUG","ts":"2025-12-10T04:26:54.330Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"INFO","ts":"2025-12-10T04:26:54.330Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dgftqv, queueLength=0"}
{"level":"DEBUG","ts":"2025-12-10T04:26:54.330Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"DEBUG","ts":"2025-12-10T04:26:54.332Z","msg":"Pod-to-variant matching successful: totalPods=1, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"DEBUG","ts":"2025-12-10T04:26:54.332Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=1"}
{"level":"DEBUG","ts":"2025-12-10T04:26:54.332Z","msg":"Collected capacity metrics for observability in model-only mode: modelID=unsloth/Meta-Llama-3.1-8B, replicaCount=1"}
{"level":"DEBUG","ts":"2025-12-10T04:26:54.352Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=1, accelerator=H100, ttft=0.00ms, itl=0.00ms, cost=100.00, maxBatch=256, arrivalRate=0.00, avgInputTokens=0.00, avgOutputTokens=0.00"}
{"level":"DEBUG","ts":"2025-12-10T04:26:54.352Z","msg":"Experimental model tuner is enabled globally: tuning model performance parameters for active VAs"}
{"level":"DEBUG","ts":"2025-12-10T04:26:54.352Z","msg":"Using state vals from VA status to tune variant ms-inference-scheduling-llm-d-modelservice-decode: alpha= 7.062454, beta= 0.055659, gamma= 16.918692, delta= 0.000256"}
{"level":"DEBUG","ts":"2025-12-10T04:26:54.352Z","msg":"Skipping tuning for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler due to insufficient load. Using fallback parameters."}
{"level":"INFO","ts":"2025-12-10T04:26:54.352Z","msg":"Using tuned parameters from status for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler"}
{"level":"DEBUG","ts":"2025-12-10T04:26:54.352Z","msg":"Updated SystemData for model=unsloth/Meta-Llama-3.1-8B, accelerator=H100: alpha=7.062454, beta=0.055659, gamma=16.918692, delta=0.000256"}
{"level":"DEBUG","ts":"2025-12-10T04:26:54.352Z","msg":"Optimization solution - system: Solution: \ns=ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler; c=Premium; m=unsloth/Meta-Llama-3.1-8B; rate=0; inTk=0; outTk=0; sol=1, sat=false, alloc={acc=H100; numRep=1; maxBatch=512; cost=100, val=0, itl=7.118113, ttft=16.918947, rho=0, maxRPM=585379.25}; slo-itl=10, slo-ttft=1000, slo-tps=0 \nAllocationByType: \nname=NVIDIA-H100-80GB-HBM3, count=1, limit=0, cost=100 \ntotalCost=100 \n"}
{"level":"DEBUG","ts":"2025-12-10T04:26:54.352Z","msg":"Setting accelerator name: Name=H100, allocationData={H100 1 512 100 7.118113 16.918947 {0 0 0}}"}
{"level":"INFO","ts":"2025-12-10T04:26:54.352Z","msg":"Model-based optimization completed for model: unsloth/Meta-Llama-3.1-8B - model-based targets: map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"WARN","ts":"2025-12-10T04:26:54.352Z","msg":"saturation analysis unavailable, using model-based targets only: modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-10T04:26:54.352Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-12-10T04:26:54.352Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1→target=1"}
{"level":"INFO","ts":"2025-12-10T04:26:54.352Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 1, desired-replicas: 1, accelerator: H100"}
{"level":"INFO","ts":"2025-12-10T04:26:54.352Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=1, accelerator=H100, SaturationOnly=false"}
{"level":"INFO","ts":"2025-12-10T04:26:54.358Z","msg":"Applied Saturation decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1, target=1, reason=model-based only (Saturation unavailable)"}
{"level":"INFO","ts":"2025-12-10T04:26:54.358Z","msg":"Reconciliation completed successfully: mode=model-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-12-10T04:27:54.359Z","msg":"Reconciling VariantAutoscaling: name=ms-inference-scheduling-llm-d-modelservice-decode, namespace=llm-d-inference-scheduler, modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-10T04:27:54.359Z","msg":"saturation scaling config cache updated: entries=1, has_default=true"}
{"level":"INFO","ts":"2025-12-10T04:27:54.359Z","msg":"saturation scaling configuration loaded successfully"}
{"level":"INFO","ts":"2025-12-10T04:27:54.359Z","msg":"Operating in MODEL-ONLY mode: model-based optimization only"}
{"level":"INFO","ts":"2025-12-10T04:27:54.359Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-12-10T04:27:54.359Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-12-10T04:27:54.368Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dgftqv, queueLength=0"}
{"level":"DEBUG","ts":"2025-12-10T04:27:54.368Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"INFO","ts":"2025-12-10T04:27:54.368Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dgftqv, usage=0.037 (3.7%)"}
{"level":"DEBUG","ts":"2025-12-10T04:27:54.368Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"DEBUG","ts":"2025-12-10T04:27:54.370Z","msg":"Pod-to-variant matching successful: totalPods=1, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"DEBUG","ts":"2025-12-10T04:27:54.370Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=1"}
{"level":"DEBUG","ts":"2025-12-10T04:27:54.370Z","msg":"Collected capacity metrics for observability in model-only mode: modelID=unsloth/Meta-Llama-3.1-8B, replicaCount=1"}
{"level":"DEBUG","ts":"2025-12-10T04:27:54.403Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=1, accelerator=H100, ttft=17.35ms, itl=8.07ms, cost=100.00, maxBatch=256, arrivalRate=223.74, avgInputTokens=278.64, avgOutputTokens=379.91"}
{"level":"DEBUG","ts":"2025-12-10T04:27:54.403Z","msg":"Experimental model tuner is enabled globally: tuning model performance parameters for active VAs"}
{"level":"DEBUG","ts":"2025-12-10T04:27:54.403Z","msg":"Using state vals from VA status to tune variant ms-inference-scheduling-llm-d-modelservice-decode: alpha= 7.062454, beta= 0.055659, gamma= 16.918692, delta= 0.000256"}
{"level":"DEBUG","ts":"2025-12-10T04:27:54.403Z","msg":"[Tuner Config] variant=ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler | Initial state: alpha=7.062454, beta=0.055659, gamma=16.918692, delta=0.000256 | Expected obs (for R): TTFT=17.35ms, ITL=8.07ms | Has covariance=true"}
{"level":"INFO","ts":"2025-12-10T04:27:54.404Z","msg":"Tuner validation succeeded for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler, server ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler - New NIS: 0.493668"}
{"level":"DEBUG","ts":"2025-12-10T04:27:54.404Z","msg":"Model tuner results: model=unsloth/Meta-Llama-3.1-8B, accelerator=H100, alpha=7.317697, beta=0.056041, gamma=16.475029, delta=0.000256, NIS=0.49"}
{"level":"DEBUG","ts":"2025-12-10T04:27:54.404Z","msg":"Updated tuner status for variant ms-inference-scheduling-llm-d-modelservice-decode: alpha=7.317697, beta=0.056041, gamma=16.475029, delta=0.000256, NIS=0.493668"}
{"level":"INFO","ts":"2025-12-10T04:27:54.404Z","msg":"Tuned performance parameters result: variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler - alpha: 7.317697, beta: 0.056041, gamma: 16.475029, delta: 0.000256"}
{"level":"DEBUG","ts":"2025-12-10T04:27:54.414Z","msg":"Optimization solution - system: Solution: \ns=ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler; c=Premium; m=unsloth/Meta-Llama-3.1-8B; rate=223.74; inTk=278; outTk=379; sol=1, sat=false, alloc={acc=H100; numRep=1; maxBatch=512; cost=100, val=0, itl=8.010105, ttft=17.3543, rho=0.022178622, maxRPM=739.97375}; slo-itl=10, slo-ttft=1000, slo-tps=0 \nAllocationByType: \nname=NVIDIA-H100-80GB-HBM3, count=1, limit=0, cost=100 \ntotalCost=100 \n"}
{"level":"DEBUG","ts":"2025-12-10T04:27:54.414Z","msg":"Setting accelerator name: Name=H100, allocationData={H100 1 512 100 8.010105 17.3543 {223.74 278 379}}"}
{"level":"INFO","ts":"2025-12-10T04:27:54.414Z","msg":"Model-based optimization completed for model: unsloth/Meta-Llama-3.1-8B - model-based targets: map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"WARN","ts":"2025-12-10T04:27:54.414Z","msg":"saturation analysis unavailable, using model-based targets only: modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-10T04:27:54.414Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-12-10T04:27:54.414Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1→target=1"}
{"level":"INFO","ts":"2025-12-10T04:27:54.414Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 1, desired-replicas: 1, accelerator: H100"}
{"level":"INFO","ts":"2025-12-10T04:27:54.414Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=1, accelerator=H100, SaturationOnly=false"}
{"level":"INFO","ts":"2025-12-10T04:27:54.420Z","msg":"Applied Saturation decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1, target=1, reason=model-based only (Saturation unavailable)"}
{"level":"INFO","ts":"2025-12-10T04:27:54.420Z","msg":"Reconciliation completed successfully: mode=model-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-12-10T04:28:54.420Z","msg":"Reconciling VariantAutoscaling: name=ms-inference-scheduling-llm-d-modelservice-decode, namespace=llm-d-inference-scheduler, modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-10T04:28:54.420Z","msg":"saturation scaling config cache updated: entries=1, has_default=true"}
{"level":"INFO","ts":"2025-12-10T04:28:54.420Z","msg":"saturation scaling configuration loaded successfully"}
{"level":"INFO","ts":"2025-12-10T04:28:54.420Z","msg":"Operating in MODEL-ONLY mode: model-based optimization only"}
{"level":"INFO","ts":"2025-12-10T04:28:54.420Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-12-10T04:28:54.420Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-12-10T04:28:54.425Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dgftqv, queueLength=0"}
{"level":"DEBUG","ts":"2025-12-10T04:28:54.425Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"INFO","ts":"2025-12-10T04:28:54.425Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dgftqv, usage=0.112 (11.2%)"}
{"level":"DEBUG","ts":"2025-12-10T04:28:54.425Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"DEBUG","ts":"2025-12-10T04:28:54.427Z","msg":"Pod-to-variant matching successful: totalPods=1, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"DEBUG","ts":"2025-12-10T04:28:54.427Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=1"}
{"level":"DEBUG","ts":"2025-12-10T04:28:54.427Z","msg":"Collected capacity metrics for observability in model-only mode: modelID=unsloth/Meta-Llama-3.1-8B, replicaCount=1"}
{"level":"DEBUG","ts":"2025-12-10T04:28:54.438Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=1, accelerator=H100, ttft=17.95ms, itl=9.43ms, cost=100.00, maxBatch=256, arrivalRate=626.21, avgInputTokens=261.78, avgOutputTokens=412.57"}
{"level":"DEBUG","ts":"2025-12-10T04:28:54.439Z","msg":"Experimental model tuner is enabled globally: tuning model performance parameters for active VAs"}
{"level":"DEBUG","ts":"2025-12-10T04:28:54.439Z","msg":"Using state vals from VA status to tune variant ms-inference-scheduling-llm-d-modelservice-decode: alpha= 7.317697, beta= 0.056041, gamma= 16.475029, delta= 0.000256"}
{"level":"DEBUG","ts":"2025-12-10T04:28:54.439Z","msg":"[Tuner Config] variant=ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler | Initial state: alpha=7.317697, beta=0.056041, gamma=16.475029, delta=0.000256 | Expected obs (for R): TTFT=17.95ms, ITL=9.43ms | Has covariance=true"}
{"level":"INFO","ts":"2025-12-10T04:28:54.439Z","msg":"Tuner validation succeeded for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler, server ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler - New NIS: 1.765544"}
{"level":"DEBUG","ts":"2025-12-10T04:28:54.439Z","msg":"Model tuner results: model=unsloth/Meta-Llama-3.1-8B, accelerator=H100, alpha=7.215838, beta=0.053528, gamma=15.429136, delta=0.000255, NIS=1.77"}
{"level":"DEBUG","ts":"2025-12-10T04:28:54.439Z","msg":"Updated tuner status for variant ms-inference-scheduling-llm-d-modelservice-decode: alpha=7.215838, beta=0.053528, gamma=15.429136, delta=0.000255, NIS=1.765544"}
{"level":"INFO","ts":"2025-12-10T04:28:54.439Z","msg":"Tuned performance parameters result: variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler - alpha: 7.215838, beta: 0.053528, gamma: 15.429136, delta: 0.000255"}
{"level":"DEBUG","ts":"2025-12-10T04:28:54.449Z","msg":"Optimization solution - system: Solution: \ns=ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler; c=Premium; m=unsloth/Meta-Llama-3.1-8B; rate=626.21; inTk=261; outTk=412; sol=1, sat=false, alloc={acc=H100; numRep=1; maxBatch=512; cost=100, val=0, itl=9.449169, ttft=18.205833, rho=0.07953631, maxRPM=741.3093}; slo-itl=10, slo-ttft=1000, slo-tps=0 \nAllocationByType: \nname=NVIDIA-H100-80GB-HBM3, count=1, limit=0, cost=100 \ntotalCost=100 \n"}
{"level":"DEBUG","ts":"2025-12-10T04:28:54.449Z","msg":"Setting accelerator name: Name=H100, allocationData={H100 1 512 100 9.449169 18.205833 {626.21 261 412}}"}
{"level":"INFO","ts":"2025-12-10T04:28:54.449Z","msg":"Model-based optimization completed for model: unsloth/Meta-Llama-3.1-8B - model-based targets: map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"WARN","ts":"2025-12-10T04:28:54.449Z","msg":"saturation analysis unavailable, using model-based targets only: modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-10T04:28:54.449Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-12-10T04:28:54.449Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1→target=1"}
{"level":"INFO","ts":"2025-12-10T04:28:54.449Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 1, desired-replicas: 1, accelerator: H100"}
{"level":"INFO","ts":"2025-12-10T04:28:54.449Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=1, accelerator=H100, SaturationOnly=false"}
{"level":"INFO","ts":"2025-12-10T04:28:54.456Z","msg":"Applied Saturation decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1, target=1, reason=model-based only (Saturation unavailable)"}
{"level":"INFO","ts":"2025-12-10T04:28:54.456Z","msg":"Reconciliation completed successfully: mode=model-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-12-10T04:29:54.457Z","msg":"Reconciling VariantAutoscaling: name=ms-inference-scheduling-llm-d-modelservice-decode, namespace=llm-d-inference-scheduler, modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-10T04:29:54.457Z","msg":"saturation scaling config cache updated: entries=1, has_default=true"}
{"level":"INFO","ts":"2025-12-10T04:29:54.457Z","msg":"saturation scaling configuration loaded successfully"}
{"level":"INFO","ts":"2025-12-10T04:29:54.457Z","msg":"Operating in MODEL-ONLY mode: model-based optimization only"}
{"level":"INFO","ts":"2025-12-10T04:29:54.457Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-12-10T04:29:54.457Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-12-10T04:29:54.467Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dgftqv, usage=0.350 (35.0%)"}
{"level":"DEBUG","ts":"2025-12-10T04:29:54.467Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"INFO","ts":"2025-12-10T04:29:54.467Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dgftqv, queueLength=0"}
{"level":"DEBUG","ts":"2025-12-10T04:29:54.467Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"DEBUG","ts":"2025-12-10T04:29:54.470Z","msg":"Pod-to-variant matching successful: totalPods=1, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"DEBUG","ts":"2025-12-10T04:29:54.470Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=1"}
{"level":"DEBUG","ts":"2025-12-10T04:29:54.470Z","msg":"Collected capacity metrics for observability in model-only mode: modelID=unsloth/Meta-Llama-3.1-8B, replicaCount=1"}
{"level":"DEBUG","ts":"2025-12-10T04:29:54.484Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=1, accelerator=H100, ttft=31.97ms, itl=16.95ms, cost=100.00, maxBatch=256, arrivalRate=1187.56, avgInputTokens=272.99, avgOutputTokens=360.37"}
{"level":"DEBUG","ts":"2025-12-10T04:29:54.484Z","msg":"Experimental model tuner is enabled globally: tuning model performance parameters for active VAs"}
{"level":"DEBUG","ts":"2025-12-10T04:29:54.484Z","msg":"Using state vals from VA status to tune variant ms-inference-scheduling-llm-d-modelservice-decode: alpha= 7.215838, beta= 0.053528, gamma= 15.429136, delta= 0.000255"}
{"level":"DEBUG","ts":"2025-12-10T04:29:54.484Z","msg":"[Tuner Config] variant=ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler | Initial state: alpha=7.215838, beta=0.053528, gamma=15.429136, delta=0.000255 | Expected obs (for R): TTFT=31.97ms, ITL=16.95ms | Has covariance=true"}
{"level":"WARN","ts":"2025-12-10T04:29:54.485Z","msg":"Tuner validation failed (NIS=74.28), validation error: normalized innovation squared (NIS=74.28) exceeds threshold (7.38), rejecting update as outlier - returning previous state: alpha=7.215838, beta=0.053528, gamma=15.429136, delta=0.000255"}
{"level":"WARN","ts":"2025-12-10T04:29:54.485Z","msg":"Tuner NIS validation failed for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler, server ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler (NIS=74.28 exceeds threshold 7.38) - Keeping previous state: alpha=7.215838, beta=0.053528, gamma=15.429136, delta=0.000255"}
{"level":"INFO","ts":"2025-12-10T04:29:54.485Z","msg":"Keeping previous tuned parameters for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler due to NIS validation failure (NIS=74.282394)"}
{"level":"DEBUG","ts":"2025-12-10T04:29:54.485Z","msg":"Model tuner results: model=unsloth/Meta-Llama-3.1-8B, accelerator=H100, alpha=7.215838, beta=0.053528, gamma=15.429136, delta=0.000255, NIS=74.28"}
{"level":"DEBUG","ts":"2025-12-10T04:29:54.485Z","msg":"Updated tuner status for variant ms-inference-scheduling-llm-d-modelservice-decode: alpha=7.215838, beta=0.053528, gamma=15.429136, delta=0.000255, NIS=74.282394"}
{"level":"DEBUG","ts":"2025-12-10T04:29:54.489Z","msg":"Optimization solution - system: Solution: \ns=ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler; c=Premium; m=unsloth/Meta-Llama-3.1-8B; rate=1187.56; inTk=272; outTk=360; sol=1, sat=false, alloc={acc=H100; numRep=2; maxBatch=512; cost=200, val=100, itl=8.988042, ttft=17.725506, rho=0.06271091, maxRPM=848.0929}; slo-itl=10, slo-ttft=1000, slo-tps=0 \nAllocationByType: \nname=NVIDIA-H100-80GB-HBM3, count=2, limit=0, cost=200 \ntotalCost=200 \n"}
{"level":"DEBUG","ts":"2025-12-10T04:29:54.489Z","msg":"Setting accelerator name: Name=H100, allocationData={H100 2 512 200 8.988042 17.725506 {1187.56 272 360}}"}
{"level":"INFO","ts":"2025-12-10T04:29:54.489Z","msg":"Model-based optimization completed for model: unsloth/Meta-Llama-3.1-8B - model-based targets: map[ms-inference-scheduling-llm-d-modelservice-decode:2]"}
{"level":"WARN","ts":"2025-12-10T04:29:54.489Z","msg":"saturation analysis unavailable, using model-based targets only: modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-10T04:29:54.489Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-12-10T04:29:54.489Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=scale-up, current=1→target=2"}
{"level":"INFO","ts":"2025-12-10T04:29:54.489Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 1, desired-replicas: 2, accelerator: H100"}
{"level":"INFO","ts":"2025-12-10T04:29:54.489Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=2, accelerator=H100, SaturationOnly=false"}
{"level":"INFO","ts":"2025-12-10T04:29:54.495Z","msg":"Applied Saturation decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=scale-up, current=1, target=2, reason=model-based only (Saturation unavailable)"}
{"level":"INFO","ts":"2025-12-10T04:29:54.495Z","msg":"Reconciliation completed successfully: mode=model-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-12-10T04:30:54.496Z","msg":"Reconciling VariantAutoscaling: name=ms-inference-scheduling-llm-d-modelservice-decode, namespace=llm-d-inference-scheduler, modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-10T04:30:54.496Z","msg":"saturation scaling config cache updated: entries=1, has_default=true"}
{"level":"INFO","ts":"2025-12-10T04:30:54.496Z","msg":"saturation scaling configuration loaded successfully"}
{"level":"INFO","ts":"2025-12-10T04:30:54.496Z","msg":"Operating in MODEL-ONLY mode: model-based optimization only"}
{"level":"INFO","ts":"2025-12-10T04:30:54.496Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-12-10T04:30:54.496Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-12-10T04:30:54.506Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dgftqv, usage=0.499 (49.9%)"}
{"level":"DEBUG","ts":"2025-12-10T04:30:54.506Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"INFO","ts":"2025-12-10T04:30:54.516Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dgftqv, queueLength=0"}
{"level":"DEBUG","ts":"2025-12-10T04:30:54.516Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"DEBUG","ts":"2025-12-10T04:30:54.521Z","msg":"Pod-to-variant matching successful: totalPods=1, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"DEBUG","ts":"2025-12-10T04:30:54.521Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=1"}
{"level":"DEBUG","ts":"2025-12-10T04:30:54.521Z","msg":"Collected capacity metrics for observability in model-only mode: modelID=unsloth/Meta-Llama-3.1-8B, replicaCount=1"}
{"level":"DEBUG","ts":"2025-12-10T04:30:54.544Z","msg":"Replica count mismatch for llm-d-inference-scheduler/unsloth/Meta-Llama-3.1-8B: spec=2, reporting_metrics=1"}
{"level":"DEBUG","ts":"2025-12-10T04:30:54.544Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=1, accelerator=H100, ttft=47.73ms, itl=28.51ms, cost=100.00, maxBatch=256, arrivalRate=1596.65, avgInputTokens=245.10, avgOutputTokens=422.90"}
{"level":"DEBUG","ts":"2025-12-10T04:30:54.544Z","msg":"Experimental model tuner is enabled globally: tuning model performance parameters for active VAs"}
{"level":"DEBUG","ts":"2025-12-10T04:30:54.544Z","msg":"Using state vals from VA status to tune variant ms-inference-scheduling-llm-d-modelservice-decode: alpha= 7.215838, beta= 0.053528, gamma= 15.429136, delta= 0.000255"}
{"level":"DEBUG","ts":"2025-12-10T04:30:54.544Z","msg":"[Tuner Config] variant=ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler | Initial state: alpha=7.215838, beta=0.053528, gamma=15.429136, delta=0.000255 | Expected obs (for R): TTFT=47.73ms, ITL=28.51ms | Has covariance=true"}
{"level":"WARN","ts":"2025-12-10T04:30:54.544Z","msg":"Tuner validation failed (NIS=162.16), validation error: normalized innovation squared (NIS=162.16) exceeds threshold (7.38), rejecting update as outlier - returning previous state: alpha=7.215838, beta=0.053528, gamma=15.429136, delta=0.000255"}
{"level":"WARN","ts":"2025-12-10T04:30:54.544Z","msg":"Tuner NIS validation failed for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler, server ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler (NIS=162.16 exceeds threshold 7.38) - Keeping previous state: alpha=7.215838, beta=0.053528, gamma=15.429136, delta=0.000255"}
{"level":"INFO","ts":"2025-12-10T04:30:54.544Z","msg":"Keeping previous tuned parameters for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler due to NIS validation failure (NIS=162.157017)"}
{"level":"DEBUG","ts":"2025-12-10T04:30:54.544Z","msg":"Model tuner results: model=unsloth/Meta-Llama-3.1-8B, accelerator=H100, alpha=7.215838, beta=0.053528, gamma=15.429136, delta=0.000255, NIS=162.16"}
{"level":"DEBUG","ts":"2025-12-10T04:30:54.544Z","msg":"Updated tuner status for variant ms-inference-scheduling-llm-d-modelservice-decode: alpha=7.215838, beta=0.053528, gamma=15.429136, delta=0.000255, NIS=162.157017"}
{"level":"DEBUG","ts":"2025-12-10T04:30:54.554Z","msg":"Optimization solution - system: Solution: \ns=ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler; c=Premium; m=unsloth/Meta-Llama-3.1-8B; rate=1596.65; inTk=245; outTk=422; sol=1, sat=false, alloc={acc=H100; numRep=3; maxBatch=512; cost=300, val=200, itl=9.095964, ttft=17.62352, rho=0.06664879, maxRPM=723.81714}; slo-itl=10, slo-ttft=1000, slo-tps=0 \nAllocationByType: \nname=NVIDIA-H100-80GB-HBM3, count=3, limit=0, cost=300 \ntotalCost=300 \n"}
{"level":"DEBUG","ts":"2025-12-10T04:30:54.554Z","msg":"Setting accelerator name: Name=H100, allocationData={H100 3 512 300 9.095964 17.62352 {1596.65 245 422}}"}
{"level":"INFO","ts":"2025-12-10T04:30:54.554Z","msg":"Model-based optimization completed for model: unsloth/Meta-Llama-3.1-8B - model-based targets: map[ms-inference-scheduling-llm-d-modelservice-decode:3]"}
{"level":"WARN","ts":"2025-12-10T04:30:54.554Z","msg":"saturation analysis unavailable, using model-based targets only: modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-10T04:30:54.554Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-12-10T04:30:54.554Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=scale-up, current=1→target=3"}
{"level":"INFO","ts":"2025-12-10T04:30:54.554Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 1, desired-replicas: 3, accelerator: H100"}
{"level":"INFO","ts":"2025-12-10T04:30:54.554Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=3, accelerator=H100, SaturationOnly=false"}
{"level":"INFO","ts":"2025-12-10T04:30:54.561Z","msg":"Applied Saturation decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=scale-up, current=1, target=3, reason=model-based only (Saturation unavailable)"}
{"level":"INFO","ts":"2025-12-10T04:30:54.561Z","msg":"Reconciliation completed successfully: mode=model-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-12-10T04:31:54.561Z","msg":"Reconciling VariantAutoscaling: name=ms-inference-scheduling-llm-d-modelservice-decode, namespace=llm-d-inference-scheduler, modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-10T04:31:54.561Z","msg":"saturation scaling config cache updated: entries=1, has_default=true"}
{"level":"INFO","ts":"2025-12-10T04:31:54.561Z","msg":"saturation scaling configuration loaded successfully"}
{"level":"INFO","ts":"2025-12-10T04:31:54.561Z","msg":"Operating in MODEL-ONLY mode: model-based optimization only"}
{"level":"INFO","ts":"2025-12-10T04:31:54.561Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-12-10T04:31:54.561Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-12-10T04:31:54.568Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dgftqv, usage=0.683 (68.3%)"}
{"level":"DEBUG","ts":"2025-12-10T04:31:54.568Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"INFO","ts":"2025-12-10T04:31:54.568Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dgftqv, queueLength=0"}
{"level":"DEBUG","ts":"2025-12-10T04:31:54.568Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"DEBUG","ts":"2025-12-10T04:31:54.571Z","msg":"Pod-to-variant matching successful: totalPods=1, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"DEBUG","ts":"2025-12-10T04:31:54.571Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=1"}
{"level":"DEBUG","ts":"2025-12-10T04:31:54.571Z","msg":"Collected capacity metrics for observability in model-only mode: modelID=unsloth/Meta-Llama-3.1-8B, replicaCount=1"}
{"level":"DEBUG","ts":"2025-12-10T04:31:54.584Z","msg":"Replica count mismatch for llm-d-inference-scheduler/unsloth/Meta-Llama-3.1-8B: spec=3, reporting_metrics=1"}
{"level":"DEBUG","ts":"2025-12-10T04:31:54.584Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=1, accelerator=H100, ttft=62.40ms, itl=38.57ms, cost=100.00, maxBatch=256, arrivalRate=1586.05, avgInputTokens=233.66, avgOutputTokens=425.81"}
{"level":"DEBUG","ts":"2025-12-10T04:31:54.584Z","msg":"Experimental model tuner is enabled globally: tuning model performance parameters for active VAs"}
{"level":"DEBUG","ts":"2025-12-10T04:31:54.584Z","msg":"Using state vals from VA status to tune variant ms-inference-scheduling-llm-d-modelservice-decode: alpha= 7.215838, beta= 0.053528, gamma= 15.429136, delta= 0.000255"}
{"level":"DEBUG","ts":"2025-12-10T04:31:54.584Z","msg":"[Tuner Config] variant=ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler | Initial state: alpha=7.215838, beta=0.053528, gamma=15.429136, delta=0.000255 | Expected obs (for R): TTFT=62.40ms, ITL=38.57ms | Has covariance=true"}
{"level":"WARN","ts":"2025-12-10T04:31:54.585Z","msg":"Tuner validation failed (NIS=347.60), validation error: normalized innovation squared (NIS=347.60) exceeds threshold (7.38), rejecting update as outlier - returning previous state: alpha=7.215838, beta=0.053528, gamma=15.429136, delta=0.000255"}
{"level":"WARN","ts":"2025-12-10T04:31:54.585Z","msg":"Tuner NIS validation failed for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler, server ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler (NIS=347.60 exceeds threshold 7.38) - Keeping previous state: alpha=7.215838, beta=0.053528, gamma=15.429136, delta=0.000255"}
{"level":"INFO","ts":"2025-12-10T04:31:54.585Z","msg":"Keeping previous tuned parameters for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler due to NIS validation failure (NIS=347.597755)"}
{"level":"DEBUG","ts":"2025-12-10T04:31:54.585Z","msg":"Model tuner results: model=unsloth/Meta-Llama-3.1-8B, accelerator=H100, alpha=7.215838, beta=0.053528, gamma=15.429136, delta=0.000255, NIS=347.60"}
{"level":"DEBUG","ts":"2025-12-10T04:31:54.585Z","msg":"Updated tuner status for variant ms-inference-scheduling-llm-d-modelservice-decode: alpha=7.215838, beta=0.053528, gamma=15.429136, delta=0.000255, NIS=347.597755"}
{"level":"DEBUG","ts":"2025-12-10T04:31:54.595Z","msg":"Optimization solution - system: Solution: \ns=ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler; c=Premium; m=unsloth/Meta-Llama-3.1-8B; rate=1586.05; inTk=233; outTk=425; sol=1, sat=false, alloc={acc=H100; numRep=3; maxBatch=512; cost=300, val=200, itl=9.096831, ttft=17.517, rho=0.066680424, maxRPM=718.745}; slo-itl=10, slo-ttft=1000, slo-tps=0 \nAllocationByType: \nname=NVIDIA-H100-80GB-HBM3, count=3, limit=0, cost=300 \ntotalCost=300 \n"}
{"level":"DEBUG","ts":"2025-12-10T04:31:54.595Z","msg":"Setting accelerator name: Name=H100, allocationData={H100 3 512 300 9.096831 17.517 {1586.05 233 425}}"}
{"level":"INFO","ts":"2025-12-10T04:31:54.595Z","msg":"Model-based optimization completed for model: unsloth/Meta-Llama-3.1-8B - model-based targets: map[ms-inference-scheduling-llm-d-modelservice-decode:3]"}
{"level":"WARN","ts":"2025-12-10T04:31:54.595Z","msg":"saturation analysis unavailable, using model-based targets only: modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-10T04:31:54.595Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-12-10T04:31:54.595Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=scale-up, current=1→target=3"}
{"level":"INFO","ts":"2025-12-10T04:31:54.595Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 1, desired-replicas: 3, accelerator: H100"}
{"level":"INFO","ts":"2025-12-10T04:31:54.595Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=3, accelerator=H100, SaturationOnly=false"}
{"level":"INFO","ts":"2025-12-10T04:31:54.601Z","msg":"Applied Saturation decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=scale-up, current=1, target=3, reason=model-based only (Saturation unavailable)"}
{"level":"INFO","ts":"2025-12-10T04:31:54.601Z","msg":"Reconciliation completed successfully: mode=model-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-12-10T04:32:54.602Z","msg":"Reconciling VariantAutoscaling: name=ms-inference-scheduling-llm-d-modelservice-decode, namespace=llm-d-inference-scheduler, modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-10T04:32:54.602Z","msg":"saturation scaling config cache updated: entries=1, has_default=true"}
{"level":"INFO","ts":"2025-12-10T04:32:54.602Z","msg":"saturation scaling configuration loaded successfully"}
{"level":"INFO","ts":"2025-12-10T04:32:54.602Z","msg":"Operating in MODEL-ONLY mode: model-based optimization only"}
{"level":"INFO","ts":"2025-12-10T04:32:54.602Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-12-10T04:32:54.602Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-12-10T04:32:54.618Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dgftqv, queueLength=0"}
{"level":"INFO","ts":"2025-12-10T04:32:54.618Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984df55dt, queueLength=0"}
{"level":"DEBUG","ts":"2025-12-10T04:32:54.618Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=2"}
{"level":"INFO","ts":"2025-12-10T04:32:54.618Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dgftqv, usage=0.767 (76.7%)"}
{"level":"INFO","ts":"2025-12-10T04:32:54.618Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984df55dt, usage=0.063 (6.3%)"}
{"level":"DEBUG","ts":"2025-12-10T04:32:54.618Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=2"}
{"level":"DEBUG","ts":"2025-12-10T04:32:54.621Z","msg":"Pod-to-variant matching successful: totalPods=2, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:2]"}
{"level":"DEBUG","ts":"2025-12-10T04:32:54.621Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=2"}
{"level":"DEBUG","ts":"2025-12-10T04:32:54.621Z","msg":"Collected capacity metrics for observability in model-only mode: modelID=unsloth/Meta-Llama-3.1-8B, replicaCount=2"}
{"level":"DEBUG","ts":"2025-12-10T04:32:54.634Z","msg":"Replica count mismatch for llm-d-inference-scheduler/unsloth/Meta-Llama-3.1-8B: spec=3, reporting_metrics=2"}
{"level":"DEBUG","ts":"2025-12-10T04:32:54.634Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=2, accelerator=H100, ttft=71.96ms, itl=44.20ms, cost=200.00, maxBatch=256, arrivalRate=1600.59, avgInputTokens=214.58, avgOutputTokens=486.65"}
{"level":"DEBUG","ts":"2025-12-10T04:32:54.634Z","msg":"Experimental model tuner is enabled globally: tuning model performance parameters for active VAs"}
{"level":"DEBUG","ts":"2025-12-10T04:32:54.634Z","msg":"Using state vals from VA status to tune variant ms-inference-scheduling-llm-d-modelservice-decode: alpha= 7.215838, beta= 0.053528, gamma= 15.429136, delta= 0.000255"}
{"level":"DEBUG","ts":"2025-12-10T04:32:54.634Z","msg":"[Tuner Config] variant=ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler | Initial state: alpha=7.215838, beta=0.053528, gamma=15.429136, delta=0.000255 | Expected obs (for R): TTFT=71.96ms, ITL=44.20ms | Has covariance=true"}
{"level":"WARN","ts":"2025-12-10T04:32:54.636Z","msg":"Tuner validation failed (NIS=1140.14), validation error: normalized innovation squared (NIS=1140.14) exceeds threshold (7.38), rejecting update as outlier - returning previous state: alpha=7.215838, beta=0.053528, gamma=15.429136, delta=0.000255"}
{"level":"WARN","ts":"2025-12-10T04:32:54.636Z","msg":"Tuner NIS validation failed for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler, server ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler (NIS=1140.14 exceeds threshold 7.38) - Keeping previous state: alpha=7.215838, beta=0.053528, gamma=15.429136, delta=0.000255"}
{"level":"INFO","ts":"2025-12-10T04:32:54.636Z","msg":"Keeping previous tuned parameters for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler due to NIS validation failure (NIS=1140.142518)"}
{"level":"DEBUG","ts":"2025-12-10T04:32:54.636Z","msg":"Model tuner results: model=unsloth/Meta-Llama-3.1-8B, accelerator=H100, alpha=7.215838, beta=0.053528, gamma=15.429136, delta=0.000255, NIS=1140.14"}
{"level":"DEBUG","ts":"2025-12-10T04:32:54.636Z","msg":"Updated tuner status for variant ms-inference-scheduling-llm-d-modelservice-decode: alpha=7.215838, beta=0.053528, gamma=15.429136, delta=0.000255, NIS=1140.142518"}
{"level":"DEBUG","ts":"2025-12-10T04:32:54.646Z","msg":"Optimization solution - system: Solution: \ns=ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler; c=Premium; m=unsloth/Meta-Llama-3.1-8B; rate=1600.59; inTk=214; outTk=486; sol=1, sat=false, alloc={acc=H100; numRep=3; maxBatch=512; cost=300, val=100, itl=9.462136, ttft=17.719162, rho=0.080009624, maxRPM=628.7232}; slo-itl=10, slo-ttft=1000, slo-tps=0 \nAllocationByType: \nname=NVIDIA-H100-80GB-HBM3, count=3, limit=0, cost=300 \ntotalCost=300 \n"}
{"level":"DEBUG","ts":"2025-12-10T04:32:54.646Z","msg":"Setting accelerator name: Name=H100, allocationData={H100 3 512 300 9.462136 17.719162 {1600.59 214 486}}"}
{"level":"INFO","ts":"2025-12-10T04:32:54.646Z","msg":"Model-based optimization completed for model: unsloth/Meta-Llama-3.1-8B - model-based targets: map[ms-inference-scheduling-llm-d-modelservice-decode:3]"}
{"level":"WARN","ts":"2025-12-10T04:32:54.646Z","msg":"saturation analysis unavailable, using model-based targets only: modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-10T04:32:54.646Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-12-10T04:32:54.646Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=scale-up, current=2→target=3"}
{"level":"INFO","ts":"2025-12-10T04:32:54.646Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 2, desired-replicas: 3, accelerator: H100"}
{"level":"INFO","ts":"2025-12-10T04:32:54.646Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=3, accelerator=H100, SaturationOnly=false"}
{"level":"INFO","ts":"2025-12-10T04:32:54.653Z","msg":"Applied Saturation decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=scale-up, current=2, target=3, reason=model-based only (Saturation unavailable)"}
{"level":"INFO","ts":"2025-12-10T04:32:54.653Z","msg":"Reconciliation completed successfully: mode=model-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-12-10T04:33:54.653Z","msg":"Reconciling VariantAutoscaling: name=ms-inference-scheduling-llm-d-modelservice-decode, namespace=llm-d-inference-scheduler, modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-10T04:33:54.654Z","msg":"saturation scaling config cache updated: entries=1, has_default=true"}
{"level":"INFO","ts":"2025-12-10T04:33:54.654Z","msg":"saturation scaling configuration loaded successfully"}
{"level":"INFO","ts":"2025-12-10T04:33:54.654Z","msg":"Operating in MODEL-ONLY mode: model-based optimization only"}
{"level":"INFO","ts":"2025-12-10T04:33:54.654Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-12-10T04:33:54.654Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-12-10T04:33:54.661Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dgftqv, queueLength=0"}
{"level":"INFO","ts":"2025-12-10T04:33:54.661Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984df55dt, queueLength=0"}
{"level":"INFO","ts":"2025-12-10T04:33:54.661Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984d9vh49, queueLength=0"}
{"level":"DEBUG","ts":"2025-12-10T04:33:54.661Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=3"}
{"level":"INFO","ts":"2025-12-10T04:33:54.661Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dgftqv, usage=0.506 (50.6%)"}
{"level":"INFO","ts":"2025-12-10T04:33:54.661Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984df55dt, usage=0.059 (5.9%)"}
{"level":"INFO","ts":"2025-12-10T04:33:54.661Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984d9vh49, usage=0.010 (1.0%)"}
{"level":"DEBUG","ts":"2025-12-10T04:33:54.661Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=3"}
{"level":"DEBUG","ts":"2025-12-10T04:33:54.664Z","msg":"Pod-to-variant matching successful: totalPods=3, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:3]"}
{"level":"DEBUG","ts":"2025-12-10T04:33:54.664Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=3"}
{"level":"DEBUG","ts":"2025-12-10T04:33:54.664Z","msg":"Collected capacity metrics for observability in model-only mode: modelID=unsloth/Meta-Llama-3.1-8B, replicaCount=3"}
{"level":"DEBUG","ts":"2025-12-10T04:33:54.677Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=3, accelerator=H100, ttft=30.04ms, itl=17.26ms, cost=300.00, maxBatch=256, arrivalRate=2039.47, avgInputTokens=210.12, avgOutputTokens=527.26"}
{"level":"DEBUG","ts":"2025-12-10T04:33:54.677Z","msg":"Experimental model tuner is enabled globally: tuning model performance parameters for active VAs"}
{"level":"DEBUG","ts":"2025-12-10T04:33:54.677Z","msg":"Using state vals from VA status to tune variant ms-inference-scheduling-llm-d-modelservice-decode: alpha= 7.215838, beta= 0.053528, gamma= 15.429136, delta= 0.000255"}
{"level":"DEBUG","ts":"2025-12-10T04:33:54.677Z","msg":"[Tuner Config] variant=ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler | Initial state: alpha=7.215838, beta=0.053528, gamma=15.429136, delta=0.000255 | Expected obs (for R): TTFT=30.04ms, ITL=17.26ms | Has covariance=true"}
{"level":"WARN","ts":"2025-12-10T04:33:54.678Z","msg":"Tuner validation failed (NIS=131.51), validation error: normalized innovation squared (NIS=131.51) exceeds threshold (7.38), rejecting update as outlier - returning previous state: alpha=7.215838, beta=0.053528, gamma=15.429136, delta=0.000255"}
{"level":"WARN","ts":"2025-12-10T04:33:54.678Z","msg":"Tuner NIS validation failed for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler, server ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler (NIS=131.51 exceeds threshold 7.38) - Keeping previous state: alpha=7.215838, beta=0.053528, gamma=15.429136, delta=0.000255"}
{"level":"INFO","ts":"2025-12-10T04:33:54.678Z","msg":"Keeping previous tuned parameters for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler due to NIS validation failure (NIS=131.512276)"}
{"level":"DEBUG","ts":"2025-12-10T04:33:54.678Z","msg":"Model tuner results: model=unsloth/Meta-Llama-3.1-8B, accelerator=H100, alpha=7.215838, beta=0.053528, gamma=15.429136, delta=0.000255, NIS=131.51"}
{"level":"DEBUG","ts":"2025-12-10T04:33:54.678Z","msg":"Updated tuner status for variant ms-inference-scheduling-llm-d-modelservice-decode: alpha=7.215838, beta=0.053528, gamma=15.429136, delta=0.000255, NIS=131.512276"}
{"level":"DEBUG","ts":"2025-12-10T04:33:54.688Z","msg":"Optimization solution - system: Solution: \ns=ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler; c=Premium; m=unsloth/Meta-Llama-3.1-8B; rate=2039.47; inTk=210; outTk=527; sol=1, sat=false, alloc={acc=H100; numRep=4; maxBatch=512; cost=400, val=100, itl=9.566301, ttft=17.780565, rho=0.08381038, maxRPM=579.89087}; slo-itl=10, slo-ttft=1000, slo-tps=0 \nAllocationByType: \nname=NVIDIA-H100-80GB-HBM3, count=4, limit=0, cost=400 \ntotalCost=400 \n"}
{"level":"DEBUG","ts":"2025-12-10T04:33:54.688Z","msg":"Setting accelerator name: Name=H100, allocationData={H100 4 512 400 9.566301 17.780565 {2039.47 210 527}}"}
{"level":"INFO","ts":"2025-12-10T04:33:54.688Z","msg":"Model-based optimization completed for model: unsloth/Meta-Llama-3.1-8B - model-based targets: map[ms-inference-scheduling-llm-d-modelservice-decode:4]"}
{"level":"WARN","ts":"2025-12-10T04:33:54.688Z","msg":"saturation analysis unavailable, using model-based targets only: modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-10T04:33:54.688Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-12-10T04:33:54.688Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=scale-up, current=3→target=4"}
{"level":"INFO","ts":"2025-12-10T04:33:54.688Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 3, desired-replicas: 4, accelerator: H100"}
{"level":"INFO","ts":"2025-12-10T04:33:54.688Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=4, accelerator=H100, SaturationOnly=false"}
{"level":"INFO","ts":"2025-12-10T04:33:54.694Z","msg":"Applied Saturation decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=scale-up, current=3, target=4, reason=model-based only (Saturation unavailable)"}
{"level":"INFO","ts":"2025-12-10T04:33:54.694Z","msg":"Reconciliation completed successfully: mode=model-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-12-10T04:34:54.696Z","msg":"Reconciling VariantAutoscaling: name=ms-inference-scheduling-llm-d-modelservice-decode, namespace=llm-d-inference-scheduler, modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-10T04:34:54.696Z","msg":"saturation scaling config cache updated: entries=1, has_default=true"}
{"level":"INFO","ts":"2025-12-10T04:34:54.696Z","msg":"saturation scaling configuration loaded successfully"}
{"level":"INFO","ts":"2025-12-10T04:34:54.696Z","msg":"Operating in MODEL-ONLY mode: model-based optimization only"}
{"level":"INFO","ts":"2025-12-10T04:34:54.696Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-12-10T04:34:54.696Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-12-10T04:34:54.717Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dgftqv, usage=0.124 (12.4%)"}
{"level":"INFO","ts":"2025-12-10T04:34:54.717Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984df55dt, usage=0.045 (4.5%)"}
{"level":"INFO","ts":"2025-12-10T04:34:54.717Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984d9vh49, usage=0.021 (2.1%)"}
{"level":"DEBUG","ts":"2025-12-10T04:34:54.717Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=3"}
{"level":"INFO","ts":"2025-12-10T04:34:54.717Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dgftqv, queueLength=0"}
{"level":"INFO","ts":"2025-12-10T04:34:54.717Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984df55dt, queueLength=0"}
{"level":"INFO","ts":"2025-12-10T04:34:54.717Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984d9vh49, queueLength=0"}
{"level":"DEBUG","ts":"2025-12-10T04:34:54.717Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=3"}
{"level":"DEBUG","ts":"2025-12-10T04:34:54.721Z","msg":"Pod-to-variant matching successful: totalPods=3, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:3]"}
{"level":"DEBUG","ts":"2025-12-10T04:34:54.721Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=3"}
{"level":"DEBUG","ts":"2025-12-10T04:34:54.721Z","msg":"Collected capacity metrics for observability in model-only mode: modelID=unsloth/Meta-Llama-3.1-8B, replicaCount=3"}
{"level":"DEBUG","ts":"2025-12-10T04:34:54.739Z","msg":"Replica count mismatch for llm-d-inference-scheduler/unsloth/Meta-Llama-3.1-8B: spec=4, reporting_metrics=3"}
{"level":"DEBUG","ts":"2025-12-10T04:34:54.739Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=3, accelerator=H100, ttft=19.38ms, itl=9.62ms, cost=300.00, maxBatch=256, arrivalRate=1761.03, avgInputTokens=224.47, avgOutputTokens=461.16"}
{"level":"DEBUG","ts":"2025-12-10T04:34:54.739Z","msg":"Experimental model tuner is enabled globally: tuning model performance parameters for active VAs"}
{"level":"DEBUG","ts":"2025-12-10T04:34:54.739Z","msg":"Using state vals from VA status to tune variant ms-inference-scheduling-llm-d-modelservice-decode: alpha= 7.215838, beta= 0.053528, gamma= 15.429136, delta= 0.000255"}
{"level":"DEBUG","ts":"2025-12-10T04:34:54.739Z","msg":"[Tuner Config] variant=ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler | Initial state: alpha=7.215838, beta=0.053528, gamma=15.429136, delta=0.000255 | Expected obs (for R): TTFT=19.38ms, ITL=9.62ms | Has covariance=true"}
{"level":"INFO","ts":"2025-12-10T04:34:54.740Z","msg":"Tuner validation succeeded for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler, server ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler - New NIS: 1.985404"}
{"level":"DEBUG","ts":"2025-12-10T04:34:54.740Z","msg":"Model tuner results: model=unsloth/Meta-Llama-3.1-8B, accelerator=H100, alpha=7.244669, beta=0.053577, gamma=16.499096, delta=0.000254, NIS=1.99"}
{"level":"DEBUG","ts":"2025-12-10T04:34:54.740Z","msg":"Updated tuner status for variant ms-inference-scheduling-llm-d-modelservice-decode: alpha=7.244669, beta=0.053577, gamma=16.499096, delta=0.000254, NIS=1.985404"}
{"level":"INFO","ts":"2025-12-10T04:34:54.740Z","msg":"Tuned performance parameters result: variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler - alpha: 7.244669, beta: 0.053577, gamma: 16.499096, delta: 0.000254"}
{"level":"DEBUG","ts":"2025-12-10T04:34:54.749Z","msg":"Optimization solution - system: Solution: \ns=ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler; c=Premium; m=unsloth/Meta-Llama-3.1-8B; rate=1761.03; inTk=224; outTk=461; sol=1, sat=false, alloc={acc=H100; numRep=3; maxBatch=512; cost=300, val=0, itl=9.630259, ttft=19.037039, rho=0.08501243, maxRPM=654.9836}; slo-itl=10, slo-ttft=1000, slo-tps=0 \nAllocationByType: \nname=NVIDIA-H100-80GB-HBM3, count=3, limit=0, cost=300 \ntotalCost=300 \n"}
{"level":"DEBUG","ts":"2025-12-10T04:34:54.749Z","msg":"Setting accelerator name: Name=H100, allocationData={H100 3 512 300 9.630259 19.037039 {1761.03 224 461}}"}
{"level":"INFO","ts":"2025-12-10T04:34:54.749Z","msg":"Model-based optimization completed for model: unsloth/Meta-Llama-3.1-8B - model-based targets: map[ms-inference-scheduling-llm-d-modelservice-decode:3]"}
{"level":"WARN","ts":"2025-12-10T04:34:54.749Z","msg":"saturation analysis unavailable, using model-based targets only: modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-10T04:34:54.749Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-12-10T04:34:54.749Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=3→target=3"}
{"level":"INFO","ts":"2025-12-10T04:34:54.749Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 3, desired-replicas: 3, accelerator: H100"}
{"level":"INFO","ts":"2025-12-10T04:34:54.749Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=3, accelerator=H100, SaturationOnly=false"}
{"level":"INFO","ts":"2025-12-10T04:34:54.755Z","msg":"Applied Saturation decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=3, target=3, reason=model-based only (Saturation unavailable)"}
{"level":"INFO","ts":"2025-12-10T04:34:54.755Z","msg":"Reconciliation completed successfully: mode=model-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-12-10T04:35:54.755Z","msg":"Reconciling VariantAutoscaling: name=ms-inference-scheduling-llm-d-modelservice-decode, namespace=llm-d-inference-scheduler, modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-10T04:35:54.755Z","msg":"saturation scaling config cache updated: entries=1, has_default=true"}
{"level":"INFO","ts":"2025-12-10T04:35:54.755Z","msg":"saturation scaling configuration loaded successfully"}
{"level":"INFO","ts":"2025-12-10T04:35:54.755Z","msg":"Operating in MODEL-ONLY mode: model-based optimization only"}
{"level":"INFO","ts":"2025-12-10T04:35:54.755Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-12-10T04:35:54.755Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-12-10T04:35:54.767Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dgftqv, queueLength=0"}
{"level":"INFO","ts":"2025-12-10T04:35:54.767Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984df55dt, queueLength=0"}
{"level":"INFO","ts":"2025-12-10T04:35:54.767Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984d9vh49, queueLength=0"}
{"level":"INFO","ts":"2025-12-10T04:35:54.767Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dgftqv, usage=0.070 (7.0%)"}
{"level":"INFO","ts":"2025-12-10T04:35:54.767Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984df55dt, usage=0.081 (8.1%)"}
{"level":"INFO","ts":"2025-12-10T04:35:54.767Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984d9vh49, usage=0.024 (2.4%)"}
{"level":"DEBUG","ts":"2025-12-10T04:35:54.767Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=3"}
{"level":"DEBUG","ts":"2025-12-10T04:35:54.767Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=3"}
{"level":"DEBUG","ts":"2025-12-10T04:35:54.770Z","msg":"Pod-to-variant matching successful: totalPods=3, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:3]"}
{"level":"DEBUG","ts":"2025-12-10T04:35:54.770Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=3"}
{"level":"DEBUG","ts":"2025-12-10T04:35:54.770Z","msg":"Collected capacity metrics for observability in model-only mode: modelID=unsloth/Meta-Llama-3.1-8B, replicaCount=3"}
{"level":"DEBUG","ts":"2025-12-10T04:35:54.785Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=3, accelerator=H100, ttft=18.13ms, itl=8.77ms, cost=300.00, maxBatch=256, arrivalRate=1186.84, avgInputTokens=223.09, avgOutputTokens=494.26"}
{"level":"DEBUG","ts":"2025-12-10T04:35:54.785Z","msg":"Experimental model tuner is enabled globally: tuning model performance parameters for active VAs"}
{"level":"DEBUG","ts":"2025-12-10T04:35:54.785Z","msg":"Using state vals from VA status to tune variant ms-inference-scheduling-llm-d-modelservice-decode: alpha= 7.244669, beta= 0.053577, gamma= 16.499096, delta= 0.000254"}
{"level":"DEBUG","ts":"2025-12-10T04:35:54.785Z","msg":"[Tuner Config] variant=ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler | Initial state: alpha=7.244669, beta=0.053577, gamma=16.499096, delta=0.000254 | Expected obs (for R): TTFT=18.13ms, ITL=8.77ms | Has covariance=true"}
{"level":"INFO","ts":"2025-12-10T04:35:54.786Z","msg":"Tuner validation succeeded for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler, server ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler - New NIS: 0.020703"}
{"level":"DEBUG","ts":"2025-12-10T04:35:54.786Z","msg":"Model tuner results: model=unsloth/Meta-Llama-3.1-8B, accelerator=H100, alpha=7.186470, beta=0.053777, gamma=16.459557, delta=0.000254, NIS=0.02"}
{"level":"DEBUG","ts":"2025-12-10T04:35:54.786Z","msg":"Updated tuner status for variant ms-inference-scheduling-llm-d-modelservice-decode: alpha=7.186470, beta=0.053777, gamma=16.459557, delta=0.000254, NIS=0.020703"}
{"level":"INFO","ts":"2025-12-10T04:35:54.786Z","msg":"Tuned performance parameters result: variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler - alpha: 7.186470, beta: 0.053777, gamma: 16.459557, delta: 0.000254"}
{"level":"DEBUG","ts":"2025-12-10T04:35:54.796Z","msg":"Optimization solution - system: Solution: \ns=ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler; c=Premium; m=unsloth/Meta-Llama-3.1-8B; rate=1186.84; inTk=223; outTk=494; sol=1, sat=false, alloc={acc=H100; numRep=2; maxBatch=512; cost=200, val=-100, itl=9.827311, ttft=19.241594, rho=0.0939602, maxRPM=622.12006}; slo-itl=10, slo-ttft=1000, slo-tps=0 \nAllocationByType: \nname=NVIDIA-H100-80GB-HBM3, count=2, limit=0, cost=200 \ntotalCost=200 \n"}
{"level":"DEBUG","ts":"2025-12-10T04:35:54.796Z","msg":"Setting accelerator name: Name=H100, allocationData={H100 2 512 200 9.827311 19.241594 {1186.84 223 494}}"}
{"level":"INFO","ts":"2025-12-10T04:35:54.796Z","msg":"Model-based optimization completed for model: unsloth/Meta-Llama-3.1-8B - model-based targets: map[ms-inference-scheduling-llm-d-modelservice-decode:2]"}
{"level":"WARN","ts":"2025-12-10T04:35:54.796Z","msg":"saturation analysis unavailable, using model-based targets only: modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-10T04:35:54.796Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-12-10T04:35:54.796Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=scale-down, current=3→target=2"}
{"level":"INFO","ts":"2025-12-10T04:35:54.796Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 3, desired-replicas: 2, accelerator: H100"}
{"level":"INFO","ts":"2025-12-10T04:35:54.796Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=2, accelerator=H100, SaturationOnly=false"}
{"level":"INFO","ts":"2025-12-10T04:35:54.802Z","msg":"Applied Saturation decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=scale-down, current=3, target=2, reason=model-based only (Saturation unavailable)"}
{"level":"INFO","ts":"2025-12-10T04:35:54.802Z","msg":"Reconciliation completed successfully: mode=model-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-12-10T04:36:54.804Z","msg":"Reconciling VariantAutoscaling: name=ms-inference-scheduling-llm-d-modelservice-decode, namespace=llm-d-inference-scheduler, modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-10T04:36:54.805Z","msg":"saturation scaling config cache updated: entries=1, has_default=true"}
{"level":"INFO","ts":"2025-12-10T04:36:54.806Z","msg":"saturation scaling configuration loaded successfully"}
{"level":"INFO","ts":"2025-12-10T04:36:54.806Z","msg":"Operating in MODEL-ONLY mode: model-based optimization only"}
{"level":"INFO","ts":"2025-12-10T04:36:54.806Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-12-10T04:36:54.806Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-12-10T04:36:54.826Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dgftqv, queueLength=0"}
{"level":"INFO","ts":"2025-12-10T04:36:54.826Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984df55dt, queueLength=0"}
{"level":"INFO","ts":"2025-12-10T04:36:54.826Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984d9vh49, queueLength=0"}
{"level":"DEBUG","ts":"2025-12-10T04:36:54.826Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=3"}
{"level":"INFO","ts":"2025-12-10T04:36:54.826Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dgftqv, usage=0.035 (3.5%)"}
{"level":"INFO","ts":"2025-12-10T04:36:54.826Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984df55dt, usage=0.037 (3.7%)"}
{"level":"INFO","ts":"2025-12-10T04:36:54.826Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984d9vh49, usage=0.033 (3.3%)"}
{"level":"DEBUG","ts":"2025-12-10T04:36:54.826Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=3"}
{"level":"DEBUG","ts":"2025-12-10T04:36:54.831Z","msg":"Filtering pod from stale vLLM metrics: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984df55dt, namespace=llm-d-inference-scheduler, model=unsloth/Meta-Llama-3.1-8B"}
{"level":"DEBUG","ts":"2025-12-10T04:36:54.831Z","msg":"Pod-to-variant matching successful: totalPods=2, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:2]"}
{"level":"DEBUG","ts":"2025-12-10T04:36:54.831Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=2"}
{"level":"DEBUG","ts":"2025-12-10T04:36:54.831Z","msg":"Collected capacity metrics for observability in model-only mode: modelID=unsloth/Meta-Llama-3.1-8B, replicaCount=2"}
{"level":"DEBUG","ts":"2025-12-10T04:36:54.846Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=2, accelerator=H100, ttft=15.27ms, itl=8.13ms, cost=200.00, maxBatch=256, arrivalRate=682.29, avgInputTokens=237.25, avgOutputTokens=423.32"}
{"level":"DEBUG","ts":"2025-12-10T04:36:54.846Z","msg":"Experimental model tuner is enabled globally: tuning model performance parameters for active VAs"}
{"level":"DEBUG","ts":"2025-12-10T04:36:54.846Z","msg":"Using state vals from VA status to tune variant ms-inference-scheduling-llm-d-modelservice-decode: alpha= 7.186470, beta= 0.053777, gamma= 16.459557, delta= 0.000254"}
{"level":"DEBUG","ts":"2025-12-10T04:36:54.847Z","msg":"[Tuner Config] variant=ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler | Initial state: alpha=7.186470, beta=0.053777, gamma=16.459557, delta=0.000254 | Expected obs (for R): TTFT=15.27ms, ITL=8.13ms | Has covariance=true"}
{"level":"INFO","ts":"2025-12-10T04:36:54.847Z","msg":"Tuner validation succeeded for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler, server ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler - New NIS: 6.047266"}
{"level":"DEBUG","ts":"2025-12-10T04:36:54.847Z","msg":"Model tuner results: model=unsloth/Meta-Llama-3.1-8B, accelerator=H100, alpha=7.028524, beta=0.054331, gamma=14.392731, delta=0.000254, NIS=6.05"}
{"level":"DEBUG","ts":"2025-12-10T04:36:54.847Z","msg":"Updated tuner status for variant ms-inference-scheduling-llm-d-modelservice-decode: alpha=7.028524, beta=0.054331, gamma=14.392731, delta=0.000254, NIS=6.047266"}
{"level":"INFO","ts":"2025-12-10T04:36:54.847Z","msg":"Tuned performance parameters result: variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler - alpha: 7.028524, beta: 0.054331, gamma: 14.392731, delta: 0.000254"}
{"level":"DEBUG","ts":"2025-12-10T04:36:54.857Z","msg":"Optimization solution - system: Solution: \ns=ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler; c=Premium; m=unsloth/Meta-Llama-3.1-8B; rate=682.29; inTk=237; outTk=423; sol=1, sat=false, alloc={acc=H100; numRep=1; maxBatch=512; cost=100, val=-100, itl=9.595189, ttft=17.24185, rho=0.09031485, maxRPM=760.2081}; slo-itl=10, slo-ttft=1000, slo-tps=0 \nAllocationByType: \nname=NVIDIA-H100-80GB-HBM3, count=1, limit=0, cost=100 \ntotalCost=100 \n"}
{"level":"DEBUG","ts":"2025-12-10T04:36:54.857Z","msg":"Setting accelerator name: Name=H100, allocationData={H100 1 512 100 9.595189 17.24185 {682.29 237 423}}"}
{"level":"INFO","ts":"2025-12-10T04:36:54.857Z","msg":"Model-based optimization completed for model: unsloth/Meta-Llama-3.1-8B - model-based targets: map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"WARN","ts":"2025-12-10T04:36:54.857Z","msg":"saturation analysis unavailable, using model-based targets only: modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-10T04:36:54.857Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-12-10T04:36:54.857Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=scale-down, current=2→target=1"}
{"level":"INFO","ts":"2025-12-10T04:36:54.857Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 2, desired-replicas: 1, accelerator: H100"}
{"level":"INFO","ts":"2025-12-10T04:36:54.857Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=1, accelerator=H100, SaturationOnly=false"}
{"level":"INFO","ts":"2025-12-10T04:36:54.863Z","msg":"Applied Saturation decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=scale-down, current=2, target=1, reason=model-based only (Saturation unavailable)"}
{"level":"INFO","ts":"2025-12-10T04:36:54.863Z","msg":"Reconciliation completed successfully: mode=model-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-12-10T04:37:54.864Z","msg":"Reconciling VariantAutoscaling: name=ms-inference-scheduling-llm-d-modelservice-decode, namespace=llm-d-inference-scheduler, modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-10T04:37:54.864Z","msg":"saturation scaling config cache updated: entries=1, has_default=true"}
{"level":"INFO","ts":"2025-12-10T04:37:54.864Z","msg":"saturation scaling configuration loaded successfully"}
{"level":"INFO","ts":"2025-12-10T04:37:54.864Z","msg":"Operating in MODEL-ONLY mode: model-based optimization only"}
{"level":"INFO","ts":"2025-12-10T04:37:54.864Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-12-10T04:37:54.864Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-12-10T04:37:54.871Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dgftqv, usage=0.090 (9.0%)"}
{"level":"INFO","ts":"2025-12-10T04:37:54.871Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984d9vh49, usage=0.028 (2.8%)"}
{"level":"DEBUG","ts":"2025-12-10T04:37:54.871Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=2"}
{"level":"INFO","ts":"2025-12-10T04:37:54.881Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dgftqv, queueLength=0"}
{"level":"INFO","ts":"2025-12-10T04:37:54.881Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984d9vh49, queueLength=0"}
{"level":"DEBUG","ts":"2025-12-10T04:37:54.881Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=2"}
{"level":"DEBUG","ts":"2025-12-10T04:37:54.884Z","msg":"Filtering pod from stale vLLM metrics: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984d9vh49, namespace=llm-d-inference-scheduler, model=unsloth/Meta-Llama-3.1-8B"}
{"level":"DEBUG","ts":"2025-12-10T04:37:54.884Z","msg":"Pod-to-variant matching successful: totalPods=1, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"DEBUG","ts":"2025-12-10T04:37:54.884Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=1"}
{"level":"DEBUG","ts":"2025-12-10T04:37:54.884Z","msg":"Collected capacity metrics for observability in model-only mode: modelID=unsloth/Meta-Llama-3.1-8B, replicaCount=1"}
{"level":"DEBUG","ts":"2025-12-10T04:37:54.897Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=1, accelerator=H100, ttft=18.63ms, itl=9.61ms, cost=100.00, maxBatch=256, arrivalRate=675.20, avgInputTokens=241.75, avgOutputTokens=414.20"}
{"level":"DEBUG","ts":"2025-12-10T04:37:54.897Z","msg":"Experimental model tuner is enabled globally: tuning model performance parameters for active VAs"}
{"level":"DEBUG","ts":"2025-12-10T04:37:54.897Z","msg":"Using state vals from VA status to tune variant ms-inference-scheduling-llm-d-modelservice-decode: alpha= 7.028524, beta= 0.054331, gamma= 14.392731, delta= 0.000254"}
{"level":"DEBUG","ts":"2025-12-10T04:37:54.897Z","msg":"[Tuner Config] variant=ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler | Initial state: alpha=7.028524, beta=0.054331, gamma=14.392731, delta=0.000254 | Expected obs (for R): TTFT=18.63ms, ITL=9.61ms | Has covariance=true"}
{"level":"INFO","ts":"2025-12-10T04:37:54.897Z","msg":"Tuner validation succeeded for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler, server ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler - New NIS: 2.379169"}
{"level":"DEBUG","ts":"2025-12-10T04:37:54.897Z","msg":"Model tuner results: model=unsloth/Meta-Llama-3.1-8B, accelerator=H100, alpha=7.075091, beta=0.055377, gamma=15.437979, delta=0.000255, NIS=2.38"}
{"level":"DEBUG","ts":"2025-12-10T04:37:54.897Z","msg":"Updated tuner status for variant ms-inference-scheduling-llm-d-modelservice-decode: alpha=7.075091, beta=0.055377, gamma=15.437979, delta=0.000255, NIS=2.379169"}
{"level":"INFO","ts":"2025-12-10T04:37:54.897Z","msg":"Tuned performance parameters result: variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler - alpha: 7.075091, beta: 0.055377, gamma: 15.437979, delta: 0.000255"}
{"level":"DEBUG","ts":"2025-12-10T04:37:54.907Z","msg":"Optimization solution - system: Solution: \ns=ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler; c=Premium; m=unsloth/Meta-Llama-3.1-8B; rate=675.2; inTk=241; outTk=414; sol=1, sat=false, alloc={acc=H100; numRep=1; maxBatch=512; cost=100, val=0, itl=9.617016, ttft=18.261566, rho=0.087698825, maxRPM=749.4102}; slo-itl=10, slo-ttft=1000, slo-tps=0 \nAllocationByType: \nname=NVIDIA-H100-80GB-HBM3, count=1, limit=0, cost=100 \ntotalCost=100 \n"}
{"level":"DEBUG","ts":"2025-12-10T04:37:54.907Z","msg":"Setting accelerator name: Name=H100, allocationData={H100 1 512 100 9.617016 18.261566 {675.2 241 414}}"}
{"level":"INFO","ts":"2025-12-10T04:37:54.907Z","msg":"Model-based optimization completed for model: unsloth/Meta-Llama-3.1-8B - model-based targets: map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"WARN","ts":"2025-12-10T04:37:54.907Z","msg":"saturation analysis unavailable, using model-based targets only: modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-10T04:37:54.907Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-12-10T04:37:54.907Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1→target=1"}
{"level":"INFO","ts":"2025-12-10T04:37:54.907Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 1, desired-replicas: 1, accelerator: H100"}
{"level":"INFO","ts":"2025-12-10T04:37:54.907Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=1, accelerator=H100, SaturationOnly=false"}
{"level":"INFO","ts":"2025-12-10T04:37:54.913Z","msg":"Applied Saturation decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1, target=1, reason=model-based only (Saturation unavailable)"}
{"level":"INFO","ts":"2025-12-10T04:37:54.913Z","msg":"Reconciliation completed successfully: mode=model-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-12-10T04:38:54.914Z","msg":"Reconciling VariantAutoscaling: name=ms-inference-scheduling-llm-d-modelservice-decode, namespace=llm-d-inference-scheduler, modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-10T04:38:54.914Z","msg":"saturation scaling config cache updated: entries=1, has_default=true"}
{"level":"INFO","ts":"2025-12-10T04:38:54.914Z","msg":"saturation scaling configuration loaded successfully"}
{"level":"INFO","ts":"2025-12-10T04:38:54.914Z","msg":"Operating in MODEL-ONLY mode: model-based optimization only"}
{"level":"INFO","ts":"2025-12-10T04:38:54.914Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-12-10T04:38:54.914Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-12-10T04:38:54.919Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dgftqv, queueLength=0"}
{"level":"DEBUG","ts":"2025-12-10T04:38:54.919Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"INFO","ts":"2025-12-10T04:38:54.919Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dgftqv, usage=0.102 (10.2%)"}
{"level":"DEBUG","ts":"2025-12-10T04:38:54.919Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"DEBUG","ts":"2025-12-10T04:38:54.921Z","msg":"Pod-to-variant matching successful: totalPods=1, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"DEBUG","ts":"2025-12-10T04:38:54.921Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=1"}
{"level":"DEBUG","ts":"2025-12-10T04:38:54.921Z","msg":"Collected capacity metrics for observability in model-only mode: modelID=unsloth/Meta-Llama-3.1-8B, replicaCount=1"}
{"level":"DEBUG","ts":"2025-12-10T04:38:54.933Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=1, accelerator=H100, ttft=21.49ms, itl=10.72ms, cost=100.00, maxBatch=256, arrivalRate=876.43, avgInputTokens=223.28, avgOutputTokens=449.93"}
{"level":"DEBUG","ts":"2025-12-10T04:38:54.933Z","msg":"Experimental model tuner is enabled globally: tuning model performance parameters for active VAs"}
{"level":"DEBUG","ts":"2025-12-10T04:38:54.933Z","msg":"Using state vals from VA status to tune variant ms-inference-scheduling-llm-d-modelservice-decode: alpha= 7.075091, beta= 0.055377, gamma= 15.437979, delta= 0.000255"}
{"level":"DEBUG","ts":"2025-12-10T04:38:54.933Z","msg":"[Tuner Config] variant=ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler | Initial state: alpha=7.075091, beta=0.055377, gamma=15.437979, delta=0.000255 | Expected obs (for R): TTFT=21.49ms, ITL=10.72ms | Has covariance=true"}
{"level":"INFO","ts":"2025-12-10T04:38:54.935Z","msg":"Tuner validation succeeded for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler, server ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler - New NIS: 3.920001"}
{"level":"DEBUG","ts":"2025-12-10T04:38:54.935Z","msg":"Model tuner results: model=unsloth/Meta-Llama-3.1-8B, accelerator=H100, alpha=7.013820, beta=0.052801, gamma=16.818996, delta=0.000256, NIS=3.92"}
{"level":"DEBUG","ts":"2025-12-10T04:38:54.935Z","msg":"Updated tuner status for variant ms-inference-scheduling-llm-d-modelservice-decode: alpha=7.013820, beta=0.052801, gamma=16.818996, delta=0.000256, NIS=3.920001"}
{"level":"INFO","ts":"2025-12-10T04:38:54.935Z","msg":"Tuned performance parameters result: variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler - alpha: 7.013820, beta: 0.052801, gamma: 16.818996, delta: 0.000256"}
{"level":"DEBUG","ts":"2025-12-10T04:38:54.940Z","msg":"Optimization solution - system: Solution: \ns=ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler; c=Premium; m=unsloth/Meta-Llama-3.1-8B; rate=876.43; inTk=223; outTk=449; sol=1, sat=false, alloc={acc=H100; numRep=2; maxBatch=512; cost=200, val=100, itl=8.551092, ttft=18.482988, rho=0.054910563, maxRPM=740.7241}; slo-itl=10, slo-ttft=1000, slo-tps=0 \nAllocationByType: \nname=NVIDIA-H100-80GB-HBM3, count=2, limit=0, cost=200 \ntotalCost=200 \n"}
{"level":"DEBUG","ts":"2025-12-10T04:38:54.940Z","msg":"Setting accelerator name: Name=H100, allocationData={H100 2 512 200 8.551092 18.482988 {876.43 223 449}}"}
{"level":"INFO","ts":"2025-12-10T04:38:54.940Z","msg":"Model-based optimization completed for model: unsloth/Meta-Llama-3.1-8B - model-based targets: map[ms-inference-scheduling-llm-d-modelservice-decode:2]"}
{"level":"WARN","ts":"2025-12-10T04:38:54.940Z","msg":"saturation analysis unavailable, using model-based targets only: modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-10T04:38:54.940Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-12-10T04:38:54.940Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=scale-up, current=1→target=2"}
{"level":"INFO","ts":"2025-12-10T04:38:54.940Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 1, desired-replicas: 2, accelerator: H100"}
{"level":"INFO","ts":"2025-12-10T04:38:54.940Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=2, accelerator=H100, SaturationOnly=false"}
{"level":"INFO","ts":"2025-12-10T04:38:54.945Z","msg":"Applied Saturation decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=scale-up, current=1, target=2, reason=model-based only (Saturation unavailable)"}
{"level":"INFO","ts":"2025-12-10T04:38:54.945Z","msg":"Reconciliation completed successfully: mode=model-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-12-10T04:39:54.945Z","msg":"Reconciling VariantAutoscaling: name=ms-inference-scheduling-llm-d-modelservice-decode, namespace=llm-d-inference-scheduler, modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-10T04:39:54.946Z","msg":"saturation scaling config cache updated: entries=1, has_default=true"}
{"level":"INFO","ts":"2025-12-10T04:39:54.946Z","msg":"saturation scaling configuration loaded successfully"}
{"level":"INFO","ts":"2025-12-10T04:39:54.946Z","msg":"Operating in MODEL-ONLY mode: model-based optimization only"}
{"level":"INFO","ts":"2025-12-10T04:39:54.946Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-12-10T04:39:54.946Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-12-10T04:39:54.955Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dgftqv, queueLength=0"}
{"level":"DEBUG","ts":"2025-12-10T04:39:54.955Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"INFO","ts":"2025-12-10T04:39:54.965Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dgftqv, usage=0.104 (10.4%)"}
{"level":"DEBUG","ts":"2025-12-10T04:39:54.965Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"DEBUG","ts":"2025-12-10T04:39:54.968Z","msg":"Pod-to-variant matching successful: totalPods=1, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"DEBUG","ts":"2025-12-10T04:39:54.968Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=1"}
{"level":"DEBUG","ts":"2025-12-10T04:39:54.968Z","msg":"Collected capacity metrics for observability in model-only mode: modelID=unsloth/Meta-Llama-3.1-8B, replicaCount=1"}
{"level":"DEBUG","ts":"2025-12-10T04:39:54.987Z","msg":"Replica count mismatch for llm-d-inference-scheduler/unsloth/Meta-Llama-3.1-8B: spec=2, reporting_metrics=1"}
{"level":"DEBUG","ts":"2025-12-10T04:39:54.987Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=1, accelerator=H100, ttft=20.44ms, itl=10.34ms, cost=100.00, maxBatch=256, arrivalRate=864.52, avgInputTokens=224.93, avgOutputTokens=446.17"}
{"level":"DEBUG","ts":"2025-12-10T04:39:54.987Z","msg":"Experimental model tuner is enabled globally: tuning model performance parameters for active VAs"}
{"level":"DEBUG","ts":"2025-12-10T04:39:54.987Z","msg":"Using state vals from VA status to tune variant ms-inference-scheduling-llm-d-modelservice-decode: alpha= 7.013820, beta= 0.052801, gamma= 16.818996, delta= 0.000256"}
{"level":"DEBUG","ts":"2025-12-10T04:39:54.987Z","msg":"[Tuner Config] variant=ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler | Initial state: alpha=7.013820, beta=0.052801, gamma=16.818996, delta=0.000256 | Expected obs (for R): TTFT=20.44ms, ITL=10.34ms | Has covariance=true"}
{"level":"INFO","ts":"2025-12-10T04:39:54.989Z","msg":"Tuner validation succeeded for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler, server ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler - New NIS: 0.343785"}
{"level":"DEBUG","ts":"2025-12-10T04:39:54.989Z","msg":"Model tuner results: model=unsloth/Meta-Llama-3.1-8B, accelerator=H100, alpha=6.845836, beta=0.052182, gamma=16.612125, delta=0.000256, NIS=0.34"}
{"level":"DEBUG","ts":"2025-12-10T04:39:54.989Z","msg":"Updated tuner status for variant ms-inference-scheduling-llm-d-modelservice-decode: alpha=6.845836, beta=0.052182, gamma=16.612125, delta=0.000256, NIS=0.343785"}
{"level":"INFO","ts":"2025-12-10T04:39:54.989Z","msg":"Tuned performance parameters result: variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler - alpha: 6.845836, beta: 0.052182, gamma: 16.612125, delta: 0.000256"}
{"level":"DEBUG","ts":"2025-12-10T04:39:54.998Z","msg":"Optimization solution - system: Solution: \ns=ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler; c=Premium; m=unsloth/Meta-Llama-3.1-8B; rate=864.52; inTk=224; outTk=446; sol=1, sat=false, alloc={acc=H100; numRep=2; maxBatch=512; cost=200, val=100, itl=8.292065, ttft=18.200579, rho=0.052177515, maxRPM=797.90466}; slo-itl=10, slo-ttft=1000, slo-tps=0 \nAllocationByType: \nname=NVIDIA-H100-80GB-HBM3, count=2, limit=0, cost=200 \ntotalCost=200 \n"}
{"level":"DEBUG","ts":"2025-12-10T04:39:54.998Z","msg":"Setting accelerator name: Name=H100, allocationData={H100 2 512 200 8.292065 18.200579 {864.52 224 446}}"}
{"level":"INFO","ts":"2025-12-10T04:39:54.998Z","msg":"Model-based optimization completed for model: unsloth/Meta-Llama-3.1-8B - model-based targets: map[ms-inference-scheduling-llm-d-modelservice-decode:2]"}
{"level":"WARN","ts":"2025-12-10T04:39:54.998Z","msg":"saturation analysis unavailable, using model-based targets only: modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-10T04:39:54.998Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-12-10T04:39:54.998Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=scale-up, current=1→target=2"}
{"level":"INFO","ts":"2025-12-10T04:39:54.998Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 1, desired-replicas: 2, accelerator: H100"}
{"level":"INFO","ts":"2025-12-10T04:39:54.998Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=2, accelerator=H100, SaturationOnly=false"}
{"level":"INFO","ts":"2025-12-10T04:39:55.004Z","msg":"Applied Saturation decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=scale-up, current=1, target=2, reason=model-based only (Saturation unavailable)"}
{"level":"INFO","ts":"2025-12-10T04:39:55.004Z","msg":"Reconciliation completed successfully: mode=model-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-12-10T04:40:55.004Z","msg":"Reconciling VariantAutoscaling: name=ms-inference-scheduling-llm-d-modelservice-decode, namespace=llm-d-inference-scheduler, modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-10T04:40:55.004Z","msg":"saturation scaling config cache updated: entries=1, has_default=true"}
{"level":"INFO","ts":"2025-12-10T04:40:55.004Z","msg":"saturation scaling configuration loaded successfully"}
{"level":"INFO","ts":"2025-12-10T04:40:55.004Z","msg":"Operating in MODEL-ONLY mode: model-based optimization only"}
{"level":"INFO","ts":"2025-12-10T04:40:55.004Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-12-10T04:40:55.004Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-12-10T04:40:55.012Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dgftqv, usage=0.092 (9.2%)"}
{"level":"DEBUG","ts":"2025-12-10T04:40:55.012Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"INFO","ts":"2025-12-10T04:40:55.012Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dgftqv, queueLength=0"}
{"level":"DEBUG","ts":"2025-12-10T04:40:55.012Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"DEBUG","ts":"2025-12-10T04:40:55.015Z","msg":"Pod-to-variant matching successful: totalPods=1, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"DEBUG","ts":"2025-12-10T04:40:55.015Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=1"}
{"level":"DEBUG","ts":"2025-12-10T04:40:55.015Z","msg":"Collected capacity metrics for observability in model-only mode: modelID=unsloth/Meta-Llama-3.1-8B, replicaCount=1"}
{"level":"DEBUG","ts":"2025-12-10T04:40:55.030Z","msg":"Replica count mismatch for llm-d-inference-scheduler/unsloth/Meta-Llama-3.1-8B: spec=2, reporting_metrics=1"}
{"level":"DEBUG","ts":"2025-12-10T04:40:55.030Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=1, accelerator=H100, ttft=20.96ms, itl=10.32ms, cost=100.00, maxBatch=256, arrivalRate=808.91, avgInputTokens=237.73, avgOutputTokens=442.73"}
{"level":"DEBUG","ts":"2025-12-10T04:40:55.030Z","msg":"Experimental model tuner is enabled globally: tuning model performance parameters for active VAs"}
{"level":"DEBUG","ts":"2025-12-10T04:40:55.030Z","msg":"Using state vals from VA status to tune variant ms-inference-scheduling-llm-d-modelservice-decode: alpha= 6.845836, beta= 0.052182, gamma= 16.612125, delta= 0.000256"}
{"level":"DEBUG","ts":"2025-12-10T04:40:55.030Z","msg":"[Tuner Config] variant=ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler | Initial state: alpha=6.845836, beta=0.052182, gamma=16.612125, delta=0.000256 | Expected obs (for R): TTFT=20.96ms, ITL=10.32ms | Has covariance=true"}
{"level":"INFO","ts":"2025-12-10T04:40:55.030Z","msg":"Tuner validation succeeded for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler, server ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler - New NIS: 0.479375"}
{"level":"DEBUG","ts":"2025-12-10T04:40:55.030Z","msg":"Model tuner results: model=unsloth/Meta-Llama-3.1-8B, accelerator=H100, alpha=7.023911, beta=0.052193, gamma=17.035286, delta=0.000256, NIS=0.48"}
{"level":"DEBUG","ts":"2025-12-10T04:40:55.030Z","msg":"Updated tuner status for variant ms-inference-scheduling-llm-d-modelservice-decode: alpha=7.023911, beta=0.052193, gamma=17.035286, delta=0.000256, NIS=0.479375"}
{"level":"INFO","ts":"2025-12-10T04:40:55.030Z","msg":"Tuned performance parameters result: variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler - alpha: 7.023911, beta: 0.052193, gamma: 17.035286, delta: 0.000256"}
{"level":"DEBUG","ts":"2025-12-10T04:40:55.039Z","msg":"Optimization solution - system: Solution: \ns=ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler; c=Premium; m=unsloth/Meta-Llama-3.1-8B; rate=808.91; inTk=237; outTk=442; sol=1, sat=false, alloc={acc=H100; numRep=2; maxBatch=512; cost=200, val=100, itl=8.383378, ttft=18.616686, rho=0.048920173, maxRPM=758.66693}; slo-itl=10, slo-ttft=1000, slo-tps=0 \nAllocationByType: \nname=NVIDIA-H100-80GB-HBM3, count=2, limit=0, cost=200 \ntotalCost=200 \n"}
{"level":"DEBUG","ts":"2025-12-10T04:40:55.039Z","msg":"Setting accelerator name: Name=H100, allocationData={H100 2 512 200 8.383378 18.616686 {808.91 237 442}}"}
{"level":"INFO","ts":"2025-12-10T04:40:55.039Z","msg":"Model-based optimization completed for model: unsloth/Meta-Llama-3.1-8B - model-based targets: map[ms-inference-scheduling-llm-d-modelservice-decode:2]"}
{"level":"WARN","ts":"2025-12-10T04:40:55.039Z","msg":"saturation analysis unavailable, using model-based targets only: modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-10T04:40:55.039Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-12-10T04:40:55.039Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=scale-up, current=1→target=2"}
{"level":"INFO","ts":"2025-12-10T04:40:55.039Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 1, desired-replicas: 2, accelerator: H100"}
{"level":"INFO","ts":"2025-12-10T04:40:55.039Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=2, accelerator=H100, SaturationOnly=false"}
{"level":"INFO","ts":"2025-12-10T04:40:55.044Z","msg":"Applied Saturation decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=scale-up, current=1, target=2, reason=model-based only (Saturation unavailable)"}
{"level":"INFO","ts":"2025-12-10T04:40:55.044Z","msg":"Reconciliation completed successfully: mode=model-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-12-10T04:41:55.044Z","msg":"Reconciling VariantAutoscaling: name=ms-inference-scheduling-llm-d-modelservice-decode, namespace=llm-d-inference-scheduler, modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-10T04:41:55.044Z","msg":"saturation scaling config cache updated: entries=1, has_default=true"}
{"level":"INFO","ts":"2025-12-10T04:41:55.044Z","msg":"saturation scaling configuration loaded successfully"}
{"level":"INFO","ts":"2025-12-10T04:41:55.044Z","msg":"Operating in MODEL-ONLY mode: model-based optimization only"}
{"level":"INFO","ts":"2025-12-10T04:41:55.044Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-12-10T04:41:55.044Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-12-10T04:41:55.055Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dgftqv, usage=0.101 (10.1%)"}
{"level":"INFO","ts":"2025-12-10T04:41:55.055Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dnwg9z, usage=0.015 (1.5%)"}
{"level":"DEBUG","ts":"2025-12-10T04:41:55.055Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=2"}
{"level":"INFO","ts":"2025-12-10T04:41:55.055Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dgftqv, queueLength=0"}
{"level":"INFO","ts":"2025-12-10T04:41:55.055Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dnwg9z, queueLength=0"}
{"level":"DEBUG","ts":"2025-12-10T04:41:55.055Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=2"}
{"level":"DEBUG","ts":"2025-12-10T04:41:55.058Z","msg":"Pod-to-variant matching successful: totalPods=2, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:2]"}
{"level":"DEBUG","ts":"2025-12-10T04:41:55.058Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=2"}
{"level":"DEBUG","ts":"2025-12-10T04:41:55.058Z","msg":"Collected capacity metrics for observability in model-only mode: modelID=unsloth/Meta-Llama-3.1-8B, replicaCount=2"}
{"level":"DEBUG","ts":"2025-12-10T04:41:55.071Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=2, accelerator=H100, ttft=21.25ms, itl=10.63ms, cost=200.00, maxBatch=256, arrivalRate=837.28, avgInputTokens=232.38, avgOutputTokens=456.02"}
{"level":"DEBUG","ts":"2025-12-10T04:41:55.071Z","msg":"Experimental model tuner is enabled globally: tuning model performance parameters for active VAs"}
{"level":"DEBUG","ts":"2025-12-10T04:41:55.071Z","msg":"Using state vals from VA status to tune variant ms-inference-scheduling-llm-d-modelservice-decode: alpha= 7.023911, beta= 0.052193, gamma= 17.035286, delta= 0.000256"}
{"level":"DEBUG","ts":"2025-12-10T04:41:55.071Z","msg":"[Tuner Config] variant=ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler | Initial state: alpha=7.023911, beta=0.052193, gamma=17.035286, delta=0.000256 | Expected obs (for R): TTFT=21.25ms, ITL=10.63ms | Has covariance=true"}
{"level":"WARN","ts":"2025-12-10T04:41:55.072Z","msg":"Tuner validation failed (NIS=15.87), validation error: normalized innovation squared (NIS=15.87) exceeds threshold (7.38), rejecting update as outlier - returning previous state: alpha=7.023911, beta=0.052193, gamma=17.035286, delta=0.000256"}
{"level":"WARN","ts":"2025-12-10T04:41:55.072Z","msg":"Tuner NIS validation failed for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler, server ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler (NIS=15.87 exceeds threshold 7.38) - Keeping previous state: alpha=7.023911, beta=0.052193, gamma=17.035286, delta=0.000256"}
{"level":"INFO","ts":"2025-12-10T04:41:55.072Z","msg":"Keeping previous tuned parameters for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler due to NIS validation failure (NIS=15.869994)"}
{"level":"DEBUG","ts":"2025-12-10T04:41:55.072Z","msg":"Model tuner results: model=unsloth/Meta-Llama-3.1-8B, accelerator=H100, alpha=7.023911, beta=0.052193, gamma=17.035286, delta=0.000256, NIS=15.87"}
{"level":"DEBUG","ts":"2025-12-10T04:41:55.072Z","msg":"Updated tuner status for variant ms-inference-scheduling-llm-d-modelservice-decode: alpha=7.023911, beta=0.052193, gamma=17.035286, delta=0.000256, NIS=15.869994"}
{"level":"DEBUG","ts":"2025-12-10T04:41:55.081Z","msg":"Optimization solution - system: Solution: \ns=ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler; c=Premium; m=unsloth/Meta-Llama-3.1-8B; rate=837.28; inTk=232; outTk=456; sol=1, sat=false, alloc={acc=H100; numRep=2; maxBatch=512; cost=200, val=0, itl=8.489614, ttft=18.703154, rho=0.05289524, maxRPM=735.4349}; slo-itl=10, slo-ttft=1000, slo-tps=0 \nAllocationByType: \nname=NVIDIA-H100-80GB-HBM3, count=2, limit=0, cost=200 \ntotalCost=200 \n"}
{"level":"DEBUG","ts":"2025-12-10T04:41:55.081Z","msg":"Setting accelerator name: Name=H100, allocationData={H100 2 512 200 8.489614 18.703154 {837.28 232 456}}"}
{"level":"INFO","ts":"2025-12-10T04:41:55.081Z","msg":"Model-based optimization completed for model: unsloth/Meta-Llama-3.1-8B - model-based targets: map[ms-inference-scheduling-llm-d-modelservice-decode:2]"}
{"level":"WARN","ts":"2025-12-10T04:41:55.081Z","msg":"saturation analysis unavailable, using model-based targets only: modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-10T04:41:55.081Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-12-10T04:41:55.081Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=2→target=2"}
{"level":"INFO","ts":"2025-12-10T04:41:55.081Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 2, desired-replicas: 2, accelerator: H100"}
{"level":"INFO","ts":"2025-12-10T04:41:55.081Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=2, accelerator=H100, SaturationOnly=false"}
{"level":"INFO","ts":"2025-12-10T04:41:55.088Z","msg":"Applied Saturation decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=2, target=2, reason=model-based only (Saturation unavailable)"}
{"level":"INFO","ts":"2025-12-10T04:41:55.088Z","msg":"Reconciliation completed successfully: mode=model-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-12-10T04:42:55.089Z","msg":"Reconciling VariantAutoscaling: name=ms-inference-scheduling-llm-d-modelservice-decode, namespace=llm-d-inference-scheduler, modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-10T04:42:55.090Z","msg":"saturation scaling config cache updated: entries=1, has_default=true"}
{"level":"INFO","ts":"2025-12-10T04:42:55.090Z","msg":"saturation scaling configuration loaded successfully"}
{"level":"INFO","ts":"2025-12-10T04:42:55.090Z","msg":"Operating in MODEL-ONLY mode: model-based optimization only"}
{"level":"INFO","ts":"2025-12-10T04:42:55.090Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-12-10T04:42:55.090Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-12-10T04:42:55.094Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dgftqv, queueLength=0"}
{"level":"INFO","ts":"2025-12-10T04:42:55.094Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dnwg9z, queueLength=0"}
{"level":"DEBUG","ts":"2025-12-10T04:42:55.094Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=2"}
{"level":"INFO","ts":"2025-12-10T04:42:55.094Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dgftqv, usage=0.061 (6.1%)"}
{"level":"INFO","ts":"2025-12-10T04:42:55.094Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dnwg9z, usage=0.026 (2.6%)"}
{"level":"DEBUG","ts":"2025-12-10T04:42:55.094Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=2"}
{"level":"DEBUG","ts":"2025-12-10T04:42:55.097Z","msg":"Pod-to-variant matching successful: totalPods=2, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:2]"}
{"level":"DEBUG","ts":"2025-12-10T04:42:55.097Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=2"}
{"level":"DEBUG","ts":"2025-12-10T04:42:55.097Z","msg":"Collected capacity metrics for observability in model-only mode: modelID=unsloth/Meta-Llama-3.1-8B, replicaCount=2"}
{"level":"DEBUG","ts":"2025-12-10T04:42:55.110Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=2, accelerator=H100, ttft=18.40ms, itl=8.82ms, cost=200.00, maxBatch=256, arrivalRate=801.91, avgInputTokens=250.86, avgOutputTokens=440.40"}
{"level":"DEBUG","ts":"2025-12-10T04:42:55.110Z","msg":"Experimental model tuner is enabled globally: tuning model performance parameters for active VAs"}
{"level":"DEBUG","ts":"2025-12-10T04:42:55.110Z","msg":"Using state vals from VA status to tune variant ms-inference-scheduling-llm-d-modelservice-decode: alpha= 7.023911, beta= 0.052193, gamma= 17.035286, delta= 0.000256"}
{"level":"DEBUG","ts":"2025-12-10T04:42:55.110Z","msg":"[Tuner Config] variant=ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler | Initial state: alpha=7.023911, beta=0.052193, gamma=17.035286, delta=0.000256 | Expected obs (for R): TTFT=18.40ms, ITL=8.82ms | Has covariance=true"}
{"level":"INFO","ts":"2025-12-10T04:42:55.111Z","msg":"Tuner validation succeeded for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler, server ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler - New NIS: 0.656715"}
{"level":"DEBUG","ts":"2025-12-10T04:42:55.111Z","msg":"Model tuner results: model=unsloth/Meta-Llama-3.1-8B, accelerator=H100, alpha=7.428271, beta=0.049402, gamma=16.745064, delta=0.000256, NIS=0.66"}
{"level":"DEBUG","ts":"2025-12-10T04:42:55.111Z","msg":"Updated tuner status for variant ms-inference-scheduling-llm-d-modelservice-decode: alpha=7.428271, beta=0.049402, gamma=16.745064, delta=0.000256, NIS=0.656715"}
{"level":"INFO","ts":"2025-12-10T04:42:55.111Z","msg":"Tuned performance parameters result: variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler - alpha: 7.428271, beta: 0.049402, gamma: 16.745064, delta: 0.000256"}
{"level":"DEBUG","ts":"2025-12-10T04:42:55.120Z","msg":"Optimization solution - system: Solution: \ns=ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler; c=Premium; m=unsloth/Meta-Llama-3.1-8B; rate=801.91; inTk=250; outTk=440; sol=1, sat=false, alloc={acc=H100; numRep=2; maxBatch=512; cost=200, val=0, itl=8.752215, ttft=18.462029, rho=0.050389346, maxRPM=694.64185}; slo-itl=10, slo-ttft=1000, slo-tps=0 \nAllocationByType: \nname=NVIDIA-H100-80GB-HBM3, count=2, limit=0, cost=200 \ntotalCost=200 \n"}
{"level":"DEBUG","ts":"2025-12-10T04:42:55.120Z","msg":"Setting accelerator name: Name=H100, allocationData={H100 2 512 200 8.752215 18.462029 {801.91 250 440}}"}
{"level":"INFO","ts":"2025-12-10T04:42:55.120Z","msg":"Model-based optimization completed for model: unsloth/Meta-Llama-3.1-8B - model-based targets: map[ms-inference-scheduling-llm-d-modelservice-decode:2]"}
{"level":"WARN","ts":"2025-12-10T04:42:55.120Z","msg":"saturation analysis unavailable, using model-based targets only: modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-10T04:42:55.120Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-12-10T04:42:55.120Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=2→target=2"}
{"level":"INFO","ts":"2025-12-10T04:42:55.120Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 2, desired-replicas: 2, accelerator: H100"}
{"level":"INFO","ts":"2025-12-10T04:42:55.120Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=2, accelerator=H100, SaturationOnly=false"}
{"level":"INFO","ts":"2025-12-10T04:42:55.125Z","msg":"Applied Saturation decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=2, target=2, reason=model-based only (Saturation unavailable)"}
{"level":"INFO","ts":"2025-12-10T04:42:55.125Z","msg":"Reconciliation completed successfully: mode=model-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-12-10T04:43:55.126Z","msg":"Reconciling VariantAutoscaling: name=ms-inference-scheduling-llm-d-modelservice-decode, namespace=llm-d-inference-scheduler, modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-10T04:43:55.126Z","msg":"saturation scaling config cache updated: entries=1, has_default=true"}
{"level":"INFO","ts":"2025-12-10T04:43:55.126Z","msg":"saturation scaling configuration loaded successfully"}
{"level":"INFO","ts":"2025-12-10T04:43:55.126Z","msg":"Operating in MODEL-ONLY mode: model-based optimization only"}
{"level":"INFO","ts":"2025-12-10T04:43:55.126Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-12-10T04:43:55.126Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-12-10T04:43:55.135Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dgftqv, usage=0.063 (6.3%)"}
{"level":"INFO","ts":"2025-12-10T04:43:55.135Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dnwg9z, usage=0.033 (3.3%)"}
{"level":"INFO","ts":"2025-12-10T04:43:55.135Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dgftqv, queueLength=0"}
{"level":"INFO","ts":"2025-12-10T04:43:55.135Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dnwg9z, queueLength=0"}
{"level":"DEBUG","ts":"2025-12-10T04:43:55.135Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=2"}
{"level":"DEBUG","ts":"2025-12-10T04:43:55.135Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=2"}
{"level":"DEBUG","ts":"2025-12-10T04:43:55.138Z","msg":"Pod-to-variant matching successful: totalPods=2, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:2]"}
{"level":"DEBUG","ts":"2025-12-10T04:43:55.138Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=2"}
{"level":"DEBUG","ts":"2025-12-10T04:43:55.138Z","msg":"Collected capacity metrics for observability in model-only mode: modelID=unsloth/Meta-Llama-3.1-8B, replicaCount=2"}
{"level":"DEBUG","ts":"2025-12-10T04:43:55.151Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=2, accelerator=H100, ttft=15.85ms, itl=8.17ms, cost=200.00, maxBatch=256, arrivalRate=736.89, avgInputTokens=252.89, avgOutputTokens=457.78"}
{"level":"DEBUG","ts":"2025-12-10T04:43:55.151Z","msg":"Experimental model tuner is enabled globally: tuning model performance parameters for active VAs"}
{"level":"DEBUG","ts":"2025-12-10T04:43:55.151Z","msg":"Using state vals from VA status to tune variant ms-inference-scheduling-llm-d-modelservice-decode: alpha= 7.428271, beta= 0.049402, gamma= 16.745064, delta= 0.000256"}
{"level":"DEBUG","ts":"2025-12-10T04:43:55.151Z","msg":"[Tuner Config] variant=ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler | Initial state: alpha=7.428271, beta=0.049402, gamma=16.745064, delta=0.000256 | Expected obs (for R): TTFT=15.85ms, ITL=8.17ms | Has covariance=true"}
{"level":"INFO","ts":"2025-12-10T04:43:55.151Z","msg":"Tuner validation succeeded for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler, server ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler - New NIS: 6.772806"}
{"level":"DEBUG","ts":"2025-12-10T04:43:55.151Z","msg":"Model tuner results: model=unsloth/Meta-Llama-3.1-8B, accelerator=H100, alpha=7.037840, beta=0.049638, gamma=14.676637, delta=0.000256, NIS=6.77"}
{"level":"DEBUG","ts":"2025-12-10T04:43:55.151Z","msg":"Updated tuner status for variant ms-inference-scheduling-llm-d-modelservice-decode: alpha=7.037840, beta=0.049638, gamma=14.676637, delta=0.000256, NIS=6.772806"}
{"level":"INFO","ts":"2025-12-10T04:43:55.151Z","msg":"Tuned performance parameters result: variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler - alpha: 7.037840, beta: 0.049638, gamma: 14.676637, delta: 0.000256"}
{"level":"DEBUG","ts":"2025-12-10T04:43:55.161Z","msg":"Optimization solution - system: Solution: \ns=ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler; c=Premium; m=unsloth/Meta-Llama-3.1-8B; rate=736.89; inTk=252; outTk=457; sol=1, sat=false, alloc={acc=H100; numRep=1; maxBatch=512; cost=100, val=-100, itl=9.831753, ttft=18.306103, rho=0.1079809, maxRPM=768.9246}; slo-itl=10, slo-ttft=1000, slo-tps=0 \nAllocationByType: \nname=NVIDIA-H100-80GB-HBM3, count=1, limit=0, cost=100 \ntotalCost=100 \n"}
{"level":"DEBUG","ts":"2025-12-10T04:43:55.161Z","msg":"Setting accelerator name: Name=H100, allocationData={H100 1 512 100 9.831753 18.306103 {736.89 252 457}}"}
{"level":"INFO","ts":"2025-12-10T04:43:55.161Z","msg":"Model-based optimization completed for model: unsloth/Meta-Llama-3.1-8B - model-based targets: map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"WARN","ts":"2025-12-10T04:43:55.161Z","msg":"saturation analysis unavailable, using model-based targets only: modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-10T04:43:55.161Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-12-10T04:43:55.161Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=scale-down, current=2→target=1"}
{"level":"INFO","ts":"2025-12-10T04:43:55.161Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 2, desired-replicas: 1, accelerator: H100"}
{"level":"INFO","ts":"2025-12-10T04:43:55.161Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=1, accelerator=H100, SaturationOnly=false"}
{"level":"INFO","ts":"2025-12-10T04:43:55.177Z","msg":"Applied Saturation decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=scale-down, current=2, target=1, reason=model-based only (Saturation unavailable)"}
{"level":"INFO","ts":"2025-12-10T04:43:55.177Z","msg":"Reconciliation completed successfully: mode=model-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-12-10T04:44:55.178Z","msg":"Reconciling VariantAutoscaling: name=ms-inference-scheduling-llm-d-modelservice-decode, namespace=llm-d-inference-scheduler, modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-10T04:44:55.178Z","msg":"saturation scaling config cache updated: entries=1, has_default=true"}
{"level":"INFO","ts":"2025-12-10T04:44:55.178Z","msg":"saturation scaling configuration loaded successfully"}
{"level":"INFO","ts":"2025-12-10T04:44:55.178Z","msg":"Operating in MODEL-ONLY mode: model-based optimization only"}
{"level":"INFO","ts":"2025-12-10T04:44:55.178Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-12-10T04:44:55.178Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-12-10T04:44:55.183Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dgftqv, usage=0.093 (9.3%)"}
{"level":"INFO","ts":"2025-12-10T04:44:55.183Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dnwg9z, usage=0.029 (2.9%)"}
{"level":"DEBUG","ts":"2025-12-10T04:44:55.183Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=2"}
{"level":"INFO","ts":"2025-12-10T04:44:55.183Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dgftqv, queueLength=0"}
{"level":"INFO","ts":"2025-12-10T04:44:55.183Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dnwg9z, queueLength=0"}
{"level":"DEBUG","ts":"2025-12-10T04:44:55.183Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=2"}
{"level":"DEBUG","ts":"2025-12-10T04:44:55.185Z","msg":"Pod-to-variant matching successful: totalPods=2, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:2]"}
{"level":"DEBUG","ts":"2025-12-10T04:44:55.185Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=2"}
{"level":"DEBUG","ts":"2025-12-10T04:44:55.185Z","msg":"Collected capacity metrics for observability in model-only mode: modelID=unsloth/Meta-Llama-3.1-8B, replicaCount=2"}
{"level":"DEBUG","ts":"2025-12-10T04:44:55.198Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=1, accelerator=H100, ttft=17.84ms, itl=9.19ms, cost=100.00, maxBatch=256, arrivalRate=556.05, avgInputTokens=243.04, avgOutputTokens=427.48"}
{"level":"DEBUG","ts":"2025-12-10T04:44:55.198Z","msg":"Experimental model tuner is enabled globally: tuning model performance parameters for active VAs"}
{"level":"DEBUG","ts":"2025-12-10T04:44:55.198Z","msg":"Using state vals from VA status to tune variant ms-inference-scheduling-llm-d-modelservice-decode: alpha= 7.037840, beta= 0.049638, gamma= 14.676637, delta= 0.000256"}
{"level":"DEBUG","ts":"2025-12-10T04:44:55.198Z","msg":"[Tuner Config] variant=ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler | Initial state: alpha=7.037840, beta=0.049638, gamma=14.676637, delta=0.000256 | Expected obs (for R): TTFT=17.84ms, ITL=9.19ms | Has covariance=true"}
{"level":"INFO","ts":"2025-12-10T04:44:55.199Z","msg":"Tuner validation succeeded for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler, server ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler - New NIS: 1.256227"}
{"level":"DEBUG","ts":"2025-12-10T04:44:55.199Z","msg":"Model tuner results: model=unsloth/Meta-Llama-3.1-8B, accelerator=H100, alpha=7.241223, beta=0.050913, gamma=15.320793, delta=0.000256, NIS=1.26"}
{"level":"DEBUG","ts":"2025-12-10T04:44:55.199Z","msg":"Updated tuner status for variant ms-inference-scheduling-llm-d-modelservice-decode: alpha=7.241223, beta=0.050913, gamma=15.320793, delta=0.000256, NIS=1.256227"}
{"level":"INFO","ts":"2025-12-10T04:44:55.199Z","msg":"Tuned performance parameters result: variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler - alpha: 7.241223, beta: 0.050913, gamma: 15.320793, delta: 0.000256"}
{"level":"DEBUG","ts":"2025-12-10T04:44:55.209Z","msg":"Optimization solution - system: Solution: \ns=ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler; c=Premium; m=unsloth/Meta-Llama-3.1-8B; rate=556.05; inTk=243; outTk=427; sol=1, sat=false, alloc={acc=H100; numRep=1; maxBatch=512; cost=100, val=0, itl=9.137008, ttft=17.640951, rho=0.0707734, maxRPM=745.8327}; slo-itl=10, slo-ttft=1000, slo-tps=0 \nAllocationByType: \nname=NVIDIA-H100-80GB-HBM3, count=1, limit=0, cost=100 \ntotalCost=100 \n"}
{"level":"DEBUG","ts":"2025-12-10T04:44:55.209Z","msg":"Setting accelerator name: Name=H100, allocationData={H100 1 512 100 9.137008 17.640951 {556.05 243 427}}"}
{"level":"INFO","ts":"2025-12-10T04:44:55.209Z","msg":"Model-based optimization completed for model: unsloth/Meta-Llama-3.1-8B - model-based targets: map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"WARN","ts":"2025-12-10T04:44:55.209Z","msg":"saturation analysis unavailable, using model-based targets only: modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-10T04:44:55.209Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-12-10T04:44:55.209Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1→target=1"}
{"level":"INFO","ts":"2025-12-10T04:44:55.209Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 1, desired-replicas: 1, accelerator: H100"}
{"level":"INFO","ts":"2025-12-10T04:44:55.209Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=1, accelerator=H100, SaturationOnly=false"}
{"level":"INFO","ts":"2025-12-10T04:44:55.215Z","msg":"Applied Saturation decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1, target=1, reason=model-based only (Saturation unavailable)"}
{"level":"INFO","ts":"2025-12-10T04:44:55.215Z","msg":"Reconciliation completed successfully: mode=model-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-12-10T04:45:55.215Z","msg":"Reconciling VariantAutoscaling: name=ms-inference-scheduling-llm-d-modelservice-decode, namespace=llm-d-inference-scheduler, modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-10T04:45:55.215Z","msg":"saturation scaling config cache updated: entries=1, has_default=true"}
{"level":"INFO","ts":"2025-12-10T04:45:55.215Z","msg":"saturation scaling configuration loaded successfully"}
{"level":"INFO","ts":"2025-12-10T04:45:55.215Z","msg":"Operating in MODEL-ONLY mode: model-based optimization only"}
{"level":"INFO","ts":"2025-12-10T04:45:55.215Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-12-10T04:45:55.215Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-12-10T04:45:55.225Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dgftqv, queueLength=0"}
{"level":"DEBUG","ts":"2025-12-10T04:45:55.225Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"INFO","ts":"2025-12-10T04:45:55.225Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dgftqv, usage=0.074 (7.4%)"}
{"level":"DEBUG","ts":"2025-12-10T04:45:55.225Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"DEBUG","ts":"2025-12-10T04:45:55.227Z","msg":"Pod-to-variant matching successful: totalPods=1, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"DEBUG","ts":"2025-12-10T04:45:55.227Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=1"}
{"level":"DEBUG","ts":"2025-12-10T04:45:55.227Z","msg":"Collected capacity metrics for observability in model-only mode: modelID=unsloth/Meta-Llama-3.1-8B, replicaCount=1"}
{"level":"DEBUG","ts":"2025-12-10T04:45:55.371Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=1, accelerator=H100, ttft=20.25ms, itl=9.83ms, cost=100.00, maxBatch=256, arrivalRate=783.76, avgInputTokens=241.01, avgOutputTokens=408.37"}
{"level":"DEBUG","ts":"2025-12-10T04:45:55.371Z","msg":"Experimental model tuner is enabled globally: tuning model performance parameters for active VAs"}
{"level":"DEBUG","ts":"2025-12-10T04:45:55.371Z","msg":"Using state vals from VA status to tune variant ms-inference-scheduling-llm-d-modelservice-decode: alpha= 7.241223, beta= 0.050913, gamma= 15.320793, delta= 0.000256"}
{"level":"DEBUG","ts":"2025-12-10T04:45:55.371Z","msg":"[Tuner Config] variant=ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler | Initial state: alpha=7.241223, beta=0.050913, gamma=15.320793, delta=0.000256 | Expected obs (for R): TTFT=20.25ms, ITL=9.83ms | Has covariance=true"}
{"level":"INFO","ts":"2025-12-10T04:45:55.372Z","msg":"Tuner validation succeeded for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler, server ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler - New NIS: 2.604428"}
{"level":"DEBUG","ts":"2025-12-10T04:45:55.372Z","msg":"Model tuner results: model=unsloth/Meta-Llama-3.1-8B, accelerator=H100, alpha=7.182811, beta=0.050177, gamma=16.489876, delta=0.000258, NIS=2.60"}
{"level":"DEBUG","ts":"2025-12-10T04:45:55.372Z","msg":"Updated tuner status for variant ms-inference-scheduling-llm-d-modelservice-decode: alpha=7.182811, beta=0.050177, gamma=16.489876, delta=0.000258, NIS=2.604428"}
{"level":"INFO","ts":"2025-12-10T04:45:55.372Z","msg":"Tuned performance parameters result: variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler - alpha: 7.182811, beta: 0.050177, gamma: 16.489876, delta: 0.000258"}
{"level":"DEBUG","ts":"2025-12-10T04:45:55.381Z","msg":"Optimization solution - system: Solution: \ns=ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler; c=Premium; m=unsloth/Meta-Llama-3.1-8B; rate=783.76; inTk=241; outTk=408; sol=1, sat=false, alloc={acc=H100; numRep=1; maxBatch=512; cost=100, val=0, itl=9.882234, ttft=19.832376, rho=0.103121124, maxRPM=808.9775}; slo-itl=10, slo-ttft=1000, slo-tps=0 \nAllocationByType: \nname=NVIDIA-H100-80GB-HBM3, count=1, limit=0, cost=100 \ntotalCost=100 \n"}
{"level":"DEBUG","ts":"2025-12-10T04:45:55.381Z","msg":"Setting accelerator name: Name=H100, allocationData={H100 1 512 100 9.882234 19.832376 {783.76 241 408}}"}
{"level":"INFO","ts":"2025-12-10T04:45:55.381Z","msg":"Model-based optimization completed for model: unsloth/Meta-Llama-3.1-8B - model-based targets: map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"WARN","ts":"2025-12-10T04:45:55.381Z","msg":"saturation analysis unavailable, using model-based targets only: modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-10T04:45:55.381Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-12-10T04:45:55.381Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1→target=1"}
{"level":"INFO","ts":"2025-12-10T04:45:55.381Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 1, desired-replicas: 1, accelerator: H100"}
{"level":"INFO","ts":"2025-12-10T04:45:55.381Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=1, accelerator=H100, SaturationOnly=false"}
{"level":"INFO","ts":"2025-12-10T04:45:55.389Z","msg":"Applied Saturation decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1, target=1, reason=model-based only (Saturation unavailable)"}
{"level":"INFO","ts":"2025-12-10T04:45:55.389Z","msg":"Reconciliation completed successfully: mode=model-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-12-10T04:46:55.390Z","msg":"Reconciling VariantAutoscaling: name=ms-inference-scheduling-llm-d-modelservice-decode, namespace=llm-d-inference-scheduler, modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-10T04:46:55.390Z","msg":"saturation scaling config cache updated: entries=1, has_default=true"}
{"level":"INFO","ts":"2025-12-10T04:46:55.390Z","msg":"saturation scaling configuration loaded successfully"}
{"level":"INFO","ts":"2025-12-10T04:46:55.390Z","msg":"Operating in MODEL-ONLY mode: model-based optimization only"}
{"level":"INFO","ts":"2025-12-10T04:46:55.390Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-12-10T04:46:55.390Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-12-10T04:46:55.395Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dgftqv, usage=0.097 (9.7%)"}
{"level":"DEBUG","ts":"2025-12-10T04:46:55.395Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"INFO","ts":"2025-12-10T04:46:55.405Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dgftqv, queueLength=0"}
{"level":"DEBUG","ts":"2025-12-10T04:46:55.405Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"DEBUG","ts":"2025-12-10T04:46:55.408Z","msg":"Pod-to-variant matching successful: totalPods=1, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"DEBUG","ts":"2025-12-10T04:46:55.408Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=1"}
{"level":"DEBUG","ts":"2025-12-10T04:46:55.408Z","msg":"Collected capacity metrics for observability in model-only mode: modelID=unsloth/Meta-Llama-3.1-8B, replicaCount=1"}
{"level":"DEBUG","ts":"2025-12-10T04:46:55.420Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=1, accelerator=H100, ttft=19.26ms, itl=9.45ms, cost=100.00, maxBatch=256, arrivalRate=602.38, avgInputTokens=214.19, avgOutputTokens=492.28"}
{"level":"DEBUG","ts":"2025-12-10T04:46:55.420Z","msg":"Experimental model tuner is enabled globally: tuning model performance parameters for active VAs"}
{"level":"DEBUG","ts":"2025-12-10T04:46:55.420Z","msg":"Using state vals from VA status to tune variant ms-inference-scheduling-llm-d-modelservice-decode: alpha= 7.182811, beta= 0.050177, gamma= 16.489876, delta= 0.000258"}
{"level":"DEBUG","ts":"2025-12-10T04:46:55.420Z","msg":"[Tuner Config] variant=ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler | Initial state: alpha=7.182811, beta=0.050177, gamma=16.489876, delta=0.000258 | Expected obs (for R): TTFT=19.26ms, ITL=9.45ms | Has covariance=true"}
{"level":"INFO","ts":"2025-12-10T04:46:55.421Z","msg":"Tuner validation succeeded for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler, server ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler - New NIS: 0.095360"}
{"level":"DEBUG","ts":"2025-12-10T04:46:55.421Z","msg":"Model tuner results: model=unsloth/Meta-Llama-3.1-8B, accelerator=H100, alpha=7.080867, beta=0.050047, gamma=16.588917, delta=0.000258, NIS=0.10"}
{"level":"DEBUG","ts":"2025-12-10T04:46:55.421Z","msg":"Updated tuner status for variant ms-inference-scheduling-llm-d-modelservice-decode: alpha=7.080867, beta=0.050047, gamma=16.588917, delta=0.000258, NIS=0.095360"}
{"level":"INFO","ts":"2025-12-10T04:46:55.421Z","msg":"Tuned performance parameters result: variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler - alpha: 7.080867, beta: 0.050047, gamma: 16.588917, delta: 0.000258"}
{"level":"DEBUG","ts":"2025-12-10T04:46:55.430Z","msg":"Optimization solution - system: Solution: \ns=ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler; c=Premium; m=unsloth/Meta-Llama-3.1-8B; rate=602.38; inTk=214; outTk=492; sol=1, sat=false, alloc={acc=H100; numRep=1; maxBatch=512; cost=100, val=0, itl=9.479146, ttft=19.234404, rho=0.09164128, maxRPM=697.72406}; slo-itl=10, slo-ttft=1000, slo-tps=0 \nAllocationByType: \nname=NVIDIA-H100-80GB-HBM3, count=1, limit=0, cost=100 \ntotalCost=100 \n"}
{"level":"DEBUG","ts":"2025-12-10T04:46:55.430Z","msg":"Setting accelerator name: Name=H100, allocationData={H100 1 512 100 9.479146 19.234404 {602.38 214 492}}"}
{"level":"INFO","ts":"2025-12-10T04:46:55.430Z","msg":"Model-based optimization completed for model: unsloth/Meta-Llama-3.1-8B - model-based targets: map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"WARN","ts":"2025-12-10T04:46:55.430Z","msg":"saturation analysis unavailable, using model-based targets only: modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-10T04:46:55.430Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-12-10T04:46:55.430Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1→target=1"}
{"level":"INFO","ts":"2025-12-10T04:46:55.430Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 1, desired-replicas: 1, accelerator: H100"}
{"level":"INFO","ts":"2025-12-10T04:46:55.430Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=1, accelerator=H100, SaturationOnly=false"}
{"level":"INFO","ts":"2025-12-10T04:46:55.436Z","msg":"Applied Saturation decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1, target=1, reason=model-based only (Saturation unavailable)"}
{"level":"INFO","ts":"2025-12-10T04:46:55.436Z","msg":"Reconciliation completed successfully: mode=model-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-12-10T04:47:55.436Z","msg":"Reconciling VariantAutoscaling: name=ms-inference-scheduling-llm-d-modelservice-decode, namespace=llm-d-inference-scheduler, modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-10T04:47:55.436Z","msg":"saturation scaling config cache updated: entries=1, has_default=true"}
{"level":"INFO","ts":"2025-12-10T04:47:55.436Z","msg":"saturation scaling configuration loaded successfully"}
{"level":"INFO","ts":"2025-12-10T04:47:55.436Z","msg":"Operating in MODEL-ONLY mode: model-based optimization only"}
{"level":"INFO","ts":"2025-12-10T04:47:55.436Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-12-10T04:47:55.436Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-12-10T04:47:55.444Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dgftqv, queueLength=0"}
{"level":"DEBUG","ts":"2025-12-10T04:47:55.444Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"INFO","ts":"2025-12-10T04:47:55.444Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dgftqv, usage=0.048 (4.8%)"}
{"level":"DEBUG","ts":"2025-12-10T04:47:55.444Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"DEBUG","ts":"2025-12-10T04:47:55.446Z","msg":"Pod-to-variant matching successful: totalPods=1, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"DEBUG","ts":"2025-12-10T04:47:55.446Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=1"}
{"level":"DEBUG","ts":"2025-12-10T04:47:55.446Z","msg":"Collected capacity metrics for observability in model-only mode: modelID=unsloth/Meta-Llama-3.1-8B, replicaCount=1"}
{"level":"DEBUG","ts":"2025-12-10T04:47:55.459Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=1, accelerator=H100, ttft=17.70ms, itl=8.35ms, cost=100.00, maxBatch=256, arrivalRate=424.98, avgInputTokens=249.80, avgOutputTokens=454.29"}
{"level":"DEBUG","ts":"2025-12-10T04:47:55.459Z","msg":"Experimental model tuner is enabled globally: tuning model performance parameters for active VAs"}
{"level":"DEBUG","ts":"2025-12-10T04:47:55.459Z","msg":"Using state vals from VA status to tune variant ms-inference-scheduling-llm-d-modelservice-decode: alpha= 7.080867, beta= 0.050047, gamma= 16.588917, delta= 0.000258"}
{"level":"DEBUG","ts":"2025-12-10T04:47:55.459Z","msg":"[Tuner Config] variant=ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler | Initial state: alpha=7.080867, beta=0.050047, gamma=16.588917, delta=0.000258 | Expected obs (for R): TTFT=17.70ms, ITL=8.35ms | Has covariance=true"}
{"level":"INFO","ts":"2025-12-10T04:47:55.460Z","msg":"Tuner validation succeeded for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler, server ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler - New NIS: 0.518263"}
{"level":"DEBUG","ts":"2025-12-10T04:47:55.460Z","msg":"Model tuner results: model=unsloth/Meta-Llama-3.1-8B, accelerator=H100, alpha=6.948398, beta=0.050750, gamma=16.029041, delta=0.000259, NIS=0.52"}
{"level":"DEBUG","ts":"2025-12-10T04:47:55.460Z","msg":"Updated tuner status for variant ms-inference-scheduling-llm-d-modelservice-decode: alpha=6.948398, beta=0.050750, gamma=16.029041, delta=0.000259, NIS=0.518263"}
{"level":"INFO","ts":"2025-12-10T04:47:55.460Z","msg":"Tuned performance parameters result: variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler - alpha: 6.948398, beta: 0.050750, gamma: 16.029041, delta: 0.000259"}
{"level":"DEBUG","ts":"2025-12-10T04:47:55.469Z","msg":"Optimization solution - system: Solution: \ns=ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler; c=Premium; m=unsloth/Meta-Llama-3.1-8B; rate=424.98; inTk=249; outTk=454; sol=1, sat=false, alloc={acc=H100; numRep=1; maxBatch=512; cost=100, val=0, itl=8.368199, ttft=17.829967, rho=0.05268845, maxRPM=779.756}; slo-itl=10, slo-ttft=1000, slo-tps=0 \nAllocationByType: \nname=NVIDIA-H100-80GB-HBM3, count=1, limit=0, cost=100 \ntotalCost=100 \n"}
{"level":"DEBUG","ts":"2025-12-10T04:47:55.469Z","msg":"Setting accelerator name: Name=H100, allocationData={H100 1 512 100 8.368199 17.829967 {424.98 249 454}}"}
{"level":"INFO","ts":"2025-12-10T04:47:55.469Z","msg":"Model-based optimization completed for model: unsloth/Meta-Llama-3.1-8B - model-based targets: map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"WARN","ts":"2025-12-10T04:47:55.469Z","msg":"saturation analysis unavailable, using model-based targets only: modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-10T04:47:55.469Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-12-10T04:47:55.469Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1→target=1"}
{"level":"INFO","ts":"2025-12-10T04:47:55.469Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 1, desired-replicas: 1, accelerator: H100"}
{"level":"INFO","ts":"2025-12-10T04:47:55.469Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=1, accelerator=H100, SaturationOnly=false"}
{"level":"INFO","ts":"2025-12-10T04:47:55.476Z","msg":"Applied Saturation decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1, target=1, reason=model-based only (Saturation unavailable)"}
{"level":"INFO","ts":"2025-12-10T04:47:55.476Z","msg":"Reconciliation completed successfully: mode=model-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-12-10T04:48:55.477Z","msg":"Reconciling VariantAutoscaling: name=ms-inference-scheduling-llm-d-modelservice-decode, namespace=llm-d-inference-scheduler, modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-10T04:48:55.477Z","msg":"saturation scaling config cache updated: entries=1, has_default=true"}
{"level":"INFO","ts":"2025-12-10T04:48:55.477Z","msg":"saturation scaling configuration loaded successfully"}
{"level":"INFO","ts":"2025-12-10T04:48:55.477Z","msg":"Operating in MODEL-ONLY mode: model-based optimization only"}
{"level":"INFO","ts":"2025-12-10T04:48:55.477Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-12-10T04:48:55.477Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-12-10T04:48:55.481Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dgftqv, usage=0.026 (2.6%)"}
{"level":"DEBUG","ts":"2025-12-10T04:48:55.481Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"INFO","ts":"2025-12-10T04:48:55.481Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dgftqv, queueLength=0"}
{"level":"DEBUG","ts":"2025-12-10T04:48:55.481Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"DEBUG","ts":"2025-12-10T04:48:55.484Z","msg":"Pod-to-variant matching successful: totalPods=1, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"DEBUG","ts":"2025-12-10T04:48:55.484Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=1"}
{"level":"DEBUG","ts":"2025-12-10T04:48:55.484Z","msg":"Collected capacity metrics for observability in model-only mode: modelID=unsloth/Meta-Llama-3.1-8B, replicaCount=1"}
{"level":"DEBUG","ts":"2025-12-10T04:48:55.585Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=1, accelerator=H100, ttft=17.36ms, itl=8.07ms, cost=100.00, maxBatch=256, arrivalRate=386.58, avgInputTokens=260.49, avgOutputTokens=428.57"}
{"level":"DEBUG","ts":"2025-12-10T04:48:55.586Z","msg":"Experimental model tuner is enabled globally: tuning model performance parameters for active VAs"}
{"level":"DEBUG","ts":"2025-12-10T04:48:55.586Z","msg":"Using state vals from VA status to tune variant ms-inference-scheduling-llm-d-modelservice-decode: alpha= 6.948398, beta= 0.050750, gamma= 16.029041, delta= 0.000259"}
{"level":"DEBUG","ts":"2025-12-10T04:48:55.586Z","msg":"[Tuner Config] variant=ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler | Initial state: alpha=6.948398, beta=0.050750, gamma=16.029041, delta=0.000259 | Expected obs (for R): TTFT=17.36ms, ITL=8.07ms | Has covariance=true"}
{"level":"INFO","ts":"2025-12-10T04:48:55.587Z","msg":"Tuner validation succeeded for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler, server ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler - New NIS: 0.078936"}
{"level":"DEBUG","ts":"2025-12-10T04:48:55.587Z","msg":"Model tuner results: model=unsloth/Meta-Llama-3.1-8B, accelerator=H100, alpha=6.893602, beta=0.050856, gamma=15.835255, delta=0.000259, NIS=0.08"}
{"level":"DEBUG","ts":"2025-12-10T04:48:55.587Z","msg":"Updated tuner status for variant ms-inference-scheduling-llm-d-modelservice-decode: alpha=6.893602, beta=0.050856, gamma=15.835255, delta=0.000259, NIS=0.078936"}
{"level":"INFO","ts":"2025-12-10T04:48:55.587Z","msg":"Tuned performance parameters result: variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler - alpha: 6.893602, beta: 0.050856, gamma: 15.835255, delta: 0.000259"}
{"level":"DEBUG","ts":"2025-12-10T04:48:55.677Z","msg":"Optimization solution - system: Solution: \ns=ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler; c=Premium; m=unsloth/Meta-Llama-3.1-8B; rate=386.58; inTk=260; outTk=428; sol=1, sat=false, alloc={acc=H100; numRep=1; maxBatch=512; cost=100, val=0, itl=8.080759, ttft=17.407177, rho=0.04363989, maxRPM=840.32544}; slo-itl=10, slo-ttft=1000, slo-tps=0 \nAllocationByType: \nname=NVIDIA-H100-80GB-HBM3, count=1, limit=0, cost=100 \ntotalCost=100 \n"}
{"level":"DEBUG","ts":"2025-12-10T04:48:55.677Z","msg":"Setting accelerator name: Name=H100, allocationData={H100 1 512 100 8.080759 17.407177 {386.58 260 428}}"}
{"level":"INFO","ts":"2025-12-10T04:48:55.677Z","msg":"Model-based optimization completed for model: unsloth/Meta-Llama-3.1-8B - model-based targets: map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"WARN","ts":"2025-12-10T04:48:55.677Z","msg":"saturation analysis unavailable, using model-based targets only: modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-10T04:48:55.677Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-12-10T04:48:55.677Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1→target=1"}
{"level":"INFO","ts":"2025-12-10T04:48:55.677Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 1, desired-replicas: 1, accelerator: H100"}
{"level":"INFO","ts":"2025-12-10T04:48:55.677Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=1, accelerator=H100, SaturationOnly=false"}
{"level":"INFO","ts":"2025-12-10T04:48:55.682Z","msg":"Applied Saturation decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1, target=1, reason=model-based only (Saturation unavailable)"}
{"level":"INFO","ts":"2025-12-10T04:48:55.682Z","msg":"Reconciliation completed successfully: mode=model-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-12-10T04:49:55.683Z","msg":"Reconciling VariantAutoscaling: name=ms-inference-scheduling-llm-d-modelservice-decode, namespace=llm-d-inference-scheduler, modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-10T04:49:55.684Z","msg":"saturation scaling config cache updated: entries=1, has_default=true"}
{"level":"INFO","ts":"2025-12-10T04:49:55.684Z","msg":"saturation scaling configuration loaded successfully"}
{"level":"INFO","ts":"2025-12-10T04:49:55.684Z","msg":"Operating in MODEL-ONLY mode: model-based optimization only"}
{"level":"INFO","ts":"2025-12-10T04:49:55.684Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-12-10T04:49:55.684Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-12-10T04:49:55.691Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dgftqv, usage=0.041 (4.1%)"}
{"level":"DEBUG","ts":"2025-12-10T04:49:55.691Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"INFO","ts":"2025-12-10T04:49:55.691Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dgftqv, queueLength=0"}
{"level":"DEBUG","ts":"2025-12-10T04:49:55.691Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"DEBUG","ts":"2025-12-10T04:49:55.694Z","msg":"Pod-to-variant matching successful: totalPods=1, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"DEBUG","ts":"2025-12-10T04:49:55.694Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=1"}
{"level":"DEBUG","ts":"2025-12-10T04:49:55.694Z","msg":"Collected capacity metrics for observability in model-only mode: modelID=unsloth/Meta-Llama-3.1-8B, replicaCount=1"}
{"level":"DEBUG","ts":"2025-12-10T04:49:55.706Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=1, accelerator=H100, ttft=17.68ms, itl=8.39ms, cost=100.00, maxBatch=256, arrivalRate=419.68, avgInputTokens=240.02, avgOutputTokens=480.51"}
{"level":"DEBUG","ts":"2025-12-10T04:49:55.706Z","msg":"Experimental model tuner is enabled globally: tuning model performance parameters for active VAs"}
{"level":"DEBUG","ts":"2025-12-10T04:49:55.706Z","msg":"Using state vals from VA status to tune variant ms-inference-scheduling-llm-d-modelservice-decode: alpha= 6.893602, beta= 0.050856, gamma= 15.835255, delta= 0.000259"}
{"level":"DEBUG","ts":"2025-12-10T04:49:55.706Z","msg":"[Tuner Config] variant=ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler | Initial state: alpha=6.893602, beta=0.050856, gamma=15.835255, delta=0.000259 | Expected obs (for R): TTFT=17.68ms, ITL=8.39ms | Has covariance=true"}
{"level":"INFO","ts":"2025-12-10T04:49:55.707Z","msg":"Tuner validation succeeded for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler, server ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler - New NIS: 0.001315"}
{"level":"DEBUG","ts":"2025-12-10T04:49:55.707Z","msg":"Model tuner results: model=unsloth/Meta-Llama-3.1-8B, accelerator=H100, alpha=6.900920, beta=0.050886, gamma=15.857373, delta=0.000259, NIS=0.00"}
{"level":"DEBUG","ts":"2025-12-10T04:49:55.707Z","msg":"Updated tuner status for variant ms-inference-scheduling-llm-d-modelservice-decode: alpha=6.900920, beta=0.050886, gamma=15.857373, delta=0.000259, NIS=0.001315"}
{"level":"INFO","ts":"2025-12-10T04:49:55.707Z","msg":"Tuned performance parameters result: variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler - alpha: 6.900920, beta: 0.050886, gamma: 15.857373, delta: 0.000259"}
{"level":"DEBUG","ts":"2025-12-10T04:49:55.711Z","msg":"Optimization solution - system: Solution: \ns=ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler; c=Premium; m=unsloth/Meta-Llama-3.1-8B; rate=419.68; inTk=240; outTk=480; sol=1, sat=false, alloc={acc=H100; numRep=1; maxBatch=512; cost=100, val=0, itl=8.388196, ttft=17.67421, rho=0.055132452, maxRPM=747.2846}; slo-itl=10, slo-ttft=1000, slo-tps=0 \nAllocationByType: \nname=NVIDIA-H100-80GB-HBM3, count=1, limit=0, cost=100 \ntotalCost=100 \n"}
{"level":"DEBUG","ts":"2025-12-10T04:49:55.711Z","msg":"Setting accelerator name: Name=H100, allocationData={H100 1 512 100 8.388196 17.67421 {419.68 240 480}}"}
{"level":"INFO","ts":"2025-12-10T04:49:55.711Z","msg":"Model-based optimization completed for model: unsloth/Meta-Llama-3.1-8B - model-based targets: map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"WARN","ts":"2025-12-10T04:49:55.711Z","msg":"saturation analysis unavailable, using model-based targets only: modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-10T04:49:55.711Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-12-10T04:49:55.711Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1→target=1"}
{"level":"INFO","ts":"2025-12-10T04:49:55.711Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 1, desired-replicas: 1, accelerator: H100"}
{"level":"INFO","ts":"2025-12-10T04:49:55.711Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=1, accelerator=H100, SaturationOnly=false"}
{"level":"INFO","ts":"2025-12-10T04:49:55.717Z","msg":"Applied Saturation decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1, target=1, reason=model-based only (Saturation unavailable)"}
{"level":"INFO","ts":"2025-12-10T04:49:55.717Z","msg":"Reconciliation completed successfully: mode=model-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-12-10T04:50:55.718Z","msg":"Reconciling VariantAutoscaling: name=ms-inference-scheduling-llm-d-modelservice-decode, namespace=llm-d-inference-scheduler, modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-10T04:50:55.718Z","msg":"saturation scaling config cache updated: entries=1, has_default=true"}
{"level":"INFO","ts":"2025-12-10T04:50:55.718Z","msg":"saturation scaling configuration loaded successfully"}
{"level":"INFO","ts":"2025-12-10T04:50:55.718Z","msg":"Operating in MODEL-ONLY mode: model-based optimization only"}
{"level":"INFO","ts":"2025-12-10T04:50:55.718Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-12-10T04:50:55.718Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-12-10T04:50:55.724Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dgftqv, usage=0.052 (5.2%)"}
{"level":"DEBUG","ts":"2025-12-10T04:50:55.724Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"INFO","ts":"2025-12-10T04:50:55.724Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dgftqv, queueLength=0"}
{"level":"DEBUG","ts":"2025-12-10T04:50:55.724Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"DEBUG","ts":"2025-12-10T04:50:55.726Z","msg":"Pod-to-variant matching successful: totalPods=1, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"DEBUG","ts":"2025-12-10T04:50:55.726Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=1"}
{"level":"DEBUG","ts":"2025-12-10T04:50:55.726Z","msg":"Collected capacity metrics for observability in model-only mode: modelID=unsloth/Meta-Llama-3.1-8B, replicaCount=1"}
{"level":"DEBUG","ts":"2025-12-10T04:50:55.738Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=1, accelerator=H100, ttft=17.50ms, itl=8.31ms, cost=100.00, maxBatch=256, arrivalRate=415.71, avgInputTokens=246.71, avgOutputTokens=493.07"}
{"level":"DEBUG","ts":"2025-12-10T04:50:55.738Z","msg":"Experimental model tuner is enabled globally: tuning model performance parameters for active VAs"}
{"level":"DEBUG","ts":"2025-12-10T04:50:55.738Z","msg":"Using state vals from VA status to tune variant ms-inference-scheduling-llm-d-modelservice-decode: alpha= 6.900920, beta= 0.050886, gamma= 15.857373, delta= 0.000259"}
{"level":"DEBUG","ts":"2025-12-10T04:50:55.738Z","msg":"[Tuner Config] variant=ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler | Initial state: alpha=6.900920, beta=0.050886, gamma=15.857373, delta=0.000259 | Expected obs (for R): TTFT=17.50ms, ITL=8.31ms | Has covariance=true"}
{"level":"INFO","ts":"2025-12-10T04:50:55.739Z","msg":"Tuner validation succeeded for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler, server ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler - New NIS: 0.102122"}
{"level":"DEBUG","ts":"2025-12-10T04:50:55.739Z","msg":"Model tuner results: model=unsloth/Meta-Llama-3.1-8B, accelerator=H100, alpha=6.830159, beta=0.050719, gamma=15.668653, delta=0.000259, NIS=0.10"}
{"level":"DEBUG","ts":"2025-12-10T04:50:55.739Z","msg":"Updated tuner status for variant ms-inference-scheduling-llm-d-modelservice-decode: alpha=6.830159, beta=0.050719, gamma=15.668653, delta=0.000259, NIS=0.102122"}
{"level":"INFO","ts":"2025-12-10T04:50:55.739Z","msg":"Tuned performance parameters result: variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler - alpha: 6.830159, beta: 0.050719, gamma: 15.668653, delta: 0.000259"}
{"level":"DEBUG","ts":"2025-12-10T04:50:55.749Z","msg":"Optimization solution - system: Solution: \ns=ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler; c=Premium; m=unsloth/Meta-Llama-3.1-8B; rate=415.71; inTk=246; outTk=493; sol=1, sat=false, alloc={acc=H100; numRep=1; maxBatch=512; cost=100, val=0, itl=8.326644, ttft=17.54817, rho=0.055675056, maxRPM=746.99927}; slo-itl=10, slo-ttft=1000, slo-tps=0 \nAllocationByType: \nname=NVIDIA-H100-80GB-HBM3, count=1, limit=0, cost=100 \ntotalCost=100 \n"}
{"level":"DEBUG","ts":"2025-12-10T04:50:55.749Z","msg":"Setting accelerator name: Name=H100, allocationData={H100 1 512 100 8.326644 17.54817 {415.71 246 493}}"}
{"level":"INFO","ts":"2025-12-10T04:50:55.749Z","msg":"Model-based optimization completed for model: unsloth/Meta-Llama-3.1-8B - model-based targets: map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"WARN","ts":"2025-12-10T04:50:55.749Z","msg":"saturation analysis unavailable, using model-based targets only: modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-10T04:50:55.749Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-12-10T04:50:55.749Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1→target=1"}
{"level":"INFO","ts":"2025-12-10T04:50:55.749Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 1, desired-replicas: 1, accelerator: H100"}
{"level":"INFO","ts":"2025-12-10T04:50:55.749Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=1, accelerator=H100, SaturationOnly=false"}
{"level":"INFO","ts":"2025-12-10T04:50:55.755Z","msg":"Applied Saturation decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1, target=1, reason=model-based only (Saturation unavailable)"}
{"level":"INFO","ts":"2025-12-10T04:50:55.755Z","msg":"Reconciliation completed successfully: mode=model-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-12-10T04:51:55.756Z","msg":"Reconciling VariantAutoscaling: name=ms-inference-scheduling-llm-d-modelservice-decode, namespace=llm-d-inference-scheduler, modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-10T04:51:55.756Z","msg":"saturation scaling config cache updated: entries=1, has_default=true"}
{"level":"INFO","ts":"2025-12-10T04:51:55.756Z","msg":"saturation scaling configuration loaded successfully"}
{"level":"INFO","ts":"2025-12-10T04:51:55.756Z","msg":"Operating in MODEL-ONLY mode: model-based optimization only"}
{"level":"INFO","ts":"2025-12-10T04:51:55.756Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-12-10T04:51:55.756Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-12-10T04:51:55.762Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dgftqv, queueLength=0"}
{"level":"DEBUG","ts":"2025-12-10T04:51:55.762Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"INFO","ts":"2025-12-10T04:51:55.762Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dgftqv, usage=0.056 (5.6%)"}
{"level":"DEBUG","ts":"2025-12-10T04:51:55.762Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"DEBUG","ts":"2025-12-10T04:51:55.765Z","msg":"Pod-to-variant matching successful: totalPods=1, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"DEBUG","ts":"2025-12-10T04:51:55.765Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=1"}
{"level":"DEBUG","ts":"2025-12-10T04:51:55.765Z","msg":"Collected capacity metrics for observability in model-only mode: modelID=unsloth/Meta-Llama-3.1-8B, replicaCount=1"}
{"level":"DEBUG","ts":"2025-12-10T04:51:55.779Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=1, accelerator=H100, ttft=17.34ms, itl=8.19ms, cost=100.00, maxBatch=256, arrivalRate=411.74, avgInputTokens=235.40, avgOutputTokens=433.51"}
{"level":"DEBUG","ts":"2025-12-10T04:51:55.779Z","msg":"Experimental model tuner is enabled globally: tuning model performance parameters for active VAs"}
{"level":"DEBUG","ts":"2025-12-10T04:51:55.779Z","msg":"Using state vals from VA status to tune variant ms-inference-scheduling-llm-d-modelservice-decode: alpha= 6.830159, beta= 0.050719, gamma= 15.668653, delta= 0.000259"}
{"level":"DEBUG","ts":"2025-12-10T04:51:55.779Z","msg":"[Tuner Config] variant=ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler | Initial state: alpha=6.830159, beta=0.050719, gamma=15.668653, delta=0.000259 | Expected obs (for R): TTFT=17.34ms, ITL=8.19ms | Has covariance=true"}
{"level":"INFO","ts":"2025-12-10T04:51:55.782Z","msg":"Tuner validation succeeded for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler, server ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler - New NIS: 0.046164"}
{"level":"DEBUG","ts":"2025-12-10T04:51:55.782Z","msg":"Model tuner results: model=unsloth/Meta-Llama-3.1-8B, accelerator=H100, alpha=6.891470, beta=0.050653, gamma=15.770497, delta=0.000259, NIS=0.05"}
{"level":"DEBUG","ts":"2025-12-10T04:51:55.782Z","msg":"Updated tuner status for variant ms-inference-scheduling-llm-d-modelservice-decode: alpha=6.891470, beta=0.050653, gamma=15.770497, delta=0.000259, NIS=0.046164"}
{"level":"INFO","ts":"2025-12-10T04:51:55.782Z","msg":"Tuned performance parameters result: variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler - alpha: 6.891470, beta: 0.050653, gamma: 15.770497, delta: 0.000259"}
{"level":"DEBUG","ts":"2025-12-10T04:51:55.875Z","msg":"Optimization solution - system: Solution: \ns=ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler; c=Premium; m=unsloth/Meta-Llama-3.1-8B; rate=411.74; inTk=235; outTk=433; sol=1, sat=false, alloc={acc=H100; numRep=1; maxBatch=512; cost=100, val=0, itl=8.175851, ttft=17.313805, rho=0.047571003, maxRPM=834.6885}; slo-itl=10, slo-ttft=1000, slo-tps=0 \nAllocationByType: \nname=NVIDIA-H100-80GB-HBM3, count=1, limit=0, cost=100 \ntotalCost=100 \n"}
{"level":"DEBUG","ts":"2025-12-10T04:51:55.875Z","msg":"Setting accelerator name: Name=H100, allocationData={H100 1 512 100 8.175851 17.313805 {411.74 235 433}}"}
{"level":"INFO","ts":"2025-12-10T04:51:55.875Z","msg":"Model-based optimization completed for model: unsloth/Meta-Llama-3.1-8B - model-based targets: map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"WARN","ts":"2025-12-10T04:51:55.875Z","msg":"saturation analysis unavailable, using model-based targets only: modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-10T04:51:55.875Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-12-10T04:51:55.875Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1→target=1"}
{"level":"INFO","ts":"2025-12-10T04:51:55.875Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 1, desired-replicas: 1, accelerator: H100"}
{"level":"INFO","ts":"2025-12-10T04:51:55.875Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=1, accelerator=H100, SaturationOnly=false"}
{"level":"INFO","ts":"2025-12-10T04:51:55.881Z","msg":"Applied Saturation decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1, target=1, reason=model-based only (Saturation unavailable)"}
{"level":"INFO","ts":"2025-12-10T04:51:55.881Z","msg":"Reconciliation completed successfully: mode=model-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-12-10T04:52:55.882Z","msg":"Reconciling VariantAutoscaling: name=ms-inference-scheduling-llm-d-modelservice-decode, namespace=llm-d-inference-scheduler, modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-10T04:52:55.882Z","msg":"saturation scaling config cache updated: entries=1, has_default=true"}
{"level":"INFO","ts":"2025-12-10T04:52:55.882Z","msg":"saturation scaling configuration loaded successfully"}
{"level":"INFO","ts":"2025-12-10T04:52:55.882Z","msg":"Operating in MODEL-ONLY mode: model-based optimization only"}
{"level":"INFO","ts":"2025-12-10T04:52:55.882Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-12-10T04:52:55.882Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-12-10T04:52:55.888Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dgftqv, queueLength=0"}
{"level":"DEBUG","ts":"2025-12-10T04:52:55.888Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"INFO","ts":"2025-12-10T04:52:55.888Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dgftqv, usage=0.045 (4.5%)"}
{"level":"DEBUG","ts":"2025-12-10T04:52:55.888Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"DEBUG","ts":"2025-12-10T04:52:55.890Z","msg":"Pod-to-variant matching successful: totalPods=1, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"DEBUG","ts":"2025-12-10T04:52:55.890Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=1"}
{"level":"DEBUG","ts":"2025-12-10T04:52:55.890Z","msg":"Collected capacity metrics for observability in model-only mode: modelID=unsloth/Meta-Llama-3.1-8B, replicaCount=1"}
{"level":"DEBUG","ts":"2025-12-10T04:52:55.903Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=1, accelerator=H100, ttft=17.13ms, itl=8.43ms, cost=100.00, maxBatch=256, arrivalRate=446.16, avgInputTokens=224.99, avgOutputTokens=489.34"}
{"level":"DEBUG","ts":"2025-12-10T04:52:55.903Z","msg":"Experimental model tuner is enabled globally: tuning model performance parameters for active VAs"}
{"level":"DEBUG","ts":"2025-12-10T04:52:55.903Z","msg":"Using state vals from VA status to tune variant ms-inference-scheduling-llm-d-modelservice-decode: alpha= 6.891470, beta= 0.050653, gamma= 15.770497, delta= 0.000259"}
{"level":"DEBUG","ts":"2025-12-10T04:52:55.903Z","msg":"[Tuner Config] variant=ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler | Initial state: alpha=6.891470, beta=0.050653, gamma=15.770497, delta=0.000259 | Expected obs (for R): TTFT=17.13ms, ITL=8.43ms | Has covariance=true"}
{"level":"INFO","ts":"2025-12-10T04:52:55.904Z","msg":"Tuner validation succeeded for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler, server ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler - New NIS: 0.267547"}
{"level":"DEBUG","ts":"2025-12-10T04:52:55.904Z","msg":"Model tuner results: model=unsloth/Meta-Llama-3.1-8B, accelerator=H100, alpha=6.841635, beta=0.050344, gamma=15.383755, delta=0.000259, NIS=0.27"}
{"level":"DEBUG","ts":"2025-12-10T04:52:55.904Z","msg":"Updated tuner status for variant ms-inference-scheduling-llm-d-modelservice-decode: alpha=6.841635, beta=0.050344, gamma=15.383755, delta=0.000259, NIS=0.267547"}
{"level":"INFO","ts":"2025-12-10T04:52:55.904Z","msg":"Tuned performance parameters result: variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler - alpha: 6.841635, beta: 0.050344, gamma: 15.383755, delta: 0.000259"}
{"level":"DEBUG","ts":"2025-12-10T04:52:55.913Z","msg":"Optimization solution - system: Solution: \ns=ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler; c=Premium; m=unsloth/Meta-Llama-3.1-8B; rate=446.16; inTk=224; outTk=489; sol=1, sat=false, alloc={acc=H100; numRep=1; maxBatch=512; cost=100, val=0, itl=8.440378, ttft=17.225405, rho=0.060070824, maxRPM=756.0955}; slo-itl=10, slo-ttft=1000, slo-tps=0 \nAllocationByType: \nname=NVIDIA-H100-80GB-HBM3, count=1, limit=0, cost=100 \ntotalCost=100 \n"}
{"level":"DEBUG","ts":"2025-12-10T04:52:55.913Z","msg":"Setting accelerator name: Name=H100, allocationData={H100 1 512 100 8.440378 17.225405 {446.16 224 489}}"}
{"level":"INFO","ts":"2025-12-10T04:52:55.913Z","msg":"Model-based optimization completed for model: unsloth/Meta-Llama-3.1-8B - model-based targets: map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"WARN","ts":"2025-12-10T04:52:55.913Z","msg":"saturation analysis unavailable, using model-based targets only: modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-10T04:52:55.913Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-12-10T04:52:55.913Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1→target=1"}
{"level":"INFO","ts":"2025-12-10T04:52:55.913Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 1, desired-replicas: 1, accelerator: H100"}
{"level":"INFO","ts":"2025-12-10T04:52:55.913Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=1, accelerator=H100, SaturationOnly=false"}
{"level":"INFO","ts":"2025-12-10T04:52:55.919Z","msg":"Applied Saturation decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1, target=1, reason=model-based only (Saturation unavailable)"}
{"level":"INFO","ts":"2025-12-10T04:52:55.919Z","msg":"Reconciliation completed successfully: mode=model-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-12-10T04:53:55.919Z","msg":"Reconciling VariantAutoscaling: name=ms-inference-scheduling-llm-d-modelservice-decode, namespace=llm-d-inference-scheduler, modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-10T04:53:55.919Z","msg":"saturation scaling config cache updated: entries=1, has_default=true"}
{"level":"INFO","ts":"2025-12-10T04:53:55.919Z","msg":"saturation scaling configuration loaded successfully"}
{"level":"INFO","ts":"2025-12-10T04:53:55.919Z","msg":"Operating in MODEL-ONLY mode: model-based optimization only"}
{"level":"INFO","ts":"2025-12-10T04:53:55.919Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-12-10T04:53:55.919Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-12-10T04:53:55.933Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dgftqv, queueLength=0"}
{"level":"DEBUG","ts":"2025-12-10T04:53:55.933Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"INFO","ts":"2025-12-10T04:53:55.933Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dgftqv, usage=0.056 (5.6%)"}
{"level":"DEBUG","ts":"2025-12-10T04:53:55.933Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"DEBUG","ts":"2025-12-10T04:53:55.935Z","msg":"Pod-to-variant matching successful: totalPods=1, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"DEBUG","ts":"2025-12-10T04:53:55.935Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=1"}
{"level":"DEBUG","ts":"2025-12-10T04:53:55.935Z","msg":"Collected capacity metrics for observability in model-only mode: modelID=unsloth/Meta-Llama-3.1-8B, replicaCount=1"}
{"level":"DEBUG","ts":"2025-12-10T04:53:55.947Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=1, accelerator=H100, ttft=17.97ms, itl=8.52ms, cost=100.00, maxBatch=256, arrivalRate=427.63, avgInputTokens=241.51, avgOutputTokens=436.73"}
{"level":"DEBUG","ts":"2025-12-10T04:53:55.947Z","msg":"Experimental model tuner is enabled globally: tuning model performance parameters for active VAs"}
{"level":"DEBUG","ts":"2025-12-10T04:53:55.947Z","msg":"Using state vals from VA status to tune variant ms-inference-scheduling-llm-d-modelservice-decode: alpha= 6.841635, beta= 0.050344, gamma= 15.383755, delta= 0.000259"}
{"level":"DEBUG","ts":"2025-12-10T04:53:55.947Z","msg":"[Tuner Config] variant=ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler | Initial state: alpha=6.841635, beta=0.050344, gamma=15.383755, delta=0.000259 | Expected obs (for R): TTFT=17.97ms, ITL=8.52ms | Has covariance=true"}
{"level":"INFO","ts":"2025-12-10T04:53:55.948Z","msg":"Tuner validation succeeded for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler, server ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler - New NIS: 1.290357"}
{"level":"DEBUG","ts":"2025-12-10T04:53:55.948Z","msg":"Model tuner results: model=unsloth/Meta-Llama-3.1-8B, accelerator=H100, alpha=7.094365, beta=0.049968, gamma=16.067303, delta=0.000259, NIS=1.29"}
{"level":"DEBUG","ts":"2025-12-10T04:53:55.948Z","msg":"Updated tuner status for variant ms-inference-scheduling-llm-d-modelservice-decode: alpha=7.094365, beta=0.049968, gamma=16.067303, delta=0.000259, NIS=1.290357"}
{"level":"INFO","ts":"2025-12-10T04:53:55.948Z","msg":"Tuned performance parameters result: variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler - alpha: 7.094365, beta: 0.049968, gamma: 16.067303, delta: 0.000259"}
{"level":"DEBUG","ts":"2025-12-10T04:53:55.958Z","msg":"Optimization solution - system: Solution: \ns=ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler; c=Premium; m=unsloth/Meta-Llama-3.1-8B; rate=427.63; inTk=241; outTk=436; sol=1, sat=false, alloc={acc=H100; numRep=1; maxBatch=512; cost=100, val=0, itl=8.461479, ttft=17.775509, rho=0.051484298, maxRPM=784.7251}; slo-itl=10, slo-ttft=1000, slo-tps=0 \nAllocationByType: \nname=NVIDIA-H100-80GB-HBM3, count=1, limit=0, cost=100 \ntotalCost=100 \n"}
{"level":"DEBUG","ts":"2025-12-10T04:53:55.958Z","msg":"Setting accelerator name: Name=H100, allocationData={H100 1 512 100 8.461479 17.775509 {427.63 241 436}}"}
{"level":"INFO","ts":"2025-12-10T04:53:55.958Z","msg":"Model-based optimization completed for model: unsloth/Meta-Llama-3.1-8B - model-based targets: map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"WARN","ts":"2025-12-10T04:53:55.958Z","msg":"saturation analysis unavailable, using model-based targets only: modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-10T04:53:55.958Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-12-10T04:53:55.958Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1→target=1"}
{"level":"INFO","ts":"2025-12-10T04:53:55.958Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 1, desired-replicas: 1, accelerator: H100"}
{"level":"INFO","ts":"2025-12-10T04:53:55.958Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=1, accelerator=H100, SaturationOnly=false"}
{"level":"INFO","ts":"2025-12-10T04:53:55.964Z","msg":"Applied Saturation decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1, target=1, reason=model-based only (Saturation unavailable)"}
{"level":"INFO","ts":"2025-12-10T04:53:55.964Z","msg":"Reconciliation completed successfully: mode=model-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-12-10T04:54:55.965Z","msg":"Reconciling VariantAutoscaling: name=ms-inference-scheduling-llm-d-modelservice-decode, namespace=llm-d-inference-scheduler, modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-10T04:54:55.965Z","msg":"saturation scaling config cache updated: entries=1, has_default=true"}
{"level":"INFO","ts":"2025-12-10T04:54:55.965Z","msg":"saturation scaling configuration loaded successfully"}
{"level":"INFO","ts":"2025-12-10T04:54:55.965Z","msg":"Operating in MODEL-ONLY mode: model-based optimization only"}
{"level":"INFO","ts":"2025-12-10T04:54:55.965Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-12-10T04:54:55.965Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-12-10T04:54:55.972Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dgftqv, queueLength=0"}
{"level":"DEBUG","ts":"2025-12-10T04:54:55.972Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"INFO","ts":"2025-12-10T04:54:55.972Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dgftqv, usage=0.048 (4.8%)"}
{"level":"DEBUG","ts":"2025-12-10T04:54:55.972Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"DEBUG","ts":"2025-12-10T04:54:55.975Z","msg":"Pod-to-variant matching successful: totalPods=1, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"DEBUG","ts":"2025-12-10T04:54:55.975Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=1"}
{"level":"DEBUG","ts":"2025-12-10T04:54:55.975Z","msg":"Collected capacity metrics for observability in model-only mode: modelID=unsloth/Meta-Llama-3.1-8B, replicaCount=1"}
{"level":"DEBUG","ts":"2025-12-10T04:54:55.987Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=1, accelerator=H100, ttft=17.45ms, itl=8.57ms, cost=100.00, maxBatch=256, arrivalRate=462.05, avgInputTokens=209.64, avgOutputTokens=464.36"}
{"level":"DEBUG","ts":"2025-12-10T04:54:55.987Z","msg":"Experimental model tuner is enabled globally: tuning model performance parameters for active VAs"}
{"level":"DEBUG","ts":"2025-12-10T04:54:55.987Z","msg":"Using state vals from VA status to tune variant ms-inference-scheduling-llm-d-modelservice-decode: alpha= 7.094365, beta= 0.049968, gamma= 16.067303, delta= 0.000259"}
{"level":"DEBUG","ts":"2025-12-10T04:54:55.987Z","msg":"[Tuner Config] variant=ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler | Initial state: alpha=7.094365, beta=0.049968, gamma=16.067303, delta=0.000259 | Expected obs (for R): TTFT=17.45ms, ITL=8.57ms | Has covariance=true"}
{"level":"INFO","ts":"2025-12-10T04:54:55.988Z","msg":"Tuner validation succeeded for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler, server ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler - New NIS: 0.174180"}
{"level":"DEBUG","ts":"2025-12-10T04:54:55.988Z","msg":"Model tuner results: model=unsloth/Meta-Llama-3.1-8B, accelerator=H100, alpha=7.015637, beta=0.049560, gamma=15.797652, delta=0.000259, NIS=0.17"}
{"level":"DEBUG","ts":"2025-12-10T04:54:55.988Z","msg":"Updated tuner status for variant ms-inference-scheduling-llm-d-modelservice-decode: alpha=7.015637, beta=0.049560, gamma=15.797652, delta=0.000259, NIS=0.174180"}
{"level":"INFO","ts":"2025-12-10T04:54:55.988Z","msg":"Tuned performance parameters result: variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler - alpha: 7.015637, beta: 0.049560, gamma: 15.797652, delta: 0.000259"}
{"level":"DEBUG","ts":"2025-12-10T04:54:55.997Z","msg":"Optimization solution - system: Solution: \ns=ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler; c=Premium; m=unsloth/Meta-Llama-3.1-8B; rate=462.05; inTk=209; outTk=464; sol=1, sat=false, alloc={acc=H100; numRep=1; maxBatch=512; cost=100, val=0, itl=8.589749, ttft=17.516651, rho=0.0600811, maxRPM=764.2406}; slo-itl=10, slo-ttft=1000, slo-tps=0 \nAllocationByType: \nname=NVIDIA-H100-80GB-HBM3, count=1, limit=0, cost=100 \ntotalCost=100 \n"}
{"level":"DEBUG","ts":"2025-12-10T04:54:55.997Z","msg":"Setting accelerator name: Name=H100, allocationData={H100 1 512 100 8.589749 17.516651 {462.05 209 464}}"}
{"level":"INFO","ts":"2025-12-10T04:54:55.997Z","msg":"Model-based optimization completed for model: unsloth/Meta-Llama-3.1-8B - model-based targets: map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"WARN","ts":"2025-12-10T04:54:55.997Z","msg":"saturation analysis unavailable, using model-based targets only: modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-10T04:54:55.997Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-12-10T04:54:55.997Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1→target=1"}
{"level":"INFO","ts":"2025-12-10T04:54:55.997Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 1, desired-replicas: 1, accelerator: H100"}
{"level":"INFO","ts":"2025-12-10T04:54:55.997Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=1, accelerator=H100, SaturationOnly=false"}
{"level":"INFO","ts":"2025-12-10T04:54:56.003Z","msg":"Applied Saturation decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1, target=1, reason=model-based only (Saturation unavailable)"}
{"level":"INFO","ts":"2025-12-10T04:54:56.003Z","msg":"Reconciliation completed successfully: mode=model-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-12-10T04:55:56.004Z","msg":"Reconciling VariantAutoscaling: name=ms-inference-scheduling-llm-d-modelservice-decode, namespace=llm-d-inference-scheduler, modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-10T04:55:56.004Z","msg":"saturation scaling config cache updated: entries=1, has_default=true"}
{"level":"INFO","ts":"2025-12-10T04:55:56.004Z","msg":"saturation scaling configuration loaded successfully"}
{"level":"INFO","ts":"2025-12-10T04:55:56.004Z","msg":"Operating in MODEL-ONLY mode: model-based optimization only"}
{"level":"INFO","ts":"2025-12-10T04:55:56.004Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-12-10T04:55:56.004Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-12-10T04:55:56.018Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dgftqv, usage=0.043 (4.3%)"}
{"level":"DEBUG","ts":"2025-12-10T04:55:56.018Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"INFO","ts":"2025-12-10T04:55:56.018Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dgftqv, queueLength=0"}
{"level":"DEBUG","ts":"2025-12-10T04:55:56.018Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"DEBUG","ts":"2025-12-10T04:55:56.020Z","msg":"Pod-to-variant matching successful: totalPods=1, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"DEBUG","ts":"2025-12-10T04:55:56.020Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=1"}
{"level":"DEBUG","ts":"2025-12-10T04:55:56.020Z","msg":"Collected capacity metrics for observability in model-only mode: modelID=unsloth/Meta-Llama-3.1-8B, replicaCount=1"}
{"level":"DEBUG","ts":"2025-12-10T04:55:56.032Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=1, accelerator=H100, ttft=17.68ms, itl=8.23ms, cost=100.00, maxBatch=256, arrivalRate=398.50, avgInputTokens=254.70, avgOutputTokens=462.99"}
{"level":"DEBUG","ts":"2025-12-10T04:55:56.032Z","msg":"Experimental model tuner is enabled globally: tuning model performance parameters for active VAs"}
{"level":"DEBUG","ts":"2025-12-10T04:55:56.032Z","msg":"Using state vals from VA status to tune variant ms-inference-scheduling-llm-d-modelservice-decode: alpha= 7.015637, beta= 0.049560, gamma= 15.797652, delta= 0.000259"}
{"level":"DEBUG","ts":"2025-12-10T04:55:56.032Z","msg":"[Tuner Config] variant=ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler | Initial state: alpha=7.015637, beta=0.049560, gamma=15.797652, delta=0.000259 | Expected obs (for R): TTFT=17.68ms, ITL=8.23ms | Has covariance=true"}
{"level":"INFO","ts":"2025-12-10T04:55:56.032Z","msg":"Tuner validation succeeded for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler, server ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler - New NIS: 0.064205"}
{"level":"DEBUG","ts":"2025-12-10T04:55:56.032Z","msg":"Model tuner results: model=unsloth/Meta-Llama-3.1-8B, accelerator=H100, alpha=6.938542, beta=0.049680, gamma=15.915035, delta=0.000259, NIS=0.06"}
{"level":"DEBUG","ts":"2025-12-10T04:55:56.032Z","msg":"Updated tuner status for variant ms-inference-scheduling-llm-d-modelservice-decode: alpha=6.938542, beta=0.049680, gamma=15.915035, delta=0.000259, NIS=0.064205"}
{"level":"INFO","ts":"2025-12-10T04:55:56.032Z","msg":"Tuned performance parameters result: variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler - alpha: 6.938542, beta: 0.049680, gamma: 15.915035, delta: 0.000259"}
{"level":"DEBUG","ts":"2025-12-10T04:55:56.041Z","msg":"Optimization solution - system: Solution: \ns=ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler; c=Premium; m=unsloth/Meta-Llama-3.1-8B; rate=398.5; inTk=254; outTk=462; sol=1, sat=false, alloc={acc=H100; numRep=1; maxBatch=512; cost=100, val=0, itl=8.248765, ttft=17.650166, rho=0.04955735, maxRPM=785.6283}; slo-itl=10, slo-ttft=1000, slo-tps=0 \nAllocationByType: \nname=NVIDIA-H100-80GB-HBM3, count=1, limit=0, cost=100 \ntotalCost=100 \n"}
{"level":"DEBUG","ts":"2025-12-10T04:55:56.041Z","msg":"Setting accelerator name: Name=H100, allocationData={H100 1 512 100 8.248765 17.650166 {398.5 254 462}}"}
{"level":"INFO","ts":"2025-12-10T04:55:56.041Z","msg":"Model-based optimization completed for model: unsloth/Meta-Llama-3.1-8B - model-based targets: map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"WARN","ts":"2025-12-10T04:55:56.041Z","msg":"saturation analysis unavailable, using model-based targets only: modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-10T04:55:56.041Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-12-10T04:55:56.041Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1→target=1"}
{"level":"INFO","ts":"2025-12-10T04:55:56.042Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 1, desired-replicas: 1, accelerator: H100"}
{"level":"INFO","ts":"2025-12-10T04:55:56.042Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=1, accelerator=H100, SaturationOnly=false"}
{"level":"INFO","ts":"2025-12-10T04:55:56.047Z","msg":"Applied Saturation decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1, target=1, reason=model-based only (Saturation unavailable)"}
{"level":"INFO","ts":"2025-12-10T04:55:56.047Z","msg":"Reconciliation completed successfully: mode=model-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-12-10T04:56:56.047Z","msg":"Reconciling VariantAutoscaling: name=ms-inference-scheduling-llm-d-modelservice-decode, namespace=llm-d-inference-scheduler, modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-10T04:56:56.047Z","msg":"saturation scaling config cache updated: entries=1, has_default=true"}
{"level":"INFO","ts":"2025-12-10T04:56:56.047Z","msg":"saturation scaling configuration loaded successfully"}
{"level":"INFO","ts":"2025-12-10T04:56:56.047Z","msg":"Operating in MODEL-ONLY mode: model-based optimization only"}
{"level":"INFO","ts":"2025-12-10T04:56:56.047Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-12-10T04:56:56.047Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-12-10T04:56:56.052Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dgftqv, usage=0.036 (3.6%)"}
{"level":"INFO","ts":"2025-12-10T04:56:56.052Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dgftqv, queueLength=0"}
{"level":"DEBUG","ts":"2025-12-10T04:56:56.052Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"DEBUG","ts":"2025-12-10T04:56:56.052Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"DEBUG","ts":"2025-12-10T04:56:56.054Z","msg":"Pod-to-variant matching successful: totalPods=1, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"DEBUG","ts":"2025-12-10T04:56:56.054Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=1"}
{"level":"DEBUG","ts":"2025-12-10T04:56:56.054Z","msg":"Collected capacity metrics for observability in model-only mode: modelID=unsloth/Meta-Llama-3.1-8B, replicaCount=1"}
{"level":"DEBUG","ts":"2025-12-10T04:56:56.066Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=1, accelerator=H100, ttft=17.78ms, itl=8.29ms, cost=100.00, maxBatch=256, arrivalRate=436.89, avgInputTokens=240.75, avgOutputTokens=453.14"}
{"level":"DEBUG","ts":"2025-12-10T04:56:56.066Z","msg":"Experimental model tuner is enabled globally: tuning model performance parameters for active VAs"}
{"level":"DEBUG","ts":"2025-12-10T04:56:56.066Z","msg":"Using state vals from VA status to tune variant ms-inference-scheduling-llm-d-modelservice-decode: alpha= 6.938542, beta= 0.049680, gamma= 15.915035, delta= 0.000259"}
{"level":"DEBUG","ts":"2025-12-10T04:56:56.066Z","msg":"[Tuner Config] variant=ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler | Initial state: alpha=6.938542, beta=0.049680, gamma=15.915035, delta=0.000259 | Expected obs (for R): TTFT=17.78ms, ITL=8.29ms | Has covariance=true"}
{"level":"INFO","ts":"2025-12-10T04:56:56.067Z","msg":"Tuner validation succeeded for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler, server ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler - New NIS: 0.028617"}
{"level":"DEBUG","ts":"2025-12-10T04:56:56.067Z","msg":"Model tuner results: model=unsloth/Meta-Llama-3.1-8B, accelerator=H100, alpha=6.892407, beta=0.049567, gamma=15.991681, delta=0.000259, NIS=0.03"}
{"level":"DEBUG","ts":"2025-12-10T04:56:56.067Z","msg":"Updated tuner status for variant ms-inference-scheduling-llm-d-modelservice-decode: alpha=6.892407, beta=0.049567, gamma=15.991681, delta=0.000259, NIS=0.028617"}
{"level":"INFO","ts":"2025-12-10T04:56:56.067Z","msg":"Tuned performance parameters result: variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler - alpha: 6.892407, beta: 0.049567, gamma: 15.991681, delta: 0.000259"}
{"level":"DEBUG","ts":"2025-12-10T04:56:56.077Z","msg":"Optimization solution - system: Solution: \ns=ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler; c=Premium; m=unsloth/Meta-Llama-3.1-8B; rate=436.89; inTk=240; outTk=453; sol=1, sat=false, alloc={acc=H100; numRep=1; maxBatch=512; cost=100, val=0, itl=8.302903, ttft=17.760614, rho=0.053625304, maxRPM=815.3654}; slo-itl=10, slo-ttft=1000, slo-tps=0 \nAllocationByType: \nname=NVIDIA-H100-80GB-HBM3, count=1, limit=0, cost=100 \ntotalCost=100 \n"}
{"level":"DEBUG","ts":"2025-12-10T04:56:56.077Z","msg":"Setting accelerator name: Name=H100, allocationData={H100 1 512 100 8.302903 17.760614 {436.89 240 453}}"}
{"level":"INFO","ts":"2025-12-10T04:56:56.077Z","msg":"Model-based optimization completed for model: unsloth/Meta-Llama-3.1-8B - model-based targets: map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"WARN","ts":"2025-12-10T04:56:56.077Z","msg":"saturation analysis unavailable, using model-based targets only: modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-10T04:56:56.077Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-12-10T04:56:56.077Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1→target=1"}
{"level":"INFO","ts":"2025-12-10T04:56:56.077Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 1, desired-replicas: 1, accelerator: H100"}
{"level":"INFO","ts":"2025-12-10T04:56:56.077Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=1, accelerator=H100, SaturationOnly=false"}
{"level":"INFO","ts":"2025-12-10T04:56:56.082Z","msg":"Applied Saturation decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1, target=1, reason=model-based only (Saturation unavailable)"}
{"level":"INFO","ts":"2025-12-10T04:56:56.082Z","msg":"Reconciliation completed successfully: mode=model-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-12-10T04:57:56.082Z","msg":"Reconciling VariantAutoscaling: name=ms-inference-scheduling-llm-d-modelservice-decode, namespace=llm-d-inference-scheduler, modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-10T04:57:56.083Z","msg":"saturation scaling config cache updated: entries=1, has_default=true"}
{"level":"INFO","ts":"2025-12-10T04:57:56.083Z","msg":"saturation scaling configuration loaded successfully"}
{"level":"INFO","ts":"2025-12-10T04:57:56.083Z","msg":"Operating in MODEL-ONLY mode: model-based optimization only"}
{"level":"INFO","ts":"2025-12-10T04:57:56.083Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-12-10T04:57:56.083Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-12-10T04:57:56.096Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dgftqv, queueLength=0"}
{"level":"DEBUG","ts":"2025-12-10T04:57:56.096Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"INFO","ts":"2025-12-10T04:57:56.096Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dgftqv, usage=0.043 (4.3%)"}
{"level":"DEBUG","ts":"2025-12-10T04:57:56.096Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"DEBUG","ts":"2025-12-10T04:57:56.098Z","msg":"Pod-to-variant matching successful: totalPods=1, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"DEBUG","ts":"2025-12-10T04:57:56.098Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=1"}
{"level":"DEBUG","ts":"2025-12-10T04:57:56.098Z","msg":"Collected capacity metrics for observability in model-only mode: modelID=unsloth/Meta-Llama-3.1-8B, replicaCount=1"}
{"level":"DEBUG","ts":"2025-12-10T04:57:56.109Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=1, accelerator=H100, ttft=17.13ms, itl=8.02ms, cost=100.00, maxBatch=256, arrivalRate=168.14, avgInputTokens=214.61, avgOutputTokens=580.61"}
{"level":"DEBUG","ts":"2025-12-10T04:57:56.109Z","msg":"Experimental model tuner is enabled globally: tuning model performance parameters for active VAs"}
{"level":"DEBUG","ts":"2025-12-10T04:57:56.109Z","msg":"Using state vals from VA status to tune variant ms-inference-scheduling-llm-d-modelservice-decode: alpha= 6.892407, beta= 0.049567, gamma= 15.991681, delta= 0.000259"}
{"level":"DEBUG","ts":"2025-12-10T04:57:56.109Z","msg":"[Tuner Config] variant=ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler | Initial state: alpha=6.892407, beta=0.049567, gamma=15.991681, delta=0.000259 | Expected obs (for R): TTFT=17.13ms, ITL=8.02ms | Has covariance=true"}
{"level":"INFO","ts":"2025-12-10T04:57:56.110Z","msg":"Tuner validation succeeded for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler, server ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler - New NIS: 1.044758"}
{"level":"DEBUG","ts":"2025-12-10T04:57:56.110Z","msg":"Model tuner results: model=unsloth/Meta-Llama-3.1-8B, accelerator=H100, alpha=7.293937, beta=0.046236, gamma=16.286325, delta=0.000259, NIS=1.04"}
{"level":"DEBUG","ts":"2025-12-10T04:57:56.110Z","msg":"Updated tuner status for variant ms-inference-scheduling-llm-d-modelservice-decode: alpha=7.293937, beta=0.046236, gamma=16.286325, delta=0.000259, NIS=1.044758"}
{"level":"INFO","ts":"2025-12-10T04:57:56.110Z","msg":"Tuned performance parameters result: variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler - alpha: 7.293937, beta: 0.046236, gamma: 16.286325, delta: 0.000259"}
{"level":"DEBUG","ts":"2025-12-10T04:57:56.119Z","msg":"Optimization solution - system: Solution: \ns=ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler; c=Premium; m=unsloth/Meta-Llama-3.1-8B; rate=168.14; inTk=214; outTk=580; sol=1, sat=false, alloc={acc=H100; numRep=1; maxBatch=512; cost=100, val=0, itl=7.9378915, ttft=17.05768, rho=0.025248894, maxRPM=594.12695}; slo-itl=10, slo-ttft=1000, slo-tps=0 \nAllocationByType: \nname=NVIDIA-H100-80GB-HBM3, count=1, limit=0, cost=100 \ntotalCost=100 \n"}
{"level":"DEBUG","ts":"2025-12-10T04:57:56.119Z","msg":"Setting accelerator name: Name=H100, allocationData={H100 1 512 100 7.9378915 17.05768 {168.14 214 580}}"}
{"level":"INFO","ts":"2025-12-10T04:57:56.119Z","msg":"Model-based optimization completed for model: unsloth/Meta-Llama-3.1-8B - model-based targets: map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"WARN","ts":"2025-12-10T04:57:56.119Z","msg":"saturation analysis unavailable, using model-based targets only: modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-10T04:57:56.119Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-12-10T04:57:56.119Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1→target=1"}
{"level":"INFO","ts":"2025-12-10T04:57:56.119Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 1, desired-replicas: 1, accelerator: H100"}
{"level":"INFO","ts":"2025-12-10T04:57:56.119Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=1, accelerator=H100, SaturationOnly=false"}
{"level":"INFO","ts":"2025-12-10T04:57:56.125Z","msg":"Applied Saturation decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1, target=1, reason=model-based only (Saturation unavailable)"}
{"level":"INFO","ts":"2025-12-10T04:57:56.125Z","msg":"Reconciliation completed successfully: mode=model-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-12-10T04:58:56.125Z","msg":"Reconciling VariantAutoscaling: name=ms-inference-scheduling-llm-d-modelservice-decode, namespace=llm-d-inference-scheduler, modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-10T04:58:56.125Z","msg":"saturation scaling config cache updated: entries=1, has_default=true"}
{"level":"INFO","ts":"2025-12-10T04:58:56.125Z","msg":"saturation scaling configuration loaded successfully"}
{"level":"INFO","ts":"2025-12-10T04:58:56.125Z","msg":"Operating in MODEL-ONLY mode: model-based optimization only"}
{"level":"INFO","ts":"2025-12-10T04:58:56.125Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-12-10T04:58:56.125Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-12-10T04:58:56.130Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dgftqv, usage=0.000 (0.0%)"}
{"level":"DEBUG","ts":"2025-12-10T04:58:56.130Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"INFO","ts":"2025-12-10T04:58:56.130Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dgftqv, queueLength=0"}
{"level":"DEBUG","ts":"2025-12-10T04:58:56.130Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"DEBUG","ts":"2025-12-10T04:58:56.132Z","msg":"Pod-to-variant matching successful: totalPods=1, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"DEBUG","ts":"2025-12-10T04:58:56.132Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=1"}
{"level":"DEBUG","ts":"2025-12-10T04:58:56.132Z","msg":"Collected capacity metrics for observability in model-only mode: modelID=unsloth/Meta-Llama-3.1-8B, replicaCount=1"}
{"level":"DEBUG","ts":"2025-12-10T04:58:56.144Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=1, accelerator=H100, ttft=0.00ms, itl=0.00ms, cost=100.00, maxBatch=256, arrivalRate=0.00, avgInputTokens=0.00, avgOutputTokens=0.00"}
{"level":"DEBUG","ts":"2025-12-10T04:58:56.144Z","msg":"Experimental model tuner is enabled globally: tuning model performance parameters for active VAs"}
{"level":"DEBUG","ts":"2025-12-10T04:58:56.144Z","msg":"Using state vals from VA status to tune variant ms-inference-scheduling-llm-d-modelservice-decode: alpha= 7.293937, beta= 0.046236, gamma= 16.286325, delta= 0.000259"}
{"level":"DEBUG","ts":"2025-12-10T04:58:56.144Z","msg":"Skipping tuning for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler due to insufficient load. Using fallback parameters."}
{"level":"INFO","ts":"2025-12-10T04:58:56.144Z","msg":"Using tuned parameters from status for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler"}
{"level":"DEBUG","ts":"2025-12-10T04:58:56.144Z","msg":"Updated SystemData for model=unsloth/Meta-Llama-3.1-8B, accelerator=H100: alpha=7.293937, beta=0.046236, gamma=16.286325, delta=0.000259"}
{"level":"DEBUG","ts":"2025-12-10T04:58:56.144Z","msg":"Optimization solution - system: Solution: \ns=ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler; c=Premium; m=unsloth/Meta-Llama-3.1-8B; rate=0; inTk=0; outTk=0; sol=1, sat=false, alloc={acc=H100; numRep=1; maxBatch=512; cost=100, val=0, itl=7.3401732, ttft=16.286585, rho=0, maxRPM=650112.56}; slo-itl=10, slo-ttft=1000, slo-tps=0 \nAllocationByType: \nname=NVIDIA-H100-80GB-HBM3, count=1, limit=0, cost=100 \ntotalCost=100 \n"}
{"level":"DEBUG","ts":"2025-12-10T04:58:56.144Z","msg":"Setting accelerator name: Name=H100, allocationData={H100 1 512 100 7.3401732 16.286585 {0 0 0}}"}
{"level":"INFO","ts":"2025-12-10T04:58:56.144Z","msg":"Model-based optimization completed for model: unsloth/Meta-Llama-3.1-8B - model-based targets: map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"WARN","ts":"2025-12-10T04:58:56.144Z","msg":"saturation analysis unavailable, using model-based targets only: modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-10T04:58:56.144Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-12-10T04:58:56.144Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1→target=1"}
{"level":"INFO","ts":"2025-12-10T04:58:56.144Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 1, desired-replicas: 1, accelerator: H100"}
{"level":"INFO","ts":"2025-12-10T04:58:56.144Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=1, accelerator=H100, SaturationOnly=false"}
{"level":"INFO","ts":"2025-12-10T04:58:56.149Z","msg":"Applied Saturation decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1, target=1, reason=model-based only (Saturation unavailable)"}
{"level":"INFO","ts":"2025-12-10T04:58:56.149Z","msg":"Reconciliation completed successfully: mode=model-only, modelsProcessed=1, decisionsApplied=1"}
