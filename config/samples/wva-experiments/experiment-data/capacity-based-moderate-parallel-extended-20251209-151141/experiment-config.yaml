# Capacity-Based WVA with Extended Parallel Overlapping Jobs
# Extended 20+ minute scenario with moderate peak load for sustained observation
# Jobs run in parallel with staggered start times to create cumulative load
# Tests capacity analyzer's response to sustained overlapping workloads

name: capacity-based-moderate-parallel-extended
description: "Capacity-based WVA with extended overlapping moderate load (20+ mins, peak ~32 req/s)"
mode: capacity-based

# Kubernetes Configuration
namespace: llm-d-inference-scheduler
controller_namespace: workload-variant-autoscaler-system
controller_pod_prefix: workload-variant-autoscaler-controller-manager
deployment: ms-inference-scheduling-llm-d-modelservice-decode
model_name: unsloth/Meta-Llama-3.1-8B

# Metrics Collection
metrics:
  interval: 5  # seconds between log polls
  log_level: INFO

# Extended Parallel Workload Sequence
# Each job runs for 10 minutes with 3-minute stagger
workloads:
  - name: phase-1-baseline
    job_manifest: ../../not_wva/workloads/sharegpt-load-job-moderate-10-extended.yaml  # 10 req/s, 10min
    duration: 600  # 10 minutes
    start_delay: 0  # Start immediately
    description: "Phase 1: 10 req/s baseline load"
    
  - name: phase-2-ramp-up
    job_manifest: ../../not_wva/workloads/sharegpt-load-job-moderate-12-extended.yaml  # 12 req/s, 10min
    duration: 600  # 10 minutes
    start_delay: 180  # Start 3min after experiment begins
    description: "Phase 2: 12 req/s ramp-up load (overlaps with phase 1)"
    
  - name: phase-3-sustained
    job_manifest: ../../not_wva/workloads/sharegpt-load-job-moderate-10-phase3.yaml  # 10 req/s
    duration: 600  # 10 minutes
    start_delay: 360  # Start 6min after experiment begins
    description: "Phase 3: 10 req/s sustained load (overlaps with phase 2)"
    
  - name: phase-4-cooldown
    job_manifest: ../../not_wva/workloads/sharegpt-load-job-moderate-10-phase4.yaml  # 10 req/s
    duration: 600  # 10 minutes
    start_delay: 540  # Start 9min after experiment begins
    description: "Phase 4: 10 req/s cooldown load (overlaps with phase 3)"

# Timeline visualization (EXTENDED - 23 minutes total):
# Time:    0s    180s  360s  540s  600s  780s  960s  1140s 1200s 1320s 1380s
# Job1:    [=================================]  (10 req/s, 10min)
# Job2:              [=================================]  (12 req/s, 10min)
# Job3:                          [=================================]  (10 req/s, 10min)
# Job4:                                      [=================================]  (10 req/s, 10min)
# Load:    10    22    32    32    22    22    20    10    10    10    0
# 
# Peak load: 32 req/s (moderate, manageable)
# Duration: ~23 minutes total runtime
# Longest overlap period: 360-600s (4 minutes at peak 32 req/s)
# 
# Expected behavior with WVA capacity-based:
# - 0-180s: Initial warmup with 10 req/s (1-2 replicas)
# - 180-360s: Ramp-up to 22 req/s (KV cache usage increases)
# - 360-600s: Peak at 32 req/s (capacity analyzer scales up to 3-4 replicas)
# - 600-780s: Gradual decrease to 22 req/s (KV cache usage decreases)
# - 780-960s: Further decrease to 20 req/s
# - 960-1140s: Cooldown to 10 req/s
# - 1140-1320s: Final cooldown at 10 req/s
# - 1320-1380s: Scale-down observation period
#
# Capacity analyzer behavior to observe:
# - KV cache and queue metrics during sustained 32 req/s period
# - Replica adjustments based on capacity constraints
# - No overshooting during pod startup (ReadyReplicas fix)
# - Capacity headroom management during load transitions

slo:
  target_itl_ms: 30  # Capacity-based target

output:
  base_dir: experiment-data
  save_logs: true
  save_metrics: true
