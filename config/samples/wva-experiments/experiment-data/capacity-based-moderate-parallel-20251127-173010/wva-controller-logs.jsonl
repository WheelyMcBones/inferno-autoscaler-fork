{"level":"INFO","ts":"2025-11-27T22:25:12.860Z","msg":"Operating in CAPACITY-ONLY mode: reactive capacity-based scaling only"}
{"level":"INFO","ts":"2025-11-27T22:25:12.860Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-11-27T22:25:12.860Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-11-27T22:25:12.874Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=1, accelerator=H100, ttft=0.00ms, itl=0.00ms, cost=10.00"}
{"level":"INFO","ts":"2025-11-27T22:25:12.875Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9wj976, usage=0.000 (0.0%)"}
{"level":"DEBUG","ts":"2025-11-27T22:25:12.875Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"INFO","ts":"2025-11-27T22:25:12.875Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9wj976, queueLength=0"}
{"level":"DEBUG","ts":"2025-11-27T22:25:12.875Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"DEBUG","ts":"2025-11-27T22:25:12.876Z","msg":"Pod-to-variant matching successful: totalPods=1, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"DEBUG","ts":"2025-11-27T22:25:12.876Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=1"}
{"level":"DEBUG","ts":"2025-11-27T22:25:12.876Z","msg":"Collected capacity metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, metricsCount=1"}
{"level":"DEBUG","ts":"2025-11-27T22:25:12.876Z","msg":"Variant analysis initialized: variant=ms-inference-scheduling-llm-d-modelservice-decode, accelerator=H100, cost=10.00, replicaCount=1"}
{"level":"DEBUG","ts":"2025-11-27T22:25:12.876Z","msg":"Scale-down unsafe: insufficient non-saturated replicas: nonSaturated=1, required=2"}
{"level":"DEBUG","ts":"2025-11-27T22:25:12.876Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, totalReplicas=1, nonSaturated=1, avgSpareKv=0.500, avgSpareQueue=5.0, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"INFO","ts":"2025-11-27T22:25:12.876Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, totalReplicas=1, nonSaturated=1, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"DEBUG","ts":"2025-11-27T22:25:12.876Z","msg":"Capacity targets: no scaling needed (avgSpareKv=0.500, avgSpareQueue=5.0, all variants stable)"}
{"level":"DEBUG","ts":"2025-11-27T22:25:12.876Z","msg":"Capacity targets calculated: modelID=unsloth/Meta-Llama-3.1-8B, targets=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"INFO","ts":"2025-11-27T22:25:12.876Z","msg":"Capacity-only decisions made for model: unsloth/Meta-Llama-3.1-8B - decision count: 0"}
{"level":"INFO","ts":"2025-11-27T22:25:12.876Z","msg":"No scaling decisions to apply"}
{"level":"INFO","ts":"2025-11-27T22:25:12.876Z","msg":"Reconciliation completed successfully: mode=capacity-only, modelsProcessed=1, decisionsApplied=0"}
{"level":"INFO","ts":"2025-11-27T22:25:13.215Z","msg":"Operating in CAPACITY-ONLY mode: reactive capacity-based scaling only"}
{"level":"INFO","ts":"2025-11-27T22:25:13.215Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-11-27T22:25:13.215Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-11-27T22:25:13.245Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=1, accelerator=H100, ttft=0.00ms, itl=0.00ms, cost=10.00"}
{"level":"INFO","ts":"2025-11-27T22:25:13.247Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9wj976, queueLength=0"}
{"level":"DEBUG","ts":"2025-11-27T22:25:13.247Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"INFO","ts":"2025-11-27T22:25:13.247Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9wj976, usage=0.000 (0.0%)"}
{"level":"DEBUG","ts":"2025-11-27T22:25:13.247Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"DEBUG","ts":"2025-11-27T22:25:13.247Z","msg":"Pod-to-variant matching successful: totalPods=1, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"DEBUG","ts":"2025-11-27T22:25:13.247Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=1"}
{"level":"DEBUG","ts":"2025-11-27T22:25:13.247Z","msg":"Collected capacity metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, metricsCount=1"}
{"level":"DEBUG","ts":"2025-11-27T22:25:13.247Z","msg":"Variant analysis initialized: variant=ms-inference-scheduling-llm-d-modelservice-decode, accelerator=H100, cost=10.00, replicaCount=1"}
{"level":"DEBUG","ts":"2025-11-27T22:25:13.247Z","msg":"Scale-down unsafe: insufficient non-saturated replicas: nonSaturated=1, required=2"}
{"level":"DEBUG","ts":"2025-11-27T22:25:13.247Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, totalReplicas=1, nonSaturated=1, avgSpareKv=0.500, avgSpareQueue=5.0, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"INFO","ts":"2025-11-27T22:25:13.247Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, totalReplicas=1, nonSaturated=1, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"DEBUG","ts":"2025-11-27T22:25:13.247Z","msg":"Capacity targets: no scaling needed (avgSpareKv=0.500, avgSpareQueue=5.0, all variants stable)"}
{"level":"DEBUG","ts":"2025-11-27T22:25:13.247Z","msg":"Capacity targets calculated: modelID=unsloth/Meta-Llama-3.1-8B, targets=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"INFO","ts":"2025-11-27T22:25:13.247Z","msg":"Capacity-only decisions made for model: unsloth/Meta-Llama-3.1-8B - decision count: 0"}
{"level":"INFO","ts":"2025-11-27T22:25:13.247Z","msg":"No scaling decisions to apply"}
{"level":"INFO","ts":"2025-11-27T22:25:13.247Z","msg":"Reconciliation completed successfully: mode=capacity-only, modelsProcessed=1, decisionsApplied=0"}
{"level":"INFO","ts":"2025-11-27T22:25:42.877Z","msg":"Operating in CAPACITY-ONLY mode: reactive capacity-based scaling only"}
{"level":"INFO","ts":"2025-11-27T22:25:42.877Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-11-27T22:25:42.877Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-11-27T22:25:42.897Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=1, accelerator=H100, ttft=0.00ms, itl=0.00ms, cost=10.00"}
{"level":"INFO","ts":"2025-11-27T22:25:42.898Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9wj976, queueLength=0"}
{"level":"DEBUG","ts":"2025-11-27T22:25:42.898Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"INFO","ts":"2025-11-27T22:25:42.898Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9wj976, usage=0.000 (0.0%)"}
{"level":"DEBUG","ts":"2025-11-27T22:25:42.898Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"DEBUG","ts":"2025-11-27T22:25:42.898Z","msg":"Pod-to-variant matching successful: totalPods=1, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"DEBUG","ts":"2025-11-27T22:25:42.898Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=1"}
{"level":"DEBUG","ts":"2025-11-27T22:25:42.898Z","msg":"Collected capacity metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, metricsCount=1"}
{"level":"DEBUG","ts":"2025-11-27T22:25:42.898Z","msg":"Variant analysis initialized: variant=ms-inference-scheduling-llm-d-modelservice-decode, accelerator=H100, cost=10.00, replicaCount=1"}
{"level":"DEBUG","ts":"2025-11-27T22:25:42.898Z","msg":"Scale-down unsafe: insufficient non-saturated replicas: nonSaturated=1, required=2"}
{"level":"DEBUG","ts":"2025-11-27T22:25:42.898Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, totalReplicas=1, nonSaturated=1, avgSpareKv=0.500, avgSpareQueue=5.0, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"INFO","ts":"2025-11-27T22:25:42.898Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, totalReplicas=1, nonSaturated=1, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"DEBUG","ts":"2025-11-27T22:25:42.898Z","msg":"Capacity targets: no scaling needed (avgSpareKv=0.500, avgSpareQueue=5.0, all variants stable)"}
{"level":"DEBUG","ts":"2025-11-27T22:25:42.898Z","msg":"Capacity targets calculated: modelID=unsloth/Meta-Llama-3.1-8B, targets=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"INFO","ts":"2025-11-27T22:25:42.898Z","msg":"Capacity-only decisions made for model: unsloth/Meta-Llama-3.1-8B - decision count: 0"}
{"level":"INFO","ts":"2025-11-27T22:25:42.898Z","msg":"No scaling decisions to apply"}
{"level":"INFO","ts":"2025-11-27T22:25:42.898Z","msg":"Reconciliation completed successfully: mode=capacity-only, modelsProcessed=1, decisionsApplied=0"}
{"level":"INFO","ts":"2025-11-27T22:25:43.250Z","msg":"Operating in CAPACITY-ONLY mode: reactive capacity-based scaling only"}
{"level":"INFO","ts":"2025-11-27T22:25:43.251Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-11-27T22:25:43.251Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-11-27T22:25:43.285Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=1, accelerator=H100, ttft=0.00ms, itl=0.00ms, cost=10.00"}
{"level":"INFO","ts":"2025-11-27T22:25:43.287Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9wj976, queueLength=0"}
{"level":"DEBUG","ts":"2025-11-27T22:25:43.287Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"INFO","ts":"2025-11-27T22:25:43.287Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9wj976, usage=0.000 (0.0%)"}
{"level":"DEBUG","ts":"2025-11-27T22:25:43.287Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"DEBUG","ts":"2025-11-27T22:25:43.287Z","msg":"Pod-to-variant matching successful: totalPods=1, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"DEBUG","ts":"2025-11-27T22:25:43.287Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=1"}
{"level":"DEBUG","ts":"2025-11-27T22:25:43.287Z","msg":"Collected capacity metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, metricsCount=1"}
{"level":"DEBUG","ts":"2025-11-27T22:25:43.287Z","msg":"Variant analysis initialized: variant=ms-inference-scheduling-llm-d-modelservice-decode, accelerator=H100, cost=10.00, replicaCount=1"}
{"level":"DEBUG","ts":"2025-11-27T22:25:43.287Z","msg":"Scale-down unsafe: insufficient non-saturated replicas: nonSaturated=1, required=2"}
{"level":"DEBUG","ts":"2025-11-27T22:25:43.287Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, totalReplicas=1, nonSaturated=1, avgSpareKv=0.500, avgSpareQueue=5.0, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"INFO","ts":"2025-11-27T22:25:43.287Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, totalReplicas=1, nonSaturated=1, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"DEBUG","ts":"2025-11-27T22:25:43.287Z","msg":"Capacity targets: no scaling needed (avgSpareKv=0.500, avgSpareQueue=5.0, all variants stable)"}
{"level":"DEBUG","ts":"2025-11-27T22:25:43.287Z","msg":"Capacity targets calculated: modelID=unsloth/Meta-Llama-3.1-8B, targets=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"INFO","ts":"2025-11-27T22:25:43.287Z","msg":"Capacity-only decisions made for model: unsloth/Meta-Llama-3.1-8B - decision count: 0"}
{"level":"INFO","ts":"2025-11-27T22:25:43.287Z","msg":"No scaling decisions to apply"}
{"level":"INFO","ts":"2025-11-27T22:25:43.287Z","msg":"Reconciliation completed successfully: mode=capacity-only, modelsProcessed=1, decisionsApplied=0"}
{"level":"INFO","ts":"2025-11-27T22:26:12.899Z","msg":"Operating in CAPACITY-ONLY mode: reactive capacity-based scaling only"}
{"level":"INFO","ts":"2025-11-27T22:26:12.899Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-11-27T22:26:12.899Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-11-27T22:26:12.914Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=1, accelerator=H100, ttft=0.00ms, itl=0.00ms, cost=10.00"}
{"level":"INFO","ts":"2025-11-27T22:26:12.915Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9wj976, usage=0.000 (0.0%)"}
{"level":"DEBUG","ts":"2025-11-27T22:26:12.915Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"INFO","ts":"2025-11-27T22:26:12.915Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9wj976, queueLength=0"}
{"level":"DEBUG","ts":"2025-11-27T22:26:12.915Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"DEBUG","ts":"2025-11-27T22:26:12.915Z","msg":"Pod-to-variant matching successful: totalPods=1, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"DEBUG","ts":"2025-11-27T22:26:12.915Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=1"}
{"level":"DEBUG","ts":"2025-11-27T22:26:12.915Z","msg":"Collected capacity metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, metricsCount=1"}
{"level":"DEBUG","ts":"2025-11-27T22:26:12.915Z","msg":"Variant analysis initialized: variant=ms-inference-scheduling-llm-d-modelservice-decode, accelerator=H100, cost=10.00, replicaCount=1"}
{"level":"DEBUG","ts":"2025-11-27T22:26:12.915Z","msg":"Scale-down unsafe: insufficient non-saturated replicas: nonSaturated=1, required=2"}
{"level":"DEBUG","ts":"2025-11-27T22:26:12.915Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, totalReplicas=1, nonSaturated=1, avgSpareKv=0.500, avgSpareQueue=5.0, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"INFO","ts":"2025-11-27T22:26:12.915Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, totalReplicas=1, nonSaturated=1, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"DEBUG","ts":"2025-11-27T22:26:12.915Z","msg":"Capacity targets: no scaling needed (avgSpareKv=0.500, avgSpareQueue=5.0, all variants stable)"}
{"level":"DEBUG","ts":"2025-11-27T22:26:12.915Z","msg":"Capacity targets calculated: modelID=unsloth/Meta-Llama-3.1-8B, targets=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"INFO","ts":"2025-11-27T22:26:12.915Z","msg":"Capacity-only decisions made for model: unsloth/Meta-Llama-3.1-8B - decision count: 0"}
{"level":"INFO","ts":"2025-11-27T22:26:12.915Z","msg":"No scaling decisions to apply"}
{"level":"INFO","ts":"2025-11-27T22:26:12.915Z","msg":"Reconciliation completed successfully: mode=capacity-only, modelsProcessed=1, decisionsApplied=0"}
{"level":"INFO","ts":"2025-11-27T22:26:13.288Z","msg":"Operating in CAPACITY-ONLY mode: reactive capacity-based scaling only"}
{"level":"INFO","ts":"2025-11-27T22:26:13.288Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-11-27T22:26:13.288Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-11-27T22:26:13.299Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=1, accelerator=H100, ttft=0.00ms, itl=0.00ms, cost=10.00"}
{"level":"INFO","ts":"2025-11-27T22:26:13.300Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9wj976, queueLength=0"}
{"level":"DEBUG","ts":"2025-11-27T22:26:13.300Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"INFO","ts":"2025-11-27T22:26:13.300Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9wj976, usage=0.000 (0.0%)"}
{"level":"DEBUG","ts":"2025-11-27T22:26:13.300Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"DEBUG","ts":"2025-11-27T22:26:13.301Z","msg":"Pod-to-variant matching successful: totalPods=1, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"DEBUG","ts":"2025-11-27T22:26:13.301Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=1"}
{"level":"DEBUG","ts":"2025-11-27T22:26:13.301Z","msg":"Collected capacity metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, metricsCount=1"}
{"level":"DEBUG","ts":"2025-11-27T22:26:13.301Z","msg":"Variant analysis initialized: variant=ms-inference-scheduling-llm-d-modelservice-decode, accelerator=H100, cost=10.00, replicaCount=1"}
{"level":"DEBUG","ts":"2025-11-27T22:26:13.301Z","msg":"Scale-down unsafe: insufficient non-saturated replicas: nonSaturated=1, required=2"}
{"level":"DEBUG","ts":"2025-11-27T22:26:13.301Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, totalReplicas=1, nonSaturated=1, avgSpareKv=0.500, avgSpareQueue=5.0, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"INFO","ts":"2025-11-27T22:26:13.301Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, totalReplicas=1, nonSaturated=1, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"DEBUG","ts":"2025-11-27T22:26:13.301Z","msg":"Capacity targets: no scaling needed (avgSpareKv=0.500, avgSpareQueue=5.0, all variants stable)"}
{"level":"DEBUG","ts":"2025-11-27T22:26:13.301Z","msg":"Capacity targets calculated: modelID=unsloth/Meta-Llama-3.1-8B, targets=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"INFO","ts":"2025-11-27T22:26:13.301Z","msg":"Capacity-only decisions made for model: unsloth/Meta-Llama-3.1-8B - decision count: 0"}
{"level":"INFO","ts":"2025-11-27T22:26:13.301Z","msg":"No scaling decisions to apply"}
{"level":"INFO","ts":"2025-11-27T22:26:13.301Z","msg":"Reconciliation completed successfully: mode=capacity-only, modelsProcessed=1, decisionsApplied=0"}
{"level":"INFO","ts":"2025-11-27T22:26:42.917Z","msg":"Operating in CAPACITY-ONLY mode: reactive capacity-based scaling only"}
{"level":"INFO","ts":"2025-11-27T22:26:42.917Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-11-27T22:26:42.917Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-11-27T22:26:42.929Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=1, accelerator=H100, ttft=0.00ms, itl=0.00ms, cost=10.00"}
{"level":"INFO","ts":"2025-11-27T22:26:42.930Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9wj976, usage=0.000 (0.0%)"}
{"level":"DEBUG","ts":"2025-11-27T22:26:42.930Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"INFO","ts":"2025-11-27T22:26:42.930Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9wj976, queueLength=0"}
{"level":"DEBUG","ts":"2025-11-27T22:26:42.930Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"DEBUG","ts":"2025-11-27T22:26:42.930Z","msg":"Pod-to-variant matching successful: totalPods=1, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"DEBUG","ts":"2025-11-27T22:26:42.930Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=1"}
{"level":"DEBUG","ts":"2025-11-27T22:26:42.930Z","msg":"Collected capacity metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, metricsCount=1"}
{"level":"DEBUG","ts":"2025-11-27T22:26:42.930Z","msg":"Variant analysis initialized: variant=ms-inference-scheduling-llm-d-modelservice-decode, accelerator=H100, cost=10.00, replicaCount=1"}
{"level":"DEBUG","ts":"2025-11-27T22:26:42.930Z","msg":"Scale-down unsafe: insufficient non-saturated replicas: nonSaturated=1, required=2"}
{"level":"DEBUG","ts":"2025-11-27T22:26:42.930Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, totalReplicas=1, nonSaturated=1, avgSpareKv=0.500, avgSpareQueue=5.0, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"INFO","ts":"2025-11-27T22:26:42.930Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, totalReplicas=1, nonSaturated=1, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"DEBUG","ts":"2025-11-27T22:26:42.930Z","msg":"Capacity targets: no scaling needed (avgSpareKv=0.500, avgSpareQueue=5.0, all variants stable)"}
{"level":"DEBUG","ts":"2025-11-27T22:26:42.930Z","msg":"Capacity targets calculated: modelID=unsloth/Meta-Llama-3.1-8B, targets=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"INFO","ts":"2025-11-27T22:26:42.930Z","msg":"Capacity-only decisions made for model: unsloth/Meta-Llama-3.1-8B - decision count: 0"}
{"level":"INFO","ts":"2025-11-27T22:26:42.930Z","msg":"No scaling decisions to apply"}
{"level":"INFO","ts":"2025-11-27T22:26:42.930Z","msg":"Reconciliation completed successfully: mode=capacity-only, modelsProcessed=1, decisionsApplied=0"}
{"level":"INFO","ts":"2025-11-27T22:26:43.301Z","msg":"Operating in CAPACITY-ONLY mode: reactive capacity-based scaling only"}
{"level":"INFO","ts":"2025-11-27T22:26:43.301Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-11-27T22:26:43.301Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-11-27T22:26:43.316Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=1, accelerator=H100, ttft=0.00ms, itl=0.00ms, cost=10.00"}
{"level":"INFO","ts":"2025-11-27T22:26:43.317Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9wj976, queueLength=0"}
{"level":"DEBUG","ts":"2025-11-27T22:26:43.317Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"INFO","ts":"2025-11-27T22:26:43.317Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9wj976, usage=0.000 (0.0%)"}
{"level":"DEBUG","ts":"2025-11-27T22:26:43.317Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"DEBUG","ts":"2025-11-27T22:26:43.317Z","msg":"Pod-to-variant matching successful: totalPods=1, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"DEBUG","ts":"2025-11-27T22:26:43.317Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=1"}
{"level":"DEBUG","ts":"2025-11-27T22:26:43.317Z","msg":"Collected capacity metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, metricsCount=1"}
{"level":"DEBUG","ts":"2025-11-27T22:26:43.317Z","msg":"Variant analysis initialized: variant=ms-inference-scheduling-llm-d-modelservice-decode, accelerator=H100, cost=10.00, replicaCount=1"}
{"level":"DEBUG","ts":"2025-11-27T22:26:43.317Z","msg":"Scale-down unsafe: insufficient non-saturated replicas: nonSaturated=1, required=2"}
{"level":"DEBUG","ts":"2025-11-27T22:26:43.317Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, totalReplicas=1, nonSaturated=1, avgSpareKv=0.500, avgSpareQueue=5.0, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"INFO","ts":"2025-11-27T22:26:43.317Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, totalReplicas=1, nonSaturated=1, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"DEBUG","ts":"2025-11-27T22:26:43.317Z","msg":"Capacity targets: no scaling needed (avgSpareKv=0.500, avgSpareQueue=5.0, all variants stable)"}
{"level":"DEBUG","ts":"2025-11-27T22:26:43.317Z","msg":"Capacity targets calculated: modelID=unsloth/Meta-Llama-3.1-8B, targets=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"INFO","ts":"2025-11-27T22:26:43.317Z","msg":"Capacity-only decisions made for model: unsloth/Meta-Llama-3.1-8B - decision count: 0"}
{"level":"INFO","ts":"2025-11-27T22:26:43.317Z","msg":"No scaling decisions to apply"}
{"level":"INFO","ts":"2025-11-27T22:26:43.317Z","msg":"Reconciliation completed successfully: mode=capacity-only, modelsProcessed=1, decisionsApplied=0"}
{"level":"INFO","ts":"2025-11-27T22:27:12.931Z","msg":"Operating in CAPACITY-ONLY mode: reactive capacity-based scaling only"}
{"level":"INFO","ts":"2025-11-27T22:27:12.931Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-11-27T22:27:12.931Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-11-27T22:27:12.944Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=1, accelerator=H100, ttft=0.00ms, itl=0.00ms, cost=10.00"}
{"level":"INFO","ts":"2025-11-27T22:27:12.945Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9wj976, usage=0.000 (0.0%)"}
{"level":"DEBUG","ts":"2025-11-27T22:27:12.945Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"INFO","ts":"2025-11-27T22:27:12.945Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9wj976, queueLength=0"}
{"level":"DEBUG","ts":"2025-11-27T22:27:12.945Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"DEBUG","ts":"2025-11-27T22:27:12.945Z","msg":"Pod-to-variant matching successful: totalPods=1, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"DEBUG","ts":"2025-11-27T22:27:12.945Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=1"}
{"level":"DEBUG","ts":"2025-11-27T22:27:12.945Z","msg":"Collected capacity metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, metricsCount=1"}
{"level":"DEBUG","ts":"2025-11-27T22:27:12.945Z","msg":"Variant analysis initialized: variant=ms-inference-scheduling-llm-d-modelservice-decode, accelerator=H100, cost=10.00, replicaCount=1"}
{"level":"DEBUG","ts":"2025-11-27T22:27:12.945Z","msg":"Scale-down unsafe: insufficient non-saturated replicas: nonSaturated=1, required=2"}
{"level":"DEBUG","ts":"2025-11-27T22:27:12.945Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, totalReplicas=1, nonSaturated=1, avgSpareKv=0.500, avgSpareQueue=5.0, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"INFO","ts":"2025-11-27T22:27:12.945Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, totalReplicas=1, nonSaturated=1, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"DEBUG","ts":"2025-11-27T22:27:12.945Z","msg":"Capacity targets: no scaling needed (avgSpareKv=0.500, avgSpareQueue=5.0, all variants stable)"}
{"level":"DEBUG","ts":"2025-11-27T22:27:12.945Z","msg":"Capacity targets calculated: modelID=unsloth/Meta-Llama-3.1-8B, targets=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"INFO","ts":"2025-11-27T22:27:12.945Z","msg":"Capacity-only decisions made for model: unsloth/Meta-Llama-3.1-8B - decision count: 0"}
{"level":"INFO","ts":"2025-11-27T22:27:12.945Z","msg":"No scaling decisions to apply"}
{"level":"INFO","ts":"2025-11-27T22:27:12.945Z","msg":"Reconciliation completed successfully: mode=capacity-only, modelsProcessed=1, decisionsApplied=0"}
{"level":"INFO","ts":"2025-11-27T22:27:13.318Z","msg":"Operating in CAPACITY-ONLY mode: reactive capacity-based scaling only"}
{"level":"INFO","ts":"2025-11-27T22:27:13.318Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-11-27T22:27:13.318Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-11-27T22:27:13.329Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=1, accelerator=H100, ttft=0.00ms, itl=0.00ms, cost=10.00"}
{"level":"INFO","ts":"2025-11-27T22:27:13.330Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9wj976, queueLength=0"}
{"level":"DEBUG","ts":"2025-11-27T22:27:13.330Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"INFO","ts":"2025-11-27T22:27:13.330Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9wj976, usage=0.000 (0.0%)"}
{"level":"DEBUG","ts":"2025-11-27T22:27:13.330Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"DEBUG","ts":"2025-11-27T22:27:13.330Z","msg":"Pod-to-variant matching successful: totalPods=1, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"DEBUG","ts":"2025-11-27T22:27:13.330Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=1"}
{"level":"DEBUG","ts":"2025-11-27T22:27:13.330Z","msg":"Collected capacity metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, metricsCount=1"}
{"level":"DEBUG","ts":"2025-11-27T22:27:13.330Z","msg":"Variant analysis initialized: variant=ms-inference-scheduling-llm-d-modelservice-decode, accelerator=H100, cost=10.00, replicaCount=1"}
{"level":"DEBUG","ts":"2025-11-27T22:27:13.330Z","msg":"Scale-down unsafe: insufficient non-saturated replicas: nonSaturated=1, required=2"}
{"level":"DEBUG","ts":"2025-11-27T22:27:13.330Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, totalReplicas=1, nonSaturated=1, avgSpareKv=0.500, avgSpareQueue=5.0, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"INFO","ts":"2025-11-27T22:27:13.330Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, totalReplicas=1, nonSaturated=1, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"DEBUG","ts":"2025-11-27T22:27:13.330Z","msg":"Capacity targets: no scaling needed (avgSpareKv=0.500, avgSpareQueue=5.0, all variants stable)"}
{"level":"DEBUG","ts":"2025-11-27T22:27:13.330Z","msg":"Capacity targets calculated: modelID=unsloth/Meta-Llama-3.1-8B, targets=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"INFO","ts":"2025-11-27T22:27:13.330Z","msg":"Capacity-only decisions made for model: unsloth/Meta-Llama-3.1-8B - decision count: 0"}
{"level":"INFO","ts":"2025-11-27T22:27:13.330Z","msg":"No scaling decisions to apply"}
{"level":"INFO","ts":"2025-11-27T22:27:13.330Z","msg":"Reconciliation completed successfully: mode=capacity-only, modelsProcessed=1, decisionsApplied=0"}
{"level":"INFO","ts":"2025-11-27T22:27:42.946Z","msg":"Operating in CAPACITY-ONLY mode: reactive capacity-based scaling only"}
{"level":"INFO","ts":"2025-11-27T22:27:42.946Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-11-27T22:27:42.946Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-11-27T22:27:42.966Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=1, accelerator=H100, ttft=0.00ms, itl=0.00ms, cost=10.00"}
{"level":"INFO","ts":"2025-11-27T22:27:42.967Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9wj976, queueLength=0"}
{"level":"DEBUG","ts":"2025-11-27T22:27:42.967Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"INFO","ts":"2025-11-27T22:27:42.967Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9wj976, usage=0.000 (0.0%)"}
{"level":"DEBUG","ts":"2025-11-27T22:27:42.967Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"DEBUG","ts":"2025-11-27T22:27:42.968Z","msg":"Pod-to-variant matching successful: totalPods=1, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"DEBUG","ts":"2025-11-27T22:27:42.968Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=1"}
{"level":"DEBUG","ts":"2025-11-27T22:27:42.968Z","msg":"Collected capacity metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, metricsCount=1"}
{"level":"DEBUG","ts":"2025-11-27T22:27:42.968Z","msg":"Variant analysis initialized: variant=ms-inference-scheduling-llm-d-modelservice-decode, accelerator=H100, cost=10.00, replicaCount=1"}
{"level":"DEBUG","ts":"2025-11-27T22:27:42.968Z","msg":"Scale-down unsafe: insufficient non-saturated replicas: nonSaturated=1, required=2"}
{"level":"DEBUG","ts":"2025-11-27T22:27:42.968Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, totalReplicas=1, nonSaturated=1, avgSpareKv=0.500, avgSpareQueue=5.0, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"INFO","ts":"2025-11-27T22:27:42.968Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, totalReplicas=1, nonSaturated=1, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"DEBUG","ts":"2025-11-27T22:27:42.968Z","msg":"Capacity targets: no scaling needed (avgSpareKv=0.500, avgSpareQueue=5.0, all variants stable)"}
{"level":"DEBUG","ts":"2025-11-27T22:27:42.968Z","msg":"Capacity targets calculated: modelID=unsloth/Meta-Llama-3.1-8B, targets=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"INFO","ts":"2025-11-27T22:27:42.968Z","msg":"Capacity-only decisions made for model: unsloth/Meta-Llama-3.1-8B - decision count: 0"}
{"level":"INFO","ts":"2025-11-27T22:27:42.968Z","msg":"No scaling decisions to apply"}
{"level":"INFO","ts":"2025-11-27T22:27:42.968Z","msg":"Reconciliation completed successfully: mode=capacity-only, modelsProcessed=1, decisionsApplied=0"}
{"level":"INFO","ts":"2025-11-27T22:27:43.331Z","msg":"Operating in CAPACITY-ONLY mode: reactive capacity-based scaling only"}
{"level":"INFO","ts":"2025-11-27T22:27:43.332Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-11-27T22:27:43.332Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-11-27T22:27:43.382Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=1, accelerator=H100, ttft=0.00ms, itl=0.00ms, cost=10.00"}
{"level":"INFO","ts":"2025-11-27T22:27:43.384Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9wj976, queueLength=0"}
{"level":"DEBUG","ts":"2025-11-27T22:27:43.384Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"INFO","ts":"2025-11-27T22:27:43.384Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9wj976, usage=0.000 (0.0%)"}
{"level":"DEBUG","ts":"2025-11-27T22:27:43.384Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"DEBUG","ts":"2025-11-27T22:27:43.384Z","msg":"Pod-to-variant matching successful: totalPods=1, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"DEBUG","ts":"2025-11-27T22:27:43.384Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=1"}
{"level":"DEBUG","ts":"2025-11-27T22:27:43.384Z","msg":"Collected capacity metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, metricsCount=1"}
{"level":"DEBUG","ts":"2025-11-27T22:27:43.384Z","msg":"Variant analysis initialized: variant=ms-inference-scheduling-llm-d-modelservice-decode, accelerator=H100, cost=10.00, replicaCount=1"}
{"level":"DEBUG","ts":"2025-11-27T22:27:43.384Z","msg":"Scale-down unsafe: insufficient non-saturated replicas: nonSaturated=1, required=2"}
{"level":"DEBUG","ts":"2025-11-27T22:27:43.384Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, totalReplicas=1, nonSaturated=1, avgSpareKv=0.500, avgSpareQueue=5.0, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"INFO","ts":"2025-11-27T22:27:43.384Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, totalReplicas=1, nonSaturated=1, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"DEBUG","ts":"2025-11-27T22:27:43.384Z","msg":"Capacity targets: no scaling needed (avgSpareKv=0.500, avgSpareQueue=5.0, all variants stable)"}
{"level":"DEBUG","ts":"2025-11-27T22:27:43.384Z","msg":"Capacity targets calculated: modelID=unsloth/Meta-Llama-3.1-8B, targets=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"INFO","ts":"2025-11-27T22:27:43.384Z","msg":"Capacity-only decisions made for model: unsloth/Meta-Llama-3.1-8B - decision count: 0"}
{"level":"INFO","ts":"2025-11-27T22:27:43.384Z","msg":"No scaling decisions to apply"}
{"level":"INFO","ts":"2025-11-27T22:27:43.384Z","msg":"Reconciliation completed successfully: mode=capacity-only, modelsProcessed=1, decisionsApplied=0"}
{"level":"INFO","ts":"2025-11-27T22:28:12.969Z","msg":"Operating in CAPACITY-ONLY mode: reactive capacity-based scaling only"}
{"level":"INFO","ts":"2025-11-27T22:28:12.969Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-11-27T22:28:12.969Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-11-27T22:28:12.996Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=1, accelerator=H100, ttft=0.00ms, itl=0.00ms, cost=10.00"}
{"level":"INFO","ts":"2025-11-27T22:28:12.998Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9wj976, usage=0.000 (0.0%)"}
{"level":"DEBUG","ts":"2025-11-27T22:28:12.998Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"INFO","ts":"2025-11-27T22:28:12.999Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9wj976, queueLength=0"}
{"level":"DEBUG","ts":"2025-11-27T22:28:12.999Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"DEBUG","ts":"2025-11-27T22:28:12.999Z","msg":"Pod-to-variant matching successful: totalPods=1, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"DEBUG","ts":"2025-11-27T22:28:12.999Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=1"}
{"level":"DEBUG","ts":"2025-11-27T22:28:12.999Z","msg":"Collected capacity metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, metricsCount=1"}
{"level":"DEBUG","ts":"2025-11-27T22:28:12.999Z","msg":"Variant analysis initialized: variant=ms-inference-scheduling-llm-d-modelservice-decode, accelerator=H100, cost=10.00, replicaCount=1"}
{"level":"DEBUG","ts":"2025-11-27T22:28:12.999Z","msg":"Scale-down unsafe: insufficient non-saturated replicas: nonSaturated=1, required=2"}
{"level":"DEBUG","ts":"2025-11-27T22:28:12.999Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, totalReplicas=1, nonSaturated=1, avgSpareKv=0.500, avgSpareQueue=5.0, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"INFO","ts":"2025-11-27T22:28:12.999Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, totalReplicas=1, nonSaturated=1, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"DEBUG","ts":"2025-11-27T22:28:12.999Z","msg":"Capacity targets: no scaling needed (avgSpareKv=0.500, avgSpareQueue=5.0, all variants stable)"}
{"level":"DEBUG","ts":"2025-11-27T22:28:12.999Z","msg":"Capacity targets calculated: modelID=unsloth/Meta-Llama-3.1-8B, targets=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"INFO","ts":"2025-11-27T22:28:12.999Z","msg":"Capacity-only decisions made for model: unsloth/Meta-Llama-3.1-8B - decision count: 0"}
{"level":"INFO","ts":"2025-11-27T22:28:12.999Z","msg":"No scaling decisions to apply"}
{"level":"INFO","ts":"2025-11-27T22:28:12.999Z","msg":"Reconciliation completed successfully: mode=capacity-only, modelsProcessed=1, decisionsApplied=0"}
{"level":"INFO","ts":"2025-11-27T22:28:13.384Z","msg":"Operating in CAPACITY-ONLY mode: reactive capacity-based scaling only"}
{"level":"INFO","ts":"2025-11-27T22:28:13.384Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-11-27T22:28:13.384Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-11-27T22:28:13.395Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=1, accelerator=H100, ttft=0.00ms, itl=0.00ms, cost=10.00"}
{"level":"INFO","ts":"2025-11-27T22:28:13.397Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9wj976, usage=0.000 (0.0%)"}
{"level":"DEBUG","ts":"2025-11-27T22:28:13.397Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"INFO","ts":"2025-11-27T22:28:13.397Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9wj976, queueLength=0"}
{"level":"DEBUG","ts":"2025-11-27T22:28:13.397Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"DEBUG","ts":"2025-11-27T22:28:13.397Z","msg":"Pod-to-variant matching successful: totalPods=1, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"DEBUG","ts":"2025-11-27T22:28:13.397Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=1"}
{"level":"DEBUG","ts":"2025-11-27T22:28:13.397Z","msg":"Collected capacity metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, metricsCount=1"}
{"level":"DEBUG","ts":"2025-11-27T22:28:13.397Z","msg":"Variant analysis initialized: variant=ms-inference-scheduling-llm-d-modelservice-decode, accelerator=H100, cost=10.00, replicaCount=1"}
{"level":"DEBUG","ts":"2025-11-27T22:28:13.397Z","msg":"Scale-down unsafe: insufficient non-saturated replicas: nonSaturated=1, required=2"}
{"level":"DEBUG","ts":"2025-11-27T22:28:13.397Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, totalReplicas=1, nonSaturated=1, avgSpareKv=0.500, avgSpareQueue=5.0, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"INFO","ts":"2025-11-27T22:28:13.397Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, totalReplicas=1, nonSaturated=1, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"DEBUG","ts":"2025-11-27T22:28:13.397Z","msg":"Capacity targets: no scaling needed (avgSpareKv=0.500, avgSpareQueue=5.0, all variants stable)"}
{"level":"DEBUG","ts":"2025-11-27T22:28:13.397Z","msg":"Capacity targets calculated: modelID=unsloth/Meta-Llama-3.1-8B, targets=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"INFO","ts":"2025-11-27T22:28:13.397Z","msg":"Capacity-only decisions made for model: unsloth/Meta-Llama-3.1-8B - decision count: 0"}
{"level":"INFO","ts":"2025-11-27T22:28:13.397Z","msg":"No scaling decisions to apply"}
{"level":"INFO","ts":"2025-11-27T22:28:13.397Z","msg":"Reconciliation completed successfully: mode=capacity-only, modelsProcessed=1, decisionsApplied=0"}
{"level":"INFO","ts":"2025-11-27T22:28:43.000Z","msg":"Operating in CAPACITY-ONLY mode: reactive capacity-based scaling only"}
{"level":"INFO","ts":"2025-11-27T22:28:43.000Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-11-27T22:28:43.000Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-11-27T22:28:43.013Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=1, accelerator=H100, ttft=0.00ms, itl=0.00ms, cost=10.00"}
{"level":"INFO","ts":"2025-11-27T22:28:43.014Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9wj976, usage=0.000 (0.0%)"}
{"level":"DEBUG","ts":"2025-11-27T22:28:43.014Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"INFO","ts":"2025-11-27T22:28:43.015Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9wj976, queueLength=0"}
{"level":"DEBUG","ts":"2025-11-27T22:28:43.015Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"DEBUG","ts":"2025-11-27T22:28:43.015Z","msg":"Pod-to-variant matching successful: totalPods=1, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"DEBUG","ts":"2025-11-27T22:28:43.015Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=1"}
{"level":"DEBUG","ts":"2025-11-27T22:28:43.015Z","msg":"Collected capacity metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, metricsCount=1"}
{"level":"DEBUG","ts":"2025-11-27T22:28:43.015Z","msg":"Variant analysis initialized: variant=ms-inference-scheduling-llm-d-modelservice-decode, accelerator=H100, cost=10.00, replicaCount=1"}
{"level":"DEBUG","ts":"2025-11-27T22:28:43.015Z","msg":"Scale-down unsafe: insufficient non-saturated replicas: nonSaturated=1, required=2"}
{"level":"DEBUG","ts":"2025-11-27T22:28:43.015Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, totalReplicas=1, nonSaturated=1, avgSpareKv=0.500, avgSpareQueue=5.0, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"INFO","ts":"2025-11-27T22:28:43.015Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, totalReplicas=1, nonSaturated=1, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"DEBUG","ts":"2025-11-27T22:28:43.015Z","msg":"Capacity targets: no scaling needed (avgSpareKv=0.500, avgSpareQueue=5.0, all variants stable)"}
{"level":"DEBUG","ts":"2025-11-27T22:28:43.015Z","msg":"Capacity targets calculated: modelID=unsloth/Meta-Llama-3.1-8B, targets=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"INFO","ts":"2025-11-27T22:28:43.015Z","msg":"Capacity-only decisions made for model: unsloth/Meta-Llama-3.1-8B - decision count: 0"}
{"level":"INFO","ts":"2025-11-27T22:28:43.015Z","msg":"No scaling decisions to apply"}
{"level":"INFO","ts":"2025-11-27T22:28:43.015Z","msg":"Reconciliation completed successfully: mode=capacity-only, modelsProcessed=1, decisionsApplied=0"}
{"level":"INFO","ts":"2025-11-27T22:28:43.398Z","msg":"Operating in CAPACITY-ONLY mode: reactive capacity-based scaling only"}
{"level":"INFO","ts":"2025-11-27T22:28:43.398Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-11-27T22:28:43.398Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-11-27T22:28:43.409Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=1, accelerator=H100, ttft=0.00ms, itl=0.00ms, cost=10.00"}
{"level":"INFO","ts":"2025-11-27T22:28:43.410Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9wj976, queueLength=0"}
{"level":"DEBUG","ts":"2025-11-27T22:28:43.410Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"INFO","ts":"2025-11-27T22:28:43.411Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9wj976, usage=0.000 (0.0%)"}
{"level":"DEBUG","ts":"2025-11-27T22:28:43.411Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"DEBUG","ts":"2025-11-27T22:28:43.411Z","msg":"Pod-to-variant matching successful: totalPods=1, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"DEBUG","ts":"2025-11-27T22:28:43.411Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=1"}
{"level":"DEBUG","ts":"2025-11-27T22:28:43.411Z","msg":"Collected capacity metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, metricsCount=1"}
{"level":"DEBUG","ts":"2025-11-27T22:28:43.411Z","msg":"Variant analysis initialized: variant=ms-inference-scheduling-llm-d-modelservice-decode, accelerator=H100, cost=10.00, replicaCount=1"}
{"level":"DEBUG","ts":"2025-11-27T22:28:43.411Z","msg":"Scale-down unsafe: insufficient non-saturated replicas: nonSaturated=1, required=2"}
{"level":"DEBUG","ts":"2025-11-27T22:28:43.411Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, totalReplicas=1, nonSaturated=1, avgSpareKv=0.500, avgSpareQueue=5.0, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"INFO","ts":"2025-11-27T22:28:43.411Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, totalReplicas=1, nonSaturated=1, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"DEBUG","ts":"2025-11-27T22:28:43.411Z","msg":"Capacity targets: no scaling needed (avgSpareKv=0.500, avgSpareQueue=5.0, all variants stable)"}
{"level":"DEBUG","ts":"2025-11-27T22:28:43.411Z","msg":"Capacity targets calculated: modelID=unsloth/Meta-Llama-3.1-8B, targets=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"INFO","ts":"2025-11-27T22:28:43.411Z","msg":"Capacity-only decisions made for model: unsloth/Meta-Llama-3.1-8B - decision count: 0"}
{"level":"INFO","ts":"2025-11-27T22:28:43.411Z","msg":"No scaling decisions to apply"}
{"level":"INFO","ts":"2025-11-27T22:28:43.411Z","msg":"Reconciliation completed successfully: mode=capacity-only, modelsProcessed=1, decisionsApplied=0"}
{"level":"INFO","ts":"2025-11-27T22:29:13.015Z","msg":"Operating in CAPACITY-ONLY mode: reactive capacity-based scaling only"}
{"level":"INFO","ts":"2025-11-27T22:29:13.015Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-11-27T22:29:13.015Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-11-27T22:29:13.028Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=1, accelerator=H100, ttft=0.00ms, itl=0.00ms, cost=10.00"}
{"level":"INFO","ts":"2025-11-27T22:29:13.029Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9wj976, queueLength=0"}
{"level":"DEBUG","ts":"2025-11-27T22:29:13.029Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"INFO","ts":"2025-11-27T22:29:13.030Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9wj976, usage=0.000 (0.0%)"}
{"level":"DEBUG","ts":"2025-11-27T22:29:13.030Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"DEBUG","ts":"2025-11-27T22:29:13.030Z","msg":"Pod-to-variant matching successful: totalPods=1, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"DEBUG","ts":"2025-11-27T22:29:13.030Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=1"}
{"level":"DEBUG","ts":"2025-11-27T22:29:13.030Z","msg":"Collected capacity metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, metricsCount=1"}
{"level":"DEBUG","ts":"2025-11-27T22:29:13.030Z","msg":"Variant analysis initialized: variant=ms-inference-scheduling-llm-d-modelservice-decode, accelerator=H100, cost=10.00, replicaCount=1"}
{"level":"DEBUG","ts":"2025-11-27T22:29:13.030Z","msg":"Scale-down unsafe: insufficient non-saturated replicas: nonSaturated=1, required=2"}
{"level":"DEBUG","ts":"2025-11-27T22:29:13.030Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, totalReplicas=1, nonSaturated=1, avgSpareKv=0.500, avgSpareQueue=5.0, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"INFO","ts":"2025-11-27T22:29:13.030Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, totalReplicas=1, nonSaturated=1, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"DEBUG","ts":"2025-11-27T22:29:13.030Z","msg":"Capacity targets: no scaling needed (avgSpareKv=0.500, avgSpareQueue=5.0, all variants stable)"}
{"level":"DEBUG","ts":"2025-11-27T22:29:13.030Z","msg":"Capacity targets calculated: modelID=unsloth/Meta-Llama-3.1-8B, targets=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"INFO","ts":"2025-11-27T22:29:13.030Z","msg":"Capacity-only decisions made for model: unsloth/Meta-Llama-3.1-8B - decision count: 0"}
{"level":"INFO","ts":"2025-11-27T22:29:13.030Z","msg":"No scaling decisions to apply"}
{"level":"INFO","ts":"2025-11-27T22:29:13.030Z","msg":"Reconciliation completed successfully: mode=capacity-only, modelsProcessed=1, decisionsApplied=0"}
{"level":"INFO","ts":"2025-11-27T22:29:13.412Z","msg":"Operating in CAPACITY-ONLY mode: reactive capacity-based scaling only"}
{"level":"INFO","ts":"2025-11-27T22:29:13.412Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-11-27T22:29:13.412Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-11-27T22:29:13.423Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=1, accelerator=H100, ttft=0.00ms, itl=0.00ms, cost=10.00"}
{"level":"INFO","ts":"2025-11-27T22:29:13.424Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9wj976, queueLength=0"}
{"level":"DEBUG","ts":"2025-11-27T22:29:13.424Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"INFO","ts":"2025-11-27T22:29:13.424Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9wj976, usage=0.000 (0.0%)"}
{"level":"DEBUG","ts":"2025-11-27T22:29:13.424Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"DEBUG","ts":"2025-11-27T22:29:13.424Z","msg":"Pod-to-variant matching successful: totalPods=1, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"DEBUG","ts":"2025-11-27T22:29:13.424Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=1"}
{"level":"DEBUG","ts":"2025-11-27T22:29:13.424Z","msg":"Collected capacity metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, metricsCount=1"}
{"level":"DEBUG","ts":"2025-11-27T22:29:13.424Z","msg":"Variant analysis initialized: variant=ms-inference-scheduling-llm-d-modelservice-decode, accelerator=H100, cost=10.00, replicaCount=1"}
{"level":"DEBUG","ts":"2025-11-27T22:29:13.424Z","msg":"Scale-down unsafe: insufficient non-saturated replicas: nonSaturated=1, required=2"}
{"level":"DEBUG","ts":"2025-11-27T22:29:13.424Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, totalReplicas=1, nonSaturated=1, avgSpareKv=0.500, avgSpareQueue=5.0, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"INFO","ts":"2025-11-27T22:29:13.424Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, totalReplicas=1, nonSaturated=1, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"DEBUG","ts":"2025-11-27T22:29:13.424Z","msg":"Capacity targets: no scaling needed (avgSpareKv=0.500, avgSpareQueue=5.0, all variants stable)"}
{"level":"DEBUG","ts":"2025-11-27T22:29:13.424Z","msg":"Capacity targets calculated: modelID=unsloth/Meta-Llama-3.1-8B, targets=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"INFO","ts":"2025-11-27T22:29:13.424Z","msg":"Capacity-only decisions made for model: unsloth/Meta-Llama-3.1-8B - decision count: 0"}
{"level":"INFO","ts":"2025-11-27T22:29:13.424Z","msg":"No scaling decisions to apply"}
{"level":"INFO","ts":"2025-11-27T22:29:13.424Z","msg":"Reconciliation completed successfully: mode=capacity-only, modelsProcessed=1, decisionsApplied=0"}
{"level":"INFO","ts":"2025-11-27T22:29:43.031Z","msg":"Operating in CAPACITY-ONLY mode: reactive capacity-based scaling only"}
{"level":"INFO","ts":"2025-11-27T22:29:43.031Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-11-27T22:29:43.031Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-11-27T22:29:43.044Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=1, accelerator=H100, ttft=0.00ms, itl=0.00ms, cost=10.00"}
{"level":"INFO","ts":"2025-11-27T22:29:43.046Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9wj976, queueLength=0"}
{"level":"DEBUG","ts":"2025-11-27T22:29:43.046Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"INFO","ts":"2025-11-27T22:29:43.046Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9wj976, usage=0.000 (0.0%)"}
{"level":"DEBUG","ts":"2025-11-27T22:29:43.046Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"DEBUG","ts":"2025-11-27T22:29:43.046Z","msg":"Pod-to-variant matching successful: totalPods=1, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"DEBUG","ts":"2025-11-27T22:29:43.046Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=1"}
{"level":"DEBUG","ts":"2025-11-27T22:29:43.046Z","msg":"Collected capacity metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, metricsCount=1"}
{"level":"DEBUG","ts":"2025-11-27T22:29:43.046Z","msg":"Variant analysis initialized: variant=ms-inference-scheduling-llm-d-modelservice-decode, accelerator=H100, cost=10.00, replicaCount=1"}
{"level":"DEBUG","ts":"2025-11-27T22:29:43.046Z","msg":"Scale-down unsafe: insufficient non-saturated replicas: nonSaturated=1, required=2"}
{"level":"DEBUG","ts":"2025-11-27T22:29:43.046Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, totalReplicas=1, nonSaturated=1, avgSpareKv=0.500, avgSpareQueue=5.0, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"INFO","ts":"2025-11-27T22:29:43.046Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, totalReplicas=1, nonSaturated=1, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"DEBUG","ts":"2025-11-27T22:29:43.046Z","msg":"Capacity targets: no scaling needed (avgSpareKv=0.500, avgSpareQueue=5.0, all variants stable)"}
{"level":"DEBUG","ts":"2025-11-27T22:29:43.046Z","msg":"Capacity targets calculated: modelID=unsloth/Meta-Llama-3.1-8B, targets=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"INFO","ts":"2025-11-27T22:29:43.046Z","msg":"Capacity-only decisions made for model: unsloth/Meta-Llama-3.1-8B - decision count: 0"}
{"level":"INFO","ts":"2025-11-27T22:29:43.046Z","msg":"No scaling decisions to apply"}
{"level":"INFO","ts":"2025-11-27T22:29:43.046Z","msg":"Reconciliation completed successfully: mode=capacity-only, modelsProcessed=1, decisionsApplied=0"}
{"level":"INFO","ts":"2025-11-27T22:29:43.425Z","msg":"Operating in CAPACITY-ONLY mode: reactive capacity-based scaling only"}
{"level":"INFO","ts":"2025-11-27T22:29:43.425Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-11-27T22:29:43.425Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-11-27T22:29:43.436Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=1, accelerator=H100, ttft=0.00ms, itl=0.00ms, cost=10.00"}
{"level":"INFO","ts":"2025-11-27T22:29:43.438Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9wj976, queueLength=0"}
{"level":"DEBUG","ts":"2025-11-27T22:29:43.438Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"INFO","ts":"2025-11-27T22:29:43.438Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9wj976, usage=0.000 (0.0%)"}
{"level":"DEBUG","ts":"2025-11-27T22:29:43.438Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"DEBUG","ts":"2025-11-27T22:29:43.438Z","msg":"Pod-to-variant matching successful: totalPods=1, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"DEBUG","ts":"2025-11-27T22:29:43.438Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=1"}
{"level":"DEBUG","ts":"2025-11-27T22:29:43.438Z","msg":"Collected capacity metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, metricsCount=1"}
{"level":"DEBUG","ts":"2025-11-27T22:29:43.438Z","msg":"Variant analysis initialized: variant=ms-inference-scheduling-llm-d-modelservice-decode, accelerator=H100, cost=10.00, replicaCount=1"}
{"level":"DEBUG","ts":"2025-11-27T22:29:43.438Z","msg":"Scale-down unsafe: insufficient non-saturated replicas: nonSaturated=1, required=2"}
{"level":"DEBUG","ts":"2025-11-27T22:29:43.438Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, totalReplicas=1, nonSaturated=1, avgSpareKv=0.500, avgSpareQueue=5.0, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"INFO","ts":"2025-11-27T22:29:43.438Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, totalReplicas=1, nonSaturated=1, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"DEBUG","ts":"2025-11-27T22:29:43.438Z","msg":"Capacity targets: no scaling needed (avgSpareKv=0.500, avgSpareQueue=5.0, all variants stable)"}
{"level":"DEBUG","ts":"2025-11-27T22:29:43.438Z","msg":"Capacity targets calculated: modelID=unsloth/Meta-Llama-3.1-8B, targets=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"INFO","ts":"2025-11-27T22:29:43.438Z","msg":"Capacity-only decisions made for model: unsloth/Meta-Llama-3.1-8B - decision count: 0"}
{"level":"INFO","ts":"2025-11-27T22:29:43.438Z","msg":"No scaling decisions to apply"}
{"level":"INFO","ts":"2025-11-27T22:29:43.438Z","msg":"Reconciliation completed successfully: mode=capacity-only, modelsProcessed=1, decisionsApplied=0"}
{"level":"INFO","ts":"2025-11-27T22:30:43.068Z","msg":"Operating in CAPACITY-ONLY mode: reactive capacity-based scaling only"}
{"level":"INFO","ts":"2025-11-27T22:30:43.068Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-11-27T22:30:43.068Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-11-27T22:30:43.080Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=1, accelerator=H100, ttft=0.00ms, itl=0.00ms, cost=10.00"}
{"level":"INFO","ts":"2025-11-27T22:30:43.082Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9wj976, usage=0.000 (0.0%)"}
{"level":"DEBUG","ts":"2025-11-27T22:30:43.082Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"INFO","ts":"2025-11-27T22:30:43.082Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9wj976, queueLength=0"}
{"level":"DEBUG","ts":"2025-11-27T22:30:43.082Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"DEBUG","ts":"2025-11-27T22:30:43.082Z","msg":"Pod-to-variant matching successful: totalPods=1, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"DEBUG","ts":"2025-11-27T22:30:43.082Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=1"}
{"level":"DEBUG","ts":"2025-11-27T22:30:43.082Z","msg":"Collected capacity metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, metricsCount=1"}
{"level":"DEBUG","ts":"2025-11-27T22:30:43.082Z","msg":"Variant analysis initialized: variant=ms-inference-scheduling-llm-d-modelservice-decode, accelerator=H100, cost=10.00, replicaCount=1"}
{"level":"DEBUG","ts":"2025-11-27T22:30:43.082Z","msg":"Scale-down unsafe: insufficient non-saturated replicas: nonSaturated=1, required=2"}
{"level":"DEBUG","ts":"2025-11-27T22:30:43.082Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, totalReplicas=1, nonSaturated=1, avgSpareKv=0.500, avgSpareQueue=5.0, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"INFO","ts":"2025-11-27T22:30:43.082Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, totalReplicas=1, nonSaturated=1, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"DEBUG","ts":"2025-11-27T22:30:43.082Z","msg":"Capacity targets: no scaling needed (avgSpareKv=0.500, avgSpareQueue=5.0, all variants stable)"}
{"level":"DEBUG","ts":"2025-11-27T22:30:43.082Z","msg":"Capacity targets calculated: modelID=unsloth/Meta-Llama-3.1-8B, targets=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"INFO","ts":"2025-11-27T22:30:43.082Z","msg":"Capacity-only decisions made for model: unsloth/Meta-Llama-3.1-8B - decision count: 0"}
{"level":"INFO","ts":"2025-11-27T22:30:43.082Z","msg":"No scaling decisions to apply"}
{"level":"INFO","ts":"2025-11-27T22:30:43.082Z","msg":"Reconciliation completed successfully: mode=capacity-only, modelsProcessed=1, decisionsApplied=0"}
{"level":"INFO","ts":"2025-11-27T22:30:43.453Z","msg":"Operating in CAPACITY-ONLY mode: reactive capacity-based scaling only"}
{"level":"INFO","ts":"2025-11-27T22:30:43.453Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-11-27T22:30:43.453Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-11-27T22:30:43.473Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=1, accelerator=H100, ttft=0.00ms, itl=0.00ms, cost=10.00"}
{"level":"INFO","ts":"2025-11-27T22:30:43.475Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9wj976, queueLength=0"}
{"level":"DEBUG","ts":"2025-11-27T22:30:43.475Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"INFO","ts":"2025-11-27T22:30:43.475Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9wj976, usage=0.000 (0.0%)"}
{"level":"DEBUG","ts":"2025-11-27T22:30:43.475Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"DEBUG","ts":"2025-11-27T22:30:43.475Z","msg":"Pod-to-variant matching successful: totalPods=1, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"DEBUG","ts":"2025-11-27T22:30:43.475Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=1"}
{"level":"DEBUG","ts":"2025-11-27T22:30:43.475Z","msg":"Collected capacity metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, metricsCount=1"}
{"level":"DEBUG","ts":"2025-11-27T22:30:43.475Z","msg":"Variant analysis initialized: variant=ms-inference-scheduling-llm-d-modelservice-decode, accelerator=H100, cost=10.00, replicaCount=1"}
{"level":"DEBUG","ts":"2025-11-27T22:30:43.475Z","msg":"Scale-down unsafe: insufficient non-saturated replicas: nonSaturated=1, required=2"}
{"level":"DEBUG","ts":"2025-11-27T22:30:43.475Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, totalReplicas=1, nonSaturated=1, avgSpareKv=0.500, avgSpareQueue=5.0, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"INFO","ts":"2025-11-27T22:30:43.475Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, totalReplicas=1, nonSaturated=1, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"DEBUG","ts":"2025-11-27T22:30:43.475Z","msg":"Capacity targets: no scaling needed (avgSpareKv=0.500, avgSpareQueue=5.0, all variants stable)"}
{"level":"DEBUG","ts":"2025-11-27T22:30:43.475Z","msg":"Capacity targets calculated: modelID=unsloth/Meta-Llama-3.1-8B, targets=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"INFO","ts":"2025-11-27T22:30:43.475Z","msg":"Capacity-only decisions made for model: unsloth/Meta-Llama-3.1-8B - decision count: 0"}
{"level":"INFO","ts":"2025-11-27T22:30:43.475Z","msg":"No scaling decisions to apply"}
{"level":"INFO","ts":"2025-11-27T22:30:43.475Z","msg":"Reconciliation completed successfully: mode=capacity-only, modelsProcessed=1, decisionsApplied=0"}
{"level":"INFO","ts":"2025-11-27T22:31:13.083Z","msg":"Operating in CAPACITY-ONLY mode: reactive capacity-based scaling only"}
{"level":"INFO","ts":"2025-11-27T22:31:13.083Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-11-27T22:31:13.083Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-11-27T22:31:13.097Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=1, accelerator=H100, ttft=0.00ms, itl=0.00ms, cost=10.00"}
{"level":"INFO","ts":"2025-11-27T22:31:13.099Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9wj976, queueLength=0"}
{"level":"DEBUG","ts":"2025-11-27T22:31:13.099Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"INFO","ts":"2025-11-27T22:31:13.099Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9wj976, usage=0.000 (0.0%)"}
{"level":"DEBUG","ts":"2025-11-27T22:31:13.099Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"DEBUG","ts":"2025-11-27T22:31:13.099Z","msg":"Pod-to-variant matching successful: totalPods=1, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"DEBUG","ts":"2025-11-27T22:31:13.099Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=1"}
{"level":"DEBUG","ts":"2025-11-27T22:31:13.099Z","msg":"Collected capacity metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, metricsCount=1"}
{"level":"DEBUG","ts":"2025-11-27T22:31:13.099Z","msg":"Variant analysis initialized: variant=ms-inference-scheduling-llm-d-modelservice-decode, accelerator=H100, cost=10.00, replicaCount=1"}
{"level":"DEBUG","ts":"2025-11-27T22:31:13.099Z","msg":"Scale-down unsafe: insufficient non-saturated replicas: nonSaturated=1, required=2"}
{"level":"DEBUG","ts":"2025-11-27T22:31:13.099Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, totalReplicas=1, nonSaturated=1, avgSpareKv=0.500, avgSpareQueue=5.0, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"INFO","ts":"2025-11-27T22:31:13.099Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, totalReplicas=1, nonSaturated=1, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"DEBUG","ts":"2025-11-27T22:31:13.099Z","msg":"Capacity targets: no scaling needed (avgSpareKv=0.500, avgSpareQueue=5.0, all variants stable)"}
{"level":"DEBUG","ts":"2025-11-27T22:31:13.099Z","msg":"Capacity targets calculated: modelID=unsloth/Meta-Llama-3.1-8B, targets=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"INFO","ts":"2025-11-27T22:31:13.099Z","msg":"Capacity-only decisions made for model: unsloth/Meta-Llama-3.1-8B - decision count: 0"}
{"level":"INFO","ts":"2025-11-27T22:31:13.099Z","msg":"No scaling decisions to apply"}
{"level":"INFO","ts":"2025-11-27T22:31:13.099Z","msg":"Reconciliation completed successfully: mode=capacity-only, modelsProcessed=1, decisionsApplied=0"}
{"level":"INFO","ts":"2025-11-27T22:31:13.476Z","msg":"Operating in CAPACITY-ONLY mode: reactive capacity-based scaling only"}
{"level":"INFO","ts":"2025-11-27T22:31:13.476Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-11-27T22:31:13.476Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-11-27T22:31:13.487Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=1, accelerator=H100, ttft=0.00ms, itl=0.00ms, cost=10.00"}
{"level":"INFO","ts":"2025-11-27T22:31:13.488Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9wj976, usage=0.000 (0.0%)"}
{"level":"DEBUG","ts":"2025-11-27T22:31:13.488Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"INFO","ts":"2025-11-27T22:31:13.488Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9wj976, queueLength=0"}
{"level":"DEBUG","ts":"2025-11-27T22:31:13.488Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"DEBUG","ts":"2025-11-27T22:31:13.488Z","msg":"Pod-to-variant matching successful: totalPods=1, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"DEBUG","ts":"2025-11-27T22:31:13.488Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=1"}
{"level":"DEBUG","ts":"2025-11-27T22:31:13.488Z","msg":"Collected capacity metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, metricsCount=1"}
{"level":"DEBUG","ts":"2025-11-27T22:31:13.488Z","msg":"Variant analysis initialized: variant=ms-inference-scheduling-llm-d-modelservice-decode, accelerator=H100, cost=10.00, replicaCount=1"}
{"level":"DEBUG","ts":"2025-11-27T22:31:13.488Z","msg":"Scale-down unsafe: insufficient non-saturated replicas: nonSaturated=1, required=2"}
{"level":"DEBUG","ts":"2025-11-27T22:31:13.488Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, totalReplicas=1, nonSaturated=1, avgSpareKv=0.500, avgSpareQueue=5.0, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"INFO","ts":"2025-11-27T22:31:13.488Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, totalReplicas=1, nonSaturated=1, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"DEBUG","ts":"2025-11-27T22:31:13.488Z","msg":"Capacity targets: no scaling needed (avgSpareKv=0.500, avgSpareQueue=5.0, all variants stable)"}
{"level":"DEBUG","ts":"2025-11-27T22:31:13.488Z","msg":"Capacity targets calculated: modelID=unsloth/Meta-Llama-3.1-8B, targets=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"INFO","ts":"2025-11-27T22:31:13.488Z","msg":"Capacity-only decisions made for model: unsloth/Meta-Llama-3.1-8B - decision count: 0"}
{"level":"INFO","ts":"2025-11-27T22:31:13.488Z","msg":"No scaling decisions to apply"}
{"level":"INFO","ts":"2025-11-27T22:31:13.488Z","msg":"Reconciliation completed successfully: mode=capacity-only, modelsProcessed=1, decisionsApplied=0"}
{"level":"INFO","ts":"2025-11-27T22:31:43.100Z","msg":"Operating in CAPACITY-ONLY mode: reactive capacity-based scaling only"}
{"level":"INFO","ts":"2025-11-27T22:31:43.100Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-11-27T22:31:43.100Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-11-27T22:31:43.120Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=1, accelerator=H100, ttft=18.20ms, itl=9.86ms, cost=10.00"}
{"level":"INFO","ts":"2025-11-27T22:31:43.121Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9wj976, usage=0.064 (6.4%)"}
{"level":"DEBUG","ts":"2025-11-27T22:31:43.121Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"INFO","ts":"2025-11-27T22:31:43.121Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9wj976, queueLength=0"}
{"level":"DEBUG","ts":"2025-11-27T22:31:43.121Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"DEBUG","ts":"2025-11-27T22:31:43.121Z","msg":"Pod-to-variant matching successful: totalPods=1, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"DEBUG","ts":"2025-11-27T22:31:43.121Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=1"}
{"level":"DEBUG","ts":"2025-11-27T22:31:43.121Z","msg":"Collected capacity metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, metricsCount=1"}
{"level":"DEBUG","ts":"2025-11-27T22:31:43.121Z","msg":"Variant analysis initialized: variant=ms-inference-scheduling-llm-d-modelservice-decode, accelerator=H100, cost=10.00, replicaCount=1"}
{"level":"DEBUG","ts":"2025-11-27T22:31:43.121Z","msg":"Scale-down unsafe: insufficient non-saturated replicas: nonSaturated=1, required=2"}
{"level":"DEBUG","ts":"2025-11-27T22:31:43.121Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, totalReplicas=1, nonSaturated=1, avgSpareKv=0.436, avgSpareQueue=5.0, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"INFO","ts":"2025-11-27T22:31:43.121Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, totalReplicas=1, nonSaturated=1, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"DEBUG","ts":"2025-11-27T22:31:43.121Z","msg":"Capacity targets: no scaling needed (avgSpareKv=0.436, avgSpareQueue=5.0, all variants stable)"}
{"level":"DEBUG","ts":"2025-11-27T22:31:43.121Z","msg":"Capacity targets calculated: modelID=unsloth/Meta-Llama-3.1-8B, targets=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"INFO","ts":"2025-11-27T22:31:43.121Z","msg":"Capacity-only decisions made for model: unsloth/Meta-Llama-3.1-8B - decision count: 0"}
{"level":"INFO","ts":"2025-11-27T22:31:43.121Z","msg":"No scaling decisions to apply"}
{"level":"INFO","ts":"2025-11-27T22:31:43.121Z","msg":"Reconciliation completed successfully: mode=capacity-only, modelsProcessed=1, decisionsApplied=0"}
{"level":"INFO","ts":"2025-11-27T22:31:43.490Z","msg":"Operating in CAPACITY-ONLY mode: reactive capacity-based scaling only"}
{"level":"INFO","ts":"2025-11-27T22:31:43.490Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-11-27T22:31:43.490Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-11-27T22:31:43.501Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=1, accelerator=H100, ttft=18.20ms, itl=9.86ms, cost=10.00"}
{"level":"INFO","ts":"2025-11-27T22:31:43.502Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9wj976, queueLength=0"}
{"level":"DEBUG","ts":"2025-11-27T22:31:43.502Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"INFO","ts":"2025-11-27T22:31:43.503Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9wj976, usage=0.064 (6.4%)"}
{"level":"DEBUG","ts":"2025-11-27T22:31:43.503Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"DEBUG","ts":"2025-11-27T22:31:43.503Z","msg":"Pod-to-variant matching successful: totalPods=1, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"DEBUG","ts":"2025-11-27T22:31:43.503Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=1"}
{"level":"DEBUG","ts":"2025-11-27T22:31:43.503Z","msg":"Collected capacity metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, metricsCount=1"}
{"level":"DEBUG","ts":"2025-11-27T22:31:43.503Z","msg":"Variant analysis initialized: variant=ms-inference-scheduling-llm-d-modelservice-decode, accelerator=H100, cost=10.00, replicaCount=1"}
{"level":"DEBUG","ts":"2025-11-27T22:31:43.503Z","msg":"Scale-down unsafe: insufficient non-saturated replicas: nonSaturated=1, required=2"}
{"level":"DEBUG","ts":"2025-11-27T22:31:43.503Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, totalReplicas=1, nonSaturated=1, avgSpareKv=0.436, avgSpareQueue=5.0, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"INFO","ts":"2025-11-27T22:31:43.503Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, totalReplicas=1, nonSaturated=1, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"DEBUG","ts":"2025-11-27T22:31:43.503Z","msg":"Capacity targets: no scaling needed (avgSpareKv=0.436, avgSpareQueue=5.0, all variants stable)"}
{"level":"DEBUG","ts":"2025-11-27T22:31:43.503Z","msg":"Capacity targets calculated: modelID=unsloth/Meta-Llama-3.1-8B, targets=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"INFO","ts":"2025-11-27T22:31:43.503Z","msg":"Capacity-only decisions made for model: unsloth/Meta-Llama-3.1-8B - decision count: 0"}
{"level":"INFO","ts":"2025-11-27T22:31:43.503Z","msg":"No scaling decisions to apply"}
{"level":"INFO","ts":"2025-11-27T22:31:43.503Z","msg":"Reconciliation completed successfully: mode=capacity-only, modelsProcessed=1, decisionsApplied=0"}
{"level":"INFO","ts":"2025-11-27T22:32:13.122Z","msg":"Operating in CAPACITY-ONLY mode: reactive capacity-based scaling only"}
{"level":"INFO","ts":"2025-11-27T22:32:13.122Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-11-27T22:32:13.122Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-11-27T22:32:13.140Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=1, accelerator=H100, ttft=22.19ms, itl=11.17ms, cost=10.00"}
{"level":"INFO","ts":"2025-11-27T22:32:13.141Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9wj976, usage=0.067 (6.7%)"}
{"level":"DEBUG","ts":"2025-11-27T22:32:13.141Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"INFO","ts":"2025-11-27T22:32:13.141Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9wj976, queueLength=0"}
{"level":"DEBUG","ts":"2025-11-27T22:32:13.141Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"DEBUG","ts":"2025-11-27T22:32:13.142Z","msg":"Pod-to-variant matching successful: totalPods=1, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"DEBUG","ts":"2025-11-27T22:32:13.142Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=1"}
{"level":"DEBUG","ts":"2025-11-27T22:32:13.142Z","msg":"Collected capacity metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, metricsCount=1"}
{"level":"DEBUG","ts":"2025-11-27T22:32:13.142Z","msg":"Variant analysis initialized: variant=ms-inference-scheduling-llm-d-modelservice-decode, accelerator=H100, cost=10.00, replicaCount=1"}
{"level":"DEBUG","ts":"2025-11-27T22:32:13.142Z","msg":"Scale-down unsafe: insufficient non-saturated replicas: nonSaturated=1, required=2"}
{"level":"DEBUG","ts":"2025-11-27T22:32:13.142Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, totalReplicas=1, nonSaturated=1, avgSpareKv=0.433, avgSpareQueue=5.0, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"INFO","ts":"2025-11-27T22:32:13.142Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, totalReplicas=1, nonSaturated=1, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"DEBUG","ts":"2025-11-27T22:32:13.142Z","msg":"Capacity targets: no scaling needed (avgSpareKv=0.433, avgSpareQueue=5.0, all variants stable)"}
{"level":"DEBUG","ts":"2025-11-27T22:32:13.142Z","msg":"Capacity targets calculated: modelID=unsloth/Meta-Llama-3.1-8B, targets=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"INFO","ts":"2025-11-27T22:32:13.142Z","msg":"Capacity-only decisions made for model: unsloth/Meta-Llama-3.1-8B - decision count: 0"}
{"level":"INFO","ts":"2025-11-27T22:32:13.142Z","msg":"No scaling decisions to apply"}
{"level":"INFO","ts":"2025-11-27T22:32:13.142Z","msg":"Reconciliation completed successfully: mode=capacity-only, modelsProcessed=1, decisionsApplied=0"}
{"level":"INFO","ts":"2025-11-27T22:32:13.503Z","msg":"Operating in CAPACITY-ONLY mode: reactive capacity-based scaling only"}
{"level":"INFO","ts":"2025-11-27T22:32:13.503Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-11-27T22:32:13.503Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-11-27T22:32:13.514Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=1, accelerator=H100, ttft=22.19ms, itl=11.17ms, cost=10.00"}
{"level":"INFO","ts":"2025-11-27T22:32:13.515Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9wj976, queueLength=0"}
{"level":"DEBUG","ts":"2025-11-27T22:32:13.515Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"INFO","ts":"2025-11-27T22:32:13.516Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9wj976, usage=0.067 (6.7%)"}
{"level":"DEBUG","ts":"2025-11-27T22:32:13.516Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"DEBUG","ts":"2025-11-27T22:32:13.516Z","msg":"Pod-to-variant matching successful: totalPods=1, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"DEBUG","ts":"2025-11-27T22:32:13.516Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=1"}
{"level":"DEBUG","ts":"2025-11-27T22:32:13.516Z","msg":"Collected capacity metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, metricsCount=1"}
{"level":"DEBUG","ts":"2025-11-27T22:32:13.516Z","msg":"Variant analysis initialized: variant=ms-inference-scheduling-llm-d-modelservice-decode, accelerator=H100, cost=10.00, replicaCount=1"}
{"level":"DEBUG","ts":"2025-11-27T22:32:13.516Z","msg":"Scale-down unsafe: insufficient non-saturated replicas: nonSaturated=1, required=2"}
{"level":"DEBUG","ts":"2025-11-27T22:32:13.516Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, totalReplicas=1, nonSaturated=1, avgSpareKv=0.433, avgSpareQueue=5.0, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"INFO","ts":"2025-11-27T22:32:13.516Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, totalReplicas=1, nonSaturated=1, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"DEBUG","ts":"2025-11-27T22:32:13.516Z","msg":"Capacity targets: no scaling needed (avgSpareKv=0.433, avgSpareQueue=5.0, all variants stable)"}
{"level":"DEBUG","ts":"2025-11-27T22:32:13.516Z","msg":"Capacity targets calculated: modelID=unsloth/Meta-Llama-3.1-8B, targets=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"INFO","ts":"2025-11-27T22:32:13.516Z","msg":"Capacity-only decisions made for model: unsloth/Meta-Llama-3.1-8B - decision count: 0"}
{"level":"INFO","ts":"2025-11-27T22:32:13.516Z","msg":"No scaling decisions to apply"}
{"level":"INFO","ts":"2025-11-27T22:32:13.516Z","msg":"Reconciliation completed successfully: mode=capacity-only, modelsProcessed=1, decisionsApplied=0"}
{"level":"INFO","ts":"2025-11-27T22:32:43.142Z","msg":"Operating in CAPACITY-ONLY mode: reactive capacity-based scaling only"}
{"level":"INFO","ts":"2025-11-27T22:32:43.142Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-11-27T22:32:43.142Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-11-27T22:32:43.157Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=1, accelerator=H100, ttft=22.40ms, itl=11.40ms, cost=10.00"}
{"level":"INFO","ts":"2025-11-27T22:32:43.158Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9wj976, usage=0.128 (12.8%)"}
{"level":"INFO","ts":"2025-11-27T22:32:43.158Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9wj976, queueLength=0"}
{"level":"DEBUG","ts":"2025-11-27T22:32:43.158Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"DEBUG","ts":"2025-11-27T22:32:43.158Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"DEBUG","ts":"2025-11-27T22:32:43.158Z","msg":"Pod-to-variant matching successful: totalPods=1, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"DEBUG","ts":"2025-11-27T22:32:43.158Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=1"}
{"level":"DEBUG","ts":"2025-11-27T22:32:43.158Z","msg":"Collected capacity metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, metricsCount=1"}
{"level":"DEBUG","ts":"2025-11-27T22:32:43.159Z","msg":"Variant analysis initialized: variant=ms-inference-scheduling-llm-d-modelservice-decode, accelerator=H100, cost=10.00, replicaCount=1"}
{"level":"DEBUG","ts":"2025-11-27T22:32:43.159Z","msg":"Scale-down unsafe: insufficient non-saturated replicas: nonSaturated=1, required=2"}
{"level":"DEBUG","ts":"2025-11-27T22:32:43.159Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, totalReplicas=1, nonSaturated=1, avgSpareKv=0.372, avgSpareQueue=5.0, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"INFO","ts":"2025-11-27T22:32:43.159Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, totalReplicas=1, nonSaturated=1, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"DEBUG","ts":"2025-11-27T22:32:43.159Z","msg":"Capacity targets: no scaling needed (avgSpareKv=0.372, avgSpareQueue=5.0, all variants stable)"}
{"level":"DEBUG","ts":"2025-11-27T22:32:43.159Z","msg":"Capacity targets calculated: modelID=unsloth/Meta-Llama-3.1-8B, targets=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"INFO","ts":"2025-11-27T22:32:43.159Z","msg":"Capacity-only decisions made for model: unsloth/Meta-Llama-3.1-8B - decision count: 0"}
{"level":"INFO","ts":"2025-11-27T22:32:43.159Z","msg":"No scaling decisions to apply"}
{"level":"INFO","ts":"2025-11-27T22:32:43.159Z","msg":"Reconciliation completed successfully: mode=capacity-only, modelsProcessed=1, decisionsApplied=0"}
{"level":"INFO","ts":"2025-11-27T22:32:43.516Z","msg":"Operating in CAPACITY-ONLY mode: reactive capacity-based scaling only"}
{"level":"INFO","ts":"2025-11-27T22:32:43.517Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-11-27T22:32:43.517Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-11-27T22:32:43.527Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=1, accelerator=H100, ttft=22.40ms, itl=11.40ms, cost=10.00"}
{"level":"INFO","ts":"2025-11-27T22:32:43.529Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9wj976, queueLength=0"}
{"level":"DEBUG","ts":"2025-11-27T22:32:43.529Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"INFO","ts":"2025-11-27T22:32:43.529Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9wj976, usage=0.128 (12.8%)"}
{"level":"DEBUG","ts":"2025-11-27T22:32:43.529Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"DEBUG","ts":"2025-11-27T22:32:43.529Z","msg":"Pod-to-variant matching successful: totalPods=1, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"DEBUG","ts":"2025-11-27T22:32:43.529Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=1"}
{"level":"DEBUG","ts":"2025-11-27T22:32:43.529Z","msg":"Collected capacity metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, metricsCount=1"}
{"level":"DEBUG","ts":"2025-11-27T22:32:43.529Z","msg":"Variant analysis initialized: variant=ms-inference-scheduling-llm-d-modelservice-decode, accelerator=H100, cost=10.00, replicaCount=1"}
{"level":"DEBUG","ts":"2025-11-27T22:32:43.529Z","msg":"Scale-down unsafe: insufficient non-saturated replicas: nonSaturated=1, required=2"}
{"level":"DEBUG","ts":"2025-11-27T22:32:43.529Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, totalReplicas=1, nonSaturated=1, avgSpareKv=0.372, avgSpareQueue=5.0, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"INFO","ts":"2025-11-27T22:32:43.529Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, totalReplicas=1, nonSaturated=1, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"DEBUG","ts":"2025-11-27T22:32:43.529Z","msg":"Capacity targets: no scaling needed (avgSpareKv=0.372, avgSpareQueue=5.0, all variants stable)"}
{"level":"DEBUG","ts":"2025-11-27T22:32:43.529Z","msg":"Capacity targets calculated: modelID=unsloth/Meta-Llama-3.1-8B, targets=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"INFO","ts":"2025-11-27T22:32:43.529Z","msg":"Capacity-only decisions made for model: unsloth/Meta-Llama-3.1-8B - decision count: 0"}
{"level":"INFO","ts":"2025-11-27T22:32:43.529Z","msg":"No scaling decisions to apply"}
{"level":"INFO","ts":"2025-11-27T22:32:43.529Z","msg":"Reconciliation completed successfully: mode=capacity-only, modelsProcessed=1, decisionsApplied=0"}
{"level":"INFO","ts":"2025-11-27T22:33:13.159Z","msg":"Operating in CAPACITY-ONLY mode: reactive capacity-based scaling only"}
{"level":"INFO","ts":"2025-11-27T22:33:13.159Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-11-27T22:33:13.159Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-11-27T22:33:13.173Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=1, accelerator=H100, ttft=37.54ms, itl=20.93ms, cost=10.00"}
{"level":"INFO","ts":"2025-11-27T22:33:13.174Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9wj976, usage=0.345 (34.5%)"}
{"level":"DEBUG","ts":"2025-11-27T22:33:13.174Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"INFO","ts":"2025-11-27T22:33:13.175Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9wj976, queueLength=0"}
{"level":"DEBUG","ts":"2025-11-27T22:33:13.175Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"DEBUG","ts":"2025-11-27T22:33:13.175Z","msg":"Pod-to-variant matching successful: totalPods=1, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"DEBUG","ts":"2025-11-27T22:33:13.175Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=1"}
{"level":"DEBUG","ts":"2025-11-27T22:33:13.175Z","msg":"Collected capacity metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, metricsCount=1"}
{"level":"DEBUG","ts":"2025-11-27T22:33:13.175Z","msg":"Variant analysis initialized: variant=ms-inference-scheduling-llm-d-modelservice-decode, accelerator=H100, cost=10.00, replicaCount=1"}
{"level":"DEBUG","ts":"2025-11-27T22:33:13.175Z","msg":"Scale-down unsafe: insufficient non-saturated replicas: nonSaturated=1, required=2"}
{"level":"DEBUG","ts":"2025-11-27T22:33:13.175Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, totalReplicas=1, nonSaturated=1, avgSpareKv=0.155, avgSpareQueue=5.0, shouldScaleUp=true, scaleDownSafe=false"}
{"level":"INFO","ts":"2025-11-27T22:33:13.175Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, totalReplicas=1, nonSaturated=1, shouldScaleUp=true, scaleDownSafe=false"}
{"level":"INFO","ts":"2025-11-27T22:33:13.175Z","msg":"Capacity target: scale-up cheapest variant: variant=ms-inference-scheduling-llm-d-modelservice-decode, cost=10.00, currentReplicas=1, readyReplicas=1, target=2, reason=KV spare capacity low (0.155 < 0.200)"}
{"level":"DEBUG","ts":"2025-11-27T22:33:13.175Z","msg":"Capacity targets calculated: modelID=unsloth/Meta-Llama-3.1-8B, targets=map[ms-inference-scheduling-llm-d-modelservice-decode:2]"}
{"level":"INFO","ts":"2025-11-27T22:33:13.175Z","msg":"Capacity-only decisions made for model: unsloth/Meta-Llama-3.1-8B - decision count: 1"}
{"level":"INFO","ts":"2025-11-27T22:33:13.175Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-11-27T22:33:13.175Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=scale-up, current=1target=2"}
{"level":"DEBUG","ts":"2025-11-27T22:33:13.175Z","msg":"Found VA in map: variant=ms-inference-scheduling-llm-d-modelservice-decode, hasCurrentAlloc=true, accelerator=H100"}
{"level":"INFO","ts":"2025-11-27T22:33:13.175Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 1, desired-replicas: 2, accelerator: H100"}
{"level":"INFO","ts":"2025-11-27T22:33:13.175Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=2, accelerator=H100, capacityOnly=true"}
{"level":"INFO","ts":"2025-11-27T22:33:13.179Z","msg":"Applied capacity decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=scale-up, current=1, target=2, reason=capacity-only mode: scale-up"}
{"level":"INFO","ts":"2025-11-27T22:33:13.179Z","msg":"Reconciliation completed successfully: mode=capacity-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-11-27T22:33:13.530Z","msg":"Operating in CAPACITY-ONLY mode: reactive capacity-based scaling only"}
{"level":"INFO","ts":"2025-11-27T22:33:13.530Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-11-27T22:33:13.530Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-11-27T22:33:13.541Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=1, accelerator=H100, ttft=37.54ms, itl=20.93ms, cost=10.00"}
{"level":"INFO","ts":"2025-11-27T22:33:13.542Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9wj976, queueLength=0"}
{"level":"DEBUG","ts":"2025-11-27T22:33:13.542Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"INFO","ts":"2025-11-27T22:33:13.542Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9wj976, usage=0.345 (34.5%)"}
{"level":"DEBUG","ts":"2025-11-27T22:33:13.542Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"DEBUG","ts":"2025-11-27T22:33:13.542Z","msg":"Pod-to-variant matching successful: totalPods=1, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"DEBUG","ts":"2025-11-27T22:33:13.542Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=1"}
{"level":"DEBUG","ts":"2025-11-27T22:33:13.542Z","msg":"Collected capacity metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, metricsCount=1"}
{"level":"DEBUG","ts":"2025-11-27T22:33:13.542Z","msg":"Variant analysis initialized: variant=ms-inference-scheduling-llm-d-modelservice-decode, accelerator=H100, cost=10.00, replicaCount=1"}
{"level":"DEBUG","ts":"2025-11-27T22:33:13.542Z","msg":"Scale-down unsafe: insufficient non-saturated replicas: nonSaturated=1, required=2"}
{"level":"DEBUG","ts":"2025-11-27T22:33:13.542Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, totalReplicas=1, nonSaturated=1, avgSpareKv=0.155, avgSpareQueue=5.0, shouldScaleUp=true, scaleDownSafe=false"}
{"level":"INFO","ts":"2025-11-27T22:33:13.542Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, totalReplicas=1, nonSaturated=1, shouldScaleUp=true, scaleDownSafe=false"}
{"level":"DEBUG","ts":"2025-11-27T22:33:13.542Z","msg":"Preserving desired replicas: variant=ms-inference-scheduling-llm-d-modelservice-decode, currentReplicas=1, readyReplicas=1, desired=2"}
{"level":"DEBUG","ts":"2025-11-27T22:33:13.542Z","msg":"Capacity targets calculated: modelID=unsloth/Meta-Llama-3.1-8B, targets=map[ms-inference-scheduling-llm-d-modelservice-decode:2]"}
{"level":"INFO","ts":"2025-11-27T22:33:13.542Z","msg":"Capacity-only decisions made for model: unsloth/Meta-Llama-3.1-8B - decision count: 1"}
{"level":"INFO","ts":"2025-11-27T22:33:13.542Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-11-27T22:33:13.542Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=scale-up, current=1target=2"}
{"level":"DEBUG","ts":"2025-11-27T22:33:13.542Z","msg":"Found VA in map: variant=ms-inference-scheduling-llm-d-modelservice-decode, hasCurrentAlloc=true, accelerator=H100"}
{"level":"INFO","ts":"2025-11-27T22:33:13.542Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 1, desired-replicas: 2, accelerator: H100"}
{"level":"INFO","ts":"2025-11-27T22:33:13.542Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=2, accelerator=H100, capacityOnly=true"}
{"level":"INFO","ts":"2025-11-27T22:33:13.546Z","msg":"Applied capacity decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=scale-up, current=1, target=2, reason=capacity-only mode: scale-up"}
{"level":"INFO","ts":"2025-11-27T22:33:13.546Z","msg":"Reconciliation completed successfully: mode=capacity-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-11-27T22:33:43.180Z","msg":"Operating in CAPACITY-ONLY mode: reactive capacity-based scaling only"}
{"level":"INFO","ts":"2025-11-27T22:33:43.181Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-11-27T22:33:43.181Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-11-27T22:33:43.201Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=2, accelerator=H100, ttft=44.22ms, itl=24.78ms, cost=20.00"}
{"level":"INFO","ts":"2025-11-27T22:33:43.202Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9wj976, queueLength=0"}
{"level":"DEBUG","ts":"2025-11-27T22:33:43.202Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"INFO","ts":"2025-11-27T22:33:43.202Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9wj976, usage=0.390 (39.0%)"}
{"level":"DEBUG","ts":"2025-11-27T22:33:43.202Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"DEBUG","ts":"2025-11-27T22:33:43.202Z","msg":"Pod-to-variant matching successful: totalPods=1, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"DEBUG","ts":"2025-11-27T22:33:43.202Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=1"}
{"level":"DEBUG","ts":"2025-11-27T22:33:43.202Z","msg":"Collected capacity metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, metricsCount=1"}
{"level":"DEBUG","ts":"2025-11-27T22:33:43.202Z","msg":"Variant analysis initialized: variant=ms-inference-scheduling-llm-d-modelservice-decode, accelerator=H100, cost=10.00, replicaCount=1"}
{"level":"DEBUG","ts":"2025-11-27T22:33:43.202Z","msg":"Scale-down unsafe: insufficient non-saturated replicas: nonSaturated=1, required=2"}
{"level":"DEBUG","ts":"2025-11-27T22:33:43.202Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, totalReplicas=1, nonSaturated=1, avgSpareKv=0.110, avgSpareQueue=5.0, shouldScaleUp=true, scaleDownSafe=false"}
{"level":"INFO","ts":"2025-11-27T22:33:43.202Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, totalReplicas=1, nonSaturated=1, shouldScaleUp=true, scaleDownSafe=false"}
{"level":"INFO","ts":"2025-11-27T22:33:43.202Z","msg":"Capacity target: scale-up cheapest variant: variant=ms-inference-scheduling-llm-d-modelservice-decode, cost=10.00, currentReplicas=2, readyReplicas=1, target=2, reason=KV spare capacity low (0.110 < 0.200)"}
{"level":"DEBUG","ts":"2025-11-27T22:33:43.202Z","msg":"Capacity targets calculated: modelID=unsloth/Meta-Llama-3.1-8B, targets=map[ms-inference-scheduling-llm-d-modelservice-decode:2]"}
{"level":"INFO","ts":"2025-11-27T22:33:43.202Z","msg":"Capacity-only decisions made for model: unsloth/Meta-Llama-3.1-8B - decision count: 0"}
{"level":"INFO","ts":"2025-11-27T22:33:43.202Z","msg":"No scaling decisions to apply"}
{"level":"INFO","ts":"2025-11-27T22:33:43.202Z","msg":"Reconciliation completed successfully: mode=capacity-only, modelsProcessed=1, decisionsApplied=0"}
{"level":"INFO","ts":"2025-11-27T22:33:43.548Z","msg":"Operating in CAPACITY-ONLY mode: reactive capacity-based scaling only"}
{"level":"INFO","ts":"2025-11-27T22:33:43.548Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-11-27T22:33:43.548Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-11-27T22:33:43.563Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=2, accelerator=H100, ttft=44.22ms, itl=24.78ms, cost=20.00"}
{"level":"INFO","ts":"2025-11-27T22:33:43.564Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9wj976, usage=0.390 (39.0%)"}
{"level":"DEBUG","ts":"2025-11-27T22:33:43.564Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"INFO","ts":"2025-11-27T22:33:43.564Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9wj976, queueLength=0"}
{"level":"DEBUG","ts":"2025-11-27T22:33:43.564Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"DEBUG","ts":"2025-11-27T22:33:43.564Z","msg":"Pod-to-variant matching successful: totalPods=1, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"DEBUG","ts":"2025-11-27T22:33:43.564Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=1"}
{"level":"DEBUG","ts":"2025-11-27T22:33:43.564Z","msg":"Collected capacity metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, metricsCount=1"}
{"level":"DEBUG","ts":"2025-11-27T22:33:43.564Z","msg":"Variant analysis initialized: variant=ms-inference-scheduling-llm-d-modelservice-decode, accelerator=H100, cost=10.00, replicaCount=1"}
{"level":"DEBUG","ts":"2025-11-27T22:33:43.564Z","msg":"Scale-down unsafe: insufficient non-saturated replicas: nonSaturated=1, required=2"}
{"level":"DEBUG","ts":"2025-11-27T22:33:43.564Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, totalReplicas=1, nonSaturated=1, avgSpareKv=0.110, avgSpareQueue=5.0, shouldScaleUp=true, scaleDownSafe=false"}
{"level":"INFO","ts":"2025-11-27T22:33:43.564Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, totalReplicas=1, nonSaturated=1, shouldScaleUp=true, scaleDownSafe=false"}
{"level":"INFO","ts":"2025-11-27T22:33:43.564Z","msg":"Capacity target: scale-up cheapest variant: variant=ms-inference-scheduling-llm-d-modelservice-decode, cost=10.00, currentReplicas=2, readyReplicas=1, target=2, reason=KV spare capacity low (0.110 < 0.200)"}
{"level":"DEBUG","ts":"2025-11-27T22:33:43.564Z","msg":"Capacity targets calculated: modelID=unsloth/Meta-Llama-3.1-8B, targets=map[ms-inference-scheduling-llm-d-modelservice-decode:2]"}
{"level":"INFO","ts":"2025-11-27T22:33:43.564Z","msg":"Capacity-only decisions made for model: unsloth/Meta-Llama-3.1-8B - decision count: 0"}
{"level":"INFO","ts":"2025-11-27T22:33:43.564Z","msg":"No scaling decisions to apply"}
{"level":"INFO","ts":"2025-11-27T22:33:43.564Z","msg":"Reconciliation completed successfully: mode=capacity-only, modelsProcessed=1, decisionsApplied=0"}
{"level":"INFO","ts":"2025-11-27T22:34:13.203Z","msg":"Operating in CAPACITY-ONLY mode: reactive capacity-based scaling only"}
{"level":"INFO","ts":"2025-11-27T22:34:13.203Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-11-27T22:34:13.203Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-11-27T22:34:13.227Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=2, accelerator=H100, ttft=65.71ms, itl=40.16ms, cost=20.00"}
{"level":"INFO","ts":"2025-11-27T22:34:13.228Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9wj976, queueLength=0"}
{"level":"DEBUG","ts":"2025-11-27T22:34:13.228Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"INFO","ts":"2025-11-27T22:34:13.229Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9wj976, usage=0.709 (70.9%)"}
{"level":"DEBUG","ts":"2025-11-27T22:34:13.229Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"DEBUG","ts":"2025-11-27T22:34:13.229Z","msg":"Pod-to-variant matching successful: totalPods=1, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"DEBUG","ts":"2025-11-27T22:34:13.229Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=1"}
{"level":"DEBUG","ts":"2025-11-27T22:34:13.229Z","msg":"Collected capacity metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, metricsCount=1"}
{"level":"DEBUG","ts":"2025-11-27T22:34:13.229Z","msg":"Variant analysis initialized: variant=ms-inference-scheduling-llm-d-modelservice-decode, accelerator=H100, cost=10.00, replicaCount=1"}
{"level":"DEBUG","ts":"2025-11-27T22:34:13.229Z","msg":"Scale-down unsafe: insufficient non-saturated replicas: nonSaturated=0, required=2"}
{"level":"DEBUG","ts":"2025-11-27T22:34:13.229Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, totalReplicas=1, nonSaturated=0, avgSpareKv=0.000, avgSpareQueue=0.0, shouldScaleUp=true, scaleDownSafe=false"}
{"level":"INFO","ts":"2025-11-27T22:34:13.229Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, totalReplicas=1, nonSaturated=0, shouldScaleUp=true, scaleDownSafe=false"}
{"level":"INFO","ts":"2025-11-27T22:34:13.229Z","msg":"Capacity target: scale-up cheapest variant: variant=ms-inference-scheduling-llm-d-modelservice-decode, cost=10.00, currentReplicas=2, readyReplicas=1, target=2, reason=both KV spare (0.000 < 0.200) and queue spare (0.0 < 3.0)"}
{"level":"DEBUG","ts":"2025-11-27T22:34:13.229Z","msg":"Capacity targets calculated: modelID=unsloth/Meta-Llama-3.1-8B, targets=map[ms-inference-scheduling-llm-d-modelservice-decode:2]"}
{"level":"INFO","ts":"2025-11-27T22:34:13.229Z","msg":"Capacity-only decisions made for model: unsloth/Meta-Llama-3.1-8B - decision count: 0"}
{"level":"INFO","ts":"2025-11-27T22:34:13.229Z","msg":"No scaling decisions to apply"}
{"level":"INFO","ts":"2025-11-27T22:34:13.229Z","msg":"Reconciliation completed successfully: mode=capacity-only, modelsProcessed=1, decisionsApplied=0"}
{"level":"INFO","ts":"2025-11-27T22:34:13.565Z","msg":"Operating in CAPACITY-ONLY mode: reactive capacity-based scaling only"}
{"level":"INFO","ts":"2025-11-27T22:34:13.565Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-11-27T22:34:13.565Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-11-27T22:34:13.576Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=2, accelerator=H100, ttft=65.71ms, itl=40.16ms, cost=20.00"}
{"level":"INFO","ts":"2025-11-27T22:34:13.578Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9wj976, usage=0.709 (70.9%)"}
{"level":"DEBUG","ts":"2025-11-27T22:34:13.578Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"INFO","ts":"2025-11-27T22:34:13.578Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9wj976, queueLength=0"}
{"level":"DEBUG","ts":"2025-11-27T22:34:13.578Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"DEBUG","ts":"2025-11-27T22:34:13.578Z","msg":"Pod-to-variant matching successful: totalPods=1, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"DEBUG","ts":"2025-11-27T22:34:13.578Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=1"}
{"level":"DEBUG","ts":"2025-11-27T22:34:13.578Z","msg":"Collected capacity metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, metricsCount=1"}
{"level":"DEBUG","ts":"2025-11-27T22:34:13.578Z","msg":"Variant analysis initialized: variant=ms-inference-scheduling-llm-d-modelservice-decode, accelerator=H100, cost=10.00, replicaCount=1"}
{"level":"DEBUG","ts":"2025-11-27T22:34:13.578Z","msg":"Scale-down unsafe: insufficient non-saturated replicas: nonSaturated=0, required=2"}
{"level":"DEBUG","ts":"2025-11-27T22:34:13.578Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, totalReplicas=1, nonSaturated=0, avgSpareKv=0.000, avgSpareQueue=0.0, shouldScaleUp=true, scaleDownSafe=false"}
{"level":"INFO","ts":"2025-11-27T22:34:13.578Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, totalReplicas=1, nonSaturated=0, shouldScaleUp=true, scaleDownSafe=false"}
{"level":"INFO","ts":"2025-11-27T22:34:13.578Z","msg":"Capacity target: scale-up cheapest variant: variant=ms-inference-scheduling-llm-d-modelservice-decode, cost=10.00, currentReplicas=2, readyReplicas=1, target=2, reason=both KV spare (0.000 < 0.200) and queue spare (0.0 < 3.0)"}
{"level":"DEBUG","ts":"2025-11-27T22:34:13.578Z","msg":"Capacity targets calculated: modelID=unsloth/Meta-Llama-3.1-8B, targets=map[ms-inference-scheduling-llm-d-modelservice-decode:2]"}
{"level":"INFO","ts":"2025-11-27T22:34:13.578Z","msg":"Capacity-only decisions made for model: unsloth/Meta-Llama-3.1-8B - decision count: 0"}
{"level":"INFO","ts":"2025-11-27T22:34:13.578Z","msg":"No scaling decisions to apply"}
{"level":"INFO","ts":"2025-11-27T22:34:13.578Z","msg":"Reconciliation completed successfully: mode=capacity-only, modelsProcessed=1, decisionsApplied=0"}
{"level":"INFO","ts":"2025-11-27T22:34:43.230Z","msg":"Operating in CAPACITY-ONLY mode: reactive capacity-based scaling only"}
{"level":"INFO","ts":"2025-11-27T22:34:43.230Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-11-27T22:34:43.230Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-11-27T22:34:43.241Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=2, accelerator=H100, ttft=88.28ms, itl=55.35ms, cost=20.00"}
{"level":"INFO","ts":"2025-11-27T22:34:43.243Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9wj976, queueLength=0"}
{"level":"DEBUG","ts":"2025-11-27T22:34:43.243Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"INFO","ts":"2025-11-27T22:34:43.243Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9wj976, usage=0.985 (98.5%)"}
{"level":"DEBUG","ts":"2025-11-27T22:34:43.243Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"DEBUG","ts":"2025-11-27T22:34:43.243Z","msg":"Pod-to-variant matching successful: totalPods=1, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"DEBUG","ts":"2025-11-27T22:34:43.243Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=1"}
{"level":"DEBUG","ts":"2025-11-27T22:34:43.243Z","msg":"Collected capacity metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, metricsCount=1"}
{"level":"DEBUG","ts":"2025-11-27T22:34:43.243Z","msg":"Variant analysis initialized: variant=ms-inference-scheduling-llm-d-modelservice-decode, accelerator=H100, cost=10.00, replicaCount=1"}
{"level":"DEBUG","ts":"2025-11-27T22:34:43.243Z","msg":"Scale-down unsafe: insufficient non-saturated replicas: nonSaturated=0, required=2"}
{"level":"DEBUG","ts":"2025-11-27T22:34:43.243Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, totalReplicas=1, nonSaturated=0, avgSpareKv=0.000, avgSpareQueue=0.0, shouldScaleUp=true, scaleDownSafe=false"}
{"level":"INFO","ts":"2025-11-27T22:34:43.243Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, totalReplicas=1, nonSaturated=0, shouldScaleUp=true, scaleDownSafe=false"}
{"level":"INFO","ts":"2025-11-27T22:34:43.243Z","msg":"Capacity target: scale-up cheapest variant: variant=ms-inference-scheduling-llm-d-modelservice-decode, cost=10.00, currentReplicas=2, readyReplicas=1, target=2, reason=both KV spare (0.000 < 0.200) and queue spare (0.0 < 3.0)"}
{"level":"DEBUG","ts":"2025-11-27T22:34:43.243Z","msg":"Capacity targets calculated: modelID=unsloth/Meta-Llama-3.1-8B, targets=map[ms-inference-scheduling-llm-d-modelservice-decode:2]"}
{"level":"INFO","ts":"2025-11-27T22:34:43.243Z","msg":"Capacity-only decisions made for model: unsloth/Meta-Llama-3.1-8B - decision count: 0"}
{"level":"INFO","ts":"2025-11-27T22:34:43.243Z","msg":"No scaling decisions to apply"}
{"level":"INFO","ts":"2025-11-27T22:34:43.243Z","msg":"Reconciliation completed successfully: mode=capacity-only, modelsProcessed=1, decisionsApplied=0"}
{"level":"INFO","ts":"2025-11-27T22:34:43.579Z","msg":"Operating in CAPACITY-ONLY mode: reactive capacity-based scaling only"}
{"level":"INFO","ts":"2025-11-27T22:34:43.579Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-11-27T22:34:43.579Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-11-27T22:34:43.589Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=2, accelerator=H100, ttft=88.28ms, itl=55.35ms, cost=20.00"}
{"level":"INFO","ts":"2025-11-27T22:34:43.591Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9wj976, usage=0.985 (98.5%)"}
{"level":"DEBUG","ts":"2025-11-27T22:34:43.591Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"INFO","ts":"2025-11-27T22:34:43.591Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9wj976, queueLength=0"}
{"level":"DEBUG","ts":"2025-11-27T22:34:43.591Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"DEBUG","ts":"2025-11-27T22:34:43.591Z","msg":"Pod-to-variant matching successful: totalPods=1, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"DEBUG","ts":"2025-11-27T22:34:43.591Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=1"}
{"level":"DEBUG","ts":"2025-11-27T22:34:43.591Z","msg":"Collected capacity metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, metricsCount=1"}
{"level":"DEBUG","ts":"2025-11-27T22:34:43.591Z","msg":"Variant analysis initialized: variant=ms-inference-scheduling-llm-d-modelservice-decode, accelerator=H100, cost=10.00, replicaCount=1"}
{"level":"DEBUG","ts":"2025-11-27T22:34:43.591Z","msg":"Scale-down unsafe: insufficient non-saturated replicas: nonSaturated=0, required=2"}
{"level":"DEBUG","ts":"2025-11-27T22:34:43.591Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, totalReplicas=1, nonSaturated=0, avgSpareKv=0.000, avgSpareQueue=0.0, shouldScaleUp=true, scaleDownSafe=false"}
{"level":"INFO","ts":"2025-11-27T22:34:43.591Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, totalReplicas=1, nonSaturated=0, shouldScaleUp=true, scaleDownSafe=false"}
{"level":"INFO","ts":"2025-11-27T22:34:43.591Z","msg":"Capacity target: scale-up cheapest variant: variant=ms-inference-scheduling-llm-d-modelservice-decode, cost=10.00, currentReplicas=2, readyReplicas=1, target=2, reason=both KV spare (0.000 < 0.200) and queue spare (0.0 < 3.0)"}
{"level":"DEBUG","ts":"2025-11-27T22:34:43.591Z","msg":"Capacity targets calculated: modelID=unsloth/Meta-Llama-3.1-8B, targets=map[ms-inference-scheduling-llm-d-modelservice-decode:2]"}
{"level":"INFO","ts":"2025-11-27T22:34:43.591Z","msg":"Capacity-only decisions made for model: unsloth/Meta-Llama-3.1-8B - decision count: 0"}
{"level":"INFO","ts":"2025-11-27T22:34:43.591Z","msg":"No scaling decisions to apply"}
{"level":"INFO","ts":"2025-11-27T22:34:43.591Z","msg":"Reconciliation completed successfully: mode=capacity-only, modelsProcessed=1, decisionsApplied=0"}
{"level":"INFO","ts":"2025-11-27T22:35:13.244Z","msg":"Operating in CAPACITY-ONLY mode: reactive capacity-based scaling only"}
{"level":"INFO","ts":"2025-11-27T22:35:13.244Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-11-27T22:35:13.244Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-11-27T22:35:13.265Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=2, accelerator=H100, ttft=449.74ms, itl=66.51ms, cost=20.00"}
{"level":"INFO","ts":"2025-11-27T22:35:13.267Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9gkhbr, queueLength=0"}
{"level":"INFO","ts":"2025-11-27T22:35:13.267Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9wj976, queueLength=185"}
{"level":"DEBUG","ts":"2025-11-27T22:35:13.267Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=2"}
{"level":"INFO","ts":"2025-11-27T22:35:13.267Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9gkhbr, usage=0.000 (0.0%)"}
{"level":"INFO","ts":"2025-11-27T22:35:13.267Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9wj976, usage=1.000 (100.0%)"}
{"level":"DEBUG","ts":"2025-11-27T22:35:13.267Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=2"}
{"level":"DEBUG","ts":"2025-11-27T22:35:13.267Z","msg":"Pod-to-variant matching successful: totalPods=2, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:2]"}
{"level":"DEBUG","ts":"2025-11-27T22:35:13.267Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=2"}
{"level":"DEBUG","ts":"2025-11-27T22:35:13.267Z","msg":"Collected capacity metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, metricsCount=2"}
{"level":"DEBUG","ts":"2025-11-27T22:35:13.267Z","msg":"Variant analysis initialized: variant=ms-inference-scheduling-llm-d-modelservice-decode, accelerator=H100, cost=10.00, replicaCount=2"}
{"level":"DEBUG","ts":"2025-11-27T22:35:13.267Z","msg":"Scale-down unsafe: insufficient non-saturated replicas: nonSaturated=1, required=2"}
{"level":"DEBUG","ts":"2025-11-27T22:35:13.267Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, totalReplicas=2, nonSaturated=1, avgSpareKv=0.500, avgSpareQueue=5.0, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"INFO","ts":"2025-11-27T22:35:13.267Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, totalReplicas=2, nonSaturated=1, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"DEBUG","ts":"2025-11-27T22:35:13.267Z","msg":"Capacity targets: no scaling needed (avgSpareKv=0.500, avgSpareQueue=5.0, all variants stable)"}
{"level":"DEBUG","ts":"2025-11-27T22:35:13.267Z","msg":"Capacity targets calculated: modelID=unsloth/Meta-Llama-3.1-8B, targets=map[ms-inference-scheduling-llm-d-modelservice-decode:2]"}
{"level":"INFO","ts":"2025-11-27T22:35:13.267Z","msg":"Capacity-only decisions made for model: unsloth/Meta-Llama-3.1-8B - decision count: 0"}
{"level":"INFO","ts":"2025-11-27T22:35:13.267Z","msg":"No scaling decisions to apply"}
{"level":"INFO","ts":"2025-11-27T22:35:13.267Z","msg":"Reconciliation completed successfully: mode=capacity-only, modelsProcessed=1, decisionsApplied=0"}
{"level":"INFO","ts":"2025-11-27T22:35:13.592Z","msg":"Operating in CAPACITY-ONLY mode: reactive capacity-based scaling only"}
{"level":"INFO","ts":"2025-11-27T22:35:13.592Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-11-27T22:35:13.592Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-11-27T22:35:13.603Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=2, accelerator=H100, ttft=449.74ms, itl=66.51ms, cost=20.00"}
{"level":"INFO","ts":"2025-11-27T22:35:13.604Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9gkhbr, queueLength=0"}
{"level":"INFO","ts":"2025-11-27T22:35:13.604Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9wj976, queueLength=185"}
{"level":"DEBUG","ts":"2025-11-27T22:35:13.604Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=2"}
{"level":"INFO","ts":"2025-11-27T22:35:13.604Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9gkhbr, usage=0.000 (0.0%)"}
{"level":"INFO","ts":"2025-11-27T22:35:13.604Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9wj976, usage=1.000 (100.0%)"}
{"level":"DEBUG","ts":"2025-11-27T22:35:13.604Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=2"}
{"level":"DEBUG","ts":"2025-11-27T22:35:13.604Z","msg":"Pod-to-variant matching successful: totalPods=2, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:2]"}
{"level":"DEBUG","ts":"2025-11-27T22:35:13.604Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=2"}
{"level":"DEBUG","ts":"2025-11-27T22:35:13.604Z","msg":"Collected capacity metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, metricsCount=2"}
{"level":"DEBUG","ts":"2025-11-27T22:35:13.604Z","msg":"Variant analysis initialized: variant=ms-inference-scheduling-llm-d-modelservice-decode, accelerator=H100, cost=10.00, replicaCount=2"}
{"level":"DEBUG","ts":"2025-11-27T22:35:13.604Z","msg":"Scale-down unsafe: insufficient non-saturated replicas: nonSaturated=1, required=2"}
{"level":"DEBUG","ts":"2025-11-27T22:35:13.604Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, totalReplicas=2, nonSaturated=1, avgSpareKv=0.500, avgSpareQueue=5.0, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"INFO","ts":"2025-11-27T22:35:13.604Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, totalReplicas=2, nonSaturated=1, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"DEBUG","ts":"2025-11-27T22:35:13.604Z","msg":"Capacity targets: no scaling needed (avgSpareKv=0.500, avgSpareQueue=5.0, all variants stable)"}
{"level":"DEBUG","ts":"2025-11-27T22:35:13.604Z","msg":"Capacity targets calculated: modelID=unsloth/Meta-Llama-3.1-8B, targets=map[ms-inference-scheduling-llm-d-modelservice-decode:2]"}
{"level":"INFO","ts":"2025-11-27T22:35:13.604Z","msg":"Capacity-only decisions made for model: unsloth/Meta-Llama-3.1-8B - decision count: 0"}
{"level":"INFO","ts":"2025-11-27T22:35:13.604Z","msg":"No scaling decisions to apply"}
{"level":"INFO","ts":"2025-11-27T22:35:13.604Z","msg":"Reconciliation completed successfully: mode=capacity-only, modelsProcessed=1, decisionsApplied=0"}
{"level":"INFO","ts":"2025-11-27T22:35:43.268Z","msg":"Operating in CAPACITY-ONLY mode: reactive capacity-based scaling only"}
{"level":"INFO","ts":"2025-11-27T22:35:43.268Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-11-27T22:35:43.268Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-11-27T22:35:43.317Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=2, accelerator=H100, ttft=8421.97ms, itl=57.89ms, cost=20.00"}
{"level":"INFO","ts":"2025-11-27T22:35:43.319Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9gkhbr, queueLength=0"}
{"level":"INFO","ts":"2025-11-27T22:35:43.319Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9wj976, queueLength=312"}
{"level":"DEBUG","ts":"2025-11-27T22:35:43.319Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=2"}
{"level":"INFO","ts":"2025-11-27T22:35:43.319Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9gkhbr, usage=0.070 (7.0%)"}
{"level":"INFO","ts":"2025-11-27T22:35:43.319Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9wj976, usage=1.000 (100.0%)"}
{"level":"DEBUG","ts":"2025-11-27T22:35:43.319Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=2"}
{"level":"DEBUG","ts":"2025-11-27T22:35:43.319Z","msg":"Pod-to-variant matching successful: totalPods=2, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:2]"}
{"level":"DEBUG","ts":"2025-11-27T22:35:43.319Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=2"}
{"level":"DEBUG","ts":"2025-11-27T22:35:43.319Z","msg":"Collected capacity metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, metricsCount=2"}
{"level":"DEBUG","ts":"2025-11-27T22:35:43.319Z","msg":"Variant analysis initialized: variant=ms-inference-scheduling-llm-d-modelservice-decode, accelerator=H100, cost=10.00, replicaCount=2"}
{"level":"DEBUG","ts":"2025-11-27T22:35:43.319Z","msg":"Scale-down unsafe: insufficient non-saturated replicas: nonSaturated=1, required=2"}
{"level":"DEBUG","ts":"2025-11-27T22:35:43.319Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, totalReplicas=2, nonSaturated=1, avgSpareKv=0.430, avgSpareQueue=5.0, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"INFO","ts":"2025-11-27T22:35:43.320Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, totalReplicas=2, nonSaturated=1, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"DEBUG","ts":"2025-11-27T22:35:43.320Z","msg":"Capacity targets: no scaling needed (avgSpareKv=0.430, avgSpareQueue=5.0, all variants stable)"}
{"level":"DEBUG","ts":"2025-11-27T22:35:43.320Z","msg":"Capacity targets calculated: modelID=unsloth/Meta-Llama-3.1-8B, targets=map[ms-inference-scheduling-llm-d-modelservice-decode:2]"}
{"level":"INFO","ts":"2025-11-27T22:35:43.320Z","msg":"Capacity-only decisions made for model: unsloth/Meta-Llama-3.1-8B - decision count: 0"}
{"level":"INFO","ts":"2025-11-27T22:35:43.320Z","msg":"No scaling decisions to apply"}
{"level":"INFO","ts":"2025-11-27T22:35:43.320Z","msg":"Reconciliation completed successfully: mode=capacity-only, modelsProcessed=1, decisionsApplied=0"}
{"level":"INFO","ts":"2025-11-27T22:35:43.605Z","msg":"Operating in CAPACITY-ONLY mode: reactive capacity-based scaling only"}
{"level":"INFO","ts":"2025-11-27T22:35:43.605Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-11-27T22:35:43.605Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-11-27T22:35:43.616Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=2, accelerator=H100, ttft=8409.98ms, itl=57.84ms, cost=20.00"}
{"level":"INFO","ts":"2025-11-27T22:35:43.618Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9gkhbr, queueLength=0"}
{"level":"INFO","ts":"2025-11-27T22:35:43.618Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9wj976, queueLength=312"}
{"level":"DEBUG","ts":"2025-11-27T22:35:43.618Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=2"}
{"level":"INFO","ts":"2025-11-27T22:35:43.618Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9gkhbr, usage=0.070 (7.0%)"}
{"level":"INFO","ts":"2025-11-27T22:35:43.618Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9wj976, usage=1.000 (100.0%)"}
{"level":"DEBUG","ts":"2025-11-27T22:35:43.618Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=2"}
{"level":"DEBUG","ts":"2025-11-27T22:35:43.618Z","msg":"Pod-to-variant matching successful: totalPods=2, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:2]"}
{"level":"DEBUG","ts":"2025-11-27T22:35:43.618Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=2"}
{"level":"DEBUG","ts":"2025-11-27T22:35:43.618Z","msg":"Collected capacity metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, metricsCount=2"}
{"level":"DEBUG","ts":"2025-11-27T22:35:43.618Z","msg":"Variant analysis initialized: variant=ms-inference-scheduling-llm-d-modelservice-decode, accelerator=H100, cost=10.00, replicaCount=2"}
{"level":"DEBUG","ts":"2025-11-27T22:35:43.618Z","msg":"Scale-down unsafe: insufficient non-saturated replicas: nonSaturated=1, required=2"}
{"level":"DEBUG","ts":"2025-11-27T22:35:43.618Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, totalReplicas=2, nonSaturated=1, avgSpareKv=0.430, avgSpareQueue=5.0, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"INFO","ts":"2025-11-27T22:35:43.618Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, totalReplicas=2, nonSaturated=1, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"DEBUG","ts":"2025-11-27T22:35:43.618Z","msg":"Capacity targets: no scaling needed (avgSpareKv=0.430, avgSpareQueue=5.0, all variants stable)"}
{"level":"DEBUG","ts":"2025-11-27T22:35:43.618Z","msg":"Capacity targets calculated: modelID=unsloth/Meta-Llama-3.1-8B, targets=map[ms-inference-scheduling-llm-d-modelservice-decode:2]"}
{"level":"INFO","ts":"2025-11-27T22:35:43.618Z","msg":"Capacity-only decisions made for model: unsloth/Meta-Llama-3.1-8B - decision count: 0"}
{"level":"INFO","ts":"2025-11-27T22:35:43.618Z","msg":"No scaling decisions to apply"}
{"level":"INFO","ts":"2025-11-27T22:35:43.618Z","msg":"Reconciliation completed successfully: mode=capacity-only, modelsProcessed=1, decisionsApplied=0"}
{"level":"INFO","ts":"2025-11-27T22:36:13.320Z","msg":"Operating in CAPACITY-ONLY mode: reactive capacity-based scaling only"}
{"level":"INFO","ts":"2025-11-27T22:36:13.321Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-11-27T22:36:13.321Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-11-27T22:36:13.335Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=2, accelerator=H100, ttft=5019.13ms, itl=60.98ms, cost=20.00"}
{"level":"INFO","ts":"2025-11-27T22:36:13.337Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9gkhbr, queueLength=0"}
{"level":"INFO","ts":"2025-11-27T22:36:13.337Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9wj976, queueLength=312"}
{"level":"DEBUG","ts":"2025-11-27T22:36:13.337Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=2"}
{"level":"INFO","ts":"2025-11-27T22:36:13.337Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9gkhbr, usage=0.070 (7.0%)"}
{"level":"INFO","ts":"2025-11-27T22:36:13.337Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9wj976, usage=0.997 (99.7%)"}
{"level":"DEBUG","ts":"2025-11-27T22:36:13.337Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=2"}
{"level":"DEBUG","ts":"2025-11-27T22:36:13.337Z","msg":"Pod-to-variant matching successful: totalPods=2, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:2]"}
{"level":"DEBUG","ts":"2025-11-27T22:36:13.337Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=2"}
{"level":"DEBUG","ts":"2025-11-27T22:36:13.337Z","msg":"Collected capacity metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, metricsCount=2"}
{"level":"DEBUG","ts":"2025-11-27T22:36:13.337Z","msg":"Variant analysis initialized: variant=ms-inference-scheduling-llm-d-modelservice-decode, accelerator=H100, cost=10.00, replicaCount=2"}
{"level":"DEBUG","ts":"2025-11-27T22:36:13.337Z","msg":"Scale-down unsafe: insufficient non-saturated replicas: nonSaturated=1, required=2"}
{"level":"DEBUG","ts":"2025-11-27T22:36:13.337Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, totalReplicas=2, nonSaturated=1, avgSpareKv=0.430, avgSpareQueue=5.0, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"INFO","ts":"2025-11-27T22:36:13.337Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, totalReplicas=2, nonSaturated=1, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"DEBUG","ts":"2025-11-27T22:36:13.337Z","msg":"Capacity targets: no scaling needed (avgSpareKv=0.430, avgSpareQueue=5.0, all variants stable)"}
{"level":"DEBUG","ts":"2025-11-27T22:36:13.337Z","msg":"Capacity targets calculated: modelID=unsloth/Meta-Llama-3.1-8B, targets=map[ms-inference-scheduling-llm-d-modelservice-decode:2]"}
{"level":"INFO","ts":"2025-11-27T22:36:13.337Z","msg":"Capacity-only decisions made for model: unsloth/Meta-Llama-3.1-8B - decision count: 0"}
{"level":"INFO","ts":"2025-11-27T22:36:13.337Z","msg":"No scaling decisions to apply"}
{"level":"INFO","ts":"2025-11-27T22:36:13.337Z","msg":"Reconciliation completed successfully: mode=capacity-only, modelsProcessed=1, decisionsApplied=0"}
{"level":"INFO","ts":"2025-11-27T22:36:13.618Z","msg":"Operating in CAPACITY-ONLY mode: reactive capacity-based scaling only"}
{"level":"INFO","ts":"2025-11-27T22:36:13.618Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-11-27T22:36:13.618Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-11-27T22:36:13.629Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=2, accelerator=H100, ttft=5019.13ms, itl=60.98ms, cost=20.00"}
{"level":"INFO","ts":"2025-11-27T22:36:13.631Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9gkhbr, queueLength=0"}
{"level":"INFO","ts":"2025-11-27T22:36:13.631Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9wj976, queueLength=312"}
{"level":"DEBUG","ts":"2025-11-27T22:36:13.631Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=2"}
{"level":"INFO","ts":"2025-11-27T22:36:13.631Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9gkhbr, usage=0.070 (7.0%)"}
{"level":"INFO","ts":"2025-11-27T22:36:13.631Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9wj976, usage=0.997 (99.7%)"}
{"level":"DEBUG","ts":"2025-11-27T22:36:13.631Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=2"}
{"level":"DEBUG","ts":"2025-11-27T22:36:13.631Z","msg":"Pod-to-variant matching successful: totalPods=2, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:2]"}
{"level":"DEBUG","ts":"2025-11-27T22:36:13.631Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=2"}
{"level":"DEBUG","ts":"2025-11-27T22:36:13.631Z","msg":"Collected capacity metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, metricsCount=2"}
{"level":"DEBUG","ts":"2025-11-27T22:36:13.631Z","msg":"Variant analysis initialized: variant=ms-inference-scheduling-llm-d-modelservice-decode, accelerator=H100, cost=10.00, replicaCount=2"}
{"level":"DEBUG","ts":"2025-11-27T22:36:13.631Z","msg":"Scale-down unsafe: insufficient non-saturated replicas: nonSaturated=1, required=2"}
{"level":"DEBUG","ts":"2025-11-27T22:36:13.631Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, totalReplicas=2, nonSaturated=1, avgSpareKv=0.430, avgSpareQueue=5.0, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"INFO","ts":"2025-11-27T22:36:13.631Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, totalReplicas=2, nonSaturated=1, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"DEBUG","ts":"2025-11-27T22:36:13.631Z","msg":"Capacity targets: no scaling needed (avgSpareKv=0.430, avgSpareQueue=5.0, all variants stable)"}
{"level":"DEBUG","ts":"2025-11-27T22:36:13.631Z","msg":"Capacity targets calculated: modelID=unsloth/Meta-Llama-3.1-8B, targets=map[ms-inference-scheduling-llm-d-modelservice-decode:2]"}
{"level":"INFO","ts":"2025-11-27T22:36:13.631Z","msg":"Capacity-only decisions made for model: unsloth/Meta-Llama-3.1-8B - decision count: 0"}
{"level":"INFO","ts":"2025-11-27T22:36:13.631Z","msg":"No scaling decisions to apply"}
{"level":"INFO","ts":"2025-11-27T22:36:13.631Z","msg":"Reconciliation completed successfully: mode=capacity-only, modelsProcessed=1, decisionsApplied=0"}
{"level":"INFO","ts":"2025-11-27T22:36:43.337Z","msg":"Operating in CAPACITY-ONLY mode: reactive capacity-based scaling only"}
{"level":"INFO","ts":"2025-11-27T22:36:43.337Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-11-27T22:36:43.337Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-11-27T22:36:43.350Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=2, accelerator=H100, ttft=41.53ms, itl=48.90ms, cost=20.00"}
{"level":"INFO","ts":"2025-11-27T22:36:43.352Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9gkhbr, usage=0.123 (12.3%)"}
{"level":"INFO","ts":"2025-11-27T22:36:43.352Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9wj976, usage=0.995 (99.5%)"}
{"level":"DEBUG","ts":"2025-11-27T22:36:43.352Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=2"}
{"level":"INFO","ts":"2025-11-27T22:36:43.352Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9gkhbr, queueLength=0"}
{"level":"INFO","ts":"2025-11-27T22:36:43.352Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9wj976, queueLength=514"}
{"level":"DEBUG","ts":"2025-11-27T22:36:43.352Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=2"}
{"level":"DEBUG","ts":"2025-11-27T22:36:43.352Z","msg":"Pod-to-variant matching successful: totalPods=2, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:2]"}
{"level":"DEBUG","ts":"2025-11-27T22:36:43.352Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=2"}
{"level":"DEBUG","ts":"2025-11-27T22:36:43.352Z","msg":"Collected capacity metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, metricsCount=2"}
{"level":"DEBUG","ts":"2025-11-27T22:36:43.352Z","msg":"Variant analysis initialized: variant=ms-inference-scheduling-llm-d-modelservice-decode, accelerator=H100, cost=10.00, replicaCount=2"}
{"level":"DEBUG","ts":"2025-11-27T22:36:43.352Z","msg":"Scale-down unsafe: insufficient non-saturated replicas: nonSaturated=1, required=2"}
{"level":"DEBUG","ts":"2025-11-27T22:36:43.352Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, totalReplicas=2, nonSaturated=1, avgSpareKv=0.377, avgSpareQueue=5.0, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"INFO","ts":"2025-11-27T22:36:43.352Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, totalReplicas=2, nonSaturated=1, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"DEBUG","ts":"2025-11-27T22:36:43.352Z","msg":"Capacity targets: no scaling needed (avgSpareKv=0.377, avgSpareQueue=5.0, all variants stable)"}
{"level":"DEBUG","ts":"2025-11-27T22:36:43.352Z","msg":"Capacity targets calculated: modelID=unsloth/Meta-Llama-3.1-8B, targets=map[ms-inference-scheduling-llm-d-modelservice-decode:2]"}
{"level":"INFO","ts":"2025-11-27T22:36:43.352Z","msg":"Capacity-only decisions made for model: unsloth/Meta-Llama-3.1-8B - decision count: 0"}
{"level":"INFO","ts":"2025-11-27T22:36:43.352Z","msg":"No scaling decisions to apply"}
{"level":"INFO","ts":"2025-11-27T22:36:43.352Z","msg":"Reconciliation completed successfully: mode=capacity-only, modelsProcessed=1, decisionsApplied=0"}
{"level":"INFO","ts":"2025-11-27T22:36:43.632Z","msg":"Operating in CAPACITY-ONLY mode: reactive capacity-based scaling only"}
{"level":"INFO","ts":"2025-11-27T22:36:43.632Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-11-27T22:36:43.632Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-11-27T22:36:43.643Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=2, accelerator=H100, ttft=41.53ms, itl=48.90ms, cost=20.00"}
{"level":"INFO","ts":"2025-11-27T22:36:43.645Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9gkhbr, usage=0.123 (12.3%)"}
{"level":"INFO","ts":"2025-11-27T22:36:43.645Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9wj976, usage=0.995 (99.5%)"}
{"level":"DEBUG","ts":"2025-11-27T22:36:43.645Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=2"}
{"level":"INFO","ts":"2025-11-27T22:36:43.645Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9gkhbr, queueLength=0"}
{"level":"INFO","ts":"2025-11-27T22:36:43.645Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9wj976, queueLength=514"}
{"level":"DEBUG","ts":"2025-11-27T22:36:43.645Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=2"}
{"level":"DEBUG","ts":"2025-11-27T22:36:43.645Z","msg":"Pod-to-variant matching successful: totalPods=2, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:2]"}
{"level":"DEBUG","ts":"2025-11-27T22:36:43.645Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=2"}
{"level":"DEBUG","ts":"2025-11-27T22:36:43.645Z","msg":"Collected capacity metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, metricsCount=2"}
{"level":"DEBUG","ts":"2025-11-27T22:36:43.645Z","msg":"Variant analysis initialized: variant=ms-inference-scheduling-llm-d-modelservice-decode, accelerator=H100, cost=10.00, replicaCount=2"}
{"level":"DEBUG","ts":"2025-11-27T22:36:43.645Z","msg":"Scale-down unsafe: insufficient non-saturated replicas: nonSaturated=1, required=2"}
{"level":"DEBUG","ts":"2025-11-27T22:36:43.645Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, totalReplicas=2, nonSaturated=1, avgSpareKv=0.377, avgSpareQueue=5.0, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"INFO","ts":"2025-11-27T22:36:43.645Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, totalReplicas=2, nonSaturated=1, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"DEBUG","ts":"2025-11-27T22:36:43.645Z","msg":"Capacity targets: no scaling needed (avgSpareKv=0.377, avgSpareQueue=5.0, all variants stable)"}
{"level":"DEBUG","ts":"2025-11-27T22:36:43.645Z","msg":"Capacity targets calculated: modelID=unsloth/Meta-Llama-3.1-8B, targets=map[ms-inference-scheduling-llm-d-modelservice-decode:2]"}
{"level":"INFO","ts":"2025-11-27T22:36:43.645Z","msg":"Capacity-only decisions made for model: unsloth/Meta-Llama-3.1-8B - decision count: 0"}
{"level":"INFO","ts":"2025-11-27T22:36:43.645Z","msg":"No scaling decisions to apply"}
{"level":"INFO","ts":"2025-11-27T22:36:43.645Z","msg":"Reconciliation completed successfully: mode=capacity-only, modelsProcessed=1, decisionsApplied=0"}
{"level":"INFO","ts":"2025-11-27T22:37:13.353Z","msg":"Operating in CAPACITY-ONLY mode: reactive capacity-based scaling only"}
{"level":"INFO","ts":"2025-11-27T22:37:13.353Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-11-27T22:37:13.353Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-11-27T22:37:13.367Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=2, accelerator=H100, ttft=7712.42ms, itl=60.50ms, cost=20.00"}
{"level":"INFO","ts":"2025-11-27T22:37:13.368Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9gkhbr, queueLength=0"}
{"level":"INFO","ts":"2025-11-27T22:37:13.368Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9wj976, queueLength=514"}
{"level":"DEBUG","ts":"2025-11-27T22:37:13.368Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=2"}
{"level":"INFO","ts":"2025-11-27T22:37:13.368Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9gkhbr, usage=0.123 (12.3%)"}
{"level":"INFO","ts":"2025-11-27T22:37:13.368Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9wj976, usage=0.995 (99.5%)"}
{"level":"DEBUG","ts":"2025-11-27T22:37:13.368Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=2"}
{"level":"DEBUG","ts":"2025-11-27T22:37:13.368Z","msg":"Pod-to-variant matching successful: totalPods=2, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:2]"}
{"level":"DEBUG","ts":"2025-11-27T22:37:13.368Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=2"}
{"level":"DEBUG","ts":"2025-11-27T22:37:13.368Z","msg":"Collected capacity metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, metricsCount=2"}
{"level":"DEBUG","ts":"2025-11-27T22:37:13.368Z","msg":"Variant analysis initialized: variant=ms-inference-scheduling-llm-d-modelservice-decode, accelerator=H100, cost=10.00, replicaCount=2"}
{"level":"DEBUG","ts":"2025-11-27T22:37:13.368Z","msg":"Scale-down unsafe: insufficient non-saturated replicas: nonSaturated=1, required=2"}
{"level":"DEBUG","ts":"2025-11-27T22:37:13.368Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, totalReplicas=2, nonSaturated=1, avgSpareKv=0.377, avgSpareQueue=5.0, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"INFO","ts":"2025-11-27T22:37:13.368Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, totalReplicas=2, nonSaturated=1, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"DEBUG","ts":"2025-11-27T22:37:13.368Z","msg":"Capacity targets: no scaling needed (avgSpareKv=0.377, avgSpareQueue=5.0, all variants stable)"}
{"level":"DEBUG","ts":"2025-11-27T22:37:13.368Z","msg":"Capacity targets calculated: modelID=unsloth/Meta-Llama-3.1-8B, targets=map[ms-inference-scheduling-llm-d-modelservice-decode:2]"}
{"level":"INFO","ts":"2025-11-27T22:37:13.368Z","msg":"Capacity-only decisions made for model: unsloth/Meta-Llama-3.1-8B - decision count: 0"}
{"level":"INFO","ts":"2025-11-27T22:37:13.368Z","msg":"No scaling decisions to apply"}
{"level":"INFO","ts":"2025-11-27T22:37:13.368Z","msg":"Reconciliation completed successfully: mode=capacity-only, modelsProcessed=1, decisionsApplied=0"}
{"level":"INFO","ts":"2025-11-27T22:37:13.646Z","msg":"Operating in CAPACITY-ONLY mode: reactive capacity-based scaling only"}
{"level":"INFO","ts":"2025-11-27T22:37:13.646Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-11-27T22:37:13.646Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-11-27T22:37:13.657Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=2, accelerator=H100, ttft=7712.42ms, itl=60.50ms, cost=20.00"}
{"level":"INFO","ts":"2025-11-27T22:37:13.658Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9gkhbr, queueLength=0"}
{"level":"INFO","ts":"2025-11-27T22:37:13.658Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9wj976, queueLength=514"}
{"level":"DEBUG","ts":"2025-11-27T22:37:13.658Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=2"}
{"level":"INFO","ts":"2025-11-27T22:37:13.658Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9gkhbr, usage=0.123 (12.3%)"}
{"level":"INFO","ts":"2025-11-27T22:37:13.658Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9wj976, usage=0.995 (99.5%)"}
{"level":"DEBUG","ts":"2025-11-27T22:37:13.658Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=2"}
{"level":"DEBUG","ts":"2025-11-27T22:37:13.659Z","msg":"Pod-to-variant matching successful: totalPods=2, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:2]"}
{"level":"DEBUG","ts":"2025-11-27T22:37:13.659Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=2"}
{"level":"DEBUG","ts":"2025-11-27T22:37:13.659Z","msg":"Collected capacity metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, metricsCount=2"}
{"level":"DEBUG","ts":"2025-11-27T22:37:13.659Z","msg":"Variant analysis initialized: variant=ms-inference-scheduling-llm-d-modelservice-decode, accelerator=H100, cost=10.00, replicaCount=2"}
{"level":"DEBUG","ts":"2025-11-27T22:37:13.659Z","msg":"Scale-down unsafe: insufficient non-saturated replicas: nonSaturated=1, required=2"}
{"level":"DEBUG","ts":"2025-11-27T22:37:13.659Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, totalReplicas=2, nonSaturated=1, avgSpareKv=0.377, avgSpareQueue=5.0, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"INFO","ts":"2025-11-27T22:37:13.659Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, totalReplicas=2, nonSaturated=1, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"DEBUG","ts":"2025-11-27T22:37:13.659Z","msg":"Capacity targets: no scaling needed (avgSpareKv=0.377, avgSpareQueue=5.0, all variants stable)"}
{"level":"DEBUG","ts":"2025-11-27T22:37:13.659Z","msg":"Capacity targets calculated: modelID=unsloth/Meta-Llama-3.1-8B, targets=map[ms-inference-scheduling-llm-d-modelservice-decode:2]"}
{"level":"INFO","ts":"2025-11-27T22:37:13.659Z","msg":"Capacity-only decisions made for model: unsloth/Meta-Llama-3.1-8B - decision count: 0"}
{"level":"INFO","ts":"2025-11-27T22:37:13.659Z","msg":"No scaling decisions to apply"}
{"level":"INFO","ts":"2025-11-27T22:37:13.659Z","msg":"Reconciliation completed successfully: mode=capacity-only, modelsProcessed=1, decisionsApplied=0"}
{"level":"INFO","ts":"2025-11-27T22:37:43.369Z","msg":"Operating in CAPACITY-ONLY mode: reactive capacity-based scaling only"}
{"level":"INFO","ts":"2025-11-27T22:37:43.369Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-11-27T22:37:43.369Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-11-27T22:37:43.389Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=2, accelerator=H100, ttft=1360.81ms, itl=46.49ms, cost=20.00"}
{"level":"INFO","ts":"2025-11-27T22:37:43.391Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9gkhbr, usage=0.156 (15.6%)"}
{"level":"INFO","ts":"2025-11-27T22:37:43.391Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9wj976, usage=0.993 (99.3%)"}
{"level":"DEBUG","ts":"2025-11-27T22:37:43.391Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=2"}
{"level":"INFO","ts":"2025-11-27T22:37:43.391Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9gkhbr, queueLength=0"}
{"level":"INFO","ts":"2025-11-27T22:37:43.391Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9wj976, queueLength=455"}
{"level":"DEBUG","ts":"2025-11-27T22:37:43.391Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=2"}
{"level":"DEBUG","ts":"2025-11-27T22:37:43.391Z","msg":"Pod-to-variant matching successful: totalPods=2, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:2]"}
{"level":"DEBUG","ts":"2025-11-27T22:37:43.391Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=2"}
{"level":"DEBUG","ts":"2025-11-27T22:37:43.391Z","msg":"Collected capacity metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, metricsCount=2"}
{"level":"DEBUG","ts":"2025-11-27T22:37:43.391Z","msg":"Variant analysis initialized: variant=ms-inference-scheduling-llm-d-modelservice-decode, accelerator=H100, cost=10.00, replicaCount=2"}
{"level":"DEBUG","ts":"2025-11-27T22:37:43.391Z","msg":"Scale-down unsafe: insufficient non-saturated replicas: nonSaturated=1, required=2"}
{"level":"DEBUG","ts":"2025-11-27T22:37:43.391Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, totalReplicas=2, nonSaturated=1, avgSpareKv=0.344, avgSpareQueue=5.0, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"INFO","ts":"2025-11-27T22:37:43.391Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, totalReplicas=2, nonSaturated=1, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"DEBUG","ts":"2025-11-27T22:37:43.391Z","msg":"Capacity targets: no scaling needed (avgSpareKv=0.344, avgSpareQueue=5.0, all variants stable)"}
{"level":"DEBUG","ts":"2025-11-27T22:37:43.391Z","msg":"Capacity targets calculated: modelID=unsloth/Meta-Llama-3.1-8B, targets=map[ms-inference-scheduling-llm-d-modelservice-decode:2]"}
{"level":"INFO","ts":"2025-11-27T22:37:43.391Z","msg":"Capacity-only decisions made for model: unsloth/Meta-Llama-3.1-8B - decision count: 0"}
{"level":"INFO","ts":"2025-11-27T22:37:43.391Z","msg":"No scaling decisions to apply"}
{"level":"INFO","ts":"2025-11-27T22:37:43.391Z","msg":"Reconciliation completed successfully: mode=capacity-only, modelsProcessed=1, decisionsApplied=0"}
{"level":"INFO","ts":"2025-11-27T22:37:43.660Z","msg":"Operating in CAPACITY-ONLY mode: reactive capacity-based scaling only"}
{"level":"INFO","ts":"2025-11-27T22:37:43.660Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-11-27T22:37:43.660Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-11-27T22:37:43.671Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=2, accelerator=H100, ttft=1360.81ms, itl=46.49ms, cost=20.00"}
{"level":"INFO","ts":"2025-11-27T22:37:43.673Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9gkhbr, queueLength=0"}
{"level":"INFO","ts":"2025-11-27T22:37:43.673Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9wj976, queueLength=455"}
{"level":"DEBUG","ts":"2025-11-27T22:37:43.673Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=2"}
{"level":"INFO","ts":"2025-11-27T22:37:43.673Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9gkhbr, usage=0.156 (15.6%)"}
{"level":"INFO","ts":"2025-11-27T22:37:43.673Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9wj976, usage=0.993 (99.3%)"}
{"level":"DEBUG","ts":"2025-11-27T22:37:43.673Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=2"}
{"level":"DEBUG","ts":"2025-11-27T22:37:43.673Z","msg":"Pod-to-variant matching successful: totalPods=2, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:2]"}
{"level":"DEBUG","ts":"2025-11-27T22:37:43.673Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=2"}
{"level":"DEBUG","ts":"2025-11-27T22:37:43.673Z","msg":"Collected capacity metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, metricsCount=2"}
{"level":"DEBUG","ts":"2025-11-27T22:37:43.673Z","msg":"Variant analysis initialized: variant=ms-inference-scheduling-llm-d-modelservice-decode, accelerator=H100, cost=10.00, replicaCount=2"}
{"level":"DEBUG","ts":"2025-11-27T22:37:43.673Z","msg":"Scale-down unsafe: insufficient non-saturated replicas: nonSaturated=1, required=2"}
{"level":"DEBUG","ts":"2025-11-27T22:37:43.673Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, totalReplicas=2, nonSaturated=1, avgSpareKv=0.344, avgSpareQueue=5.0, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"INFO","ts":"2025-11-27T22:37:43.673Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, totalReplicas=2, nonSaturated=1, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"DEBUG","ts":"2025-11-27T22:37:43.673Z","msg":"Capacity targets: no scaling needed (avgSpareKv=0.344, avgSpareQueue=5.0, all variants stable)"}
{"level":"DEBUG","ts":"2025-11-27T22:37:43.673Z","msg":"Capacity targets calculated: modelID=unsloth/Meta-Llama-3.1-8B, targets=map[ms-inference-scheduling-llm-d-modelservice-decode:2]"}
{"level":"INFO","ts":"2025-11-27T22:37:43.673Z","msg":"Capacity-only decisions made for model: unsloth/Meta-Llama-3.1-8B - decision count: 0"}
{"level":"INFO","ts":"2025-11-27T22:37:43.673Z","msg":"No scaling decisions to apply"}
{"level":"INFO","ts":"2025-11-27T22:37:43.673Z","msg":"Reconciliation completed successfully: mode=capacity-only, modelsProcessed=1, decisionsApplied=0"}
{"level":"INFO","ts":"2025-11-27T22:38:13.392Z","msg":"Operating in CAPACITY-ONLY mode: reactive capacity-based scaling only"}
{"level":"INFO","ts":"2025-11-27T22:38:13.392Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-11-27T22:38:13.392Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-11-27T22:38:13.406Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=2, accelerator=H100, ttft=7302.69ms, itl=48.54ms, cost=20.00"}
{"level":"INFO","ts":"2025-11-27T22:38:13.408Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9gkhbr, queueLength=0"}
{"level":"INFO","ts":"2025-11-27T22:38:13.408Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9wj976, queueLength=455"}
{"level":"DEBUG","ts":"2025-11-27T22:38:13.408Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=2"}
{"level":"INFO","ts":"2025-11-27T22:38:13.408Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9gkhbr, usage=0.156 (15.6%)"}
{"level":"INFO","ts":"2025-11-27T22:38:13.408Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9wj976, usage=0.993 (99.3%)"}
{"level":"DEBUG","ts":"2025-11-27T22:38:13.408Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=2"}
{"level":"DEBUG","ts":"2025-11-27T22:38:13.408Z","msg":"Pod-to-variant matching successful: totalPods=2, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:2]"}
{"level":"DEBUG","ts":"2025-11-27T22:38:13.408Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=2"}
{"level":"DEBUG","ts":"2025-11-27T22:38:13.408Z","msg":"Collected capacity metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, metricsCount=2"}
{"level":"DEBUG","ts":"2025-11-27T22:38:13.408Z","msg":"Variant analysis initialized: variant=ms-inference-scheduling-llm-d-modelservice-decode, accelerator=H100, cost=10.00, replicaCount=2"}
{"level":"DEBUG","ts":"2025-11-27T22:38:13.408Z","msg":"Scale-down unsafe: insufficient non-saturated replicas: nonSaturated=1, required=2"}
{"level":"DEBUG","ts":"2025-11-27T22:38:13.408Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, totalReplicas=2, nonSaturated=1, avgSpareKv=0.344, avgSpareQueue=5.0, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"INFO","ts":"2025-11-27T22:38:13.408Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, totalReplicas=2, nonSaturated=1, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"DEBUG","ts":"2025-11-27T22:38:13.408Z","msg":"Capacity targets: no scaling needed (avgSpareKv=0.344, avgSpareQueue=5.0, all variants stable)"}
{"level":"DEBUG","ts":"2025-11-27T22:38:13.408Z","msg":"Capacity targets calculated: modelID=unsloth/Meta-Llama-3.1-8B, targets=map[ms-inference-scheduling-llm-d-modelservice-decode:2]"}
{"level":"INFO","ts":"2025-11-27T22:38:13.408Z","msg":"Capacity-only decisions made for model: unsloth/Meta-Llama-3.1-8B - decision count: 0"}
{"level":"INFO","ts":"2025-11-27T22:38:13.408Z","msg":"No scaling decisions to apply"}
{"level":"INFO","ts":"2025-11-27T22:38:13.408Z","msg":"Reconciliation completed successfully: mode=capacity-only, modelsProcessed=1, decisionsApplied=0"}
{"level":"INFO","ts":"2025-11-27T22:38:13.674Z","msg":"Operating in CAPACITY-ONLY mode: reactive capacity-based scaling only"}
{"level":"INFO","ts":"2025-11-27T22:38:13.675Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-11-27T22:38:13.675Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-11-27T22:38:13.700Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=2, accelerator=H100, ttft=7302.69ms, itl=48.54ms, cost=20.00"}
{"level":"INFO","ts":"2025-11-27T22:38:13.702Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9gkhbr, usage=0.156 (15.6%)"}
{"level":"INFO","ts":"2025-11-27T22:38:13.702Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9wj976, usage=0.993 (99.3%)"}
{"level":"DEBUG","ts":"2025-11-27T22:38:13.702Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=2"}
{"level":"INFO","ts":"2025-11-27T22:38:13.702Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9gkhbr, queueLength=0"}
{"level":"INFO","ts":"2025-11-27T22:38:13.702Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9wj976, queueLength=455"}
{"level":"DEBUG","ts":"2025-11-27T22:38:13.702Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=2"}
{"level":"DEBUG","ts":"2025-11-27T22:38:13.702Z","msg":"Pod-to-variant matching successful: totalPods=2, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:2]"}
{"level":"DEBUG","ts":"2025-11-27T22:38:13.702Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=2"}
{"level":"DEBUG","ts":"2025-11-27T22:38:13.702Z","msg":"Collected capacity metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, metricsCount=2"}
{"level":"DEBUG","ts":"2025-11-27T22:38:13.702Z","msg":"Variant analysis initialized: variant=ms-inference-scheduling-llm-d-modelservice-decode, accelerator=H100, cost=10.00, replicaCount=2"}
{"level":"DEBUG","ts":"2025-11-27T22:38:13.702Z","msg":"Scale-down unsafe: insufficient non-saturated replicas: nonSaturated=1, required=2"}
{"level":"DEBUG","ts":"2025-11-27T22:38:13.702Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, totalReplicas=2, nonSaturated=1, avgSpareKv=0.344, avgSpareQueue=5.0, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"INFO","ts":"2025-11-27T22:38:13.702Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, totalReplicas=2, nonSaturated=1, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"DEBUG","ts":"2025-11-27T22:38:13.702Z","msg":"Capacity targets: no scaling needed (avgSpareKv=0.344, avgSpareQueue=5.0, all variants stable)"}
{"level":"DEBUG","ts":"2025-11-27T22:38:13.702Z","msg":"Capacity targets calculated: modelID=unsloth/Meta-Llama-3.1-8B, targets=map[ms-inference-scheduling-llm-d-modelservice-decode:2]"}
{"level":"INFO","ts":"2025-11-27T22:38:13.702Z","msg":"Capacity-only decisions made for model: unsloth/Meta-Llama-3.1-8B - decision count: 0"}
{"level":"INFO","ts":"2025-11-27T22:38:13.702Z","msg":"No scaling decisions to apply"}
{"level":"INFO","ts":"2025-11-27T22:38:13.702Z","msg":"Reconciliation completed successfully: mode=capacity-only, modelsProcessed=1, decisionsApplied=0"}
{"level":"INFO","ts":"2025-11-27T22:38:43.409Z","msg":"Operating in CAPACITY-ONLY mode: reactive capacity-based scaling only"}
{"level":"INFO","ts":"2025-11-27T22:38:43.409Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-11-27T22:38:43.409Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-11-27T22:38:43.430Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=2, accelerator=H100, ttft=57.73ms, itl=37.46ms, cost=20.00"}
{"level":"INFO","ts":"2025-11-27T22:38:43.432Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9gkhbr, queueLength=0"}
{"level":"INFO","ts":"2025-11-27T22:38:43.432Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9wj976, queueLength=0"}
{"level":"DEBUG","ts":"2025-11-27T22:38:43.432Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=2"}
{"level":"INFO","ts":"2025-11-27T22:38:43.432Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9gkhbr, usage=0.049 (4.9%)"}
{"level":"INFO","ts":"2025-11-27T22:38:43.432Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9wj976, usage=0.758 (75.8%)"}
{"level":"DEBUG","ts":"2025-11-27T22:38:43.432Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=2"}
{"level":"DEBUG","ts":"2025-11-27T22:38:43.432Z","msg":"Pod-to-variant matching successful: totalPods=2, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:2]"}
{"level":"DEBUG","ts":"2025-11-27T22:38:43.432Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=2"}
{"level":"DEBUG","ts":"2025-11-27T22:38:43.432Z","msg":"Collected capacity metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, metricsCount=2"}
{"level":"DEBUG","ts":"2025-11-27T22:38:43.432Z","msg":"Variant analysis initialized: variant=ms-inference-scheduling-llm-d-modelservice-decode, accelerator=H100, cost=10.00, replicaCount=2"}
{"level":"DEBUG","ts":"2025-11-27T22:38:43.432Z","msg":"Scale-down unsafe: insufficient non-saturated replicas: nonSaturated=1, required=2"}
{"level":"DEBUG","ts":"2025-11-27T22:38:43.432Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, totalReplicas=2, nonSaturated=1, avgSpareKv=0.451, avgSpareQueue=5.0, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"INFO","ts":"2025-11-27T22:38:43.432Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, totalReplicas=2, nonSaturated=1, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"DEBUG","ts":"2025-11-27T22:38:43.432Z","msg":"Capacity targets: no scaling needed (avgSpareKv=0.451, avgSpareQueue=5.0, all variants stable)"}
{"level":"DEBUG","ts":"2025-11-27T22:38:43.432Z","msg":"Capacity targets calculated: modelID=unsloth/Meta-Llama-3.1-8B, targets=map[ms-inference-scheduling-llm-d-modelservice-decode:2]"}
{"level":"INFO","ts":"2025-11-27T22:38:43.432Z","msg":"Capacity-only decisions made for model: unsloth/Meta-Llama-3.1-8B - decision count: 0"}
{"level":"INFO","ts":"2025-11-27T22:38:43.432Z","msg":"No scaling decisions to apply"}
{"level":"INFO","ts":"2025-11-27T22:38:43.432Z","msg":"Reconciliation completed successfully: mode=capacity-only, modelsProcessed=1, decisionsApplied=0"}
{"level":"INFO","ts":"2025-11-27T22:38:43.702Z","msg":"Operating in CAPACITY-ONLY mode: reactive capacity-based scaling only"}
{"level":"INFO","ts":"2025-11-27T22:38:43.702Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-11-27T22:38:43.702Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-11-27T22:38:43.731Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=2, accelerator=H100, ttft=57.73ms, itl=37.46ms, cost=20.00"}
{"level":"INFO","ts":"2025-11-27T22:38:43.733Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9gkhbr, queueLength=0"}
{"level":"INFO","ts":"2025-11-27T22:38:43.733Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9wj976, queueLength=0"}
{"level":"DEBUG","ts":"2025-11-27T22:38:43.733Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=2"}
{"level":"INFO","ts":"2025-11-27T22:38:43.733Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9gkhbr, usage=0.049 (4.9%)"}
{"level":"INFO","ts":"2025-11-27T22:38:43.733Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9wj976, usage=0.758 (75.8%)"}
{"level":"DEBUG","ts":"2025-11-27T22:38:43.733Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=2"}
{"level":"DEBUG","ts":"2025-11-27T22:38:43.733Z","msg":"Pod-to-variant matching successful: totalPods=2, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:2]"}
{"level":"DEBUG","ts":"2025-11-27T22:38:43.733Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=2"}
{"level":"DEBUG","ts":"2025-11-27T22:38:43.733Z","msg":"Collected capacity metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, metricsCount=2"}
{"level":"DEBUG","ts":"2025-11-27T22:38:43.733Z","msg":"Variant analysis initialized: variant=ms-inference-scheduling-llm-d-modelservice-decode, accelerator=H100, cost=10.00, replicaCount=2"}
{"level":"DEBUG","ts":"2025-11-27T22:38:43.733Z","msg":"Scale-down unsafe: insufficient non-saturated replicas: nonSaturated=1, required=2"}
{"level":"DEBUG","ts":"2025-11-27T22:38:43.733Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, totalReplicas=2, nonSaturated=1, avgSpareKv=0.451, avgSpareQueue=5.0, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"INFO","ts":"2025-11-27T22:38:43.733Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, totalReplicas=2, nonSaturated=1, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"DEBUG","ts":"2025-11-27T22:38:43.733Z","msg":"Capacity targets: no scaling needed (avgSpareKv=0.451, avgSpareQueue=5.0, all variants stable)"}
{"level":"DEBUG","ts":"2025-11-27T22:38:43.733Z","msg":"Capacity targets calculated: modelID=unsloth/Meta-Llama-3.1-8B, targets=map[ms-inference-scheduling-llm-d-modelservice-decode:2]"}
{"level":"INFO","ts":"2025-11-27T22:38:43.733Z","msg":"Capacity-only decisions made for model: unsloth/Meta-Llama-3.1-8B - decision count: 0"}
{"level":"INFO","ts":"2025-11-27T22:38:43.733Z","msg":"No scaling decisions to apply"}
{"level":"INFO","ts":"2025-11-27T22:38:43.733Z","msg":"Reconciliation completed successfully: mode=capacity-only, modelsProcessed=1, decisionsApplied=0"}
{"level":"INFO","ts":"2025-11-27T22:39:13.433Z","msg":"Operating in CAPACITY-ONLY mode: reactive capacity-based scaling only"}
{"level":"INFO","ts":"2025-11-27T22:39:13.433Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-11-27T22:39:13.433Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-11-27T22:39:13.459Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=2, accelerator=H100, ttft=28.36ms, itl=17.17ms, cost=20.00"}
{"level":"INFO","ts":"2025-11-27T22:39:13.461Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9gkhbr, queueLength=0"}
{"level":"INFO","ts":"2025-11-27T22:39:13.461Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9wj976, queueLength=0"}
{"level":"DEBUG","ts":"2025-11-27T22:39:13.461Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=2"}
{"level":"INFO","ts":"2025-11-27T22:39:13.461Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9gkhbr, usage=0.020 (2.0%)"}
{"level":"INFO","ts":"2025-11-27T22:39:13.461Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9wj976, usage=0.436 (43.6%)"}
{"level":"DEBUG","ts":"2025-11-27T22:39:13.461Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=2"}
{"level":"DEBUG","ts":"2025-11-27T22:39:13.461Z","msg":"Pod-to-variant matching successful: totalPods=2, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:2]"}
{"level":"DEBUG","ts":"2025-11-27T22:39:13.461Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=2"}
{"level":"DEBUG","ts":"2025-11-27T22:39:13.461Z","msg":"Collected capacity metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, metricsCount=2"}
{"level":"DEBUG","ts":"2025-11-27T22:39:13.461Z","msg":"Variant analysis initialized: variant=ms-inference-scheduling-llm-d-modelservice-decode, accelerator=H100, cost=10.00, replicaCount=2"}
{"level":"DEBUG","ts":"2025-11-27T22:39:13.461Z","msg":"Scale-down unsafe: insufficient headroom after redistribution: remainingSpareKv=0.044, kvTrigger=0.200, kvSafe=false, remainingSpareQueue=5.0, queueTrigger=3.0%!(EXTRA bool=true)"}
{"level":"DEBUG","ts":"2025-11-27T22:39:13.461Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, totalReplicas=2, nonSaturated=2, avgSpareKv=0.272, avgSpareQueue=5.0, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"INFO","ts":"2025-11-27T22:39:13.461Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, totalReplicas=2, nonSaturated=2, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"DEBUG","ts":"2025-11-27T22:39:13.461Z","msg":"Capacity targets: no scaling needed (avgSpareKv=0.272, avgSpareQueue=5.0, all variants stable)"}
{"level":"DEBUG","ts":"2025-11-27T22:39:13.461Z","msg":"Capacity targets calculated: modelID=unsloth/Meta-Llama-3.1-8B, targets=map[ms-inference-scheduling-llm-d-modelservice-decode:2]"}
{"level":"INFO","ts":"2025-11-27T22:39:13.461Z","msg":"Capacity-only decisions made for model: unsloth/Meta-Llama-3.1-8B - decision count: 0"}
{"level":"INFO","ts":"2025-11-27T22:39:13.461Z","msg":"No scaling decisions to apply"}
{"level":"INFO","ts":"2025-11-27T22:39:13.461Z","msg":"Reconciliation completed successfully: mode=capacity-only, modelsProcessed=1, decisionsApplied=0"}
{"level":"INFO","ts":"2025-11-27T22:39:13.734Z","msg":"Operating in CAPACITY-ONLY mode: reactive capacity-based scaling only"}
{"level":"INFO","ts":"2025-11-27T22:39:13.734Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-11-27T22:39:13.734Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-11-27T22:39:13.745Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=2, accelerator=H100, ttft=28.36ms, itl=17.17ms, cost=20.00"}
{"level":"INFO","ts":"2025-11-27T22:39:13.746Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9gkhbr, queueLength=0"}
{"level":"INFO","ts":"2025-11-27T22:39:13.746Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9wj976, queueLength=0"}
{"level":"DEBUG","ts":"2025-11-27T22:39:13.746Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=2"}
{"level":"INFO","ts":"2025-11-27T22:39:13.746Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9gkhbr, usage=0.020 (2.0%)"}
{"level":"INFO","ts":"2025-11-27T22:39:13.746Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9wj976, usage=0.436 (43.6%)"}
{"level":"DEBUG","ts":"2025-11-27T22:39:13.746Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=2"}
{"level":"DEBUG","ts":"2025-11-27T22:39:13.746Z","msg":"Pod-to-variant matching successful: totalPods=2, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:2]"}
{"level":"DEBUG","ts":"2025-11-27T22:39:13.746Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=2"}
{"level":"DEBUG","ts":"2025-11-27T22:39:13.746Z","msg":"Collected capacity metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, metricsCount=2"}
{"level":"DEBUG","ts":"2025-11-27T22:39:13.746Z","msg":"Variant analysis initialized: variant=ms-inference-scheduling-llm-d-modelservice-decode, accelerator=H100, cost=10.00, replicaCount=2"}
{"level":"DEBUG","ts":"2025-11-27T22:39:13.746Z","msg":"Scale-down unsafe: insufficient headroom after redistribution: remainingSpareKv=0.044, kvTrigger=0.200, kvSafe=false, remainingSpareQueue=5.0, queueTrigger=3.0%!(EXTRA bool=true)"}
{"level":"DEBUG","ts":"2025-11-27T22:39:13.746Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, totalReplicas=2, nonSaturated=2, avgSpareKv=0.272, avgSpareQueue=5.0, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"INFO","ts":"2025-11-27T22:39:13.746Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, totalReplicas=2, nonSaturated=2, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"DEBUG","ts":"2025-11-27T22:39:13.746Z","msg":"Capacity targets: no scaling needed (avgSpareKv=0.272, avgSpareQueue=5.0, all variants stable)"}
{"level":"DEBUG","ts":"2025-11-27T22:39:13.746Z","msg":"Capacity targets calculated: modelID=unsloth/Meta-Llama-3.1-8B, targets=map[ms-inference-scheduling-llm-d-modelservice-decode:2]"}
{"level":"INFO","ts":"2025-11-27T22:39:13.746Z","msg":"Capacity-only decisions made for model: unsloth/Meta-Llama-3.1-8B - decision count: 0"}
{"level":"INFO","ts":"2025-11-27T22:39:13.746Z","msg":"No scaling decisions to apply"}
{"level":"INFO","ts":"2025-11-27T22:39:13.746Z","msg":"Reconciliation completed successfully: mode=capacity-only, modelsProcessed=1, decisionsApplied=0"}
{"level":"INFO","ts":"2025-11-27T22:39:43.462Z","msg":"Operating in CAPACITY-ONLY mode: reactive capacity-based scaling only"}
{"level":"INFO","ts":"2025-11-27T22:39:43.462Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-11-27T22:39:43.462Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-11-27T22:39:43.475Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=2, accelerator=H100, ttft=21.25ms, itl=10.45ms, cost=20.00"}
{"level":"INFO","ts":"2025-11-27T22:39:43.476Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9gkhbr, queueLength=0"}
{"level":"INFO","ts":"2025-11-27T22:39:43.476Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9wj976, queueLength=0"}
{"level":"DEBUG","ts":"2025-11-27T22:39:43.476Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=2"}
{"level":"INFO","ts":"2025-11-27T22:39:43.476Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9gkhbr, usage=0.004 (0.4%)"}
{"level":"INFO","ts":"2025-11-27T22:39:43.476Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9wj976, usage=0.073 (7.3%)"}
{"level":"DEBUG","ts":"2025-11-27T22:39:43.476Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=2"}
{"level":"DEBUG","ts":"2025-11-27T22:39:43.477Z","msg":"Pod-to-variant matching successful: totalPods=2, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:2]"}
{"level":"DEBUG","ts":"2025-11-27T22:39:43.477Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=2"}
{"level":"DEBUG","ts":"2025-11-27T22:39:43.477Z","msg":"Collected capacity metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, metricsCount=2"}
{"level":"DEBUG","ts":"2025-11-27T22:39:43.477Z","msg":"Variant analysis initialized: variant=ms-inference-scheduling-llm-d-modelservice-decode, accelerator=H100, cost=10.00, replicaCount=2"}
{"level":"DEBUG","ts":"2025-11-27T22:39:43.477Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, totalReplicas=2, nonSaturated=2, avgSpareKv=0.462, avgSpareQueue=5.0, shouldScaleUp=false, scaleDownSafe=true"}
{"level":"INFO","ts":"2025-11-27T22:39:43.477Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, totalReplicas=2, nonSaturated=2, shouldScaleUp=false, scaleDownSafe=true"}
{"level":"INFO","ts":"2025-11-27T22:39:43.477Z","msg":"Capacity target: scale-down most expensive variant: variant=ms-inference-scheduling-llm-d-modelservice-decode, cost=10.00, currentReplicas=2, readyReplicas=2, target=1"}
{"level":"DEBUG","ts":"2025-11-27T22:39:43.477Z","msg":"Capacity targets calculated: modelID=unsloth/Meta-Llama-3.1-8B, targets=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"INFO","ts":"2025-11-27T22:39:43.477Z","msg":"Capacity-only decisions made for model: unsloth/Meta-Llama-3.1-8B - decision count: 1"}
{"level":"INFO","ts":"2025-11-27T22:39:43.477Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-11-27T22:39:43.477Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=scale-down, current=2target=1"}
{"level":"DEBUG","ts":"2025-11-27T22:39:43.477Z","msg":"Found VA in map: variant=ms-inference-scheduling-llm-d-modelservice-decode, hasCurrentAlloc=true, accelerator=H100"}
{"level":"INFO","ts":"2025-11-27T22:39:43.477Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 2, desired-replicas: 1, accelerator: H100"}
{"level":"INFO","ts":"2025-11-27T22:39:43.477Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=1, accelerator=H100, capacityOnly=true"}
{"level":"INFO","ts":"2025-11-27T22:39:43.482Z","msg":"Applied capacity decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=scale-down, current=2, target=1, reason=capacity-only mode: scale-down"}
{"level":"INFO","ts":"2025-11-27T22:39:43.482Z","msg":"Reconciliation completed successfully: mode=capacity-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-11-27T22:39:43.747Z","msg":"Operating in CAPACITY-ONLY mode: reactive capacity-based scaling only"}
{"level":"INFO","ts":"2025-11-27T22:39:43.747Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-11-27T22:39:43.747Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-11-27T22:39:43.758Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=2, accelerator=H100, ttft=21.25ms, itl=10.45ms, cost=20.00"}
{"level":"INFO","ts":"2025-11-27T22:39:43.760Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9gkhbr, queueLength=0"}
{"level":"INFO","ts":"2025-11-27T22:39:43.760Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9wj976, queueLength=0"}
{"level":"DEBUG","ts":"2025-11-27T22:39:43.760Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=2"}
{"level":"INFO","ts":"2025-11-27T22:39:43.760Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9gkhbr, usage=0.004 (0.4%)"}
{"level":"INFO","ts":"2025-11-27T22:39:43.760Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9wj976, usage=0.073 (7.3%)"}
{"level":"DEBUG","ts":"2025-11-27T22:39:43.760Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=2"}
{"level":"DEBUG","ts":"2025-11-27T22:39:43.760Z","msg":"Pod-to-variant matching successful: totalPods=2, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:2]"}
{"level":"DEBUG","ts":"2025-11-27T22:39:43.760Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=2"}
{"level":"DEBUG","ts":"2025-11-27T22:39:43.760Z","msg":"Collected capacity metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, metricsCount=2"}
{"level":"DEBUG","ts":"2025-11-27T22:39:43.760Z","msg":"Variant analysis initialized: variant=ms-inference-scheduling-llm-d-modelservice-decode, accelerator=H100, cost=10.00, replicaCount=2"}
{"level":"DEBUG","ts":"2025-11-27T22:39:43.760Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, totalReplicas=2, nonSaturated=2, avgSpareKv=0.462, avgSpareQueue=5.0, shouldScaleUp=false, scaleDownSafe=true"}
{"level":"INFO","ts":"2025-11-27T22:39:43.760Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, totalReplicas=2, nonSaturated=2, shouldScaleUp=false, scaleDownSafe=true"}
{"level":"DEBUG","ts":"2025-11-27T22:39:43.760Z","msg":"Preserving desired replicas: variant=ms-inference-scheduling-llm-d-modelservice-decode, currentReplicas=2, readyReplicas=2, desired=1"}
{"level":"DEBUG","ts":"2025-11-27T22:39:43.760Z","msg":"Capacity targets calculated: modelID=unsloth/Meta-Llama-3.1-8B, targets=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"INFO","ts":"2025-11-27T22:39:43.760Z","msg":"Capacity-only decisions made for model: unsloth/Meta-Llama-3.1-8B - decision count: 1"}
{"level":"INFO","ts":"2025-11-27T22:39:43.760Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-11-27T22:39:43.760Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=scale-down, current=2target=1"}
{"level":"DEBUG","ts":"2025-11-27T22:39:43.760Z","msg":"Found VA in map: variant=ms-inference-scheduling-llm-d-modelservice-decode, hasCurrentAlloc=true, accelerator=H100"}
{"level":"INFO","ts":"2025-11-27T22:39:43.760Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 2, desired-replicas: 1, accelerator: H100"}
{"level":"INFO","ts":"2025-11-27T22:39:43.760Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=1, accelerator=H100, capacityOnly=true"}
{"level":"INFO","ts":"2025-11-27T22:39:43.764Z","msg":"Applied capacity decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=scale-down, current=2, target=1, reason=capacity-only mode: scale-down"}
{"level":"INFO","ts":"2025-11-27T22:39:43.764Z","msg":"Reconciliation completed successfully: mode=capacity-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-11-27T22:40:13.483Z","msg":"Operating in CAPACITY-ONLY mode: reactive capacity-based scaling only"}
{"level":"INFO","ts":"2025-11-27T22:40:13.483Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-11-27T22:40:13.483Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-11-27T22:40:13.509Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=1, accelerator=H100, ttft=0.00ms, itl=9.49ms, cost=10.00"}
{"level":"INFO","ts":"2025-11-27T22:40:13.511Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9gkhbr, queueLength=0"}
{"level":"INFO","ts":"2025-11-27T22:40:13.511Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9wj976, queueLength=0"}
{"level":"DEBUG","ts":"2025-11-27T22:40:13.511Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=2"}
{"level":"INFO","ts":"2025-11-27T22:40:13.511Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9gkhbr, usage=0.000 (0.0%)"}
{"level":"INFO","ts":"2025-11-27T22:40:13.511Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9wj976, usage=0.068 (6.8%)"}
{"level":"DEBUG","ts":"2025-11-27T22:40:13.511Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=2"}
{"level":"DEBUG","ts":"2025-11-27T22:40:13.511Z","msg":"Pod-to-variant matching successful: totalPods=2, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:2]"}
{"level":"DEBUG","ts":"2025-11-27T22:40:13.511Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=2"}
{"level":"DEBUG","ts":"2025-11-27T22:40:13.511Z","msg":"Collected capacity metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, metricsCount=2"}
{"level":"DEBUG","ts":"2025-11-27T22:40:13.511Z","msg":"Variant analysis initialized: variant=ms-inference-scheduling-llm-d-modelservice-decode, accelerator=H100, cost=10.00, replicaCount=2"}
{"level":"DEBUG","ts":"2025-11-27T22:40:13.511Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, totalReplicas=2, nonSaturated=2, avgSpareKv=0.466, avgSpareQueue=5.0, shouldScaleUp=false, scaleDownSafe=true"}
{"level":"INFO","ts":"2025-11-27T22:40:13.511Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, totalReplicas=2, nonSaturated=2, shouldScaleUp=false, scaleDownSafe=true"}
{"level":"INFO","ts":"2025-11-27T22:40:13.511Z","msg":"Capacity target: scale-down most expensive variant: variant=ms-inference-scheduling-llm-d-modelservice-decode, cost=10.00, currentReplicas=1, readyReplicas=2, target=1"}
{"level":"DEBUG","ts":"2025-11-27T22:40:13.511Z","msg":"Capacity targets calculated: modelID=unsloth/Meta-Llama-3.1-8B, targets=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"INFO","ts":"2025-11-27T22:40:13.511Z","msg":"Capacity-only decisions made for model: unsloth/Meta-Llama-3.1-8B - decision count: 0"}
{"level":"INFO","ts":"2025-11-27T22:40:13.511Z","msg":"No scaling decisions to apply"}
{"level":"INFO","ts":"2025-11-27T22:40:13.511Z","msg":"Reconciliation completed successfully: mode=capacity-only, modelsProcessed=1, decisionsApplied=0"}
{"level":"INFO","ts":"2025-11-27T22:40:13.765Z","msg":"Operating in CAPACITY-ONLY mode: reactive capacity-based scaling only"}
{"level":"INFO","ts":"2025-11-27T22:40:13.765Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-11-27T22:40:13.765Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-11-27T22:40:13.777Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=1, accelerator=H100, ttft=0.00ms, itl=9.49ms, cost=10.00"}
{"level":"INFO","ts":"2025-11-27T22:40:13.778Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9gkhbr, queueLength=0"}
{"level":"INFO","ts":"2025-11-27T22:40:13.778Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9wj976, queueLength=0"}
{"level":"DEBUG","ts":"2025-11-27T22:40:13.778Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=2"}
{"level":"INFO","ts":"2025-11-27T22:40:13.778Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9gkhbr, usage=0.000 (0.0%)"}
{"level":"INFO","ts":"2025-11-27T22:40:13.778Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9wj976, usage=0.068 (6.8%)"}
{"level":"DEBUG","ts":"2025-11-27T22:40:13.778Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=2"}
{"level":"DEBUG","ts":"2025-11-27T22:40:13.778Z","msg":"Pod-to-variant matching successful: totalPods=2, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:2]"}
{"level":"DEBUG","ts":"2025-11-27T22:40:13.778Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=2"}
{"level":"DEBUG","ts":"2025-11-27T22:40:13.778Z","msg":"Collected capacity metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, metricsCount=2"}
{"level":"DEBUG","ts":"2025-11-27T22:40:13.778Z","msg":"Variant analysis initialized: variant=ms-inference-scheduling-llm-d-modelservice-decode, accelerator=H100, cost=10.00, replicaCount=2"}
{"level":"DEBUG","ts":"2025-11-27T22:40:13.778Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, totalReplicas=2, nonSaturated=2, avgSpareKv=0.466, avgSpareQueue=5.0, shouldScaleUp=false, scaleDownSafe=true"}
{"level":"INFO","ts":"2025-11-27T22:40:13.778Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, totalReplicas=2, nonSaturated=2, shouldScaleUp=false, scaleDownSafe=true"}
{"level":"INFO","ts":"2025-11-27T22:40:13.778Z","msg":"Capacity target: scale-down most expensive variant: variant=ms-inference-scheduling-llm-d-modelservice-decode, cost=10.00, currentReplicas=1, readyReplicas=2, target=1"}
{"level":"DEBUG","ts":"2025-11-27T22:40:13.778Z","msg":"Capacity targets calculated: modelID=unsloth/Meta-Llama-3.1-8B, targets=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"INFO","ts":"2025-11-27T22:40:13.778Z","msg":"Capacity-only decisions made for model: unsloth/Meta-Llama-3.1-8B - decision count: 0"}
{"level":"INFO","ts":"2025-11-27T22:40:13.778Z","msg":"No scaling decisions to apply"}
{"level":"INFO","ts":"2025-11-27T22:40:13.778Z","msg":"Reconciliation completed successfully: mode=capacity-only, modelsProcessed=1, decisionsApplied=0"}
{"level":"INFO","ts":"2025-11-27T22:40:43.512Z","msg":"Operating in CAPACITY-ONLY mode: reactive capacity-based scaling only"}
{"level":"INFO","ts":"2025-11-27T22:40:43.512Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-11-27T22:40:43.512Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-11-27T22:40:43.525Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=1, accelerator=H100, ttft=0.00ms, itl=0.00ms, cost=10.00"}
{"level":"INFO","ts":"2025-11-27T22:40:43.527Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9gkhbr, queueLength=0"}
{"level":"INFO","ts":"2025-11-27T22:40:43.527Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9wj976, queueLength=0"}
{"level":"DEBUG","ts":"2025-11-27T22:40:43.527Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=2"}
{"level":"INFO","ts":"2025-11-27T22:40:43.527Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9gkhbr, usage=0.000 (0.0%)"}
{"level":"INFO","ts":"2025-11-27T22:40:43.527Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9wj976, usage=0.000 (0.0%)"}
{"level":"DEBUG","ts":"2025-11-27T22:40:43.527Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=2"}
{"level":"DEBUG","ts":"2025-11-27T22:40:43.527Z","msg":"Pod-to-variant matching successful: totalPods=2, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:2]"}
{"level":"DEBUG","ts":"2025-11-27T22:40:43.527Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=2"}
{"level":"DEBUG","ts":"2025-11-27T22:40:43.527Z","msg":"Collected capacity metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, metricsCount=2"}
{"level":"DEBUG","ts":"2025-11-27T22:40:43.527Z","msg":"Variant analysis initialized: variant=ms-inference-scheduling-llm-d-modelservice-decode, accelerator=H100, cost=10.00, replicaCount=2"}
{"level":"DEBUG","ts":"2025-11-27T22:40:43.527Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, totalReplicas=2, nonSaturated=2, avgSpareKv=0.500, avgSpareQueue=5.0, shouldScaleUp=false, scaleDownSafe=true"}
{"level":"INFO","ts":"2025-11-27T22:40:43.527Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, totalReplicas=2, nonSaturated=2, shouldScaleUp=false, scaleDownSafe=true"}
{"level":"INFO","ts":"2025-11-27T22:40:43.527Z","msg":"Capacity target: scale-down most expensive variant: variant=ms-inference-scheduling-llm-d-modelservice-decode, cost=10.00, currentReplicas=1, readyReplicas=2, target=1"}
{"level":"DEBUG","ts":"2025-11-27T22:40:43.527Z","msg":"Capacity targets calculated: modelID=unsloth/Meta-Llama-3.1-8B, targets=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"INFO","ts":"2025-11-27T22:40:43.527Z","msg":"Capacity-only decisions made for model: unsloth/Meta-Llama-3.1-8B - decision count: 0"}
{"level":"INFO","ts":"2025-11-27T22:40:43.527Z","msg":"No scaling decisions to apply"}
{"level":"INFO","ts":"2025-11-27T22:40:43.527Z","msg":"Reconciliation completed successfully: mode=capacity-only, modelsProcessed=1, decisionsApplied=0"}
{"level":"INFO","ts":"2025-11-27T22:40:43.779Z","msg":"Operating in CAPACITY-ONLY mode: reactive capacity-based scaling only"}
{"level":"INFO","ts":"2025-11-27T22:40:43.779Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-11-27T22:40:43.779Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-11-27T22:40:43.790Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=1, accelerator=H100, ttft=0.00ms, itl=0.00ms, cost=10.00"}
{"level":"INFO","ts":"2025-11-27T22:40:43.792Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9gkhbr, queueLength=0"}
{"level":"INFO","ts":"2025-11-27T22:40:43.792Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9wj976, queueLength=0"}
{"level":"DEBUG","ts":"2025-11-27T22:40:43.792Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=2"}
{"level":"INFO","ts":"2025-11-27T22:40:43.792Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9gkhbr, usage=0.000 (0.0%)"}
{"level":"INFO","ts":"2025-11-27T22:40:43.792Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9wj976, usage=0.000 (0.0%)"}
{"level":"DEBUG","ts":"2025-11-27T22:40:43.792Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=2"}
{"level":"DEBUG","ts":"2025-11-27T22:40:43.792Z","msg":"Pod-to-variant matching successful: totalPods=2, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:2]"}
{"level":"DEBUG","ts":"2025-11-27T22:40:43.792Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=2"}
{"level":"DEBUG","ts":"2025-11-27T22:40:43.792Z","msg":"Collected capacity metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, metricsCount=2"}
{"level":"DEBUG","ts":"2025-11-27T22:40:43.792Z","msg":"Variant analysis initialized: variant=ms-inference-scheduling-llm-d-modelservice-decode, accelerator=H100, cost=10.00, replicaCount=2"}
{"level":"DEBUG","ts":"2025-11-27T22:40:43.792Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, totalReplicas=2, nonSaturated=2, avgSpareKv=0.500, avgSpareQueue=5.0, shouldScaleUp=false, scaleDownSafe=true"}
{"level":"INFO","ts":"2025-11-27T22:40:43.792Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, totalReplicas=2, nonSaturated=2, shouldScaleUp=false, scaleDownSafe=true"}
{"level":"INFO","ts":"2025-11-27T22:40:43.792Z","msg":"Capacity target: scale-down most expensive variant: variant=ms-inference-scheduling-llm-d-modelservice-decode, cost=10.00, currentReplicas=1, readyReplicas=2, target=1"}
{"level":"DEBUG","ts":"2025-11-27T22:40:43.792Z","msg":"Capacity targets calculated: modelID=unsloth/Meta-Llama-3.1-8B, targets=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"INFO","ts":"2025-11-27T22:40:43.792Z","msg":"Capacity-only decisions made for model: unsloth/Meta-Llama-3.1-8B - decision count: 0"}
{"level":"INFO","ts":"2025-11-27T22:40:43.792Z","msg":"No scaling decisions to apply"}
{"level":"INFO","ts":"2025-11-27T22:40:43.792Z","msg":"Reconciliation completed successfully: mode=capacity-only, modelsProcessed=1, decisionsApplied=0"}
{"level":"INFO","ts":"2025-11-27T22:41:13.528Z","msg":"Operating in CAPACITY-ONLY mode: reactive capacity-based scaling only"}
{"level":"INFO","ts":"2025-11-27T22:41:13.528Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-11-27T22:41:13.528Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-11-27T22:41:13.541Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=1, accelerator=H100, ttft=0.00ms, itl=0.00ms, cost=10.00"}
{"level":"INFO","ts":"2025-11-27T22:41:13.542Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9wj976, queueLength=0"}
{"level":"DEBUG","ts":"2025-11-27T22:41:13.542Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"INFO","ts":"2025-11-27T22:41:13.542Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9wj976, usage=0.000 (0.0%)"}
{"level":"DEBUG","ts":"2025-11-27T22:41:13.542Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"DEBUG","ts":"2025-11-27T22:41:13.542Z","msg":"Pod-to-variant matching successful: totalPods=1, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"DEBUG","ts":"2025-11-27T22:41:13.542Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=1"}
{"level":"DEBUG","ts":"2025-11-27T22:41:13.542Z","msg":"Collected capacity metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, metricsCount=1"}
{"level":"DEBUG","ts":"2025-11-27T22:41:13.542Z","msg":"Variant analysis initialized: variant=ms-inference-scheduling-llm-d-modelservice-decode, accelerator=H100, cost=10.00, replicaCount=1"}
{"level":"DEBUG","ts":"2025-11-27T22:41:13.542Z","msg":"Scale-down unsafe: insufficient non-saturated replicas: nonSaturated=1, required=2"}
{"level":"DEBUG","ts":"2025-11-27T22:41:13.542Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, totalReplicas=1, nonSaturated=1, avgSpareKv=0.500, avgSpareQueue=5.0, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"INFO","ts":"2025-11-27T22:41:13.542Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, totalReplicas=1, nonSaturated=1, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"DEBUG","ts":"2025-11-27T22:41:13.543Z","msg":"Capacity targets: no scaling needed (avgSpareKv=0.500, avgSpareQueue=5.0, all variants stable)"}
{"level":"DEBUG","ts":"2025-11-27T22:41:13.543Z","msg":"Capacity targets calculated: modelID=unsloth/Meta-Llama-3.1-8B, targets=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"INFO","ts":"2025-11-27T22:41:13.543Z","msg":"Capacity-only decisions made for model: unsloth/Meta-Llama-3.1-8B - decision count: 0"}
{"level":"INFO","ts":"2025-11-27T22:41:13.543Z","msg":"No scaling decisions to apply"}
{"level":"INFO","ts":"2025-11-27T22:41:13.543Z","msg":"Reconciliation completed successfully: mode=capacity-only, modelsProcessed=1, decisionsApplied=0"}
{"level":"INFO","ts":"2025-11-27T22:41:13.793Z","msg":"Operating in CAPACITY-ONLY mode: reactive capacity-based scaling only"}
{"level":"INFO","ts":"2025-11-27T22:41:13.793Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-11-27T22:41:13.793Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-11-27T22:41:13.804Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=1, accelerator=H100, ttft=0.00ms, itl=0.00ms, cost=10.00"}
{"level":"INFO","ts":"2025-11-27T22:41:13.807Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9wj976, queueLength=0"}
{"level":"DEBUG","ts":"2025-11-27T22:41:13.807Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"INFO","ts":"2025-11-27T22:41:13.807Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9wj976, usage=0.000 (0.0%)"}
{"level":"DEBUG","ts":"2025-11-27T22:41:13.807Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"DEBUG","ts":"2025-11-27T22:41:13.808Z","msg":"Pod-to-variant matching successful: totalPods=1, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"DEBUG","ts":"2025-11-27T22:41:13.808Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=1"}
{"level":"DEBUG","ts":"2025-11-27T22:41:13.808Z","msg":"Collected capacity metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, metricsCount=1"}
{"level":"DEBUG","ts":"2025-11-27T22:41:13.808Z","msg":"Variant analysis initialized: variant=ms-inference-scheduling-llm-d-modelservice-decode, accelerator=H100, cost=10.00, replicaCount=1"}
{"level":"DEBUG","ts":"2025-11-27T22:41:13.808Z","msg":"Scale-down unsafe: insufficient non-saturated replicas: nonSaturated=1, required=2"}
{"level":"DEBUG","ts":"2025-11-27T22:41:13.808Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, totalReplicas=1, nonSaturated=1, avgSpareKv=0.500, avgSpareQueue=5.0, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"INFO","ts":"2025-11-27T22:41:13.808Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, totalReplicas=1, nonSaturated=1, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"DEBUG","ts":"2025-11-27T22:41:13.808Z","msg":"Capacity targets: no scaling needed (avgSpareKv=0.500, avgSpareQueue=5.0, all variants stable)"}
{"level":"DEBUG","ts":"2025-11-27T22:41:13.808Z","msg":"Capacity targets calculated: modelID=unsloth/Meta-Llama-3.1-8B, targets=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"INFO","ts":"2025-11-27T22:41:13.808Z","msg":"Capacity-only decisions made for model: unsloth/Meta-Llama-3.1-8B - decision count: 0"}
{"level":"INFO","ts":"2025-11-27T22:41:13.808Z","msg":"No scaling decisions to apply"}
{"level":"INFO","ts":"2025-11-27T22:41:13.808Z","msg":"Reconciliation completed successfully: mode=capacity-only, modelsProcessed=1, decisionsApplied=0"}
{"level":"INFO","ts":"2025-11-27T22:41:43.543Z","msg":"Operating in CAPACITY-ONLY mode: reactive capacity-based scaling only"}
{"level":"INFO","ts":"2025-11-27T22:41:43.543Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-11-27T22:41:43.543Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-11-27T22:41:43.556Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=1, accelerator=H100, ttft=0.00ms, itl=0.00ms, cost=10.00"}
{"level":"INFO","ts":"2025-11-27T22:41:43.557Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9wj976, queueLength=0"}
{"level":"DEBUG","ts":"2025-11-27T22:41:43.557Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"INFO","ts":"2025-11-27T22:41:43.557Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9wj976, usage=0.000 (0.0%)"}
{"level":"DEBUG","ts":"2025-11-27T22:41:43.557Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"DEBUG","ts":"2025-11-27T22:41:43.558Z","msg":"Pod-to-variant matching successful: totalPods=1, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"DEBUG","ts":"2025-11-27T22:41:43.558Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=1"}
{"level":"DEBUG","ts":"2025-11-27T22:41:43.558Z","msg":"Collected capacity metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, metricsCount=1"}
{"level":"DEBUG","ts":"2025-11-27T22:41:43.558Z","msg":"Variant analysis initialized: variant=ms-inference-scheduling-llm-d-modelservice-decode, accelerator=H100, cost=10.00, replicaCount=1"}
{"level":"DEBUG","ts":"2025-11-27T22:41:43.558Z","msg":"Scale-down unsafe: insufficient non-saturated replicas: nonSaturated=1, required=2"}
{"level":"DEBUG","ts":"2025-11-27T22:41:43.558Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, totalReplicas=1, nonSaturated=1, avgSpareKv=0.500, avgSpareQueue=5.0, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"INFO","ts":"2025-11-27T22:41:43.558Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, totalReplicas=1, nonSaturated=1, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"DEBUG","ts":"2025-11-27T22:41:43.558Z","msg":"Capacity targets: no scaling needed (avgSpareKv=0.500, avgSpareQueue=5.0, all variants stable)"}
{"level":"DEBUG","ts":"2025-11-27T22:41:43.558Z","msg":"Capacity targets calculated: modelID=unsloth/Meta-Llama-3.1-8B, targets=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"INFO","ts":"2025-11-27T22:41:43.558Z","msg":"Capacity-only decisions made for model: unsloth/Meta-Llama-3.1-8B - decision count: 0"}
{"level":"INFO","ts":"2025-11-27T22:41:43.558Z","msg":"No scaling decisions to apply"}
{"level":"INFO","ts":"2025-11-27T22:41:43.558Z","msg":"Reconciliation completed successfully: mode=capacity-only, modelsProcessed=1, decisionsApplied=0"}
{"level":"INFO","ts":"2025-11-27T22:41:43.808Z","msg":"Operating in CAPACITY-ONLY mode: reactive capacity-based scaling only"}
{"level":"INFO","ts":"2025-11-27T22:41:43.808Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-11-27T22:41:43.808Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-11-27T22:41:43.819Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=1, accelerator=H100, ttft=0.00ms, itl=0.00ms, cost=10.00"}
{"level":"INFO","ts":"2025-11-27T22:41:43.820Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9wj976, queueLength=0"}
{"level":"DEBUG","ts":"2025-11-27T22:41:43.820Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"INFO","ts":"2025-11-27T22:41:43.820Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9wj976, usage=0.000 (0.0%)"}
{"level":"DEBUG","ts":"2025-11-27T22:41:43.820Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"DEBUG","ts":"2025-11-27T22:41:43.820Z","msg":"Pod-to-variant matching successful: totalPods=1, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"DEBUG","ts":"2025-11-27T22:41:43.820Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=1"}
{"level":"DEBUG","ts":"2025-11-27T22:41:43.820Z","msg":"Collected capacity metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, metricsCount=1"}
{"level":"DEBUG","ts":"2025-11-27T22:41:43.820Z","msg":"Variant analysis initialized: variant=ms-inference-scheduling-llm-d-modelservice-decode, accelerator=H100, cost=10.00, replicaCount=1"}
{"level":"DEBUG","ts":"2025-11-27T22:41:43.820Z","msg":"Scale-down unsafe: insufficient non-saturated replicas: nonSaturated=1, required=2"}
{"level":"DEBUG","ts":"2025-11-27T22:41:43.820Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, totalReplicas=1, nonSaturated=1, avgSpareKv=0.500, avgSpareQueue=5.0, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"INFO","ts":"2025-11-27T22:41:43.820Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, totalReplicas=1, nonSaturated=1, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"DEBUG","ts":"2025-11-27T22:41:43.820Z","msg":"Capacity targets: no scaling needed (avgSpareKv=0.500, avgSpareQueue=5.0, all variants stable)"}
{"level":"DEBUG","ts":"2025-11-27T22:41:43.820Z","msg":"Capacity targets calculated: modelID=unsloth/Meta-Llama-3.1-8B, targets=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"INFO","ts":"2025-11-27T22:41:43.820Z","msg":"Capacity-only decisions made for model: unsloth/Meta-Llama-3.1-8B - decision count: 0"}
{"level":"INFO","ts":"2025-11-27T22:41:43.820Z","msg":"No scaling decisions to apply"}
{"level":"INFO","ts":"2025-11-27T22:41:43.820Z","msg":"Reconciliation completed successfully: mode=capacity-only, modelsProcessed=1, decisionsApplied=0"}
{"level":"INFO","ts":"2025-11-27T22:42:13.558Z","msg":"Operating in CAPACITY-ONLY mode: reactive capacity-based scaling only"}
{"level":"INFO","ts":"2025-11-27T22:42:13.558Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-11-27T22:42:13.558Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-11-27T22:42:13.573Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=1, accelerator=H100, ttft=0.00ms, itl=0.00ms, cost=10.00"}
{"level":"INFO","ts":"2025-11-27T22:42:13.574Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9wj976, queueLength=0"}
{"level":"DEBUG","ts":"2025-11-27T22:42:13.574Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"INFO","ts":"2025-11-27T22:42:13.575Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9wj976, usage=0.000 (0.0%)"}
{"level":"DEBUG","ts":"2025-11-27T22:42:13.575Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"DEBUG","ts":"2025-11-27T22:42:13.575Z","msg":"Pod-to-variant matching successful: totalPods=1, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"DEBUG","ts":"2025-11-27T22:42:13.575Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=1"}
{"level":"DEBUG","ts":"2025-11-27T22:42:13.575Z","msg":"Collected capacity metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, metricsCount=1"}
{"level":"DEBUG","ts":"2025-11-27T22:42:13.575Z","msg":"Variant analysis initialized: variant=ms-inference-scheduling-llm-d-modelservice-decode, accelerator=H100, cost=10.00, replicaCount=1"}
{"level":"DEBUG","ts":"2025-11-27T22:42:13.575Z","msg":"Scale-down unsafe: insufficient non-saturated replicas: nonSaturated=1, required=2"}
{"level":"DEBUG","ts":"2025-11-27T22:42:13.575Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, totalReplicas=1, nonSaturated=1, avgSpareKv=0.500, avgSpareQueue=5.0, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"INFO","ts":"2025-11-27T22:42:13.575Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, totalReplicas=1, nonSaturated=1, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"DEBUG","ts":"2025-11-27T22:42:13.575Z","msg":"Capacity targets: no scaling needed (avgSpareKv=0.500, avgSpareQueue=5.0, all variants stable)"}
{"level":"DEBUG","ts":"2025-11-27T22:42:13.575Z","msg":"Capacity targets calculated: modelID=unsloth/Meta-Llama-3.1-8B, targets=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"INFO","ts":"2025-11-27T22:42:13.575Z","msg":"Capacity-only decisions made for model: unsloth/Meta-Llama-3.1-8B - decision count: 0"}
{"level":"INFO","ts":"2025-11-27T22:42:13.575Z","msg":"No scaling decisions to apply"}
{"level":"INFO","ts":"2025-11-27T22:42:13.575Z","msg":"Reconciliation completed successfully: mode=capacity-only, modelsProcessed=1, decisionsApplied=0"}
{"level":"INFO","ts":"2025-11-27T22:42:13.822Z","msg":"Operating in CAPACITY-ONLY mode: reactive capacity-based scaling only"}
{"level":"INFO","ts":"2025-11-27T22:42:13.822Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-11-27T22:42:13.822Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-11-27T22:42:13.833Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=1, accelerator=H100, ttft=0.00ms, itl=0.00ms, cost=10.00"}
{"level":"INFO","ts":"2025-11-27T22:42:13.834Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9wj976, queueLength=0"}
{"level":"DEBUG","ts":"2025-11-27T22:42:13.834Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"INFO","ts":"2025-11-27T22:42:13.834Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-789945f9wj976, usage=0.000 (0.0%)"}
{"level":"DEBUG","ts":"2025-11-27T22:42:13.834Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"DEBUG","ts":"2025-11-27T22:42:13.834Z","msg":"Pod-to-variant matching successful: totalPods=1, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"DEBUG","ts":"2025-11-27T22:42:13.834Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=1"}
{"level":"DEBUG","ts":"2025-11-27T22:42:13.834Z","msg":"Collected capacity metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, metricsCount=1"}
{"level":"DEBUG","ts":"2025-11-27T22:42:13.834Z","msg":"Variant analysis initialized: variant=ms-inference-scheduling-llm-d-modelservice-decode, accelerator=H100, cost=10.00, replicaCount=1"}
{"level":"DEBUG","ts":"2025-11-27T22:42:13.834Z","msg":"Scale-down unsafe: insufficient non-saturated replicas: nonSaturated=1, required=2"}
{"level":"DEBUG","ts":"2025-11-27T22:42:13.834Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, totalReplicas=1, nonSaturated=1, avgSpareKv=0.500, avgSpareQueue=5.0, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"INFO","ts":"2025-11-27T22:42:13.834Z","msg":"Capacity analysis completed: modelID=unsloth/Meta-Llama-3.1-8B, totalReplicas=1, nonSaturated=1, shouldScaleUp=false, scaleDownSafe=false"}
{"level":"DEBUG","ts":"2025-11-27T22:42:13.834Z","msg":"Capacity targets: no scaling needed (avgSpareKv=0.500, avgSpareQueue=5.0, all variants stable)"}
{"level":"DEBUG","ts":"2025-11-27T22:42:13.834Z","msg":"Capacity targets calculated: modelID=unsloth/Meta-Llama-3.1-8B, targets=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"INFO","ts":"2025-11-27T22:42:13.834Z","msg":"Capacity-only decisions made for model: unsloth/Meta-Llama-3.1-8B - decision count: 0"}
{"level":"INFO","ts":"2025-11-27T22:42:13.834Z","msg":"No scaling decisions to apply"}
{"level":"INFO","ts":"2025-11-27T22:42:13.834Z","msg":"Reconciliation completed successfully: mode=capacity-only, modelsProcessed=1, decisionsApplied=0"}
