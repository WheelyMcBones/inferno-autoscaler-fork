{"level":"INFO","ts":"2025-12-05T23:16:16.825Z","msg":"Zap logger initialized"}
{"level":"INFO","ts":"2025-12-05T23:16:16.827Z","msg":"Creating metrics emitter instance"}
{"level":"INFO","ts":"2025-12-05T23:16:16.827Z","msg":"Metrics emitter created successfully"}
{"level":"INFO","ts":"2025-12-05T23:16:16.827Z","msg":"Using Prometheus configuration from environment variables: address=https://thanos-querier.openshift-monitoring.svc.cluster.local:9091"}
{"level":"INFO","ts":"2025-12-05T23:16:16.828Z","msg":"Initializing Prometheus client -> address: https://thanos-querier.openshift-monitoring.svc.cluster.local:9091, tls_enabled: true"}
{"level":"INFO","ts":"2025-12-05T23:16:16.828Z","msg":"CA certificate loaded successfullypath/etc/ssl/certs/prometheus-ca.crt"}
{"level":"INFO","ts":"2025-12-05T23:16:16.828Z","msg":"TLS configuration applied to Prometheus HTTPS transport"}
{"level":"INFO","ts":"2025-12-05T23:16:16.828Z","msg":"Bearer token loaded from filepath/var/run/secrets/kubernetes.io/serviceaccount/token"}
{"level":"INFO","ts":"2025-12-05T23:16:16.872Z","msg":"Prometheus API validation successful with queryqueryup"}
{"level":"INFO","ts":"2025-12-05T23:16:16.873Z","msg":"Prometheus client and API wrapper initialized and validated successfully"}
{"level":"INFO","ts":"2025-12-05T23:16:16.873Z","msg":"Starting manager"}
{"level":"INFO","ts":"2025-12-05T23:16:16.873Z","msg":"Registering custom metrics with Prometheus registry"}
{"level":"info","ts":"2025-12-05T23:16:16Z","logger":"controller-runtime.metrics","msg":"Starting metrics server"}
{"level":"INFO","ts":"2025-12-05T23:16:16.873Z","msg":"disabling http/2"}
{"level":"info","ts":"2025-12-05T23:16:16Z","msg":"starting server","name":"health probe","addr":"[::]:8081"}
I1205 23:16:16.873522       1 leaderelection.go:257] attempting to acquire leader lease workload-variant-autoscaler-system/72dd1cf1.llm-d.ai...
I1205 23:16:16.916768       1 leaderelection.go:271] successfully acquired lease workload-variant-autoscaler-system/72dd1cf1.llm-d.ai
{"level":"info","ts":"2025-12-05T23:16:16Z","msg":"Starting EventSource","controller":"variantAutoscaling","controllerGroup":"llmd.ai","controllerKind":"VariantAutoscaling","source":"kind source: *v1alpha1.VariantAutoscaling"}
{"level":"info","ts":"2025-12-05T23:16:16Z","msg":"Starting EventSource","controller":"variantAutoscaling","controllerGroup":"llmd.ai","controllerKind":"VariantAutoscaling","source":"kind source: *v1.ServiceMonitor"}
{"level":"info","ts":"2025-12-05T23:16:16Z","msg":"Starting EventSource","controller":"variantAutoscaling","controllerGroup":"llmd.ai","controllerKind":"VariantAutoscaling","source":"kind source: *v1.ConfigMap"}
{"level":"info","ts":"2025-12-05T23:16:17Z","logger":"controller-runtime.metrics","msg":"Serving metrics server","bindAddress":":8443","secure":true}
{"level":"info","ts":"2025-12-05T23:16:18Z","msg":"Starting Controller","controller":"variantAutoscaling","controllerGroup":"llmd.ai","controllerKind":"VariantAutoscaling"}
{"level":"info","ts":"2025-12-05T23:16:18Z","msg":"Starting workers","controller":"variantAutoscaling","controllerGroup":"llmd.ai","controllerKind":"VariantAutoscaling","worker count":1}
{"level":"INFO","ts":"2025-12-05T23:16:18.313Z","msg":"saturation scaling config cache updated: entries=1, has_default=true"}
{"level":"INFO","ts":"2025-12-05T23:16:18.313Z","msg":"saturation scaling configuration loaded successfully"}
{"level":"INFO","ts":"2025-12-05T23:16:18.313Z","msg":"Operating in MODEL-ONLY mode: model-based optimization only"}
{"level":"INFO","ts":"2025-12-05T23:16:18.313Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-12-05T23:16:18.313Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"DEBUG","ts":"2025-12-05T23:16:18.414Z","msg":"Could not get deployment for VA in model-only metrics logging: variant=ms-inference-scheduling-llm-d-modelservice-decode, error=Deployment.apps \"ms-inference-scheduling-llm-d-modelservice-decode\" not found"}
{"level":"DEBUG","ts":"2025-12-05T23:16:18.414Z","msg":"Failed to collect capacity metrics for logging in model-only mode: modelID=unsloth/Meta-Llama-3.1-8B, error=no deployments found for model: unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-05T23:16:18.414Z","msg":"Found SLO for model: model=unsloth/Meta-Llama-3.1-8B, class=Premium, slo-tpot=10, slo-ttft=1000"}
{"level":"ERROR","ts":"2025-12-05T23:16:18.414Z","msg":"failed to get Deployment after retries: variantAutoscaling-name=ms-inference-scheduling-llm-d-modelservice-decode, error=Deployment.apps \"ms-inference-scheduling-llm-d-modelservice-decode\" not found"}
{"level":"DEBUG","ts":"2025-12-05T23:16:18.414Z","msg":"Experimental model tuner is enabled globally (EXPERIMENTAL_MODEL_TUNER_ENABLED=true) tuning model performance parameters for active VAs"}
{"level":"ERROR","ts":"2025-12-05T23:16:18.414Z","msg":"Model-based optimization failed: no feasible allocations found for all variants: "}
{"level":"WARN","ts":"2025-12-05T23:16:18.414Z","msg":"Both Saturation and model-based failed, activating safety net: modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"WARN","ts":"2025-12-05T23:16:18.414Z","msg":"Safety net: failed to get current replicas, using VA status: variant=ms-inference-scheduling-llm-d-modelservice-decode, error=failed to get Deployment llm-d-inference-scheduler/ms-inference-scheduling-llm-d-modelservice-decode: Deployment.apps \"ms-inference-scheduling-llm-d-modelservice-decode\" not found"}
{"level":"WARN","ts":"2025-12-05T23:16:18.414Z","msg":"Safety net: failed to get current replicas for metrics: variant=ms-inference-scheduling-llm-d-modelservice-decode, error=failed to get Deployment llm-d-inference-scheduler/ms-inference-scheduling-llm-d-modelservice-decode: Deployment.apps \"ms-inference-scheduling-llm-d-modelservice-decode\" not found"}
{"level":"INFO","ts":"2025-12-05T23:16:18.414Z","msg":"Safety net activated: emitted fallback metrics: variant=ms-inference-scheduling-llm-d-modelservice-decode, currentReplicas=0, desiredReplicas=0, accelerator=H100, fallbackSource=current-replicas"}
{"level":"INFO","ts":"2025-12-05T23:16:18.414Z","msg":"No scaling decisions to apply"}
{"level":"WARN","ts":"2025-12-05T23:16:18.414Z","msg":"Reconciliation completed with errors: mode=model-only, modelsProcessed=1, modelsFailed=1, decisionsApplied=0"}
{"level":"INFO","ts":"2025-12-05T23:16:18.414Z","msg":"saturation scaling config cache updated: entries=1, has_default=true"}
{"level":"INFO","ts":"2025-12-05T23:16:18.414Z","msg":"saturation scaling configuration loaded successfully"}
{"level":"INFO","ts":"2025-12-05T23:16:18.414Z","msg":"Operating in MODEL-ONLY mode: model-based optimization only"}
{"level":"INFO","ts":"2025-12-05T23:16:18.414Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-12-05T23:16:18.414Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"error","ts":"2025-12-05T23:16:21Z","logger":"controller-runtime.metrics","msg":"Authentication failed","path":"/metrics","error":"[invalid bearer token, Token does not match server's copy, token lookup failed]","errorCauses":[{"error":"[invalid bearer token, Token does not match server's copy, token lookup failed]"}],"stacktrace":"sigs.k8s.io/controller-runtime/pkg/metrics/filters.WithAuthenticationAndAuthorization.func1.1\n\t/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.20.4/pkg/metrics/filters/filters.go:89\nnet/http.HandlerFunc.ServeHTTP\n\t/usr/local/go/src/net/http/server.go:2294\nnet/http.(*ServeMux).ServeHTTP\n\t/usr/local/go/src/net/http/server.go:2822\nnet/http.serverHandler.ServeHTTP\n\t/usr/local/go/src/net/http/server.go:3301\nnet/http.(*conn).serve\n\t/usr/local/go/src/net/http/server.go:2102"}
{"level":"error","ts":"2025-12-05T23:16:31Z","logger":"controller-runtime.metrics","msg":"Authentication failed","path":"/metrics","error":"[invalid bearer token, Token does not match server's copy, token lookup failed]","errorCauses":[{"error":"[invalid bearer token, Token does not match server's copy, token lookup failed]"}],"stacktrace":"sigs.k8s.io/controller-runtime/pkg/metrics/filters.WithAuthenticationAndAuthorization.func1.1\n\t/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.20.4/pkg/metrics/filters/filters.go:89\nnet/http.HandlerFunc.ServeHTTP\n\t/usr/local/go/src/net/http/server.go:2294\nnet/http.(*ServeMux).ServeHTTP\n\t/usr/local/go/src/net/http/server.go:2822\nnet/http.serverHandler.ServeHTTP\n\t/usr/local/go/src/net/http/server.go:3301\nnet/http.(*conn).serve\n\t/usr/local/go/src/net/http/server.go:2102"}
{"level":"error","ts":"2025-12-05T23:16:41Z","logger":"controller-runtime.metrics","msg":"Authentication failed","path":"/metrics","error":"[invalid bearer token, Token does not match server's copy, token lookup failed]","errorCauses":[{"error":"[invalid bearer token, Token does not match server's copy, token lookup failed]"}],"stacktrace":"sigs.k8s.io/controller-runtime/pkg/metrics/filters.WithAuthenticationAndAuthorization.func1.1\n\t/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.20.4/pkg/metrics/filters/filters.go:89\nnet/http.HandlerFunc.ServeHTTP\n\t/usr/local/go/src/net/http/server.go:2294\nnet/http.(*ServeMux).ServeHTTP\n\t/usr/local/go/src/net/http/server.go:2822\nnet/http.serverHandler.ServeHTTP\n\t/usr/local/go/src/net/http/server.go:3301\nnet/http.(*conn).serve\n\t/usr/local/go/src/net/http/server.go:2102"}
{"level":"error","ts":"2025-12-05T23:16:51Z","logger":"controller-runtime.metrics","msg":"Authentication failed","path":"/metrics","error":"[invalid bearer token, Token does not match server's copy, token lookup failed]","errorCauses":[{"error":"[invalid bearer token, Token does not match server's copy, token lookup failed]"}],"stacktrace":"sigs.k8s.io/controller-runtime/pkg/metrics/filters.WithAuthenticationAndAuthorization.func1.1\n\t/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.20.4/pkg/metrics/filters/filters.go:89\nnet/http.HandlerFunc.ServeHTTP\n\t/usr/local/go/src/net/http/server.go:2294\nnet/http.(*ServeMux).ServeHTTP\n\t/usr/local/go/src/net/http/server.go:2822\nnet/http.serverHandler.ServeHTTP\n\t/usr/local/go/src/net/http/server.go:3301\nnet/http.(*conn).serve\n\t/usr/local/go/src/net/http/server.go:2102"}
{"level":"error","ts":"2025-12-05T23:17:01Z","logger":"controller-runtime.metrics","msg":"Authentication failed","path":"/metrics","error":"[invalid bearer token, Token does not match server's copy, token lookup failed]","errorCauses":[{"error":"[invalid bearer token, Token does not match server's copy, token lookup failed]"}],"stacktrace":"sigs.k8s.io/controller-runtime/pkg/metrics/filters.WithAuthenticationAndAuthorization.func1.1\n\t/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.20.4/pkg/metrics/filters/filters.go:89\nnet/http.HandlerFunc.ServeHTTP\n\t/usr/local/go/src/net/http/server.go:2294\nnet/http.(*ServeMux).ServeHTTP\n\t/usr/local/go/src/net/http/server.go:2822\nnet/http.serverHandler.ServeHTTP\n\t/usr/local/go/src/net/http/server.go:3301\nnet/http.(*conn).serve\n\t/usr/local/go/src/net/http/server.go:2102"}
{"level":"error","ts":"2025-12-05T23:17:11Z","logger":"controller-runtime.metrics","msg":"Authentication failed","path":"/metrics","error":"[invalid bearer token, Token does not match server's copy, token lookup failed]","errorCauses":[{"error":"[invalid bearer token, Token does not match server's copy, token lookup failed]"}],"stacktrace":"sigs.k8s.io/controller-runtime/pkg/metrics/filters.WithAuthenticationAndAuthorization.func1.1\n\t/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.20.4/pkg/metrics/filters/filters.go:89\nnet/http.HandlerFunc.ServeHTTP\n\t/usr/local/go/src/net/http/server.go:2294\nnet/http.(*ServeMux).ServeHTTP\n\t/usr/local/go/src/net/http/server.go:2822\nnet/http.serverHandler.ServeHTTP\n\t/usr/local/go/src/net/http/server.go:3301\nnet/http.(*conn).serve\n\t/usr/local/go/src/net/http/server.go:2102"}
{"level":"INFO","ts":"2025-12-05T23:17:18.414Z","msg":"saturation scaling config cache updated: entries=1, has_default=true"}
{"level":"INFO","ts":"2025-12-05T23:17:18.414Z","msg":"saturation scaling configuration loaded successfully"}
{"level":"INFO","ts":"2025-12-05T23:17:18.414Z","msg":"Operating in MODEL-ONLY mode: model-based optimization only"}
{"level":"INFO","ts":"2025-12-05T23:17:18.414Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-12-05T23:17:18.414Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"DEBUG","ts":"2025-12-05T23:17:18.418Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=0"}
{"level":"DEBUG","ts":"2025-12-05T23:17:18.418Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=0"}
{"level":"DEBUG","ts":"2025-12-05T23:17:18.420Z","msg":"Pod-to-variant matching successful: totalPods=0, variantCounts=map[]"}
{"level":"DEBUG","ts":"2025-12-05T23:17:18.420Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=0"}
{"level":"DEBUG","ts":"2025-12-05T23:17:18.420Z","msg":"Collected capacity metrics for observability in model-only mode: modelID=unsloth/Meta-Llama-3.1-8B, replicaCount=0"}
{"level":"INFO","ts":"2025-12-05T23:17:18.420Z","msg":"Found SLO for model: model=unsloth/Meta-Llama-3.1-8B, class=Premium, slo-tpot=10, slo-ttft=1000"}
{"level":"INFO","ts":"2025-12-05T23:17:18.424Z","msg":"Set ownerReference on VariantAutoscaling: variantAutoscaling-name=ms-inference-scheduling-llm-d-modelservice-decode, owner=ms-inference-scheduling-llm-d-modelservice-decode"}
{"level":"WARN","ts":"2025-12-05T23:17:18.427Z","msg":"Metrics unavailable, skipping optimization for variant: variant=ms-inference-scheduling-llm-d-modelservice-decode, namespace=llm-d-inference-scheduler, model=unsloth/Meta-Llama-3.1-8B, reason=MetricsMissing, troubleshooting=No vLLM metrics found for model 'unsloth/Meta-Llama-3.1-8B' in namespace 'llm-d-inference-scheduler'. Check: (1) ServiceMonitor exists in monitoring namespace, (2) ServiceMonitor selector matches vLLM service labels, (3) vLLM pods are running and exposing /metrics endpoint, (4) Prometheus is scraping the monitoring namespace"}
{"level":"DEBUG","ts":"2025-12-05T23:17:18.427Z","msg":"Experimental model tuner is enabled globally (EXPERIMENTAL_MODEL_TUNER_ENABLED=true) tuning model performance parameters for active VAs"}
{"level":"ERROR","ts":"2025-12-05T23:17:18.427Z","msg":"Model-based optimization failed: no feasible allocations found for all variants: "}
{"level":"WARN","ts":"2025-12-05T23:17:18.427Z","msg":"Both Saturation and model-based failed, activating safety net: modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-05T23:17:18.427Z","msg":"Safety net activated: emitted fallback metrics: variant=ms-inference-scheduling-llm-d-modelservice-decode, currentReplicas=2, desiredReplicas=2, accelerator=H100, fallbackSource=current-replicas"}
{"level":"INFO","ts":"2025-12-05T23:17:18.427Z","msg":"No scaling decisions to apply"}
{"level":"WARN","ts":"2025-12-05T23:17:18.427Z","msg":"Reconciliation completed with errors: mode=model-only, modelsProcessed=1, modelsFailed=1, decisionsApplied=0"}
{"level":"INFO","ts":"2025-12-05T23:17:18.428Z","msg":"saturation scaling config cache updated: entries=1, has_default=true"}
{"level":"INFO","ts":"2025-12-05T23:17:18.428Z","msg":"saturation scaling configuration loaded successfully"}
{"level":"INFO","ts":"2025-12-05T23:17:18.428Z","msg":"Operating in MODEL-ONLY mode: model-based optimization only"}
{"level":"INFO","ts":"2025-12-05T23:17:18.428Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-12-05T23:17:18.428Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"DEBUG","ts":"2025-12-05T23:17:18.429Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=0"}
{"level":"DEBUG","ts":"2025-12-05T23:17:18.429Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=0"}
{"level":"DEBUG","ts":"2025-12-05T23:17:18.431Z","msg":"Pod-to-variant matching successful: totalPods=0, variantCounts=map[]"}
{"level":"DEBUG","ts":"2025-12-05T23:17:18.431Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=0"}
{"level":"DEBUG","ts":"2025-12-05T23:17:18.431Z","msg":"Collected capacity metrics for observability in model-only mode: modelID=unsloth/Meta-Llama-3.1-8B, replicaCount=0"}
{"level":"INFO","ts":"2025-12-05T23:17:18.432Z","msg":"Found SLO for model: model=unsloth/Meta-Llama-3.1-8B, class=Premium, slo-tpot=10, slo-ttft=1000"}
{"level":"WARN","ts":"2025-12-05T23:17:18.434Z","msg":"Metrics unavailable, skipping optimization for variant: variant=ms-inference-scheduling-llm-d-modelservice-decode, namespace=llm-d-inference-scheduler, model=unsloth/Meta-Llama-3.1-8B, reason=MetricsMissing, troubleshooting=No vLLM metrics found for model 'unsloth/Meta-Llama-3.1-8B' in namespace 'llm-d-inference-scheduler'. Check: (1) ServiceMonitor exists in monitoring namespace, (2) ServiceMonitor selector matches vLLM service labels, (3) vLLM pods are running and exposing /metrics endpoint, (4) Prometheus is scraping the monitoring namespace"}
{"level":"DEBUG","ts":"2025-12-05T23:17:18.434Z","msg":"Experimental model tuner is enabled globally (EXPERIMENTAL_MODEL_TUNER_ENABLED=true) tuning model performance parameters for active VAs"}
{"level":"ERROR","ts":"2025-12-05T23:17:18.434Z","msg":"Model-based optimization failed: no feasible allocations found for all variants: "}
{"level":"WARN","ts":"2025-12-05T23:17:18.434Z","msg":"Both Saturation and model-based failed, activating safety net: modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-05T23:17:18.434Z","msg":"Safety net activated: emitted fallback metrics: variant=ms-inference-scheduling-llm-d-modelservice-decode, currentReplicas=2, desiredReplicas=2, accelerator=H100, fallbackSource=current-replicas"}
{"level":"INFO","ts":"2025-12-05T23:17:18.434Z","msg":"No scaling decisions to apply"}
{"level":"WARN","ts":"2025-12-05T23:17:18.434Z","msg":"Reconciliation completed with errors: mode=model-only, modelsProcessed=1, modelsFailed=1, decisionsApplied=0"}
{"level":"error","ts":"2025-12-05T23:17:21Z","logger":"controller-runtime.metrics","msg":"Authentication failed","path":"/metrics","error":"[invalid bearer token, Token does not match server's copy, token lookup failed]","errorCauses":[{"error":"[invalid bearer token, Token does not match server's copy, token lookup failed]"}],"stacktrace":"sigs.k8s.io/controller-runtime/pkg/metrics/filters.WithAuthenticationAndAuthorization.func1.1\n\t/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.20.4/pkg/metrics/filters/filters.go:89\nnet/http.HandlerFunc.ServeHTTP\n\t/usr/local/go/src/net/http/server.go:2294\nnet/http.(*ServeMux).ServeHTTP\n\t/usr/local/go/src/net/http/server.go:2822\nnet/http.serverHandler.ServeHTTP\n\t/usr/local/go/src/net/http/server.go:3301\nnet/http.(*conn).serve\n\t/usr/local/go/src/net/http/server.go:2102"}
{"level":"error","ts":"2025-12-05T23:17:31Z","logger":"controller-runtime.metrics","msg":"Authentication failed","path":"/metrics","error":"[invalid bearer token, Token does not match server's copy, token lookup failed]","errorCauses":[{"error":"[invalid bearer token, Token does not match server's copy, token lookup failed]"}],"stacktrace":"sigs.k8s.io/controller-runtime/pkg/metrics/filters.WithAuthenticationAndAuthorization.func1.1\n\t/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.20.4/pkg/metrics/filters/filters.go:89\nnet/http.HandlerFunc.ServeHTTP\n\t/usr/local/go/src/net/http/server.go:2294\nnet/http.(*ServeMux).ServeHTTP\n\t/usr/local/go/src/net/http/server.go:2822\nnet/http.serverHandler.ServeHTTP\n\t/usr/local/go/src/net/http/server.go:3301\nnet/http.(*conn).serve\n\t/usr/local/go/src/net/http/server.go:2102"}
{"level":"error","ts":"2025-12-05T23:17:41Z","logger":"controller-runtime.metrics","msg":"Authentication failed","path":"/metrics","error":"[invalid bearer token, Token does not match server's copy, token lookup failed]","errorCauses":[{"error":"[invalid bearer token, Token does not match server's copy, token lookup failed]"}],"stacktrace":"sigs.k8s.io/controller-runtime/pkg/metrics/filters.WithAuthenticationAndAuthorization.func1.1\n\t/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.20.4/pkg/metrics/filters/filters.go:89\nnet/http.HandlerFunc.ServeHTTP\n\t/usr/local/go/src/net/http/server.go:2294\nnet/http.(*ServeMux).ServeHTTP\n\t/usr/local/go/src/net/http/server.go:2822\nnet/http.serverHandler.ServeHTTP\n\t/usr/local/go/src/net/http/server.go:3301\nnet/http.(*conn).serve\n\t/usr/local/go/src/net/http/server.go:2102"}
{"level":"error","ts":"2025-12-05T23:17:51Z","logger":"controller-runtime.metrics","msg":"Authentication failed","path":"/metrics","error":"[invalid bearer token, Token does not match server's copy, token lookup failed]","errorCauses":[{"error":"[invalid bearer token, Token does not match server's copy, token lookup failed]"}],"stacktrace":"sigs.k8s.io/controller-runtime/pkg/metrics/filters.WithAuthenticationAndAuthorization.func1.1\n\t/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.20.4/pkg/metrics/filters/filters.go:89\nnet/http.HandlerFunc.ServeHTTP\n\t/usr/local/go/src/net/http/server.go:2294\nnet/http.(*ServeMux).ServeHTTP\n\t/usr/local/go/src/net/http/server.go:2822\nnet/http.serverHandler.ServeHTTP\n\t/usr/local/go/src/net/http/server.go:3301\nnet/http.(*conn).serve\n\t/usr/local/go/src/net/http/server.go:2102"}
{"level":"error","ts":"2025-12-05T23:18:01Z","logger":"controller-runtime.metrics","msg":"Authentication failed","path":"/metrics","error":"[invalid bearer token, Token does not match server's copy, token lookup failed]","errorCauses":[{"error":"[invalid bearer token, Token does not match server's copy, token lookup failed]"}],"stacktrace":"sigs.k8s.io/controller-runtime/pkg/metrics/filters.WithAuthenticationAndAuthorization.func1.1\n\t/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.20.4/pkg/metrics/filters/filters.go:89\nnet/http.HandlerFunc.ServeHTTP\n\t/usr/local/go/src/net/http/server.go:2294\nnet/http.(*ServeMux).ServeHTTP\n\t/usr/local/go/src/net/http/server.go:2822\nnet/http.serverHandler.ServeHTTP\n\t/usr/local/go/src/net/http/server.go:3301\nnet/http.(*conn).serve\n\t/usr/local/go/src/net/http/server.go:2102"}
{"level":"error","ts":"2025-12-05T23:18:11Z","logger":"controller-runtime.metrics","msg":"Authentication failed","path":"/metrics","error":"[invalid bearer token, Token does not match server's copy, token lookup failed]","errorCauses":[{"error":"[invalid bearer token, Token does not match server's copy, token lookup failed]"}],"stacktrace":"sigs.k8s.io/controller-runtime/pkg/metrics/filters.WithAuthenticationAndAuthorization.func1.1\n\t/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.20.4/pkg/metrics/filters/filters.go:89\nnet/http.HandlerFunc.ServeHTTP\n\t/usr/local/go/src/net/http/server.go:2294\nnet/http.(*ServeMux).ServeHTTP\n\t/usr/local/go/src/net/http/server.go:2822\nnet/http.serverHandler.ServeHTTP\n\t/usr/local/go/src/net/http/server.go:3301\nnet/http.(*conn).serve\n\t/usr/local/go/src/net/http/server.go:2102"}
{"level":"INFO","ts":"2025-12-05T23:18:18.428Z","msg":"saturation scaling config cache updated: entries=1, has_default=true"}
{"level":"INFO","ts":"2025-12-05T23:18:18.428Z","msg":"saturation scaling configuration loaded successfully"}
{"level":"INFO","ts":"2025-12-05T23:18:18.428Z","msg":"Operating in MODEL-ONLY mode: model-based optimization only"}
{"level":"INFO","ts":"2025-12-05T23:18:18.428Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-12-05T23:18:18.428Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"DEBUG","ts":"2025-12-05T23:18:18.436Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=0"}
{"level":"DEBUG","ts":"2025-12-05T23:18:18.437Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=0"}
{"level":"DEBUG","ts":"2025-12-05T23:18:18.439Z","msg":"Pod-to-variant matching successful: totalPods=0, variantCounts=map[]"}
{"level":"DEBUG","ts":"2025-12-05T23:18:18.439Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=0"}
{"level":"DEBUG","ts":"2025-12-05T23:18:18.439Z","msg":"Collected capacity metrics for observability in model-only mode: modelID=unsloth/Meta-Llama-3.1-8B, replicaCount=0"}
{"level":"INFO","ts":"2025-12-05T23:18:18.439Z","msg":"Found SLO for model: model=unsloth/Meta-Llama-3.1-8B, class=Premium, slo-tpot=10, slo-ttft=1000"}
{"level":"WARN","ts":"2025-12-05T23:18:18.442Z","msg":"Metrics unavailable, skipping optimization for variant: variant=ms-inference-scheduling-llm-d-modelservice-decode, namespace=llm-d-inference-scheduler, model=unsloth/Meta-Llama-3.1-8B, reason=MetricsMissing, troubleshooting=No vLLM metrics found for model 'unsloth/Meta-Llama-3.1-8B' in namespace 'llm-d-inference-scheduler'. Check: (1) ServiceMonitor exists in monitoring namespace, (2) ServiceMonitor selector matches vLLM service labels, (3) vLLM pods are running and exposing /metrics endpoint, (4) Prometheus is scraping the monitoring namespace"}
{"level":"DEBUG","ts":"2025-12-05T23:18:18.442Z","msg":"Experimental model tuner is enabled globally (EXPERIMENTAL_MODEL_TUNER_ENABLED=true) tuning model performance parameters for active VAs"}
{"level":"ERROR","ts":"2025-12-05T23:18:18.442Z","msg":"Model-based optimization failed: no feasible allocations found for all variants: "}
{"level":"WARN","ts":"2025-12-05T23:18:18.442Z","msg":"Both Saturation and model-based failed, activating safety net: modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-05T23:18:18.442Z","msg":"Safety net activated: emitted fallback metrics: variant=ms-inference-scheduling-llm-d-modelservice-decode, currentReplicas=2, desiredReplicas=2, accelerator=H100, fallbackSource=current-replicas"}
{"level":"INFO","ts":"2025-12-05T23:18:18.442Z","msg":"No scaling decisions to apply"}
{"level":"WARN","ts":"2025-12-05T23:18:18.442Z","msg":"Reconciliation completed with errors: mode=model-only, modelsProcessed=1, modelsFailed=1, decisionsApplied=0"}
{"level":"INFO","ts":"2025-12-05T23:18:18.442Z","msg":"saturation scaling config cache updated: entries=1, has_default=true"}
{"level":"INFO","ts":"2025-12-05T23:18:18.442Z","msg":"saturation scaling configuration loaded successfully"}
{"level":"INFO","ts":"2025-12-05T23:18:18.442Z","msg":"Operating in MODEL-ONLY mode: model-based optimization only"}
{"level":"INFO","ts":"2025-12-05T23:18:18.442Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-12-05T23:18:18.442Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"DEBUG","ts":"2025-12-05T23:18:18.443Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=0"}
{"level":"DEBUG","ts":"2025-12-05T23:18:18.444Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=0"}
{"level":"DEBUG","ts":"2025-12-05T23:18:18.445Z","msg":"Pod-to-variant matching successful: totalPods=0, variantCounts=map[]"}
{"level":"DEBUG","ts":"2025-12-05T23:18:18.445Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=0"}
{"level":"DEBUG","ts":"2025-12-05T23:18:18.445Z","msg":"Collected capacity metrics for observability in model-only mode: modelID=unsloth/Meta-Llama-3.1-8B, replicaCount=0"}
{"level":"INFO","ts":"2025-12-05T23:18:18.445Z","msg":"Found SLO for model: model=unsloth/Meta-Llama-3.1-8B, class=Premium, slo-tpot=10, slo-ttft=1000"}
{"level":"WARN","ts":"2025-12-05T23:18:18.448Z","msg":"Metrics unavailable, skipping optimization for variant: variant=ms-inference-scheduling-llm-d-modelservice-decode, namespace=llm-d-inference-scheduler, model=unsloth/Meta-Llama-3.1-8B, reason=MetricsMissing, troubleshooting=No vLLM metrics found for model 'unsloth/Meta-Llama-3.1-8B' in namespace 'llm-d-inference-scheduler'. Check: (1) ServiceMonitor exists in monitoring namespace, (2) ServiceMonitor selector matches vLLM service labels, (3) vLLM pods are running and exposing /metrics endpoint, (4) Prometheus is scraping the monitoring namespace"}
{"level":"DEBUG","ts":"2025-12-05T23:18:18.448Z","msg":"Experimental model tuner is enabled globally (EXPERIMENTAL_MODEL_TUNER_ENABLED=true) tuning model performance parameters for active VAs"}
{"level":"ERROR","ts":"2025-12-05T23:18:18.448Z","msg":"Model-based optimization failed: no feasible allocations found for all variants: "}
{"level":"WARN","ts":"2025-12-05T23:18:18.448Z","msg":"Both Saturation and model-based failed, activating safety net: modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-05T23:18:18.448Z","msg":"Safety net activated: emitted fallback metrics: variant=ms-inference-scheduling-llm-d-modelservice-decode, currentReplicas=2, desiredReplicas=2, accelerator=H100, fallbackSource=current-replicas"}
{"level":"INFO","ts":"2025-12-05T23:18:18.448Z","msg":"No scaling decisions to apply"}
{"level":"WARN","ts":"2025-12-05T23:18:18.448Z","msg":"Reconciliation completed with errors: mode=model-only, modelsProcessed=1, modelsFailed=1, decisionsApplied=0"}
{"level":"error","ts":"2025-12-05T23:18:21Z","logger":"controller-runtime.metrics","msg":"Authentication failed","path":"/metrics","error":"[invalid bearer token, Token does not match server's copy, token lookup failed]","errorCauses":[{"error":"[invalid bearer token, Token does not match server's copy, token lookup failed]"}],"stacktrace":"sigs.k8s.io/controller-runtime/pkg/metrics/filters.WithAuthenticationAndAuthorization.func1.1\n\t/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.20.4/pkg/metrics/filters/filters.go:89\nnet/http.HandlerFunc.ServeHTTP\n\t/usr/local/go/src/net/http/server.go:2294\nnet/http.(*ServeMux).ServeHTTP\n\t/usr/local/go/src/net/http/server.go:2822\nnet/http.serverHandler.ServeHTTP\n\t/usr/local/go/src/net/http/server.go:3301\nnet/http.(*conn).serve\n\t/usr/local/go/src/net/http/server.go:2102"}
{"level":"error","ts":"2025-12-05T23:18:31Z","logger":"controller-runtime.metrics","msg":"Authentication failed","path":"/metrics","error":"[invalid bearer token, Token does not match server's copy, token lookup failed]","errorCauses":[{"error":"[invalid bearer token, Token does not match server's copy, token lookup failed]"}],"stacktrace":"sigs.k8s.io/controller-runtime/pkg/metrics/filters.WithAuthenticationAndAuthorization.func1.1\n\t/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.20.4/pkg/metrics/filters/filters.go:89\nnet/http.HandlerFunc.ServeHTTP\n\t/usr/local/go/src/net/http/server.go:2294\nnet/http.(*ServeMux).ServeHTTP\n\t/usr/local/go/src/net/http/server.go:2822\nnet/http.serverHandler.ServeHTTP\n\t/usr/local/go/src/net/http/server.go:3301\nnet/http.(*conn).serve\n\t/usr/local/go/src/net/http/server.go:2102"}
{"level":"INFO","ts":"2025-12-05T23:19:18.442Z","msg":"saturation scaling config cache updated: entries=1, has_default=true"}
{"level":"INFO","ts":"2025-12-05T23:19:18.442Z","msg":"saturation scaling configuration loaded successfully"}
{"level":"INFO","ts":"2025-12-05T23:19:18.442Z","msg":"Operating in MODEL-ONLY mode: model-based optimization only"}
{"level":"INFO","ts":"2025-12-05T23:19:18.442Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-12-05T23:19:18.442Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-12-05T23:19:18.447Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dczzts, queueLength=0"}
{"level":"DEBUG","ts":"2025-12-05T23:19:18.447Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"INFO","ts":"2025-12-05T23:19:18.447Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dczzts, usage=0.000 (0.0%)"}
{"level":"DEBUG","ts":"2025-12-05T23:19:18.447Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"DEBUG","ts":"2025-12-05T23:19:18.651Z","msg":"Pod-to-variant matching successful: totalPods=1, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"DEBUG","ts":"2025-12-05T23:19:18.651Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=1"}
{"level":"DEBUG","ts":"2025-12-05T23:19:18.651Z","msg":"Collected capacity metrics for observability in model-only mode: modelID=unsloth/Meta-Llama-3.1-8B, replicaCount=1"}
{"level":"INFO","ts":"2025-12-05T23:19:18.651Z","msg":"Found SLO for model: model=unsloth/Meta-Llama-3.1-8B, class=Premium, slo-tpot=10, slo-ttft=1000"}
{"level":"DEBUG","ts":"2025-12-05T23:19:18.662Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=2, accelerator=H100, ttft=0.00ms, itl=0.00ms, cost=200.00, maxBatch=256, arrivalRate=0.00, avgInputTokens=0.00, avgOutputTokens=0.00"}
{"level":"DEBUG","ts":"2025-12-05T23:19:18.662Z","msg":"Experimental model tuner is enabled globally (EXPERIMENTAL_MODEL_TUNER_ENABLED=true) tuning model performance parameters for active VAs"}
{"level":"DEBUG","ts":"2025-12-05T23:19:18.662Z","msg":"[Tuner Config] variant=ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler | Initial state: alpha=7.470000, beta=0.044000, gamma=15.415000, delta=0.000337 | SLO targets: TTFT=1000.00ms, ITL=10.00ms | Has covariance=false"}
{"level":"WARN","ts":"2025-12-05T23:19:18.662Z","msg":"Tuner failed completely for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler: failed to get/create tuner: failed to create tuner: invalid environment: &{0 0 0 256 0 0 2}. Using fallback parameters."}
{"level":"INFO","ts":"2025-12-05T23:19:18.662Z","msg":"Using fallback parameters for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler: alpha=7.470000, beta=0.044000, gamma=15.415000, delta=0.000337"}
{"level":"DEBUG","ts":"2025-12-05T23:19:18.662Z","msg":"Updated SystemData for model=unsloth/Meta-Llama-3.1-8B, accelerator=H100: alpha=7.470000, beta=0.044000, gamma=15.415000, delta=0.000337"}
{"level":"DEBUG","ts":"2025-12-05T23:19:18.662Z","msg":"Updated VA status for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler, model unsloth/Meta-Llama-3.1-8B, accelerator H100: state=[7.470000, 0.044000, 15.415000, 0.000337]"}
{"level":"DEBUG","ts":"2025-12-05T23:19:18.662Z","msg":"Optimization solution - system: Solution: \ns=ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler; c=Premium; m=unsloth/Meta-Llama-3.1-8B; rate=0; inTk=0; outTk=0; sol=1, sat=false, alloc={acc=H100; numRep=1; maxBatch=512; cost=100, val=-100, itl=7.514, ttft=15.415337, rho=0, maxRPM=676453.25}; slo-itl=10, slo-ttft=1000, slo-tps=0 \nAllocationByType: \nname=NVIDIA-H100-80GB-HBM3, count=1, limit=0, cost=100 \ntotalCost=100 \n"}
{"level":"DEBUG","ts":"2025-12-05T23:19:18.662Z","msg":"Setting accelerator name: Name=H100, allocationData={H100 1 512 100 7.514 15.415337 {0 0 0}}"}
{"level":"INFO","ts":"2025-12-05T23:19:18.662Z","msg":"Model-based optimization completed for model: unsloth/Meta-Llama-3.1-8B - model-based targets: map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"WARN","ts":"2025-12-05T23:19:18.662Z","msg":"saturation analysis unavailable, using model-based targets only: modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-05T23:19:18.662Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-12-05T23:19:18.662Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=scale-down, current=2→target=1"}
{"level":"DEBUG","ts":"2025-12-05T23:19:18.662Z","msg":"Found VA in map: variant=ms-inference-scheduling-llm-d-modelservice-decode, hasCurrentAlloc=true, accelerator=H100"}
{"level":"INFO","ts":"2025-12-05T23:19:18.662Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 2, desired-replicas: 1, accelerator: H100"}
{"level":"INFO","ts":"2025-12-05T23:19:18.662Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=1, accelerator=H100, SaturationOnly=false"}
{"level":"INFO","ts":"2025-12-05T23:19:18.668Z","msg":"Applied Saturation decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=scale-down, current=2, target=1, reason=model-based only (Saturation unavailable)"}
{"level":"INFO","ts":"2025-12-05T23:19:18.668Z","msg":"Reconciliation completed successfully: mode=model-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-12-05T23:19:18.668Z","msg":"saturation scaling config cache updated: entries=1, has_default=true"}
{"level":"INFO","ts":"2025-12-05T23:19:18.668Z","msg":"saturation scaling configuration loaded successfully"}
{"level":"INFO","ts":"2025-12-05T23:19:18.668Z","msg":"Operating in MODEL-ONLY mode: model-based optimization only"}
{"level":"INFO","ts":"2025-12-05T23:19:18.668Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-12-05T23:19:18.668Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-12-05T23:19:18.669Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dczzts, queueLength=0"}
{"level":"DEBUG","ts":"2025-12-05T23:19:18.669Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"INFO","ts":"2025-12-05T23:19:18.669Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dczzts, usage=0.000 (0.0%)"}
{"level":"DEBUG","ts":"2025-12-05T23:19:18.669Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"DEBUG","ts":"2025-12-05T23:19:18.671Z","msg":"Pod-to-variant matching successful: totalPods=1, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"DEBUG","ts":"2025-12-05T23:19:18.671Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=1"}
{"level":"DEBUG","ts":"2025-12-05T23:19:18.671Z","msg":"Collected capacity metrics for observability in model-only mode: modelID=unsloth/Meta-Llama-3.1-8B, replicaCount=1"}
{"level":"INFO","ts":"2025-12-05T23:19:18.672Z","msg":"Found SLO for model: model=unsloth/Meta-Llama-3.1-8B, class=Premium, slo-tpot=10, slo-ttft=1000"}
{"level":"DEBUG","ts":"2025-12-05T23:19:18.679Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=2, accelerator=H100, ttft=0.00ms, itl=0.00ms, cost=200.00, maxBatch=256, arrivalRate=0.00, avgInputTokens=0.00, avgOutputTokens=0.00"}
{"level":"DEBUG","ts":"2025-12-05T23:19:18.679Z","msg":"Experimental model tuner is enabled globally (EXPERIMENTAL_MODEL_TUNER_ENABLED=true) tuning model performance parameters for active VAs"}
{"level":"DEBUG","ts":"2025-12-05T23:19:18.679Z","msg":"[Tuner Config] variant=ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler | Initial state: alpha=7.470000, beta=0.044000, gamma=15.415000, delta=0.000337 | SLO targets: TTFT=1000.00ms, ITL=10.00ms | Has covariance=false"}
{"level":"WARN","ts":"2025-12-05T23:19:18.679Z","msg":"Tuner failed completely for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler: failed to get/create tuner: failed to create tuner: invalid environment: &{0 0 0 256 0 0 2}. Using fallback parameters."}
{"level":"INFO","ts":"2025-12-05T23:19:18.679Z","msg":"Using fallback parameters for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler: alpha=7.470000, beta=0.044000, gamma=15.415000, delta=0.000337"}
{"level":"DEBUG","ts":"2025-12-05T23:19:18.679Z","msg":"Updated SystemData for model=unsloth/Meta-Llama-3.1-8B, accelerator=H100: alpha=7.470000, beta=0.044000, gamma=15.415000, delta=0.000337"}
{"level":"DEBUG","ts":"2025-12-05T23:19:18.679Z","msg":"Updated VA status for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler, model unsloth/Meta-Llama-3.1-8B, accelerator H100: state=[7.470000, 0.044000, 15.415000, 0.000337]"}
{"level":"DEBUG","ts":"2025-12-05T23:19:18.680Z","msg":"Optimization solution - system: Solution: \ns=ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler; c=Premium; m=unsloth/Meta-Llama-3.1-8B; rate=0; inTk=0; outTk=0; sol=1, sat=false, alloc={acc=H100; numRep=1; maxBatch=512; cost=100, val=-100, itl=7.514, ttft=15.415337, rho=0, maxRPM=676453.25}; slo-itl=10, slo-ttft=1000, slo-tps=0 \nAllocationByType: \nname=NVIDIA-H100-80GB-HBM3, count=1, limit=0, cost=100 \ntotalCost=100 \n"}
{"level":"DEBUG","ts":"2025-12-05T23:19:18.680Z","msg":"Setting accelerator name: Name=H100, allocationData={H100 1 512 100 7.514 15.415337 {0 0 0}}"}
{"level":"INFO","ts":"2025-12-05T23:19:18.680Z","msg":"Model-based optimization completed for model: unsloth/Meta-Llama-3.1-8B - model-based targets: map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"WARN","ts":"2025-12-05T23:19:18.680Z","msg":"saturation analysis unavailable, using model-based targets only: modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-05T23:19:18.680Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-12-05T23:19:18.680Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=scale-down, current=2→target=1"}
{"level":"DEBUG","ts":"2025-12-05T23:19:18.680Z","msg":"Found VA in map: variant=ms-inference-scheduling-llm-d-modelservice-decode, hasCurrentAlloc=true, accelerator=H100"}
{"level":"INFO","ts":"2025-12-05T23:19:18.680Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 2, desired-replicas: 1, accelerator: H100"}
{"level":"INFO","ts":"2025-12-05T23:19:18.680Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=1, accelerator=H100, SaturationOnly=false"}
{"level":"INFO","ts":"2025-12-05T23:19:18.684Z","msg":"Applied Saturation decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=scale-down, current=2, target=1, reason=model-based only (Saturation unavailable)"}
{"level":"INFO","ts":"2025-12-05T23:19:18.684Z","msg":"Reconciliation completed successfully: mode=model-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-12-05T23:20:18.668Z","msg":"saturation scaling config cache updated: entries=1, has_default=true"}
{"level":"INFO","ts":"2025-12-05T23:20:18.668Z","msg":"saturation scaling configuration loaded successfully"}
{"level":"INFO","ts":"2025-12-05T23:20:18.668Z","msg":"Operating in MODEL-ONLY mode: model-based optimization only"}
{"level":"INFO","ts":"2025-12-05T23:20:18.668Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-12-05T23:20:18.668Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-12-05T23:20:18.677Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dgxfr7, queueLength=0"}
{"level":"INFO","ts":"2025-12-05T23:20:18.677Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dczzts, queueLength=0"}
{"level":"DEBUG","ts":"2025-12-05T23:20:18.677Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=2"}
{"level":"INFO","ts":"2025-12-05T23:20:18.677Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dgxfr7, usage=0.000 (0.0%)"}
{"level":"INFO","ts":"2025-12-05T23:20:18.677Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dczzts, usage=0.000 (0.0%)"}
{"level":"DEBUG","ts":"2025-12-05T23:20:18.677Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=2"}
{"level":"DEBUG","ts":"2025-12-05T23:20:18.679Z","msg":"Filtering pod from stale vLLM metrics: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dgxfr7, namespace=llm-d-inference-scheduler, model=unsloth/Meta-Llama-3.1-8B"}
{"level":"DEBUG","ts":"2025-12-05T23:20:18.679Z","msg":"Pod-to-variant matching successful: totalPods=1, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"DEBUG","ts":"2025-12-05T23:20:18.679Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=1"}
{"level":"DEBUG","ts":"2025-12-05T23:20:18.679Z","msg":"Collected capacity metrics for observability in model-only mode: modelID=unsloth/Meta-Llama-3.1-8B, replicaCount=1"}
{"level":"INFO","ts":"2025-12-05T23:20:18.679Z","msg":"Found SLO for model: model=unsloth/Meta-Llama-3.1-8B, class=Premium, slo-tpot=10, slo-ttft=1000"}
{"level":"DEBUG","ts":"2025-12-05T23:20:18.688Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=1, accelerator=H100, ttft=0.00ms, itl=0.00ms, cost=100.00, maxBatch=256, arrivalRate=0.00, avgInputTokens=0.00, avgOutputTokens=0.00"}
{"level":"DEBUG","ts":"2025-12-05T23:20:18.688Z","msg":"Experimental model tuner is enabled globally (EXPERIMENTAL_MODEL_TUNER_ENABLED=true) tuning model performance parameters for active VAs"}
{"level":"DEBUG","ts":"2025-12-05T23:20:18.688Z","msg":"[Tuner Config] variant=ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler | Initial state: alpha=7.470000, beta=0.044000, gamma=15.415000, delta=0.000337 | SLO targets: TTFT=1000.00ms, ITL=10.00ms | Has covariance=false"}
{"level":"WARN","ts":"2025-12-05T23:20:18.688Z","msg":"Tuner failed completely for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler: failed to get/create tuner: failed to create tuner: invalid environment: &{0 0 0 256 0 0 1}. Using fallback parameters."}
{"level":"INFO","ts":"2025-12-05T23:20:18.688Z","msg":"Using fallback parameters for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler: alpha=7.470000, beta=0.044000, gamma=15.415000, delta=0.000337"}
{"level":"DEBUG","ts":"2025-12-05T23:20:18.688Z","msg":"Updated SystemData for model=unsloth/Meta-Llama-3.1-8B, accelerator=H100: alpha=7.470000, beta=0.044000, gamma=15.415000, delta=0.000337"}
{"level":"DEBUG","ts":"2025-12-05T23:20:18.688Z","msg":"Updated VA status for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler, model unsloth/Meta-Llama-3.1-8B, accelerator H100: state=[7.470000, 0.044000, 15.415000, 0.000337]"}
{"level":"DEBUG","ts":"2025-12-05T23:20:18.688Z","msg":"Optimization solution - system: Solution: \ns=ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler; c=Premium; m=unsloth/Meta-Llama-3.1-8B; rate=0; inTk=0; outTk=0; sol=1, sat=false, alloc={acc=H100; numRep=1; maxBatch=512; cost=100, val=0, itl=7.514, ttft=15.415337, rho=0, maxRPM=676453.25}; slo-itl=10, slo-ttft=1000, slo-tps=0 \nAllocationByType: \nname=NVIDIA-H100-80GB-HBM3, count=1, limit=0, cost=100 \ntotalCost=100 \n"}
{"level":"DEBUG","ts":"2025-12-05T23:20:18.688Z","msg":"Setting accelerator name: Name=H100, allocationData={H100 1 512 100 7.514 15.415337 {0 0 0}}"}
{"level":"INFO","ts":"2025-12-05T23:20:18.688Z","msg":"Model-based optimization completed for model: unsloth/Meta-Llama-3.1-8B - model-based targets: map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"WARN","ts":"2025-12-05T23:20:18.688Z","msg":"saturation analysis unavailable, using model-based targets only: modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-05T23:20:18.688Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-12-05T23:20:18.688Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1→target=1"}
{"level":"DEBUG","ts":"2025-12-05T23:20:18.688Z","msg":"Found VA in map: variant=ms-inference-scheduling-llm-d-modelservice-decode, hasCurrentAlloc=true, accelerator=H100"}
{"level":"INFO","ts":"2025-12-05T23:20:18.688Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 1, desired-replicas: 1, accelerator: H100"}
{"level":"INFO","ts":"2025-12-05T23:20:18.688Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=1, accelerator=H100, SaturationOnly=false"}
{"level":"INFO","ts":"2025-12-05T23:20:18.694Z","msg":"Applied Saturation decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1, target=1, reason=model-based only (Saturation unavailable)"}
{"level":"INFO","ts":"2025-12-05T23:20:18.694Z","msg":"Reconciliation completed successfully: mode=model-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-12-05T23:20:18.694Z","msg":"saturation scaling config cache updated: entries=1, has_default=true"}
{"level":"INFO","ts":"2025-12-05T23:20:18.694Z","msg":"saturation scaling configuration loaded successfully"}
{"level":"INFO","ts":"2025-12-05T23:20:18.694Z","msg":"Operating in MODEL-ONLY mode: model-based optimization only"}
{"level":"INFO","ts":"2025-12-05T23:20:18.694Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-12-05T23:20:18.694Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-12-05T23:20:18.696Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dgxfr7, usage=0.000 (0.0%)"}
{"level":"INFO","ts":"2025-12-05T23:20:18.696Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dczzts, usage=0.000 (0.0%)"}
{"level":"DEBUG","ts":"2025-12-05T23:20:18.696Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=2"}
{"level":"INFO","ts":"2025-12-05T23:20:18.696Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dgxfr7, queueLength=0"}
{"level":"INFO","ts":"2025-12-05T23:20:18.696Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dczzts, queueLength=0"}
{"level":"DEBUG","ts":"2025-12-05T23:20:18.696Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=2"}
{"level":"DEBUG","ts":"2025-12-05T23:20:18.697Z","msg":"Filtering pod from stale vLLM metrics: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dgxfr7, namespace=llm-d-inference-scheduler, model=unsloth/Meta-Llama-3.1-8B"}
{"level":"DEBUG","ts":"2025-12-05T23:20:18.697Z","msg":"Pod-to-variant matching successful: totalPods=1, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"DEBUG","ts":"2025-12-05T23:20:18.697Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=1"}
{"level":"DEBUG","ts":"2025-12-05T23:20:18.697Z","msg":"Collected capacity metrics for observability in model-only mode: modelID=unsloth/Meta-Llama-3.1-8B, replicaCount=1"}
{"level":"INFO","ts":"2025-12-05T23:20:18.698Z","msg":"Found SLO for model: model=unsloth/Meta-Llama-3.1-8B, class=Premium, slo-tpot=10, slo-ttft=1000"}
{"level":"DEBUG","ts":"2025-12-05T23:20:18.705Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=1, accelerator=H100, ttft=0.00ms, itl=0.00ms, cost=100.00, maxBatch=256, arrivalRate=0.00, avgInputTokens=0.00, avgOutputTokens=0.00"}
{"level":"DEBUG","ts":"2025-12-05T23:20:18.705Z","msg":"Experimental model tuner is enabled globally (EXPERIMENTAL_MODEL_TUNER_ENABLED=true) tuning model performance parameters for active VAs"}
{"level":"DEBUG","ts":"2025-12-05T23:20:18.705Z","msg":"[Tuner Config] variant=ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler | Initial state: alpha=7.470000, beta=0.044000, gamma=15.415000, delta=0.000337 | SLO targets: TTFT=1000.00ms, ITL=10.00ms | Has covariance=false"}
{"level":"WARN","ts":"2025-12-05T23:20:18.705Z","msg":"Tuner failed completely for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler: failed to get/create tuner: failed to create tuner: invalid environment: &{0 0 0 256 0 0 1}. Using fallback parameters."}
{"level":"INFO","ts":"2025-12-05T23:20:18.705Z","msg":"Using fallback parameters for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler: alpha=7.470000, beta=0.044000, gamma=15.415000, delta=0.000337"}
{"level":"DEBUG","ts":"2025-12-05T23:20:18.705Z","msg":"Updated SystemData for model=unsloth/Meta-Llama-3.1-8B, accelerator=H100: alpha=7.470000, beta=0.044000, gamma=15.415000, delta=0.000337"}
{"level":"DEBUG","ts":"2025-12-05T23:20:18.705Z","msg":"Updated VA status for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler, model unsloth/Meta-Llama-3.1-8B, accelerator H100: state=[7.470000, 0.044000, 15.415000, 0.000337]"}
{"level":"DEBUG","ts":"2025-12-05T23:20:18.705Z","msg":"Optimization solution - system: Solution: \ns=ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler; c=Premium; m=unsloth/Meta-Llama-3.1-8B; rate=0; inTk=0; outTk=0; sol=1, sat=false, alloc={acc=H100; numRep=1; maxBatch=512; cost=100, val=0, itl=7.514, ttft=15.415337, rho=0, maxRPM=676453.25}; slo-itl=10, slo-ttft=1000, slo-tps=0 \nAllocationByType: \nname=NVIDIA-H100-80GB-HBM3, count=1, limit=0, cost=100 \ntotalCost=100 \n"}
{"level":"DEBUG","ts":"2025-12-05T23:20:18.705Z","msg":"Setting accelerator name: Name=H100, allocationData={H100 1 512 100 7.514 15.415337 {0 0 0}}"}
{"level":"INFO","ts":"2025-12-05T23:20:18.705Z","msg":"Model-based optimization completed for model: unsloth/Meta-Llama-3.1-8B - model-based targets: map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"WARN","ts":"2025-12-05T23:20:18.705Z","msg":"saturation analysis unavailable, using model-based targets only: modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-05T23:20:18.705Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-12-05T23:20:18.705Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1→target=1"}
{"level":"DEBUG","ts":"2025-12-05T23:20:18.705Z","msg":"Found VA in map: variant=ms-inference-scheduling-llm-d-modelservice-decode, hasCurrentAlloc=true, accelerator=H100"}
{"level":"INFO","ts":"2025-12-05T23:20:18.705Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 1, desired-replicas: 1, accelerator: H100"}
{"level":"INFO","ts":"2025-12-05T23:20:18.705Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=1, accelerator=H100, SaturationOnly=false"}
{"level":"INFO","ts":"2025-12-05T23:20:18.711Z","msg":"Applied Saturation decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1, target=1, reason=model-based only (Saturation unavailable)"}
{"level":"INFO","ts":"2025-12-05T23:20:18.711Z","msg":"Reconciliation completed successfully: mode=model-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-12-05T23:21:18.695Z","msg":"saturation scaling config cache updated: entries=1, has_default=true"}
{"level":"INFO","ts":"2025-12-05T23:21:18.695Z","msg":"saturation scaling configuration loaded successfully"}
{"level":"INFO","ts":"2025-12-05T23:21:18.695Z","msg":"Operating in MODEL-ONLY mode: model-based optimization only"}
{"level":"INFO","ts":"2025-12-05T23:21:18.695Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-12-05T23:21:18.695Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-12-05T23:21:18.725Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dczzts, queueLength=0"}
{"level":"DEBUG","ts":"2025-12-05T23:21:18.725Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"INFO","ts":"2025-12-05T23:21:18.725Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dczzts, usage=0.075 (7.5%)"}
{"level":"DEBUG","ts":"2025-12-05T23:21:18.725Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"DEBUG","ts":"2025-12-05T23:21:18.771Z","msg":"Pod-to-variant matching successful: totalPods=1, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"DEBUG","ts":"2025-12-05T23:21:18.771Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=1"}
{"level":"DEBUG","ts":"2025-12-05T23:21:18.771Z","msg":"Collected capacity metrics for observability in model-only mode: modelID=unsloth/Meta-Llama-3.1-8B, replicaCount=1"}
{"level":"INFO","ts":"2025-12-05T23:21:18.771Z","msg":"Found SLO for model: model=unsloth/Meta-Llama-3.1-8B, class=Premium, slo-tpot=10, slo-ttft=1000"}
{"level":"DEBUG","ts":"2025-12-05T23:21:18.809Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=1, accelerator=H100, ttft=19.59ms, itl=9.42ms, cost=100.00, maxBatch=256, arrivalRate=343.87, avgInputTokens=265.47, avgOutputTokens=425.02"}
{"level":"DEBUG","ts":"2025-12-05T23:21:18.809Z","msg":"Experimental model tuner is enabled globally (EXPERIMENTAL_MODEL_TUNER_ENABLED=true) tuning model performance parameters for active VAs"}
{"level":"DEBUG","ts":"2025-12-05T23:21:18.809Z","msg":"[Tuner Config] variant=ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler | Initial state: alpha=8.478000, beta=0.040855, gamma=17.631001, delta=0.000321 | SLO targets: TTFT=1000.00ms, ITL=10.00ms | Has covariance=false"}
{"level":"INFO","ts":"2025-12-05T23:21:18.809Z","msg":"Tuner validation succeeded for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler, server ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler - New NIS: 0.003596"}
{"level":"DEBUG","ts":"2025-12-05T23:21:18.809Z","msg":"Model tuner results: model=unsloth/Meta-Llama-3.1-8B, accelerator=H100, alpha=8.444675, beta=0.040837, gamma=17.630774, delta=0.000321, NIS=0.00"}
{"level":"DEBUG","ts":"2025-12-05T23:21:18.809Z","msg":"Updated tuner status for variant ms-inference-scheduling-llm-d-modelservice-decode: alpha=8.444675, beta=0.040837, gamma=17.630774, delta=0.000321, NIS=0.003596"}
{"level":"INFO","ts":"2025-12-05T23:21:18.809Z","msg":"Tuned performance parameters result: variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler - alpha: 8.444675, beta: 0.040837, gamma: 17.630774, delta: 0.000321"}
{"level":"DEBUG","ts":"2025-12-05T23:21:18.812Z","msg":"Optimization solution - system: Solution: \ns=ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler; c=Premium; m=unsloth/Meta-Llama-3.1-8B; rate=343.87; inTk=265; outTk=425; sol=1, sat=false, alloc={acc=H100; numRep=1; maxBatch=512; cost=100, val=0, itl=9.425436, ttft=19.671307, rho=0.044954464, maxRPM=522.23645}; slo-itl=10, slo-ttft=1000, slo-tps=0 \nAllocationByType: \nname=NVIDIA-H100-80GB-HBM3, count=1, limit=0, cost=100 \ntotalCost=100 \n"}
{"level":"DEBUG","ts":"2025-12-05T23:21:18.812Z","msg":"Setting accelerator name: Name=H100, allocationData={H100 1 512 100 9.425436 19.671307 {343.87 265 425}}"}
{"level":"INFO","ts":"2025-12-05T23:21:18.812Z","msg":"Model-based optimization completed for model: unsloth/Meta-Llama-3.1-8B - model-based targets: map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"WARN","ts":"2025-12-05T23:21:18.812Z","msg":"saturation analysis unavailable, using model-based targets only: modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-05T23:21:18.812Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-12-05T23:21:18.812Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1→target=1"}
{"level":"DEBUG","ts":"2025-12-05T23:21:18.812Z","msg":"Found VA in map: variant=ms-inference-scheduling-llm-d-modelservice-decode, hasCurrentAlloc=true, accelerator=H100"}
{"level":"INFO","ts":"2025-12-05T23:21:18.812Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 1, desired-replicas: 1, accelerator: H100"}
{"level":"INFO","ts":"2025-12-05T23:21:18.812Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=1, accelerator=H100, SaturationOnly=false"}
{"level":"INFO","ts":"2025-12-05T23:21:18.819Z","msg":"Applied Saturation decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1, target=1, reason=model-based only (Saturation unavailable)"}
{"level":"INFO","ts":"2025-12-05T23:21:18.819Z","msg":"Reconciliation completed successfully: mode=model-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-12-05T23:21:18.819Z","msg":"saturation scaling config cache updated: entries=1, has_default=true"}
{"level":"INFO","ts":"2025-12-05T23:21:18.819Z","msg":"saturation scaling configuration loaded successfully"}
{"level":"INFO","ts":"2025-12-05T23:21:18.819Z","msg":"Operating in MODEL-ONLY mode: model-based optimization only"}
{"level":"INFO","ts":"2025-12-05T23:21:18.819Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-12-05T23:21:18.819Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-12-05T23:21:18.828Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dczzts, usage=0.075 (7.5%)"}
{"level":"DEBUG","ts":"2025-12-05T23:21:18.828Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"INFO","ts":"2025-12-05T23:21:18.828Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dczzts, queueLength=0"}
{"level":"DEBUG","ts":"2025-12-05T23:21:18.828Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"DEBUG","ts":"2025-12-05T23:21:18.845Z","msg":"Pod-to-variant matching successful: totalPods=1, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"DEBUG","ts":"2025-12-05T23:21:18.846Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=1"}
{"level":"DEBUG","ts":"2025-12-05T23:21:18.846Z","msg":"Collected capacity metrics for observability in model-only mode: modelID=unsloth/Meta-Llama-3.1-8B, replicaCount=1"}
{"level":"INFO","ts":"2025-12-05T23:21:18.846Z","msg":"Found SLO for model: model=unsloth/Meta-Llama-3.1-8B, class=Premium, slo-tpot=10, slo-ttft=1000"}
{"level":"DEBUG","ts":"2025-12-05T23:21:18.865Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=1, accelerator=H100, ttft=19.59ms, itl=9.42ms, cost=100.00, maxBatch=256, arrivalRate=344.32, avgInputTokens=265.47, avgOutputTokens=425.02"}
{"level":"DEBUG","ts":"2025-12-05T23:21:18.865Z","msg":"Experimental model tuner is enabled globally (EXPERIMENTAL_MODEL_TUNER_ENABLED=true) tuning model performance parameters for active VAs"}
{"level":"DEBUG","ts":"2025-12-05T23:21:18.865Z","msg":"Using state vals from VA status to tune variant ms-inference-scheduling-llm-d-modelservice-decode: alpha= 8.444675, beta= 0.040837, gamma= 17.630774, delta= 0.000321"}
{"level":"DEBUG","ts":"2025-12-05T23:21:18.865Z","msg":"[Tuner Config] variant=ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler | Initial state: alpha=8.444675, beta=0.040837, gamma=17.630774, delta=0.000321 | SLO targets: TTFT=1000.00ms, ITL=10.00ms | Has covariance=true"}
{"level":"INFO","ts":"2025-12-05T23:21:18.865Z","msg":"Tuner validation succeeded for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler, server ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler - New NIS: 0.000146"}
{"level":"DEBUG","ts":"2025-12-05T23:21:18.865Z","msg":"Model tuner results: model=unsloth/Meta-Llama-3.1-8B, accelerator=H100, alpha=8.439760, beta=0.040834, gamma=17.630465, delta=0.000321, NIS=0.00"}
{"level":"DEBUG","ts":"2025-12-05T23:21:18.865Z","msg":"Updated tuner status for variant ms-inference-scheduling-llm-d-modelservice-decode: alpha=8.439760, beta=0.040834, gamma=17.630465, delta=0.000321, NIS=0.000146"}
{"level":"INFO","ts":"2025-12-05T23:21:18.865Z","msg":"Tuned performance parameters result: variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler - alpha: 8.439760, beta: 0.040834, gamma: 17.630465, delta: 0.000321"}
{"level":"DEBUG","ts":"2025-12-05T23:21:18.874Z","msg":"Optimization solution - system: Solution: \ns=ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler; c=Premium; m=unsloth/Meta-Llama-3.1-8B; rate=344.32; inTk=265; outTk=425; sol=1, sat=false, alloc={acc=H100; numRep=1; maxBatch=512; cost=100, val=0, itl=9.421286, ttft=19.675148, rho=0.044993617, maxRPM=523.9616}; slo-itl=10, slo-ttft=1000, slo-tps=0 \nAllocationByType: \nname=NVIDIA-H100-80GB-HBM3, count=1, limit=0, cost=100 \ntotalCost=100 \n"}
{"level":"DEBUG","ts":"2025-12-05T23:21:18.874Z","msg":"Setting accelerator name: Name=H100, allocationData={H100 1 512 100 9.421286 19.675148 {344.32 265 425}}"}
{"level":"INFO","ts":"2025-12-05T23:21:18.874Z","msg":"Model-based optimization completed for model: unsloth/Meta-Llama-3.1-8B - model-based targets: map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"WARN","ts":"2025-12-05T23:21:18.874Z","msg":"saturation analysis unavailable, using model-based targets only: modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-05T23:21:18.874Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-12-05T23:21:18.874Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1→target=1"}
{"level":"DEBUG","ts":"2025-12-05T23:21:18.874Z","msg":"Found VA in map: variant=ms-inference-scheduling-llm-d-modelservice-decode, hasCurrentAlloc=true, accelerator=H100"}
{"level":"INFO","ts":"2025-12-05T23:21:18.874Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 1, desired-replicas: 1, accelerator: H100"}
{"level":"INFO","ts":"2025-12-05T23:21:18.874Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=1, accelerator=H100, SaturationOnly=false"}
{"level":"INFO","ts":"2025-12-05T23:21:18.881Z","msg":"Applied Saturation decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1, target=1, reason=model-based only (Saturation unavailable)"}
{"level":"INFO","ts":"2025-12-05T23:21:18.881Z","msg":"Reconciliation completed successfully: mode=model-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-12-05T23:22:18.819Z","msg":"saturation scaling config cache updated: entries=1, has_default=true"}
{"level":"INFO","ts":"2025-12-05T23:22:18.819Z","msg":"saturation scaling configuration loaded successfully"}
{"level":"INFO","ts":"2025-12-05T23:22:18.819Z","msg":"Operating in MODEL-ONLY mode: model-based optimization only"}
{"level":"INFO","ts":"2025-12-05T23:22:18.819Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-12-05T23:22:18.819Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-12-05T23:22:18.827Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dczzts, usage=0.078 (7.8%)"}
{"level":"DEBUG","ts":"2025-12-05T23:22:18.827Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"INFO","ts":"2025-12-05T23:22:18.827Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dczzts, queueLength=0"}
{"level":"DEBUG","ts":"2025-12-05T23:22:18.827Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"DEBUG","ts":"2025-12-05T23:22:18.829Z","msg":"Pod-to-variant matching successful: totalPods=1, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"DEBUG","ts":"2025-12-05T23:22:18.829Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=1"}
{"level":"DEBUG","ts":"2025-12-05T23:22:18.829Z","msg":"Collected capacity metrics for observability in model-only mode: modelID=unsloth/Meta-Llama-3.1-8B, replicaCount=1"}
{"level":"INFO","ts":"2025-12-05T23:22:18.829Z","msg":"Found SLO for model: model=unsloth/Meta-Llama-3.1-8B, class=Premium, slo-tpot=10, slo-ttft=1000"}
{"level":"DEBUG","ts":"2025-12-05T23:22:18.837Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=1, accelerator=H100, ttft=18.96ms, itl=9.13ms, cost=100.00, maxBatch=256, arrivalRate=585.49, avgInputTokens=234.79, avgOutputTokens=463.44"}
{"level":"DEBUG","ts":"2025-12-05T23:22:18.837Z","msg":"Experimental model tuner is enabled globally (EXPERIMENTAL_MODEL_TUNER_ENABLED=true) tuning model performance parameters for active VAs"}
{"level":"DEBUG","ts":"2025-12-05T23:22:18.837Z","msg":"Using state vals from VA status to tune variant ms-inference-scheduling-llm-d-modelservice-decode: alpha= 8.439760, beta= 0.040834, gamma= 17.630465, delta= 0.000321"}
{"level":"DEBUG","ts":"2025-12-05T23:22:18.837Z","msg":"[Tuner Config] variant=ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler | Initial state: alpha=8.439760, beta=0.040834, gamma=17.630465, delta=0.000321 | SLO targets: TTFT=1000.00ms, ITL=10.00ms | Has covariance=true"}
{"level":"INFO","ts":"2025-12-05T23:22:18.838Z","msg":"Tuner validation succeeded for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler, server ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler - New NIS: 3.843030"}
{"level":"DEBUG","ts":"2025-12-05T23:22:18.838Z","msg":"Model tuner results: model=unsloth/Meta-Llama-3.1-8B, accelerator=H100, alpha=7.650324, beta=0.038955, gamma=17.618820, delta=0.000321, NIS=3.84"}
{"level":"DEBUG","ts":"2025-12-05T23:22:18.838Z","msg":"Updated tuner status for variant ms-inference-scheduling-llm-d-modelservice-decode: alpha=7.650324, beta=0.038955, gamma=17.618820, delta=0.000321, NIS=3.843030"}
{"level":"INFO","ts":"2025-12-05T23:22:18.838Z","msg":"Tuned performance parameters result: variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler - alpha: 7.650324, beta: 0.038955, gamma: 17.618820, delta: 0.000321"}
{"level":"DEBUG","ts":"2025-12-05T23:22:18.847Z","msg":"Optimization solution - system: Solution: \ns=ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler; c=Premium; m=unsloth/Meta-Llama-3.1-8B; rate=585.49; inTk=234; outTk=463; sol=1, sat=false, alloc={acc=H100; numRep=1; maxBatch=512; cost=100, val=0, itl=9.33699, ttft=20.87093, rho=0.08261203, maxRPM=766.6759}; slo-itl=10, slo-ttft=1000, slo-tps=0 \nAllocationByType: \nname=NVIDIA-H100-80GB-HBM3, count=1, limit=0, cost=100 \ntotalCost=100 \n"}
{"level":"DEBUG","ts":"2025-12-05T23:22:18.847Z","msg":"Setting accelerator name: Name=H100, allocationData={H100 1 512 100 9.33699 20.87093 {585.49 234 463}}"}
{"level":"INFO","ts":"2025-12-05T23:22:18.847Z","msg":"Model-based optimization completed for model: unsloth/Meta-Llama-3.1-8B - model-based targets: map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"WARN","ts":"2025-12-05T23:22:18.847Z","msg":"saturation analysis unavailable, using model-based targets only: modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-05T23:22:18.847Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-12-05T23:22:18.847Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1→target=1"}
{"level":"DEBUG","ts":"2025-12-05T23:22:18.847Z","msg":"Found VA in map: variant=ms-inference-scheduling-llm-d-modelservice-decode, hasCurrentAlloc=true, accelerator=H100"}
{"level":"INFO","ts":"2025-12-05T23:22:18.847Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 1, desired-replicas: 1, accelerator: H100"}
{"level":"INFO","ts":"2025-12-05T23:22:18.847Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=1, accelerator=H100, SaturationOnly=false"}
{"level":"INFO","ts":"2025-12-05T23:22:18.853Z","msg":"Applied Saturation decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1, target=1, reason=model-based only (Saturation unavailable)"}
{"level":"INFO","ts":"2025-12-05T23:22:18.853Z","msg":"Reconciliation completed successfully: mode=model-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-12-05T23:22:18.881Z","msg":"saturation scaling config cache updated: entries=1, has_default=true"}
{"level":"INFO","ts":"2025-12-05T23:22:18.881Z","msg":"saturation scaling configuration loaded successfully"}
{"level":"INFO","ts":"2025-12-05T23:22:18.881Z","msg":"Operating in MODEL-ONLY mode: model-based optimization only"}
{"level":"INFO","ts":"2025-12-05T23:22:18.881Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-12-05T23:22:18.881Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-12-05T23:22:18.883Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dczzts, queueLength=0"}
{"level":"DEBUG","ts":"2025-12-05T23:22:18.883Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"INFO","ts":"2025-12-05T23:22:18.883Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dczzts, usage=0.078 (7.8%)"}
{"level":"DEBUG","ts":"2025-12-05T23:22:18.883Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"DEBUG","ts":"2025-12-05T23:22:18.885Z","msg":"Pod-to-variant matching successful: totalPods=1, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"DEBUG","ts":"2025-12-05T23:22:18.885Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=1"}
{"level":"DEBUG","ts":"2025-12-05T23:22:18.885Z","msg":"Collected capacity metrics for observability in model-only mode: modelID=unsloth/Meta-Llama-3.1-8B, replicaCount=1"}
{"level":"INFO","ts":"2025-12-05T23:22:18.885Z","msg":"Found SLO for model: model=unsloth/Meta-Llama-3.1-8B, class=Premium, slo-tpot=10, slo-ttft=1000"}
{"level":"DEBUG","ts":"2025-12-05T23:22:18.896Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=1, accelerator=H100, ttft=18.96ms, itl=9.13ms, cost=100.00, maxBatch=256, arrivalRate=585.49, avgInputTokens=234.79, avgOutputTokens=463.44"}
{"level":"DEBUG","ts":"2025-12-05T23:22:18.896Z","msg":"Experimental model tuner is enabled globally (EXPERIMENTAL_MODEL_TUNER_ENABLED=true) tuning model performance parameters for active VAs"}
{"level":"DEBUG","ts":"2025-12-05T23:22:18.896Z","msg":"Using state vals from VA status to tune variant ms-inference-scheduling-llm-d-modelservice-decode: alpha= 7.650324, beta= 0.038955, gamma= 17.618820, delta= 0.000321"}
{"level":"DEBUG","ts":"2025-12-05T23:22:18.896Z","msg":"[Tuner Config] variant=ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler | Initial state: alpha=7.650324, beta=0.038955, gamma=17.618820, delta=0.000321 | SLO targets: TTFT=1000.00ms, ITL=10.00ms | Has covariance=true"}
{"level":"INFO","ts":"2025-12-05T23:22:18.896Z","msg":"Tuner validation succeeded for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler, server ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler - New NIS: 0.129909"}
{"level":"DEBUG","ts":"2025-12-05T23:22:18.896Z","msg":"Model tuner results: model=unsloth/Meta-Llama-3.1-8B, accelerator=H100, alpha=7.517021, beta=0.038839, gamma=17.607670, delta=0.000321, NIS=0.13"}
{"level":"DEBUG","ts":"2025-12-05T23:22:18.896Z","msg":"Updated tuner status for variant ms-inference-scheduling-llm-d-modelservice-decode: alpha=7.517021, beta=0.038839, gamma=17.607670, delta=0.000321, NIS=0.129909"}
{"level":"INFO","ts":"2025-12-05T23:22:18.896Z","msg":"Tuned performance parameters result: variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler - alpha: 7.517021, beta: 0.038839, gamma: 17.607670, delta: 0.000321"}
{"level":"DEBUG","ts":"2025-12-05T23:22:18.901Z","msg":"Optimization solution - system: Solution: \ns=ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler; c=Premium; m=unsloth/Meta-Llama-3.1-8B; rate=585.49; inTk=234; outTk=463; sol=1, sat=false, alloc={acc=H100; numRep=1; maxBatch=512; cost=100, val=0, itl=9.1692505, ttft=20.802977, rho=0.081133746, maxRPM=813.3268}; slo-itl=10, slo-ttft=1000, slo-tps=0 \nAllocationByType: \nname=NVIDIA-H100-80GB-HBM3, count=1, limit=0, cost=100 \ntotalCost=100 \n"}
{"level":"DEBUG","ts":"2025-12-05T23:22:18.901Z","msg":"Setting accelerator name: Name=H100, allocationData={H100 1 512 100 9.1692505 20.802977 {585.49 234 463}}"}
{"level":"INFO","ts":"2025-12-05T23:22:18.901Z","msg":"Model-based optimization completed for model: unsloth/Meta-Llama-3.1-8B - model-based targets: map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"WARN","ts":"2025-12-05T23:22:18.901Z","msg":"saturation analysis unavailable, using model-based targets only: modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-05T23:22:18.901Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-12-05T23:22:18.901Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1→target=1"}
{"level":"DEBUG","ts":"2025-12-05T23:22:18.901Z","msg":"Found VA in map: variant=ms-inference-scheduling-llm-d-modelservice-decode, hasCurrentAlloc=true, accelerator=H100"}
{"level":"INFO","ts":"2025-12-05T23:22:18.901Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 1, desired-replicas: 1, accelerator: H100"}
{"level":"INFO","ts":"2025-12-05T23:22:18.901Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=1, accelerator=H100, SaturationOnly=false"}
{"level":"INFO","ts":"2025-12-05T23:22:18.906Z","msg":"Applied Saturation decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1, target=1, reason=model-based only (Saturation unavailable)"}
{"level":"INFO","ts":"2025-12-05T23:22:18.906Z","msg":"Reconciliation completed successfully: mode=model-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-12-05T23:23:18.854Z","msg":"saturation scaling config cache updated: entries=1, has_default=true"}
{"level":"INFO","ts":"2025-12-05T23:23:18.854Z","msg":"saturation scaling configuration loaded successfully"}
{"level":"INFO","ts":"2025-12-05T23:23:18.854Z","msg":"Operating in MODEL-ONLY mode: model-based optimization only"}
{"level":"INFO","ts":"2025-12-05T23:23:18.854Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-12-05T23:23:18.854Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-12-05T23:23:18.862Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dczzts, queueLength=0"}
{"level":"DEBUG","ts":"2025-12-05T23:23:18.862Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"INFO","ts":"2025-12-05T23:23:18.862Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dczzts, usage=0.071 (7.1%)"}
{"level":"DEBUG","ts":"2025-12-05T23:23:18.862Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"DEBUG","ts":"2025-12-05T23:23:18.865Z","msg":"Pod-to-variant matching successful: totalPods=1, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"DEBUG","ts":"2025-12-05T23:23:18.865Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=1"}
{"level":"DEBUG","ts":"2025-12-05T23:23:18.865Z","msg":"Collected capacity metrics for observability in model-only mode: modelID=unsloth/Meta-Llama-3.1-8B, replicaCount=1"}
{"level":"INFO","ts":"2025-12-05T23:23:18.865Z","msg":"Found SLO for model: model=unsloth/Meta-Llama-3.1-8B, class=Premium, slo-tpot=10, slo-ttft=1000"}
{"level":"DEBUG","ts":"2025-12-05T23:23:18.874Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=1, accelerator=H100, ttft=19.38ms, itl=9.37ms, cost=100.00, maxBatch=256, arrivalRate=636.85, avgInputTokens=236.89, avgOutputTokens=425.23"}
{"level":"DEBUG","ts":"2025-12-05T23:23:18.874Z","msg":"Experimental model tuner is enabled globally (EXPERIMENTAL_MODEL_TUNER_ENABLED=true) tuning model performance parameters for active VAs"}
{"level":"DEBUG","ts":"2025-12-05T23:23:18.874Z","msg":"Using state vals from VA status to tune variant ms-inference-scheduling-llm-d-modelservice-decode: alpha= 7.517021, beta= 0.038839, gamma= 17.607670, delta= 0.000321"}
{"level":"DEBUG","ts":"2025-12-05T23:23:18.874Z","msg":"[Tuner Config] variant=ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler | Initial state: alpha=7.517021, beta=0.038839, gamma=17.607670, delta=0.000321 | SLO targets: TTFT=1000.00ms, ITL=10.00ms | Has covariance=true"}
{"level":"INFO","ts":"2025-12-05T23:23:18.874Z","msg":"Tuner validation succeeded for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler, server ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler - New NIS: 0.126439"}
{"level":"DEBUG","ts":"2025-12-05T23:23:18.874Z","msg":"Model tuner results: model=unsloth/Meta-Llama-3.1-8B, accelerator=H100, alpha=7.646378, beta=0.038977, gamma=17.597416, delta=0.000321, NIS=0.13"}
{"level":"DEBUG","ts":"2025-12-05T23:23:18.874Z","msg":"Updated tuner status for variant ms-inference-scheduling-llm-d-modelservice-decode: alpha=7.646378, beta=0.038977, gamma=17.597416, delta=0.000321, NIS=0.126439"}
{"level":"INFO","ts":"2025-12-05T23:23:18.874Z","msg":"Tuned performance parameters result: variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler - alpha: 7.646378, beta: 0.038977, gamma: 17.597416, delta: 0.000321"}
{"level":"DEBUG","ts":"2025-12-05T23:23:18.882Z","msg":"Optimization solution - system: Solution: \ns=ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler; c=Premium; m=unsloth/Meta-Llama-3.1-8B; rate=636.85; inTk=236; outTk=425; sol=1, sat=false, alloc={acc=H100; numRep=1; maxBatch=512; cost=100, val=0, itl=9.33071, ttft=20.871046, rho=0.082448274, maxRPM=835.9785}; slo-itl=10, slo-ttft=1000, slo-tps=0 \nAllocationByType: \nname=NVIDIA-H100-80GB-HBM3, count=1, limit=0, cost=100 \ntotalCost=100 \n"}
{"level":"DEBUG","ts":"2025-12-05T23:23:18.882Z","msg":"Setting accelerator name: Name=H100, allocationData={H100 1 512 100 9.33071 20.871046 {636.85 236 425}}"}
{"level":"INFO","ts":"2025-12-05T23:23:18.882Z","msg":"Model-based optimization completed for model: unsloth/Meta-Llama-3.1-8B - model-based targets: map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"WARN","ts":"2025-12-05T23:23:18.882Z","msg":"saturation analysis unavailable, using model-based targets only: modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-05T23:23:18.882Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-12-05T23:23:18.882Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1→target=1"}
{"level":"DEBUG","ts":"2025-12-05T23:23:18.882Z","msg":"Found VA in map: variant=ms-inference-scheduling-llm-d-modelservice-decode, hasCurrentAlloc=true, accelerator=H100"}
{"level":"INFO","ts":"2025-12-05T23:23:18.882Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 1, desired-replicas: 1, accelerator: H100"}
{"level":"INFO","ts":"2025-12-05T23:23:18.882Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=1, accelerator=H100, SaturationOnly=false"}
{"level":"INFO","ts":"2025-12-05T23:23:18.888Z","msg":"Applied Saturation decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1, target=1, reason=model-based only (Saturation unavailable)"}
{"level":"INFO","ts":"2025-12-05T23:23:18.888Z","msg":"Reconciliation completed successfully: mode=model-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-12-05T23:23:18.907Z","msg":"saturation scaling config cache updated: entries=1, has_default=true"}
{"level":"INFO","ts":"2025-12-05T23:23:18.907Z","msg":"saturation scaling configuration loaded successfully"}
{"level":"INFO","ts":"2025-12-05T23:23:18.907Z","msg":"Operating in MODEL-ONLY mode: model-based optimization only"}
{"level":"INFO","ts":"2025-12-05T23:23:18.907Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-12-05T23:23:18.907Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-12-05T23:23:18.909Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dczzts, usage=0.071 (7.1%)"}
{"level":"DEBUG","ts":"2025-12-05T23:23:18.909Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"INFO","ts":"2025-12-05T23:23:18.909Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dczzts, queueLength=0"}
{"level":"DEBUG","ts":"2025-12-05T23:23:18.909Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"DEBUG","ts":"2025-12-05T23:23:18.911Z","msg":"Pod-to-variant matching successful: totalPods=1, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"DEBUG","ts":"2025-12-05T23:23:18.911Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=1"}
{"level":"DEBUG","ts":"2025-12-05T23:23:18.911Z","msg":"Collected capacity metrics for observability in model-only mode: modelID=unsloth/Meta-Llama-3.1-8B, replicaCount=1"}
{"level":"INFO","ts":"2025-12-05T23:23:18.911Z","msg":"Found SLO for model: model=unsloth/Meta-Llama-3.1-8B, class=Premium, slo-tpot=10, slo-ttft=1000"}
{"level":"DEBUG","ts":"2025-12-05T23:23:18.920Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=1, accelerator=H100, ttft=19.38ms, itl=9.37ms, cost=100.00, maxBatch=256, arrivalRate=636.85, avgInputTokens=236.89, avgOutputTokens=425.23"}
{"level":"DEBUG","ts":"2025-12-05T23:23:18.920Z","msg":"Experimental model tuner is enabled globally (EXPERIMENTAL_MODEL_TUNER_ENABLED=true) tuning model performance parameters for active VAs"}
{"level":"DEBUG","ts":"2025-12-05T23:23:18.920Z","msg":"Using state vals from VA status to tune variant ms-inference-scheduling-llm-d-modelservice-decode: alpha= 7.646378, beta= 0.038977, gamma= 17.597416, delta= 0.000321"}
{"level":"DEBUG","ts":"2025-12-05T23:23:18.920Z","msg":"[Tuner Config] variant=ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler | Initial state: alpha=7.646378, beta=0.038977, gamma=17.597416, delta=0.000321 | SLO targets: TTFT=1000.00ms, ITL=10.00ms | Has covariance=true"}
{"level":"INFO","ts":"2025-12-05T23:23:18.920Z","msg":"Tuner validation succeeded for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler, server ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler - New NIS: 0.007922"}
{"level":"DEBUG","ts":"2025-12-05T23:23:18.920Z","msg":"Model tuner results: model=unsloth/Meta-Llama-3.1-8B, accelerator=H100, alpha=7.671219, beta=0.039009, gamma=17.585199, delta=0.000321, NIS=0.01"}
{"level":"DEBUG","ts":"2025-12-05T23:23:18.920Z","msg":"Updated tuner status for variant ms-inference-scheduling-llm-d-modelservice-decode: alpha=7.671219, beta=0.039009, gamma=17.585199, delta=0.000321, NIS=0.007922"}
{"level":"INFO","ts":"2025-12-05T23:23:18.920Z","msg":"Tuned performance parameters result: variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler - alpha: 7.671219, beta: 0.039009, gamma: 17.585199, delta: 0.000321"}
{"level":"DEBUG","ts":"2025-12-05T23:23:18.928Z","msg":"Optimization solution - system: Solution: \ns=ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler; c=Premium; m=unsloth/Meta-Llama-3.1-8B; rate=636.85; inTk=236; outTk=425; sol=1, sat=false, alloc={acc=H100; numRep=1; maxBatch=512; cost=100, val=0, itl=9.362538, ttft=20.869673, rho=0.082728006, maxRPM=826.3234}; slo-itl=10, slo-ttft=1000, slo-tps=0 \nAllocationByType: \nname=NVIDIA-H100-80GB-HBM3, count=1, limit=0, cost=100 \ntotalCost=100 \n"}
{"level":"DEBUG","ts":"2025-12-05T23:23:18.928Z","msg":"Setting accelerator name: Name=H100, allocationData={H100 1 512 100 9.362538 20.869673 {636.85 236 425}}"}
{"level":"INFO","ts":"2025-12-05T23:23:18.928Z","msg":"Model-based optimization completed for model: unsloth/Meta-Llama-3.1-8B - model-based targets: map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"WARN","ts":"2025-12-05T23:23:18.928Z","msg":"saturation analysis unavailable, using model-based targets only: modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-05T23:23:18.928Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-12-05T23:23:18.928Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1→target=1"}
{"level":"DEBUG","ts":"2025-12-05T23:23:18.928Z","msg":"Found VA in map: variant=ms-inference-scheduling-llm-d-modelservice-decode, hasCurrentAlloc=true, accelerator=H100"}
{"level":"INFO","ts":"2025-12-05T23:23:18.928Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 1, desired-replicas: 1, accelerator: H100"}
{"level":"INFO","ts":"2025-12-05T23:23:18.928Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=1, accelerator=H100, SaturationOnly=false"}
{"level":"INFO","ts":"2025-12-05T23:23:18.933Z","msg":"Applied Saturation decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1, target=1, reason=model-based only (Saturation unavailable)"}
{"level":"INFO","ts":"2025-12-05T23:23:18.933Z","msg":"Reconciliation completed successfully: mode=model-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-12-05T23:24:18.888Z","msg":"saturation scaling config cache updated: entries=1, has_default=true"}
{"level":"INFO","ts":"2025-12-05T23:24:18.888Z","msg":"saturation scaling configuration loaded successfully"}
{"level":"INFO","ts":"2025-12-05T23:24:18.888Z","msg":"Operating in MODEL-ONLY mode: model-based optimization only"}
{"level":"INFO","ts":"2025-12-05T23:24:18.888Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-12-05T23:24:18.888Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-12-05T23:24:18.897Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dczzts, usage=0.246 (24.6%)"}
{"level":"DEBUG","ts":"2025-12-05T23:24:18.897Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"INFO","ts":"2025-12-05T23:24:18.897Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dczzts, queueLength=0"}
{"level":"DEBUG","ts":"2025-12-05T23:24:18.897Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"DEBUG","ts":"2025-12-05T23:24:18.899Z","msg":"Pod-to-variant matching successful: totalPods=1, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"DEBUG","ts":"2025-12-05T23:24:18.899Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=1"}
{"level":"DEBUG","ts":"2025-12-05T23:24:18.899Z","msg":"Collected capacity metrics for observability in model-only mode: modelID=unsloth/Meta-Llama-3.1-8B, replicaCount=1"}
{"level":"INFO","ts":"2025-12-05T23:24:18.899Z","msg":"Found SLO for model: model=unsloth/Meta-Llama-3.1-8B, class=Premium, slo-tpot=10, slo-ttft=1000"}
{"level":"DEBUG","ts":"2025-12-05T23:24:18.908Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=1, accelerator=H100, ttft=27.20ms, itl=13.58ms, cost=100.00, maxBatch=256, arrivalRate=921.52, avgInputTokens=265.21, avgOutputTokens=380.66"}
{"level":"DEBUG","ts":"2025-12-05T23:24:18.908Z","msg":"Experimental model tuner is enabled globally (EXPERIMENTAL_MODEL_TUNER_ENABLED=true) tuning model performance parameters for active VAs"}
{"level":"DEBUG","ts":"2025-12-05T23:24:18.908Z","msg":"Using state vals from VA status to tune variant ms-inference-scheduling-llm-d-modelservice-decode: alpha= 7.671219, beta= 0.039009, gamma= 17.585199, delta= 0.000321"}
{"level":"DEBUG","ts":"2025-12-05T23:24:18.908Z","msg":"[Tuner Config] variant=ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler | Initial state: alpha=7.671219, beta=0.039009, gamma=17.585199, delta=0.000321 | SLO targets: TTFT=1000.00ms, ITL=10.00ms | Has covariance=true"}
{"level":"WARN","ts":"2025-12-05T23:24:18.909Z","msg":"Tuner validation failed (NIS=31.33), validation error: normalized innovation squared (NIS=31.33) exceeds threshold (7.38), rejecting update as outlier - returning previous state: alpha=7.671219, beta=0.039009, gamma=17.585199, delta=0.000321"}
{"level":"WARN","ts":"2025-12-05T23:24:18.909Z","msg":"Tuner NIS validation failed for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler, server ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler (NIS=31.33 exceeds threshold 7.38) - Keeping previous state: alpha=7.671219, beta=0.039009, gamma=17.585199, delta=0.000321"}
{"level":"INFO","ts":"2025-12-05T23:24:18.909Z","msg":"Keeping previous tuned parameters for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler due to NIS validation failure (NIS=31.325438)"}
{"level":"DEBUG","ts":"2025-12-05T23:24:18.909Z","msg":"Model tuner results: model=unsloth/Meta-Llama-3.1-8B, accelerator=H100, alpha=7.671219, beta=0.039009, gamma=17.585199, delta=0.000321, NIS=31.33"}
{"level":"DEBUG","ts":"2025-12-05T23:24:18.909Z","msg":"Updated tuner status for variant ms-inference-scheduling-llm-d-modelservice-decode: alpha=7.671219, beta=0.039009, gamma=17.585199, delta=0.000321, NIS=31.325438"}
{"level":"DEBUG","ts":"2025-12-05T23:24:18.918Z","msg":"Optimization solution - system: Solution: \ns=ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler; c=Premium; m=unsloth/Meta-Llama-3.1-8B; rate=921.52; inTk=265; outTk=380; sol=1, sat=false, alloc={acc=H100; numRep=1; maxBatch=512; cost=100, val=0, itl=9.992866, ttft=22.6479, rho=0.114288405, maxRPM=923.74316}; slo-itl=10, slo-ttft=1000, slo-tps=0 \nAllocationByType: \nname=NVIDIA-H100-80GB-HBM3, count=1, limit=0, cost=100 \ntotalCost=100 \n"}
{"level":"DEBUG","ts":"2025-12-05T23:24:18.918Z","msg":"Setting accelerator name: Name=H100, allocationData={H100 1 512 100 9.992866 22.6479 {921.52 265 380}}"}
{"level":"INFO","ts":"2025-12-05T23:24:18.918Z","msg":"Model-based optimization completed for model: unsloth/Meta-Llama-3.1-8B - model-based targets: map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"WARN","ts":"2025-12-05T23:24:18.918Z","msg":"saturation analysis unavailable, using model-based targets only: modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-05T23:24:18.918Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-12-05T23:24:18.918Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1→target=1"}
{"level":"DEBUG","ts":"2025-12-05T23:24:18.918Z","msg":"Found VA in map: variant=ms-inference-scheduling-llm-d-modelservice-decode, hasCurrentAlloc=true, accelerator=H100"}
{"level":"INFO","ts":"2025-12-05T23:24:18.918Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 1, desired-replicas: 1, accelerator: H100"}
{"level":"INFO","ts":"2025-12-05T23:24:18.918Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=1, accelerator=H100, SaturationOnly=false"}
{"level":"INFO","ts":"2025-12-05T23:24:18.923Z","msg":"Applied Saturation decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1, target=1, reason=model-based only (Saturation unavailable)"}
{"level":"INFO","ts":"2025-12-05T23:24:18.923Z","msg":"Reconciliation completed successfully: mode=model-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-12-05T23:24:18.933Z","msg":"saturation scaling config cache updated: entries=1, has_default=true"}
{"level":"INFO","ts":"2025-12-05T23:24:18.933Z","msg":"saturation scaling configuration loaded successfully"}
{"level":"INFO","ts":"2025-12-05T23:24:18.933Z","msg":"Operating in MODEL-ONLY mode: model-based optimization only"}
{"level":"INFO","ts":"2025-12-05T23:24:18.933Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-12-05T23:24:18.934Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-12-05T23:24:18.935Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dczzts, usage=0.246 (24.6%)"}
{"level":"DEBUG","ts":"2025-12-05T23:24:18.935Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"INFO","ts":"2025-12-05T23:24:18.935Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dczzts, queueLength=0"}
{"level":"DEBUG","ts":"2025-12-05T23:24:18.935Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"DEBUG","ts":"2025-12-05T23:24:18.937Z","msg":"Pod-to-variant matching successful: totalPods=1, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"DEBUG","ts":"2025-12-05T23:24:18.937Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=1"}
{"level":"DEBUG","ts":"2025-12-05T23:24:18.937Z","msg":"Collected capacity metrics for observability in model-only mode: modelID=unsloth/Meta-Llama-3.1-8B, replicaCount=1"}
{"level":"INFO","ts":"2025-12-05T23:24:18.937Z","msg":"Found SLO for model: model=unsloth/Meta-Llama-3.1-8B, class=Premium, slo-tpot=10, slo-ttft=1000"}
{"level":"DEBUG","ts":"2025-12-05T23:24:18.951Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=1, accelerator=H100, ttft=27.20ms, itl=13.58ms, cost=100.00, maxBatch=256, arrivalRate=921.52, avgInputTokens=265.21, avgOutputTokens=380.66"}
{"level":"DEBUG","ts":"2025-12-05T23:24:18.951Z","msg":"Experimental model tuner is enabled globally (EXPERIMENTAL_MODEL_TUNER_ENABLED=true) tuning model performance parameters for active VAs"}
{"level":"DEBUG","ts":"2025-12-05T23:24:18.951Z","msg":"Using state vals from VA status to tune variant ms-inference-scheduling-llm-d-modelservice-decode: alpha= 7.671219, beta= 0.039009, gamma= 17.585199, delta= 0.000321"}
{"level":"DEBUG","ts":"2025-12-05T23:24:18.951Z","msg":"[Tuner Config] variant=ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler | Initial state: alpha=7.671219, beta=0.039009, gamma=17.585199, delta=0.000321 | SLO targets: TTFT=1000.00ms, ITL=10.00ms | Has covariance=true"}
{"level":"WARN","ts":"2025-12-05T23:24:18.951Z","msg":"Tuner validation failed (NIS=31.33), validation error: normalized innovation squared (NIS=31.33) exceeds threshold (7.38), rejecting update as outlier - returning previous state: alpha=7.671219, beta=0.039009, gamma=17.585199, delta=0.000321"}
{"level":"WARN","ts":"2025-12-05T23:24:18.951Z","msg":"Tuner NIS validation failed for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler, server ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler (NIS=31.33 exceeds threshold 7.38) - Keeping previous state: alpha=7.671219, beta=0.039009, gamma=17.585199, delta=0.000321"}
{"level":"INFO","ts":"2025-12-05T23:24:18.951Z","msg":"Keeping previous tuned parameters for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler due to NIS validation failure (NIS=31.325438)"}
{"level":"DEBUG","ts":"2025-12-05T23:24:18.951Z","msg":"Model tuner results: model=unsloth/Meta-Llama-3.1-8B, accelerator=H100, alpha=7.671219, beta=0.039009, gamma=17.585199, delta=0.000321, NIS=31.33"}
{"level":"DEBUG","ts":"2025-12-05T23:24:18.951Z","msg":"Tuned parameters unchanged for variant ms-inference-scheduling-llm-d-modelservice-decode, skipping status update"}
{"level":"DEBUG","ts":"2025-12-05T23:24:18.960Z","msg":"Optimization solution - system: Solution: \ns=ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler; c=Premium; m=unsloth/Meta-Llama-3.1-8B; rate=921.52; inTk=265; outTk=380; sol=1, sat=false, alloc={acc=H100; numRep=1; maxBatch=512; cost=100, val=0, itl=9.992866, ttft=22.6479, rho=0.114288405, maxRPM=923.74316}; slo-itl=10, slo-ttft=1000, slo-tps=0 \nAllocationByType: \nname=NVIDIA-H100-80GB-HBM3, count=1, limit=0, cost=100 \ntotalCost=100 \n"}
{"level":"DEBUG","ts":"2025-12-05T23:24:18.960Z","msg":"Setting accelerator name: Name=H100, allocationData={H100 1 512 100 9.992866 22.6479 {921.52 265 380}}"}
{"level":"INFO","ts":"2025-12-05T23:24:18.960Z","msg":"Model-based optimization completed for model: unsloth/Meta-Llama-3.1-8B - model-based targets: map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"WARN","ts":"2025-12-05T23:24:18.960Z","msg":"saturation analysis unavailable, using model-based targets only: modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-05T23:24:18.960Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-12-05T23:24:18.960Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1→target=1"}
{"level":"DEBUG","ts":"2025-12-05T23:24:18.960Z","msg":"Found VA in map: variant=ms-inference-scheduling-llm-d-modelservice-decode, hasCurrentAlloc=true, accelerator=H100"}
{"level":"INFO","ts":"2025-12-05T23:24:18.960Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 1, desired-replicas: 1, accelerator: H100"}
{"level":"INFO","ts":"2025-12-05T23:24:18.960Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=1, accelerator=H100, SaturationOnly=false"}
{"level":"INFO","ts":"2025-12-05T23:24:18.965Z","msg":"Applied Saturation decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1, target=1, reason=model-based only (Saturation unavailable)"}
{"level":"INFO","ts":"2025-12-05T23:24:18.965Z","msg":"Reconciliation completed successfully: mode=model-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-12-05T23:25:18.924Z","msg":"saturation scaling config cache updated: entries=1, has_default=true"}
{"level":"INFO","ts":"2025-12-05T23:25:18.924Z","msg":"saturation scaling configuration loaded successfully"}
{"level":"INFO","ts":"2025-12-05T23:25:18.924Z","msg":"Operating in MODEL-ONLY mode: model-based optimization only"}
{"level":"INFO","ts":"2025-12-05T23:25:18.924Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-12-05T23:25:18.924Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-12-05T23:25:18.930Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dczzts, usage=0.241 (24.1%)"}
{"level":"DEBUG","ts":"2025-12-05T23:25:18.930Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"INFO","ts":"2025-12-05T23:25:18.930Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dczzts, queueLength=0"}
{"level":"DEBUG","ts":"2025-12-05T23:25:18.930Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"DEBUG","ts":"2025-12-05T23:25:18.932Z","msg":"Pod-to-variant matching successful: totalPods=1, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"DEBUG","ts":"2025-12-05T23:25:18.932Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=1"}
{"level":"DEBUG","ts":"2025-12-05T23:25:18.932Z","msg":"Collected capacity metrics for observability in model-only mode: modelID=unsloth/Meta-Llama-3.1-8B, replicaCount=1"}
{"level":"INFO","ts":"2025-12-05T23:25:18.932Z","msg":"Found SLO for model: model=unsloth/Meta-Llama-3.1-8B, class=Premium, slo-tpot=10, slo-ttft=1000"}
{"level":"DEBUG","ts":"2025-12-05T23:25:18.942Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=1, accelerator=H100, ttft=31.90ms, itl=16.75ms, cost=100.00, maxBatch=256, arrivalRate=1304.51, avgInputTokens=222.61, avgOutputTokens=484.40"}
{"level":"DEBUG","ts":"2025-12-05T23:25:18.942Z","msg":"Experimental model tuner is enabled globally (EXPERIMENTAL_MODEL_TUNER_ENABLED=true) tuning model performance parameters for active VAs"}
{"level":"DEBUG","ts":"2025-12-05T23:25:18.942Z","msg":"Using state vals from VA status to tune variant ms-inference-scheduling-llm-d-modelservice-decode: alpha= 7.671219, beta= 0.039009, gamma= 17.585199, delta= 0.000321"}
{"level":"DEBUG","ts":"2025-12-05T23:25:18.942Z","msg":"[Tuner Config] variant=ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler | Initial state: alpha=7.671219, beta=0.039009, gamma=17.585199, delta=0.000321 | SLO targets: TTFT=1000.00ms, ITL=10.00ms | Has covariance=true"}
{"level":"WARN","ts":"2025-12-05T23:25:18.942Z","msg":"Tuner validation failed (NIS=8.57), validation error: normalized innovation squared (NIS=8.57) exceeds threshold (7.38), rejecting update as outlier - returning previous state: alpha=7.671219, beta=0.039009, gamma=17.585199, delta=0.000321"}
{"level":"WARN","ts":"2025-12-05T23:25:18.942Z","msg":"Tuner NIS validation failed for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler, server ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler (NIS=8.57 exceeds threshold 7.38) - Keeping previous state: alpha=7.671219, beta=0.039009, gamma=17.585199, delta=0.000321"}
{"level":"INFO","ts":"2025-12-05T23:25:18.942Z","msg":"Keeping previous tuned parameters for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler due to NIS validation failure (NIS=8.573486)"}
{"level":"DEBUG","ts":"2025-12-05T23:25:18.942Z","msg":"Model tuner results: model=unsloth/Meta-Llama-3.1-8B, accelerator=H100, alpha=7.671219, beta=0.039009, gamma=17.585199, delta=0.000321, NIS=8.57"}
{"level":"DEBUG","ts":"2025-12-05T23:25:18.942Z","msg":"Updated tuner status for variant ms-inference-scheduling-llm-d-modelservice-decode: alpha=7.671219, beta=0.039009, gamma=17.585199, delta=0.000321, NIS=8.573486"}
{"level":"DEBUG","ts":"2025-12-05T23:25:18.952Z","msg":"Optimization solution - system: Solution: \ns=ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler; c=Premium; m=unsloth/Meta-Llama-3.1-8B; rate=1304.51; inTk=222; outTk=484; sol=1, sat=false, alloc={acc=H100; numRep=2; maxBatch=512; cost=200, val=100, itl=9.7076, ttft=21.305277, rho=0.10000555, maxRPM=725.891}; slo-itl=10, slo-ttft=1000, slo-tps=0 \nAllocationByType: \nname=NVIDIA-H100-80GB-HBM3, count=2, limit=0, cost=200 \ntotalCost=200 \n"}
{"level":"DEBUG","ts":"2025-12-05T23:25:18.952Z","msg":"Setting accelerator name: Name=H100, allocationData={H100 2 512 200 9.7076 21.305277 {1304.51 222 484}}"}
{"level":"INFO","ts":"2025-12-05T23:25:18.952Z","msg":"Model-based optimization completed for model: unsloth/Meta-Llama-3.1-8B - model-based targets: map[ms-inference-scheduling-llm-d-modelservice-decode:2]"}
{"level":"WARN","ts":"2025-12-05T23:25:18.952Z","msg":"saturation analysis unavailable, using model-based targets only: modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-05T23:25:18.952Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-12-05T23:25:18.952Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=scale-up, current=1→target=2"}
{"level":"DEBUG","ts":"2025-12-05T23:25:18.952Z","msg":"Found VA in map: variant=ms-inference-scheduling-llm-d-modelservice-decode, hasCurrentAlloc=true, accelerator=H100"}
{"level":"INFO","ts":"2025-12-05T23:25:18.952Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 1, desired-replicas: 2, accelerator: H100"}
{"level":"INFO","ts":"2025-12-05T23:25:18.952Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=2, accelerator=H100, SaturationOnly=false"}
{"level":"INFO","ts":"2025-12-05T23:25:18.958Z","msg":"Applied Saturation decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=scale-up, current=1, target=2, reason=model-based only (Saturation unavailable)"}
{"level":"INFO","ts":"2025-12-05T23:25:18.958Z","msg":"Reconciliation completed successfully: mode=model-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-12-05T23:25:18.966Z","msg":"saturation scaling config cache updated: entries=1, has_default=true"}
{"level":"INFO","ts":"2025-12-05T23:25:18.967Z","msg":"saturation scaling configuration loaded successfully"}
{"level":"INFO","ts":"2025-12-05T23:25:18.967Z","msg":"Operating in MODEL-ONLY mode: model-based optimization only"}
{"level":"INFO","ts":"2025-12-05T23:25:18.967Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-12-05T23:25:18.967Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-12-05T23:25:18.968Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dczzts, queueLength=0"}
{"level":"DEBUG","ts":"2025-12-05T23:25:18.968Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"INFO","ts":"2025-12-05T23:25:18.968Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dczzts, usage=0.241 (24.1%)"}
{"level":"DEBUG","ts":"2025-12-05T23:25:18.968Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"DEBUG","ts":"2025-12-05T23:25:18.970Z","msg":"Pod-to-variant matching successful: totalPods=1, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"DEBUG","ts":"2025-12-05T23:25:18.970Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=1"}
{"level":"DEBUG","ts":"2025-12-05T23:25:18.970Z","msg":"Collected capacity metrics for observability in model-only mode: modelID=unsloth/Meta-Llama-3.1-8B, replicaCount=1"}
{"level":"INFO","ts":"2025-12-05T23:25:18.970Z","msg":"Found SLO for model: model=unsloth/Meta-Llama-3.1-8B, class=Premium, slo-tpot=10, slo-ttft=1000"}
{"level":"DEBUG","ts":"2025-12-05T23:25:18.978Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=1, accelerator=H100, ttft=31.90ms, itl=16.75ms, cost=100.00, maxBatch=256, arrivalRate=1304.51, avgInputTokens=222.61, avgOutputTokens=484.40"}
{"level":"DEBUG","ts":"2025-12-05T23:25:18.978Z","msg":"Experimental model tuner is enabled globally (EXPERIMENTAL_MODEL_TUNER_ENABLED=true) tuning model performance parameters for active VAs"}
{"level":"DEBUG","ts":"2025-12-05T23:25:18.978Z","msg":"Using state vals from VA status to tune variant ms-inference-scheduling-llm-d-modelservice-decode: alpha= 7.671219, beta= 0.039009, gamma= 17.585199, delta= 0.000321"}
{"level":"DEBUG","ts":"2025-12-05T23:25:18.978Z","msg":"[Tuner Config] variant=ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler | Initial state: alpha=7.671219, beta=0.039009, gamma=17.585199, delta=0.000321 | SLO targets: TTFT=1000.00ms, ITL=10.00ms | Has covariance=true"}
{"level":"WARN","ts":"2025-12-05T23:25:18.979Z","msg":"Tuner validation failed (NIS=8.57), validation error: normalized innovation squared (NIS=8.57) exceeds threshold (7.38), rejecting update as outlier - returning previous state: alpha=7.671219, beta=0.039009, gamma=17.585199, delta=0.000321"}
{"level":"WARN","ts":"2025-12-05T23:25:18.979Z","msg":"Tuner NIS validation failed for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler, server ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler (NIS=8.57 exceeds threshold 7.38) - Keeping previous state: alpha=7.671219, beta=0.039009, gamma=17.585199, delta=0.000321"}
{"level":"INFO","ts":"2025-12-05T23:25:18.979Z","msg":"Keeping previous tuned parameters for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler due to NIS validation failure (NIS=8.573486)"}
{"level":"DEBUG","ts":"2025-12-05T23:25:18.979Z","msg":"Model tuner results: model=unsloth/Meta-Llama-3.1-8B, accelerator=H100, alpha=7.671219, beta=0.039009, gamma=17.585199, delta=0.000321, NIS=8.57"}
{"level":"DEBUG","ts":"2025-12-05T23:25:18.979Z","msg":"Tuned parameters unchanged for variant ms-inference-scheduling-llm-d-modelservice-decode, skipping status update"}
{"level":"DEBUG","ts":"2025-12-05T23:25:18.986Z","msg":"Optimization solution - system: Solution: \ns=ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler; c=Premium; m=unsloth/Meta-Llama-3.1-8B; rate=1304.51; inTk=222; outTk=484; sol=1, sat=false, alloc={acc=H100; numRep=2; maxBatch=512; cost=200, val=100, itl=9.7076, ttft=21.305277, rho=0.10000555, maxRPM=725.891}; slo-itl=10, slo-ttft=1000, slo-tps=0 \nAllocationByType: \nname=NVIDIA-H100-80GB-HBM3, count=2, limit=0, cost=200 \ntotalCost=200 \n"}
{"level":"DEBUG","ts":"2025-12-05T23:25:18.986Z","msg":"Setting accelerator name: Name=H100, allocationData={H100 2 512 200 9.7076 21.305277 {1304.51 222 484}}"}
{"level":"INFO","ts":"2025-12-05T23:25:18.986Z","msg":"Model-based optimization completed for model: unsloth/Meta-Llama-3.1-8B - model-based targets: map[ms-inference-scheduling-llm-d-modelservice-decode:2]"}
{"level":"WARN","ts":"2025-12-05T23:25:18.986Z","msg":"saturation analysis unavailable, using model-based targets only: modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-05T23:25:18.986Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-12-05T23:25:18.986Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=scale-up, current=1→target=2"}
{"level":"DEBUG","ts":"2025-12-05T23:25:18.986Z","msg":"Found VA in map: variant=ms-inference-scheduling-llm-d-modelservice-decode, hasCurrentAlloc=true, accelerator=H100"}
{"level":"INFO","ts":"2025-12-05T23:25:18.986Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 1, desired-replicas: 2, accelerator: H100"}
{"level":"INFO","ts":"2025-12-05T23:25:18.986Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=2, accelerator=H100, SaturationOnly=false"}
{"level":"INFO","ts":"2025-12-05T23:25:18.993Z","msg":"Applied Saturation decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=scale-up, current=1, target=2, reason=model-based only (Saturation unavailable)"}
{"level":"INFO","ts":"2025-12-05T23:25:18.993Z","msg":"Reconciliation completed successfully: mode=model-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-12-05T23:26:18.959Z","msg":"saturation scaling config cache updated: entries=1, has_default=true"}
{"level":"INFO","ts":"2025-12-05T23:26:18.959Z","msg":"saturation scaling configuration loaded successfully"}
{"level":"INFO","ts":"2025-12-05T23:26:18.959Z","msg":"Operating in MODEL-ONLY mode: model-based optimization only"}
{"level":"INFO","ts":"2025-12-05T23:26:18.959Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-12-05T23:26:18.959Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-12-05T23:26:18.968Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dczzts, queueLength=0"}
{"level":"DEBUG","ts":"2025-12-05T23:26:18.968Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"INFO","ts":"2025-12-05T23:26:18.968Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dczzts, usage=0.261 (26.1%)"}
{"level":"DEBUG","ts":"2025-12-05T23:26:18.968Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"DEBUG","ts":"2025-12-05T23:26:18.971Z","msg":"Pod-to-variant matching successful: totalPods=1, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"DEBUG","ts":"2025-12-05T23:26:18.971Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=1"}
{"level":"DEBUG","ts":"2025-12-05T23:26:18.971Z","msg":"Collected capacity metrics for observability in model-only mode: modelID=unsloth/Meta-Llama-3.1-8B, replicaCount=1"}
{"level":"INFO","ts":"2025-12-05T23:26:18.971Z","msg":"Found SLO for model: model=unsloth/Meta-Llama-3.1-8B, class=Premium, slo-tpot=10, slo-ttft=1000"}
{"level":"DEBUG","ts":"2025-12-05T23:26:18.980Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=2, accelerator=H100, ttft=33.40ms, itl=17.56ms, cost=200.00, maxBatch=256, arrivalRate=1383.75, avgInputTokens=223.30, avgOutputTokens=462.70"}
{"level":"DEBUG","ts":"2025-12-05T23:26:18.981Z","msg":"Experimental model tuner is enabled globally (EXPERIMENTAL_MODEL_TUNER_ENABLED=true) tuning model performance parameters for active VAs"}
{"level":"DEBUG","ts":"2025-12-05T23:26:18.981Z","msg":"Using state vals from VA status to tune variant ms-inference-scheduling-llm-d-modelservice-decode: alpha= 7.671219, beta= 0.039009, gamma= 17.585199, delta= 0.000321"}
{"level":"DEBUG","ts":"2025-12-05T23:26:18.981Z","msg":"[Tuner Config] variant=ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler | Initial state: alpha=7.671219, beta=0.039009, gamma=17.585199, delta=0.000321 | SLO targets: TTFT=1000.00ms, ITL=10.00ms | Has covariance=true"}
{"level":"WARN","ts":"2025-12-05T23:26:18.983Z","msg":"Tuner validation failed (NIS=160.73), validation error: normalized innovation squared (NIS=160.73) exceeds threshold (7.38), rejecting update as outlier - returning previous state: alpha=7.671219, beta=0.039009, gamma=17.585199, delta=0.000321"}
{"level":"WARN","ts":"2025-12-05T23:26:18.983Z","msg":"Tuner NIS validation failed for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler, server ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler (NIS=160.73 exceeds threshold 7.38) - Keeping previous state: alpha=7.671219, beta=0.039009, gamma=17.585199, delta=0.000321"}
{"level":"INFO","ts":"2025-12-05T23:26:18.983Z","msg":"Keeping previous tuned parameters for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler due to NIS validation failure (NIS=160.726846)"}
{"level":"DEBUG","ts":"2025-12-05T23:26:18.983Z","msg":"Model tuner results: model=unsloth/Meta-Llama-3.1-8B, accelerator=H100, alpha=7.671219, beta=0.039009, gamma=17.585199, delta=0.000321, NIS=160.73"}
{"level":"DEBUG","ts":"2025-12-05T23:26:18.983Z","msg":"Updated tuner status for variant ms-inference-scheduling-llm-d-modelservice-decode: alpha=7.671219, beta=0.039009, gamma=17.585199, delta=0.000321, NIS=160.726846"}
{"level":"DEBUG","ts":"2025-12-05T23:26:18.997Z","msg":"Optimization solution - system: Solution: \ns=ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler; c=Premium; m=unsloth/Meta-Llama-3.1-8B; rate=1383.75; inTk=223; outTk=462; sol=1, sat=false, alloc={acc=H100; numRep=2; maxBatch=512; cost=200, val=0, itl=9.739511, ttft=21.380594, rho=0.101603255, maxRPM=760.367}; slo-itl=10, slo-ttft=1000, slo-tps=0 \nAllocationByType: \nname=NVIDIA-H100-80GB-HBM3, count=2, limit=0, cost=200 \ntotalCost=200 \n"}
{"level":"DEBUG","ts":"2025-12-05T23:26:18.998Z","msg":"Setting accelerator name: Name=H100, allocationData={H100 2 512 200 9.739511 21.380594 {1383.75 223 462}}"}
{"level":"INFO","ts":"2025-12-05T23:26:18.998Z","msg":"Model-based optimization completed for model: unsloth/Meta-Llama-3.1-8B - model-based targets: map[ms-inference-scheduling-llm-d-modelservice-decode:2]"}
{"level":"WARN","ts":"2025-12-05T23:26:18.998Z","msg":"saturation analysis unavailable, using model-based targets only: modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-05T23:26:18.998Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-12-05T23:26:18.998Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=2→target=2"}
{"level":"DEBUG","ts":"2025-12-05T23:26:18.998Z","msg":"Found VA in map: variant=ms-inference-scheduling-llm-d-modelservice-decode, hasCurrentAlloc=true, accelerator=H100"}
{"level":"INFO","ts":"2025-12-05T23:26:18.998Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 2, desired-replicas: 2, accelerator: H100"}
{"level":"INFO","ts":"2025-12-05T23:26:18.998Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=2, accelerator=H100, SaturationOnly=false"}
{"level":"INFO","ts":"2025-12-05T23:26:19.007Z","msg":"Applied Saturation decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=2, target=2, reason=model-based only (Saturation unavailable)"}
{"level":"INFO","ts":"2025-12-05T23:26:19.007Z","msg":"Reconciliation completed successfully: mode=model-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-12-05T23:26:19.008Z","msg":"saturation scaling config cache updated: entries=1, has_default=true"}
{"level":"INFO","ts":"2025-12-05T23:26:19.008Z","msg":"saturation scaling configuration loaded successfully"}
{"level":"INFO","ts":"2025-12-05T23:26:19.008Z","msg":"Operating in MODEL-ONLY mode: model-based optimization only"}
{"level":"INFO","ts":"2025-12-05T23:26:19.008Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-12-05T23:26:19.008Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-12-05T23:26:19.011Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dczzts, queueLength=0"}
{"level":"DEBUG","ts":"2025-12-05T23:26:19.011Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"INFO","ts":"2025-12-05T23:26:19.012Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dczzts, usage=0.261 (26.1%)"}
{"level":"DEBUG","ts":"2025-12-05T23:26:19.013Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"DEBUG","ts":"2025-12-05T23:26:19.019Z","msg":"Pod-to-variant matching successful: totalPods=1, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"DEBUG","ts":"2025-12-05T23:26:19.019Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=1"}
{"level":"DEBUG","ts":"2025-12-05T23:26:19.019Z","msg":"Collected capacity metrics for observability in model-only mode: modelID=unsloth/Meta-Llama-3.1-8B, replicaCount=1"}
{"level":"INFO","ts":"2025-12-05T23:26:19.019Z","msg":"Found SLO for model: model=unsloth/Meta-Llama-3.1-8B, class=Premium, slo-tpot=10, slo-ttft=1000"}
{"level":"DEBUG","ts":"2025-12-05T23:26:19.029Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=2, accelerator=H100, ttft=33.40ms, itl=17.56ms, cost=200.00, maxBatch=256, arrivalRate=1383.75, avgInputTokens=223.30, avgOutputTokens=462.70"}
{"level":"DEBUG","ts":"2025-12-05T23:26:19.029Z","msg":"Experimental model tuner is enabled globally (EXPERIMENTAL_MODEL_TUNER_ENABLED=true) tuning model performance parameters for active VAs"}
{"level":"DEBUG","ts":"2025-12-05T23:26:19.029Z","msg":"Using state vals from VA status to tune variant ms-inference-scheduling-llm-d-modelservice-decode: alpha= 7.671219, beta= 0.039009, gamma= 17.585199, delta= 0.000321"}
{"level":"DEBUG","ts":"2025-12-05T23:26:19.030Z","msg":"[Tuner Config] variant=ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler | Initial state: alpha=7.671219, beta=0.039009, gamma=17.585199, delta=0.000321 | SLO targets: TTFT=1000.00ms, ITL=10.00ms | Has covariance=true"}
{"level":"WARN","ts":"2025-12-05T23:26:19.030Z","msg":"Tuner validation failed (NIS=160.73), validation error: normalized innovation squared (NIS=160.73) exceeds threshold (7.38), rejecting update as outlier - returning previous state: alpha=7.671219, beta=0.039009, gamma=17.585199, delta=0.000321"}
{"level":"WARN","ts":"2025-12-05T23:26:19.030Z","msg":"Tuner NIS validation failed for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler, server ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler (NIS=160.73 exceeds threshold 7.38) - Keeping previous state: alpha=7.671219, beta=0.039009, gamma=17.585199, delta=0.000321"}
{"level":"INFO","ts":"2025-12-05T23:26:19.030Z","msg":"Keeping previous tuned parameters for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler due to NIS validation failure (NIS=160.726846)"}
{"level":"DEBUG","ts":"2025-12-05T23:26:19.030Z","msg":"Model tuner results: model=unsloth/Meta-Llama-3.1-8B, accelerator=H100, alpha=7.671219, beta=0.039009, gamma=17.585199, delta=0.000321, NIS=160.73"}
{"level":"DEBUG","ts":"2025-12-05T23:26:19.030Z","msg":"Tuned parameters unchanged for variant ms-inference-scheduling-llm-d-modelservice-decode, skipping status update"}
{"level":"DEBUG","ts":"2025-12-05T23:26:19.038Z","msg":"Optimization solution - system: Solution: \ns=ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler; c=Premium; m=unsloth/Meta-Llama-3.1-8B; rate=1383.75; inTk=223; outTk=462; sol=1, sat=false, alloc={acc=H100; numRep=2; maxBatch=512; cost=200, val=0, itl=9.739511, ttft=21.380594, rho=0.101603255, maxRPM=760.367}; slo-itl=10, slo-ttft=1000, slo-tps=0 \nAllocationByType: \nname=NVIDIA-H100-80GB-HBM3, count=2, limit=0, cost=200 \ntotalCost=200 \n"}
{"level":"DEBUG","ts":"2025-12-05T23:26:19.038Z","msg":"Setting accelerator name: Name=H100, allocationData={H100 2 512 200 9.739511 21.380594 {1383.75 223 462}}"}
{"level":"INFO","ts":"2025-12-05T23:26:19.038Z","msg":"Model-based optimization completed for model: unsloth/Meta-Llama-3.1-8B - model-based targets: map[ms-inference-scheduling-llm-d-modelservice-decode:2]"}
{"level":"WARN","ts":"2025-12-05T23:26:19.038Z","msg":"saturation analysis unavailable, using model-based targets only: modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-05T23:26:19.038Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-12-05T23:26:19.038Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=2→target=2"}
{"level":"DEBUG","ts":"2025-12-05T23:26:19.038Z","msg":"Found VA in map: variant=ms-inference-scheduling-llm-d-modelservice-decode, hasCurrentAlloc=true, accelerator=H100"}
{"level":"INFO","ts":"2025-12-05T23:26:19.038Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 2, desired-replicas: 2, accelerator: H100"}
{"level":"INFO","ts":"2025-12-05T23:26:19.038Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=2, accelerator=H100, SaturationOnly=false"}
{"level":"INFO","ts":"2025-12-05T23:26:19.045Z","msg":"Applied Saturation decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=2, target=2, reason=model-based only (Saturation unavailable)"}
{"level":"INFO","ts":"2025-12-05T23:26:19.045Z","msg":"Reconciliation completed successfully: mode=model-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-12-05T23:27:19.008Z","msg":"saturation scaling config cache updated: entries=1, has_default=true"}
{"level":"INFO","ts":"2025-12-05T23:27:19.008Z","msg":"saturation scaling configuration loaded successfully"}
{"level":"INFO","ts":"2025-12-05T23:27:19.008Z","msg":"Operating in MODEL-ONLY mode: model-based optimization only"}
{"level":"INFO","ts":"2025-12-05T23:27:19.008Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-12-05T23:27:19.008Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-12-05T23:27:19.013Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dczzts, usage=0.355 (35.5%)"}
{"level":"DEBUG","ts":"2025-12-05T23:27:19.013Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"INFO","ts":"2025-12-05T23:27:19.013Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dczzts, queueLength=0"}
{"level":"DEBUG","ts":"2025-12-05T23:27:19.013Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"DEBUG","ts":"2025-12-05T23:27:19.015Z","msg":"Pod-to-variant matching successful: totalPods=1, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"DEBUG","ts":"2025-12-05T23:27:19.015Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=1"}
{"level":"DEBUG","ts":"2025-12-05T23:27:19.015Z","msg":"Collected capacity metrics for observability in model-only mode: modelID=unsloth/Meta-Llama-3.1-8B, replicaCount=1"}
{"level":"INFO","ts":"2025-12-05T23:27:19.016Z","msg":"Found SLO for model: model=unsloth/Meta-Llama-3.1-8B, class=Premium, slo-tpot=10, slo-ttft=1000"}
{"level":"DEBUG","ts":"2025-12-05T23:27:19.025Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=2, accelerator=H100, ttft=33.94ms, itl=17.55ms, cost=200.00, maxBatch=256, arrivalRate=1404.29, avgInputTokens=253.63, avgOutputTokens=383.56"}
{"level":"DEBUG","ts":"2025-12-05T23:27:19.025Z","msg":"Experimental model tuner is enabled globally (EXPERIMENTAL_MODEL_TUNER_ENABLED=true) tuning model performance parameters for active VAs"}
{"level":"DEBUG","ts":"2025-12-05T23:27:19.025Z","msg":"Using state vals from VA status to tune variant ms-inference-scheduling-llm-d-modelservice-decode: alpha= 7.671219, beta= 0.039009, gamma= 17.585199, delta= 0.000321"}
{"level":"DEBUG","ts":"2025-12-05T23:27:19.025Z","msg":"[Tuner Config] variant=ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler | Initial state: alpha=7.671219, beta=0.039009, gamma=17.585199, delta=0.000321 | SLO targets: TTFT=1000.00ms, ITL=10.00ms | Has covariance=true"}
{"level":"WARN","ts":"2025-12-05T23:27:19.026Z","msg":"Tuner validation failed (NIS=194.98), validation error: normalized innovation squared (NIS=194.98) exceeds threshold (7.38), rejecting update as outlier - returning previous state: alpha=7.671219, beta=0.039009, gamma=17.585199, delta=0.000321"}
{"level":"WARN","ts":"2025-12-05T23:27:19.026Z","msg":"Tuner NIS validation failed for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler, server ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler (NIS=194.98 exceeds threshold 7.38) - Keeping previous state: alpha=7.671219, beta=0.039009, gamma=17.585199, delta=0.000321"}
{"level":"INFO","ts":"2025-12-05T23:27:19.026Z","msg":"Keeping previous tuned parameters for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler due to NIS validation failure (NIS=194.981538)"}
{"level":"DEBUG","ts":"2025-12-05T23:27:19.026Z","msg":"Model tuner results: model=unsloth/Meta-Llama-3.1-8B, accelerator=H100, alpha=7.671219, beta=0.039009, gamma=17.585199, delta=0.000321, NIS=194.98"}
{"level":"DEBUG","ts":"2025-12-05T23:27:19.026Z","msg":"Updated tuner status for variant ms-inference-scheduling-llm-d-modelservice-decode: alpha=7.671219, beta=0.039009, gamma=17.585199, delta=0.000321, NIS=194.981538"}
{"level":"DEBUG","ts":"2025-12-05T23:27:19.034Z","msg":"Optimization solution - system: Solution: \ns=ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler; c=Premium; m=unsloth/Meta-Llama-3.1-8B; rate=1404.29; inTk=253; outTk=383; sol=1, sat=false, alloc={acc=H100; numRep=2; maxBatch=512; cost=200, val=0, itl=9.350401, ttft=21.081097, rho=0.08212113, maxRPM=916.5849}; slo-itl=10, slo-ttft=1000, slo-tps=0 \nAllocationByType: \nname=NVIDIA-H100-80GB-HBM3, count=2, limit=0, cost=200 \ntotalCost=200 \n"}
{"level":"DEBUG","ts":"2025-12-05T23:27:19.034Z","msg":"Setting accelerator name: Name=H100, allocationData={H100 2 512 200 9.350401 21.081097 {1404.29 253 383}}"}
{"level":"INFO","ts":"2025-12-05T23:27:19.034Z","msg":"Model-based optimization completed for model: unsloth/Meta-Llama-3.1-8B - model-based targets: map[ms-inference-scheduling-llm-d-modelservice-decode:2]"}
{"level":"WARN","ts":"2025-12-05T23:27:19.034Z","msg":"saturation analysis unavailable, using model-based targets only: modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-05T23:27:19.034Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-12-05T23:27:19.034Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=2→target=2"}
{"level":"DEBUG","ts":"2025-12-05T23:27:19.034Z","msg":"Found VA in map: variant=ms-inference-scheduling-llm-d-modelservice-decode, hasCurrentAlloc=true, accelerator=H100"}
{"level":"INFO","ts":"2025-12-05T23:27:19.034Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 2, desired-replicas: 2, accelerator: H100"}
{"level":"INFO","ts":"2025-12-05T23:27:19.034Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=2, accelerator=H100, SaturationOnly=false"}
{"level":"INFO","ts":"2025-12-05T23:27:19.041Z","msg":"Applied Saturation decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=2, target=2, reason=model-based only (Saturation unavailable)"}
{"level":"INFO","ts":"2025-12-05T23:27:19.041Z","msg":"Reconciliation completed successfully: mode=model-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-12-05T23:27:19.046Z","msg":"saturation scaling config cache updated: entries=1, has_default=true"}
{"level":"INFO","ts":"2025-12-05T23:27:19.046Z","msg":"saturation scaling configuration loaded successfully"}
{"level":"INFO","ts":"2025-12-05T23:27:19.046Z","msg":"Operating in MODEL-ONLY mode: model-based optimization only"}
{"level":"INFO","ts":"2025-12-05T23:27:19.046Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-12-05T23:27:19.046Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-12-05T23:27:19.048Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dczzts, usage=0.355 (35.5%)"}
{"level":"DEBUG","ts":"2025-12-05T23:27:19.048Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"INFO","ts":"2025-12-05T23:27:19.048Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dczzts, queueLength=0"}
{"level":"DEBUG","ts":"2025-12-05T23:27:19.048Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"DEBUG","ts":"2025-12-05T23:27:19.050Z","msg":"Pod-to-variant matching successful: totalPods=1, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"DEBUG","ts":"2025-12-05T23:27:19.050Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=1"}
{"level":"DEBUG","ts":"2025-12-05T23:27:19.050Z","msg":"Collected capacity metrics for observability in model-only mode: modelID=unsloth/Meta-Llama-3.1-8B, replicaCount=1"}
{"level":"INFO","ts":"2025-12-05T23:27:19.050Z","msg":"Found SLO for model: model=unsloth/Meta-Llama-3.1-8B, class=Premium, slo-tpot=10, slo-ttft=1000"}
{"level":"DEBUG","ts":"2025-12-05T23:27:19.058Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=2, accelerator=H100, ttft=33.94ms, itl=17.55ms, cost=200.00, maxBatch=256, arrivalRate=1404.29, avgInputTokens=253.63, avgOutputTokens=383.56"}
{"level":"DEBUG","ts":"2025-12-05T23:27:19.058Z","msg":"Experimental model tuner is enabled globally (EXPERIMENTAL_MODEL_TUNER_ENABLED=true) tuning model performance parameters for active VAs"}
{"level":"DEBUG","ts":"2025-12-05T23:27:19.058Z","msg":"Using state vals from VA status to tune variant ms-inference-scheduling-llm-d-modelservice-decode: alpha= 7.671219, beta= 0.039009, gamma= 17.585199, delta= 0.000321"}
{"level":"DEBUG","ts":"2025-12-05T23:27:19.058Z","msg":"[Tuner Config] variant=ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler | Initial state: alpha=7.671219, beta=0.039009, gamma=17.585199, delta=0.000321 | SLO targets: TTFT=1000.00ms, ITL=10.00ms | Has covariance=true"}
{"level":"WARN","ts":"2025-12-05T23:27:19.059Z","msg":"Tuner validation failed (NIS=194.98), validation error: normalized innovation squared (NIS=194.98) exceeds threshold (7.38), rejecting update as outlier - returning previous state: alpha=7.671219, beta=0.039009, gamma=17.585199, delta=0.000321"}
{"level":"WARN","ts":"2025-12-05T23:27:19.059Z","msg":"Tuner NIS validation failed for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler, server ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler (NIS=194.98 exceeds threshold 7.38) - Keeping previous state: alpha=7.671219, beta=0.039009, gamma=17.585199, delta=0.000321"}
{"level":"INFO","ts":"2025-12-05T23:27:19.059Z","msg":"Keeping previous tuned parameters for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler due to NIS validation failure (NIS=194.981538)"}
{"level":"DEBUG","ts":"2025-12-05T23:27:19.059Z","msg":"Model tuner results: model=unsloth/Meta-Llama-3.1-8B, accelerator=H100, alpha=7.671219, beta=0.039009, gamma=17.585199, delta=0.000321, NIS=194.98"}
{"level":"DEBUG","ts":"2025-12-05T23:27:19.059Z","msg":"Tuned parameters unchanged for variant ms-inference-scheduling-llm-d-modelservice-decode, skipping status update"}
{"level":"DEBUG","ts":"2025-12-05T23:27:19.067Z","msg":"Optimization solution - system: Solution: \ns=ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler; c=Premium; m=unsloth/Meta-Llama-3.1-8B; rate=1404.29; inTk=253; outTk=383; sol=1, sat=false, alloc={acc=H100; numRep=2; maxBatch=512; cost=200, val=0, itl=9.350401, ttft=21.081097, rho=0.08212113, maxRPM=916.5849}; slo-itl=10, slo-ttft=1000, slo-tps=0 \nAllocationByType: \nname=NVIDIA-H100-80GB-HBM3, count=2, limit=0, cost=200 \ntotalCost=200 \n"}
{"level":"DEBUG","ts":"2025-12-05T23:27:19.067Z","msg":"Setting accelerator name: Name=H100, allocationData={H100 2 512 200 9.350401 21.081097 {1404.29 253 383}}"}
{"level":"INFO","ts":"2025-12-05T23:27:19.067Z","msg":"Model-based optimization completed for model: unsloth/Meta-Llama-3.1-8B - model-based targets: map[ms-inference-scheduling-llm-d-modelservice-decode:2]"}
{"level":"WARN","ts":"2025-12-05T23:27:19.067Z","msg":"saturation analysis unavailable, using model-based targets only: modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-05T23:27:19.067Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-12-05T23:27:19.067Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=2→target=2"}
{"level":"DEBUG","ts":"2025-12-05T23:27:19.067Z","msg":"Found VA in map: variant=ms-inference-scheduling-llm-d-modelservice-decode, hasCurrentAlloc=true, accelerator=H100"}
{"level":"INFO","ts":"2025-12-05T23:27:19.067Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 2, desired-replicas: 2, accelerator: H100"}
{"level":"INFO","ts":"2025-12-05T23:27:19.067Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=2, accelerator=H100, SaturationOnly=false"}
{"level":"INFO","ts":"2025-12-05T23:27:19.073Z","msg":"Applied Saturation decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=2, target=2, reason=model-based only (Saturation unavailable)"}
{"level":"INFO","ts":"2025-12-05T23:27:19.073Z","msg":"Reconciliation completed successfully: mode=model-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-12-05T23:28:19.041Z","msg":"saturation scaling config cache updated: entries=1, has_default=true"}
{"level":"INFO","ts":"2025-12-05T23:28:19.041Z","msg":"saturation scaling configuration loaded successfully"}
{"level":"INFO","ts":"2025-12-05T23:28:19.041Z","msg":"Operating in MODEL-ONLY mode: model-based optimization only"}
{"level":"INFO","ts":"2025-12-05T23:28:19.041Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-12-05T23:28:19.041Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-12-05T23:28:19.049Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dgzckd, queueLength=0"}
{"level":"INFO","ts":"2025-12-05T23:28:19.049Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dczzts, queueLength=0"}
{"level":"DEBUG","ts":"2025-12-05T23:28:19.049Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=2"}
{"level":"INFO","ts":"2025-12-05T23:28:19.049Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dgzckd, usage=0.093 (9.3%)"}
{"level":"INFO","ts":"2025-12-05T23:28:19.049Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dczzts, usage=0.523 (52.3%)"}
{"level":"DEBUG","ts":"2025-12-05T23:28:19.049Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=2"}
{"level":"DEBUG","ts":"2025-12-05T23:28:19.054Z","msg":"Pod-to-variant matching successful: totalPods=2, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:2]"}
{"level":"DEBUG","ts":"2025-12-05T23:28:19.054Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=2"}
{"level":"DEBUG","ts":"2025-12-05T23:28:19.054Z","msg":"Collected capacity metrics for observability in model-only mode: modelID=unsloth/Meta-Llama-3.1-8B, replicaCount=2"}
{"level":"INFO","ts":"2025-12-05T23:28:19.054Z","msg":"Found SLO for model: model=unsloth/Meta-Llama-3.1-8B, class=Premium, slo-tpot=10, slo-ttft=1000"}
{"level":"DEBUG","ts":"2025-12-05T23:28:19.063Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=2, accelerator=H100, ttft=33.22ms, itl=19.99ms, cost=200.00, maxBatch=256, arrivalRate=2062.27, avgInputTokens=214.71, avgOutputTokens=528.77"}
{"level":"DEBUG","ts":"2025-12-05T23:28:19.063Z","msg":"Experimental model tuner is enabled globally (EXPERIMENTAL_MODEL_TUNER_ENABLED=true) tuning model performance parameters for active VAs"}
{"level":"DEBUG","ts":"2025-12-05T23:28:19.063Z","msg":"Using state vals from VA status to tune variant ms-inference-scheduling-llm-d-modelservice-decode: alpha= 7.671219, beta= 0.039009, gamma= 17.585199, delta= 0.000321"}
{"level":"DEBUG","ts":"2025-12-05T23:28:19.063Z","msg":"[Tuner Config] variant=ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler | Initial state: alpha=7.671219, beta=0.039009, gamma=17.585199, delta=0.000321 | SLO targets: TTFT=1000.00ms, ITL=10.00ms | Has covariance=true"}
{"level":"WARN","ts":"2025-12-05T23:28:19.065Z","msg":"Tuner validation failed (NIS=70.32), validation error: normalized innovation squared (NIS=70.32) exceeds threshold (7.38), rejecting update as outlier - returning previous state: alpha=7.671219, beta=0.039009, gamma=17.585199, delta=0.000321"}
{"level":"WARN","ts":"2025-12-05T23:28:19.065Z","msg":"Tuner NIS validation failed for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler, server ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler (NIS=70.32 exceeds threshold 7.38) - Keeping previous state: alpha=7.671219, beta=0.039009, gamma=17.585199, delta=0.000321"}
{"level":"INFO","ts":"2025-12-05T23:28:19.065Z","msg":"Keeping previous tuned parameters for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler due to NIS validation failure (NIS=70.319381)"}
{"level":"DEBUG","ts":"2025-12-05T23:28:19.065Z","msg":"Model tuner results: model=unsloth/Meta-Llama-3.1-8B, accelerator=H100, alpha=7.671219, beta=0.039009, gamma=17.585199, delta=0.000321, NIS=70.32"}
{"level":"DEBUG","ts":"2025-12-05T23:28:19.065Z","msg":"Updated tuner status for variant ms-inference-scheduling-llm-d-modelservice-decode: alpha=7.671219, beta=0.039009, gamma=17.585199, delta=0.000321, NIS=70.319381"}
{"level":"DEBUG","ts":"2025-12-05T23:28:19.074Z","msg":"Optimization solution - system: Solution: \ns=ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler; c=Premium; m=unsloth/Meta-Llama-3.1-8B; rate=2062.27; inTk=214; outTk=528; sol=1, sat=false, alloc={acc=H100; numRep=4; maxBatch=512; cost=400, val=200, itl=9.372821, ttft=20.581682, rho=0.083243646, maxRPM=665.5573}; slo-itl=10, slo-ttft=1000, slo-tps=0 \nAllocationByType: \nname=NVIDIA-H100-80GB-HBM3, count=4, limit=0, cost=400 \ntotalCost=400 \n"}
{"level":"DEBUG","ts":"2025-12-05T23:28:19.074Z","msg":"Setting accelerator name: Name=H100, allocationData={H100 4 512 400 9.372821 20.581682 {2062.27 214 528}}"}
{"level":"INFO","ts":"2025-12-05T23:28:19.074Z","msg":"Model-based optimization completed for model: unsloth/Meta-Llama-3.1-8B - model-based targets: map[ms-inference-scheduling-llm-d-modelservice-decode:4]"}
{"level":"WARN","ts":"2025-12-05T23:28:19.074Z","msg":"saturation analysis unavailable, using model-based targets only: modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-05T23:28:19.074Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-12-05T23:28:19.074Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=scale-up, current=2→target=4"}
{"level":"DEBUG","ts":"2025-12-05T23:28:19.074Z","msg":"Found VA in map: variant=ms-inference-scheduling-llm-d-modelservice-decode, hasCurrentAlloc=true, accelerator=H100"}
{"level":"INFO","ts":"2025-12-05T23:28:19.074Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 2, desired-replicas: 4, accelerator: H100"}
{"level":"INFO","ts":"2025-12-05T23:28:19.074Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=4, accelerator=H100, SaturationOnly=false"}
{"level":"INFO","ts":"2025-12-05T23:28:19.079Z","msg":"Applied Saturation decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=scale-up, current=2, target=4, reason=model-based only (Saturation unavailable)"}
{"level":"INFO","ts":"2025-12-05T23:28:19.079Z","msg":"Reconciliation completed successfully: mode=model-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-12-05T23:28:19.079Z","msg":"saturation scaling config cache updated: entries=1, has_default=true"}
{"level":"INFO","ts":"2025-12-05T23:28:19.079Z","msg":"saturation scaling configuration loaded successfully"}
{"level":"INFO","ts":"2025-12-05T23:28:19.079Z","msg":"Operating in MODEL-ONLY mode: model-based optimization only"}
{"level":"INFO","ts":"2025-12-05T23:28:19.079Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-12-05T23:28:19.079Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-12-05T23:28:19.080Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dgzckd, queueLength=0"}
{"level":"INFO","ts":"2025-12-05T23:28:19.080Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dczzts, queueLength=0"}
{"level":"DEBUG","ts":"2025-12-05T23:28:19.080Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=2"}
{"level":"INFO","ts":"2025-12-05T23:28:19.081Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dgzckd, usage=0.093 (9.3%)"}
{"level":"INFO","ts":"2025-12-05T23:28:19.081Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dczzts, usage=0.523 (52.3%)"}
{"level":"DEBUG","ts":"2025-12-05T23:28:19.081Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=2"}
{"level":"DEBUG","ts":"2025-12-05T23:28:19.083Z","msg":"Pod-to-variant matching successful: totalPods=2, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:2]"}
{"level":"DEBUG","ts":"2025-12-05T23:28:19.083Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=2"}
{"level":"DEBUG","ts":"2025-12-05T23:28:19.083Z","msg":"Collected capacity metrics for observability in model-only mode: modelID=unsloth/Meta-Llama-3.1-8B, replicaCount=2"}
{"level":"INFO","ts":"2025-12-05T23:28:19.083Z","msg":"Found SLO for model: model=unsloth/Meta-Llama-3.1-8B, class=Premium, slo-tpot=10, slo-ttft=1000"}
{"level":"DEBUG","ts":"2025-12-05T23:28:19.091Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=2, accelerator=H100, ttft=33.22ms, itl=19.99ms, cost=200.00, maxBatch=256, arrivalRate=2062.70, avgInputTokens=214.71, avgOutputTokens=528.74"}
{"level":"DEBUG","ts":"2025-12-05T23:28:19.091Z","msg":"Experimental model tuner is enabled globally (EXPERIMENTAL_MODEL_TUNER_ENABLED=true) tuning model performance parameters for active VAs"}
{"level":"DEBUG","ts":"2025-12-05T23:28:19.091Z","msg":"Using state vals from VA status to tune variant ms-inference-scheduling-llm-d-modelservice-decode: alpha= 7.671219, beta= 0.039009, gamma= 17.585199, delta= 0.000321"}
{"level":"DEBUG","ts":"2025-12-05T23:28:19.091Z","msg":"[Tuner Config] variant=ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler | Initial state: alpha=7.671219, beta=0.039009, gamma=17.585199, delta=0.000321 | SLO targets: TTFT=1000.00ms, ITL=10.00ms | Has covariance=true"}
{"level":"WARN","ts":"2025-12-05T23:28:19.092Z","msg":"Tuner validation failed (NIS=70.25), validation error: normalized innovation squared (NIS=70.25) exceeds threshold (7.38), rejecting update as outlier - returning previous state: alpha=7.671219, beta=0.039009, gamma=17.585199, delta=0.000321"}
{"level":"WARN","ts":"2025-12-05T23:28:19.093Z","msg":"Tuner NIS validation failed for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler, server ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler (NIS=70.25 exceeds threshold 7.38) - Keeping previous state: alpha=7.671219, beta=0.039009, gamma=17.585199, delta=0.000321"}
{"level":"INFO","ts":"2025-12-05T23:28:19.093Z","msg":"Keeping previous tuned parameters for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler due to NIS validation failure (NIS=70.251410)"}
{"level":"DEBUG","ts":"2025-12-05T23:28:19.093Z","msg":"Model tuner results: model=unsloth/Meta-Llama-3.1-8B, accelerator=H100, alpha=7.671219, beta=0.039009, gamma=17.585199, delta=0.000321, NIS=70.25"}
{"level":"DEBUG","ts":"2025-12-05T23:28:19.093Z","msg":"Updated tuner status for variant ms-inference-scheduling-llm-d-modelservice-decode: alpha=7.671219, beta=0.039009, gamma=17.585199, delta=0.000321, NIS=70.251410"}
{"level":"DEBUG","ts":"2025-12-05T23:28:19.101Z","msg":"Optimization solution - system: Solution: \ns=ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler; c=Premium; m=unsloth/Meta-Llama-3.1-8B; rate=2062.7; inTk=214; outTk=528; sol=1, sat=false, alloc={acc=H100; numRep=4; maxBatch=512; cost=400, val=200, itl=9.373241, ttft=20.582424, rho=0.083264746, maxRPM=665.5573}; slo-itl=10, slo-ttft=1000, slo-tps=0 \nAllocationByType: \nname=NVIDIA-H100-80GB-HBM3, count=4, limit=0, cost=400 \ntotalCost=400 \n"}
{"level":"DEBUG","ts":"2025-12-05T23:28:19.101Z","msg":"Setting accelerator name: Name=H100, allocationData={H100 4 512 400 9.373241 20.582424 {2062.7 214 528}}"}
{"level":"INFO","ts":"2025-12-05T23:28:19.101Z","msg":"Model-based optimization completed for model: unsloth/Meta-Llama-3.1-8B - model-based targets: map[ms-inference-scheduling-llm-d-modelservice-decode:4]"}
{"level":"WARN","ts":"2025-12-05T23:28:19.101Z","msg":"saturation analysis unavailable, using model-based targets only: modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-05T23:28:19.101Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-12-05T23:28:19.101Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=scale-up, current=2→target=4"}
{"level":"DEBUG","ts":"2025-12-05T23:28:19.101Z","msg":"Found VA in map: variant=ms-inference-scheduling-llm-d-modelservice-decode, hasCurrentAlloc=true, accelerator=H100"}
{"level":"INFO","ts":"2025-12-05T23:28:19.101Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 2, desired-replicas: 4, accelerator: H100"}
{"level":"INFO","ts":"2025-12-05T23:28:19.101Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=4, accelerator=H100, SaturationOnly=false"}
{"level":"INFO","ts":"2025-12-05T23:28:19.107Z","msg":"Applied Saturation decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=scale-up, current=2, target=4, reason=model-based only (Saturation unavailable)"}
{"level":"INFO","ts":"2025-12-05T23:28:19.107Z","msg":"Reconciliation completed successfully: mode=model-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-12-05T23:29:19.080Z","msg":"saturation scaling config cache updated: entries=1, has_default=true"}
{"level":"INFO","ts":"2025-12-05T23:29:19.080Z","msg":"saturation scaling configuration loaded successfully"}
{"level":"INFO","ts":"2025-12-05T23:29:19.080Z","msg":"Operating in MODEL-ONLY mode: model-based optimization only"}
{"level":"INFO","ts":"2025-12-05T23:29:19.080Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-12-05T23:29:19.080Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-12-05T23:29:19.084Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dgzckd, queueLength=0"}
{"level":"INFO","ts":"2025-12-05T23:29:19.084Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dczzts, queueLength=0"}
{"level":"DEBUG","ts":"2025-12-05T23:29:19.084Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=2"}
{"level":"INFO","ts":"2025-12-05T23:29:19.084Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dgzckd, usage=0.127 (12.7%)"}
{"level":"INFO","ts":"2025-12-05T23:29:19.084Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dczzts, usage=0.136 (13.6%)"}
{"level":"DEBUG","ts":"2025-12-05T23:29:19.084Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=2"}
{"level":"DEBUG","ts":"2025-12-05T23:29:19.087Z","msg":"Pod-to-variant matching successful: totalPods=2, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:2]"}
{"level":"DEBUG","ts":"2025-12-05T23:29:19.087Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=2"}
{"level":"DEBUG","ts":"2025-12-05T23:29:19.087Z","msg":"Collected capacity metrics for observability in model-only mode: modelID=unsloth/Meta-Llama-3.1-8B, replicaCount=2"}
{"level":"INFO","ts":"2025-12-05T23:29:19.088Z","msg":"Found SLO for model: model=unsloth/Meta-Llama-3.1-8B, class=Premium, slo-tpot=10, slo-ttft=1000"}
{"level":"DEBUG","ts":"2025-12-05T23:29:19.097Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=4, accelerator=H100, ttft=22.41ms, itl=11.12ms, cost=400.00, maxBatch=256, arrivalRate=1898.32, avgInputTokens=232.65, avgOutputTokens=443.75"}
{"level":"DEBUG","ts":"2025-12-05T23:29:19.097Z","msg":"Experimental model tuner is enabled globally (EXPERIMENTAL_MODEL_TUNER_ENABLED=true) tuning model performance parameters for active VAs"}
{"level":"DEBUG","ts":"2025-12-05T23:29:19.097Z","msg":"Using state vals from VA status to tune variant ms-inference-scheduling-llm-d-modelservice-decode: alpha= 7.671219, beta= 0.039009, gamma= 17.585199, delta= 0.000321"}
{"level":"DEBUG","ts":"2025-12-05T23:29:19.097Z","msg":"[Tuner Config] variant=ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler | Initial state: alpha=7.671219, beta=0.039009, gamma=17.585199, delta=0.000321 | SLO targets: TTFT=1000.00ms, ITL=10.00ms | Has covariance=true"}
{"level":"WARN","ts":"2025-12-05T23:29:19.097Z","msg":"Tuner validation failed (NIS=14.96), validation error: normalized innovation squared (NIS=14.96) exceeds threshold (7.38), rejecting update as outlier - returning previous state: alpha=7.671219, beta=0.039009, gamma=17.585199, delta=0.000321"}
{"level":"WARN","ts":"2025-12-05T23:29:19.097Z","msg":"Tuner NIS validation failed for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler, server ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler (NIS=14.96 exceeds threshold 7.38) - Keeping previous state: alpha=7.671219, beta=0.039009, gamma=17.585199, delta=0.000321"}
{"level":"INFO","ts":"2025-12-05T23:29:19.097Z","msg":"Keeping previous tuned parameters for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler due to NIS validation failure (NIS=14.959150)"}
{"level":"DEBUG","ts":"2025-12-05T23:29:19.097Z","msg":"Model tuner results: model=unsloth/Meta-Llama-3.1-8B, accelerator=H100, alpha=7.671219, beta=0.039009, gamma=17.585199, delta=0.000321, NIS=14.96"}
{"level":"DEBUG","ts":"2025-12-05T23:29:19.097Z","msg":"Updated tuner status for variant ms-inference-scheduling-llm-d-modelservice-decode: alpha=7.671219, beta=0.039009, gamma=17.585199, delta=0.000321, NIS=14.959150"}
{"level":"DEBUG","ts":"2025-12-05T23:29:19.107Z","msg":"Optimization solution - system: Solution: \ns=ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler; c=Premium; m=unsloth/Meta-Llama-3.1-8B; rate=1898.32; inTk=232; outTk=443; sol=1, sat=false, alloc={acc=H100; numRep=3; maxBatch=512; cost=300, val=-100, itl=9.434372, ttft=20.951233, rho=0.08632543, maxRPM=792.86005}; slo-itl=10, slo-ttft=1000, slo-tps=0 \nAllocationByType: \nname=NVIDIA-H100-80GB-HBM3, count=3, limit=0, cost=300 \ntotalCost=300 \n"}
{"level":"DEBUG","ts":"2025-12-05T23:29:19.107Z","msg":"Setting accelerator name: Name=H100, allocationData={H100 3 512 300 9.434372 20.951233 {1898.32 232 443}}"}
{"level":"INFO","ts":"2025-12-05T23:29:19.107Z","msg":"Model-based optimization completed for model: unsloth/Meta-Llama-3.1-8B - model-based targets: map[ms-inference-scheduling-llm-d-modelservice-decode:3]"}
{"level":"WARN","ts":"2025-12-05T23:29:19.107Z","msg":"saturation analysis unavailable, using model-based targets only: modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-05T23:29:19.107Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-12-05T23:29:19.107Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=scale-down, current=4→target=3"}
{"level":"DEBUG","ts":"2025-12-05T23:29:19.107Z","msg":"Found VA in map: variant=ms-inference-scheduling-llm-d-modelservice-decode, hasCurrentAlloc=true, accelerator=H100"}
{"level":"INFO","ts":"2025-12-05T23:29:19.108Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 4, desired-replicas: 3, accelerator: H100"}
{"level":"INFO","ts":"2025-12-05T23:29:19.108Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=3, accelerator=H100, SaturationOnly=false"}
{"level":"INFO","ts":"2025-12-05T23:29:19.114Z","msg":"Applied Saturation decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=scale-down, current=4, target=3, reason=model-based only (Saturation unavailable)"}
{"level":"INFO","ts":"2025-12-05T23:29:19.114Z","msg":"Reconciliation completed successfully: mode=model-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-12-05T23:29:19.114Z","msg":"saturation scaling config cache updated: entries=1, has_default=true"}
{"level":"INFO","ts":"2025-12-05T23:29:19.114Z","msg":"saturation scaling configuration loaded successfully"}
{"level":"INFO","ts":"2025-12-05T23:29:19.114Z","msg":"Operating in MODEL-ONLY mode: model-based optimization only"}
{"level":"INFO","ts":"2025-12-05T23:29:19.114Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-12-05T23:29:19.114Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-12-05T23:29:19.116Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dgzckd, usage=0.127 (12.7%)"}
{"level":"INFO","ts":"2025-12-05T23:29:19.116Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dczzts, usage=0.136 (13.6%)"}
{"level":"DEBUG","ts":"2025-12-05T23:29:19.116Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=2"}
{"level":"INFO","ts":"2025-12-05T23:29:19.116Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dgzckd, queueLength=0"}
{"level":"INFO","ts":"2025-12-05T23:29:19.116Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dczzts, queueLength=0"}
{"level":"DEBUG","ts":"2025-12-05T23:29:19.116Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=2"}
{"level":"DEBUG","ts":"2025-12-05T23:29:19.119Z","msg":"Pod-to-variant matching successful: totalPods=2, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:2]"}
{"level":"DEBUG","ts":"2025-12-05T23:29:19.119Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=2"}
{"level":"DEBUG","ts":"2025-12-05T23:29:19.119Z","msg":"Collected capacity metrics for observability in model-only mode: modelID=unsloth/Meta-Llama-3.1-8B, replicaCount=2"}
{"level":"INFO","ts":"2025-12-05T23:29:19.119Z","msg":"Found SLO for model: model=unsloth/Meta-Llama-3.1-8B, class=Premium, slo-tpot=10, slo-ttft=1000"}
{"level":"DEBUG","ts":"2025-12-05T23:29:19.131Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=4, accelerator=H100, ttft=22.41ms, itl=11.12ms, cost=400.00, maxBatch=256, arrivalRate=1898.32, avgInputTokens=232.65, avgOutputTokens=443.75"}
{"level":"DEBUG","ts":"2025-12-05T23:29:19.131Z","msg":"Experimental model tuner is enabled globally (EXPERIMENTAL_MODEL_TUNER_ENABLED=true) tuning model performance parameters for active VAs"}
{"level":"DEBUG","ts":"2025-12-05T23:29:19.131Z","msg":"Using state vals from VA status to tune variant ms-inference-scheduling-llm-d-modelservice-decode: alpha= 7.671219, beta= 0.039009, gamma= 17.585199, delta= 0.000321"}
{"level":"DEBUG","ts":"2025-12-05T23:29:19.131Z","msg":"[Tuner Config] variant=ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler | Initial state: alpha=7.671219, beta=0.039009, gamma=17.585199, delta=0.000321 | SLO targets: TTFT=1000.00ms, ITL=10.00ms | Has covariance=true"}
{"level":"WARN","ts":"2025-12-05T23:29:19.132Z","msg":"Tuner validation failed (NIS=14.96), validation error: normalized innovation squared (NIS=14.96) exceeds threshold (7.38), rejecting update as outlier - returning previous state: alpha=7.671219, beta=0.039009, gamma=17.585199, delta=0.000321"}
{"level":"WARN","ts":"2025-12-05T23:29:19.132Z","msg":"Tuner NIS validation failed for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler, server ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler (NIS=14.96 exceeds threshold 7.38) - Keeping previous state: alpha=7.671219, beta=0.039009, gamma=17.585199, delta=0.000321"}
{"level":"INFO","ts":"2025-12-05T23:29:19.132Z","msg":"Keeping previous tuned parameters for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler due to NIS validation failure (NIS=14.959150)"}
{"level":"DEBUG","ts":"2025-12-05T23:29:19.132Z","msg":"Model tuner results: model=unsloth/Meta-Llama-3.1-8B, accelerator=H100, alpha=7.671219, beta=0.039009, gamma=17.585199, delta=0.000321, NIS=14.96"}
{"level":"DEBUG","ts":"2025-12-05T23:29:19.132Z","msg":"Tuned parameters unchanged for variant ms-inference-scheduling-llm-d-modelservice-decode, skipping status update"}
{"level":"DEBUG","ts":"2025-12-05T23:29:19.142Z","msg":"Optimization solution - system: Solution: \ns=ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler; c=Premium; m=unsloth/Meta-Llama-3.1-8B; rate=1898.32; inTk=232; outTk=443; sol=1, sat=false, alloc={acc=H100; numRep=3; maxBatch=512; cost=300, val=-100, itl=9.434372, ttft=20.951233, rho=0.08632543, maxRPM=792.86005}; slo-itl=10, slo-ttft=1000, slo-tps=0 \nAllocationByType: \nname=NVIDIA-H100-80GB-HBM3, count=3, limit=0, cost=300 \ntotalCost=300 \n"}
{"level":"DEBUG","ts":"2025-12-05T23:29:19.142Z","msg":"Setting accelerator name: Name=H100, allocationData={H100 3 512 300 9.434372 20.951233 {1898.32 232 443}}"}
{"level":"INFO","ts":"2025-12-05T23:29:19.142Z","msg":"Model-based optimization completed for model: unsloth/Meta-Llama-3.1-8B - model-based targets: map[ms-inference-scheduling-llm-d-modelservice-decode:3]"}
{"level":"WARN","ts":"2025-12-05T23:29:19.142Z","msg":"saturation analysis unavailable, using model-based targets only: modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-05T23:29:19.142Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-12-05T23:29:19.142Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=scale-down, current=4→target=3"}
{"level":"DEBUG","ts":"2025-12-05T23:29:19.142Z","msg":"Found VA in map: variant=ms-inference-scheduling-llm-d-modelservice-decode, hasCurrentAlloc=true, accelerator=H100"}
{"level":"INFO","ts":"2025-12-05T23:29:19.142Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 4, desired-replicas: 3, accelerator: H100"}
{"level":"INFO","ts":"2025-12-05T23:29:19.142Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=3, accelerator=H100, SaturationOnly=false"}
{"level":"INFO","ts":"2025-12-05T23:29:19.147Z","msg":"Applied Saturation decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=scale-down, current=4, target=3, reason=model-based only (Saturation unavailable)"}
{"level":"INFO","ts":"2025-12-05T23:29:19.147Z","msg":"Reconciliation completed successfully: mode=model-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-12-05T23:30:19.115Z","msg":"saturation scaling config cache updated: entries=1, has_default=true"}
{"level":"INFO","ts":"2025-12-05T23:30:19.115Z","msg":"saturation scaling configuration loaded successfully"}
{"level":"INFO","ts":"2025-12-05T23:30:19.115Z","msg":"Operating in MODEL-ONLY mode: model-based optimization only"}
{"level":"INFO","ts":"2025-12-05T23:30:19.115Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-12-05T23:30:19.115Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-12-05T23:30:19.122Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dgzckd, queueLength=0"}
{"level":"INFO","ts":"2025-12-05T23:30:19.122Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dczzts, queueLength=0"}
{"level":"DEBUG","ts":"2025-12-05T23:30:19.122Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=2"}
{"level":"INFO","ts":"2025-12-05T23:30:19.122Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dgzckd, usage=0.172 (17.2%)"}
{"level":"INFO","ts":"2025-12-05T23:30:19.122Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dczzts, usage=0.218 (21.8%)"}
{"level":"DEBUG","ts":"2025-12-05T23:30:19.122Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=2"}
{"level":"DEBUG","ts":"2025-12-05T23:30:19.124Z","msg":"Pod-to-variant matching successful: totalPods=2, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:2]"}
{"level":"DEBUG","ts":"2025-12-05T23:30:19.124Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=2"}
{"level":"DEBUG","ts":"2025-12-05T23:30:19.124Z","msg":"Collected capacity metrics for observability in model-only mode: modelID=unsloth/Meta-Llama-3.1-8B, replicaCount=2"}
{"level":"INFO","ts":"2025-12-05T23:30:19.125Z","msg":"Found SLO for model: model=unsloth/Meta-Llama-3.1-8B, class=Premium, slo-tpot=10, slo-ttft=1000"}
{"level":"DEBUG","ts":"2025-12-05T23:30:19.134Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=3, accelerator=H100, ttft=26.01ms, itl=13.23ms, cost=300.00, maxBatch=256, arrivalRate=2175.32, avgInputTokens=237.02, avgOutputTokens=432.74"}
{"level":"DEBUG","ts":"2025-12-05T23:30:19.134Z","msg":"Experimental model tuner is enabled globally (EXPERIMENTAL_MODEL_TUNER_ENABLED=true) tuning model performance parameters for active VAs"}
{"level":"DEBUG","ts":"2025-12-05T23:30:19.134Z","msg":"Using state vals from VA status to tune variant ms-inference-scheduling-llm-d-modelservice-decode: alpha= 7.671219, beta= 0.039009, gamma= 17.585199, delta= 0.000321"}
{"level":"DEBUG","ts":"2025-12-05T23:30:19.134Z","msg":"[Tuner Config] variant=ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler | Initial state: alpha=7.671219, beta=0.039009, gamma=17.585199, delta=0.000321 | SLO targets: TTFT=1000.00ms, ITL=10.00ms | Has covariance=true"}
{"level":"WARN","ts":"2025-12-05T23:30:19.135Z","msg":"Tuner validation failed (NIS=33.41), validation error: normalized innovation squared (NIS=33.41) exceeds threshold (7.38), rejecting update as outlier - returning previous state: alpha=7.671219, beta=0.039009, gamma=17.585199, delta=0.000321"}
{"level":"WARN","ts":"2025-12-05T23:30:19.135Z","msg":"Tuner NIS validation failed for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler, server ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler (NIS=33.41 exceeds threshold 7.38) - Keeping previous state: alpha=7.671219, beta=0.039009, gamma=17.585199, delta=0.000321"}
{"level":"INFO","ts":"2025-12-05T23:30:19.135Z","msg":"Keeping previous tuned parameters for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler due to NIS validation failure (NIS=33.409822)"}
{"level":"DEBUG","ts":"2025-12-05T23:30:19.135Z","msg":"Model tuner results: model=unsloth/Meta-Llama-3.1-8B, accelerator=H100, alpha=7.671219, beta=0.039009, gamma=17.585199, delta=0.000321, NIS=33.41"}
{"level":"DEBUG","ts":"2025-12-05T23:30:19.135Z","msg":"Updated tuner status for variant ms-inference-scheduling-llm-d-modelservice-decode: alpha=7.671219, beta=0.039009, gamma=17.585199, delta=0.000321, NIS=33.409822"}
{"level":"DEBUG","ts":"2025-12-05T23:30:19.144Z","msg":"Optimization solution - system: Solution: \ns=ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler; c=Premium; m=unsloth/Meta-Llama-3.1-8B; rate=2175.32; inTk=237; outTk=432; sol=1, sat=false, alloc={acc=H100; numRep=3; maxBatch=512; cost=300, val=0, itl=9.689047, ttft=21.520452, rho=0.09907666, maxRPM=812.975}; slo-itl=10, slo-ttft=1000, slo-tps=0 \nAllocationByType: \nname=NVIDIA-H100-80GB-HBM3, count=3, limit=0, cost=300 \ntotalCost=300 \n"}
{"level":"DEBUG","ts":"2025-12-05T23:30:19.144Z","msg":"Setting accelerator name: Name=H100, allocationData={H100 3 512 300 9.689047 21.520452 {2175.32 237 432}}"}
{"level":"INFO","ts":"2025-12-05T23:30:19.144Z","msg":"Model-based optimization completed for model: unsloth/Meta-Llama-3.1-8B - model-based targets: map[ms-inference-scheduling-llm-d-modelservice-decode:3]"}
{"level":"WARN","ts":"2025-12-05T23:30:19.144Z","msg":"saturation analysis unavailable, using model-based targets only: modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-05T23:30:19.144Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-12-05T23:30:19.144Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=3→target=3"}
{"level":"DEBUG","ts":"2025-12-05T23:30:19.144Z","msg":"Found VA in map: variant=ms-inference-scheduling-llm-d-modelservice-decode, hasCurrentAlloc=true, accelerator=H100"}
{"level":"INFO","ts":"2025-12-05T23:30:19.144Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 3, desired-replicas: 3, accelerator: H100"}
{"level":"INFO","ts":"2025-12-05T23:30:19.144Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=3, accelerator=H100, SaturationOnly=false"}
{"level":"INFO","ts":"2025-12-05T23:30:19.150Z","msg":"Applied Saturation decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=3, target=3, reason=model-based only (Saturation unavailable)"}
{"level":"INFO","ts":"2025-12-05T23:30:19.150Z","msg":"Reconciliation completed successfully: mode=model-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-12-05T23:30:19.150Z","msg":"saturation scaling config cache updated: entries=1, has_default=true"}
{"level":"INFO","ts":"2025-12-05T23:30:19.150Z","msg":"saturation scaling configuration loaded successfully"}
{"level":"INFO","ts":"2025-12-05T23:30:19.150Z","msg":"Operating in MODEL-ONLY mode: model-based optimization only"}
{"level":"INFO","ts":"2025-12-05T23:30:19.150Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-12-05T23:30:19.150Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-12-05T23:30:19.152Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dgzckd, queueLength=0"}
{"level":"INFO","ts":"2025-12-05T23:30:19.152Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dczzts, queueLength=0"}
{"level":"DEBUG","ts":"2025-12-05T23:30:19.152Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=2"}
{"level":"INFO","ts":"2025-12-05T23:30:19.152Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dgzckd, usage=0.172 (17.2%)"}
{"level":"INFO","ts":"2025-12-05T23:30:19.152Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dczzts, usage=0.218 (21.8%)"}
{"level":"DEBUG","ts":"2025-12-05T23:30:19.152Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=2"}
{"level":"DEBUG","ts":"2025-12-05T23:30:19.154Z","msg":"Pod-to-variant matching successful: totalPods=2, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:2]"}
{"level":"DEBUG","ts":"2025-12-05T23:30:19.154Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=2"}
{"level":"DEBUG","ts":"2025-12-05T23:30:19.154Z","msg":"Collected capacity metrics for observability in model-only mode: modelID=unsloth/Meta-Llama-3.1-8B, replicaCount=2"}
{"level":"INFO","ts":"2025-12-05T23:30:19.154Z","msg":"Found SLO for model: model=unsloth/Meta-Llama-3.1-8B, class=Premium, slo-tpot=10, slo-ttft=1000"}
{"level":"DEBUG","ts":"2025-12-05T23:30:19.162Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=3, accelerator=H100, ttft=26.01ms, itl=13.23ms, cost=300.00, maxBatch=256, arrivalRate=2175.32, avgInputTokens=237.02, avgOutputTokens=432.74"}
{"level":"DEBUG","ts":"2025-12-05T23:30:19.162Z","msg":"Experimental model tuner is enabled globally (EXPERIMENTAL_MODEL_TUNER_ENABLED=true) tuning model performance parameters for active VAs"}
{"level":"DEBUG","ts":"2025-12-05T23:30:19.162Z","msg":"Using state vals from VA status to tune variant ms-inference-scheduling-llm-d-modelservice-decode: alpha= 7.671219, beta= 0.039009, gamma= 17.585199, delta= 0.000321"}
{"level":"DEBUG","ts":"2025-12-05T23:30:19.162Z","msg":"[Tuner Config] variant=ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler | Initial state: alpha=7.671219, beta=0.039009, gamma=17.585199, delta=0.000321 | SLO targets: TTFT=1000.00ms, ITL=10.00ms | Has covariance=true"}
{"level":"WARN","ts":"2025-12-05T23:30:19.163Z","msg":"Tuner validation failed (NIS=33.41), validation error: normalized innovation squared (NIS=33.41) exceeds threshold (7.38), rejecting update as outlier - returning previous state: alpha=7.671219, beta=0.039009, gamma=17.585199, delta=0.000321"}
{"level":"WARN","ts":"2025-12-05T23:30:19.163Z","msg":"Tuner NIS validation failed for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler, server ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler (NIS=33.41 exceeds threshold 7.38) - Keeping previous state: alpha=7.671219, beta=0.039009, gamma=17.585199, delta=0.000321"}
{"level":"INFO","ts":"2025-12-05T23:30:19.163Z","msg":"Keeping previous tuned parameters for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler due to NIS validation failure (NIS=33.409822)"}
{"level":"DEBUG","ts":"2025-12-05T23:30:19.163Z","msg":"Model tuner results: model=unsloth/Meta-Llama-3.1-8B, accelerator=H100, alpha=7.671219, beta=0.039009, gamma=17.585199, delta=0.000321, NIS=33.41"}
{"level":"DEBUG","ts":"2025-12-05T23:30:19.163Z","msg":"Tuned parameters unchanged for variant ms-inference-scheduling-llm-d-modelservice-decode, skipping status update"}
{"level":"DEBUG","ts":"2025-12-05T23:30:19.172Z","msg":"Optimization solution - system: Solution: \ns=ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler; c=Premium; m=unsloth/Meta-Llama-3.1-8B; rate=2175.32; inTk=237; outTk=432; sol=1, sat=false, alloc={acc=H100; numRep=3; maxBatch=512; cost=300, val=0, itl=9.689047, ttft=21.520452, rho=0.09907666, maxRPM=812.975}; slo-itl=10, slo-ttft=1000, slo-tps=0 \nAllocationByType: \nname=NVIDIA-H100-80GB-HBM3, count=3, limit=0, cost=300 \ntotalCost=300 \n"}
{"level":"DEBUG","ts":"2025-12-05T23:30:19.172Z","msg":"Setting accelerator name: Name=H100, allocationData={H100 3 512 300 9.689047 21.520452 {2175.32 237 432}}"}
{"level":"INFO","ts":"2025-12-05T23:30:19.172Z","msg":"Model-based optimization completed for model: unsloth/Meta-Llama-3.1-8B - model-based targets: map[ms-inference-scheduling-llm-d-modelservice-decode:3]"}
{"level":"WARN","ts":"2025-12-05T23:30:19.172Z","msg":"saturation analysis unavailable, using model-based targets only: modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-05T23:30:19.172Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-12-05T23:30:19.172Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=3→target=3"}
{"level":"DEBUG","ts":"2025-12-05T23:30:19.172Z","msg":"Found VA in map: variant=ms-inference-scheduling-llm-d-modelservice-decode, hasCurrentAlloc=true, accelerator=H100"}
{"level":"INFO","ts":"2025-12-05T23:30:19.172Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 3, desired-replicas: 3, accelerator: H100"}
{"level":"INFO","ts":"2025-12-05T23:30:19.172Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=3, accelerator=H100, SaturationOnly=false"}
{"level":"INFO","ts":"2025-12-05T23:30:19.178Z","msg":"Applied Saturation decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=3, target=3, reason=model-based only (Saturation unavailable)"}
{"level":"INFO","ts":"2025-12-05T23:30:19.178Z","msg":"Reconciliation completed successfully: mode=model-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-12-05T23:31:19.150Z","msg":"saturation scaling config cache updated: entries=1, has_default=true"}
{"level":"INFO","ts":"2025-12-05T23:31:19.150Z","msg":"saturation scaling configuration loaded successfully"}
{"level":"INFO","ts":"2025-12-05T23:31:19.150Z","msg":"Operating in MODEL-ONLY mode: model-based optimization only"}
{"level":"INFO","ts":"2025-12-05T23:31:19.150Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-12-05T23:31:19.150Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-12-05T23:31:19.157Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dgzckd, usage=0.133 (13.3%)"}
{"level":"INFO","ts":"2025-12-05T23:31:19.157Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dgv5kp, usage=0.000 (0.0%)"}
{"level":"INFO","ts":"2025-12-05T23:31:19.157Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dczzts, usage=0.140 (14.0%)"}
{"level":"DEBUG","ts":"2025-12-05T23:31:19.157Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=3"}
{"level":"INFO","ts":"2025-12-05T23:31:19.157Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dgzckd, queueLength=0"}
{"level":"INFO","ts":"2025-12-05T23:31:19.157Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dgv5kp, queueLength=0"}
{"level":"INFO","ts":"2025-12-05T23:31:19.157Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dczzts, queueLength=0"}
{"level":"DEBUG","ts":"2025-12-05T23:31:19.157Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=3"}
{"level":"DEBUG","ts":"2025-12-05T23:31:19.160Z","msg":"Pod-to-variant matching successful: totalPods=3, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:3]"}
{"level":"DEBUG","ts":"2025-12-05T23:31:19.160Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=3"}
{"level":"DEBUG","ts":"2025-12-05T23:31:19.160Z","msg":"Collected capacity metrics for observability in model-only mode: modelID=unsloth/Meta-Llama-3.1-8B, replicaCount=3"}
{"level":"INFO","ts":"2025-12-05T23:31:19.160Z","msg":"Found SLO for model: model=unsloth/Meta-Llama-3.1-8B, class=Premium, slo-tpot=10, slo-ttft=1000"}
{"level":"DEBUG","ts":"2025-12-05T23:31:19.169Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=3, accelerator=H100, ttft=22.66ms, itl=11.38ms, cost=300.00, maxBatch=256, arrivalRate=1938.48, avgInputTokens=219.18, avgOutputTokens=475.12"}
{"level":"DEBUG","ts":"2025-12-05T23:31:19.170Z","msg":"Experimental model tuner is enabled globally (EXPERIMENTAL_MODEL_TUNER_ENABLED=true) tuning model performance parameters for active VAs"}
{"level":"DEBUG","ts":"2025-12-05T23:31:19.170Z","msg":"Using state vals from VA status to tune variant ms-inference-scheduling-llm-d-modelservice-decode: alpha= 7.671219, beta= 0.039009, gamma= 17.585199, delta= 0.000321"}
{"level":"DEBUG","ts":"2025-12-05T23:31:19.170Z","msg":"[Tuner Config] variant=ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler | Initial state: alpha=7.671219, beta=0.039009, gamma=17.585199, delta=0.000321 | SLO targets: TTFT=1000.00ms, ITL=10.00ms | Has covariance=true"}
{"level":"WARN","ts":"2025-12-05T23:31:19.170Z","msg":"Tuner validation failed (NIS=8.19), validation error: normalized innovation squared (NIS=8.19) exceeds threshold (7.38), rejecting update as outlier - returning previous state: alpha=7.671219, beta=0.039009, gamma=17.585199, delta=0.000321"}
{"level":"WARN","ts":"2025-12-05T23:31:19.170Z","msg":"Tuner NIS validation failed for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler, server ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler (NIS=8.19 exceeds threshold 7.38) - Keeping previous state: alpha=7.671219, beta=0.039009, gamma=17.585199, delta=0.000321"}
{"level":"INFO","ts":"2025-12-05T23:31:19.170Z","msg":"Keeping previous tuned parameters for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler due to NIS validation failure (NIS=8.190544)"}
{"level":"DEBUG","ts":"2025-12-05T23:31:19.170Z","msg":"Model tuner results: model=unsloth/Meta-Llama-3.1-8B, accelerator=H100, alpha=7.671219, beta=0.039009, gamma=17.585199, delta=0.000321, NIS=8.19"}
{"level":"DEBUG","ts":"2025-12-05T23:31:19.170Z","msg":"Updated tuner status for variant ms-inference-scheduling-llm-d-modelservice-decode: alpha=7.671219, beta=0.039009, gamma=17.585199, delta=0.000321, NIS=8.190544"}
{"level":"DEBUG","ts":"2025-12-05T23:31:19.180Z","msg":"Optimization solution - system: Solution: \ns=ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler; c=Premium; m=unsloth/Meta-Llama-3.1-8B; rate=1938.48; inTk=219; outTk=475; sol=1, sat=false, alloc={acc=H100; numRep=3; maxBatch=512; cost=300, val=0, itl=9.638373, ttft=21.130253, rho=0.096539475, maxRPM=739.6226}; slo-itl=10, slo-ttft=1000, slo-tps=0 \nAllocationByType: \nname=NVIDIA-H100-80GB-HBM3, count=3, limit=0, cost=300 \ntotalCost=300 \n"}
{"level":"DEBUG","ts":"2025-12-05T23:31:19.180Z","msg":"Setting accelerator name: Name=H100, allocationData={H100 3 512 300 9.638373 21.130253 {1938.48 219 475}}"}
{"level":"INFO","ts":"2025-12-05T23:31:19.180Z","msg":"Model-based optimization completed for model: unsloth/Meta-Llama-3.1-8B - model-based targets: map[ms-inference-scheduling-llm-d-modelservice-decode:3]"}
{"level":"WARN","ts":"2025-12-05T23:31:19.180Z","msg":"saturation analysis unavailable, using model-based targets only: modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-05T23:31:19.180Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-12-05T23:31:19.180Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=3→target=3"}
{"level":"DEBUG","ts":"2025-12-05T23:31:19.180Z","msg":"Found VA in map: variant=ms-inference-scheduling-llm-d-modelservice-decode, hasCurrentAlloc=true, accelerator=H100"}
{"level":"INFO","ts":"2025-12-05T23:31:19.180Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 3, desired-replicas: 3, accelerator: H100"}
{"level":"INFO","ts":"2025-12-05T23:31:19.180Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=3, accelerator=H100, SaturationOnly=false"}
{"level":"INFO","ts":"2025-12-05T23:31:19.186Z","msg":"Applied Saturation decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=3, target=3, reason=model-based only (Saturation unavailable)"}
{"level":"INFO","ts":"2025-12-05T23:31:19.186Z","msg":"Reconciliation completed successfully: mode=model-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-12-05T23:31:19.187Z","msg":"saturation scaling config cache updated: entries=1, has_default=true"}
{"level":"INFO","ts":"2025-12-05T23:31:19.187Z","msg":"saturation scaling configuration loaded successfully"}
{"level":"INFO","ts":"2025-12-05T23:31:19.187Z","msg":"Operating in MODEL-ONLY mode: model-based optimization only"}
{"level":"INFO","ts":"2025-12-05T23:31:19.187Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-12-05T23:31:19.187Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-12-05T23:31:19.188Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dgzckd, usage=0.133 (13.3%)"}
{"level":"INFO","ts":"2025-12-05T23:31:19.188Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dgv5kp, usage=0.000 (0.0%)"}
{"level":"INFO","ts":"2025-12-05T23:31:19.188Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dczzts, usage=0.140 (14.0%)"}
{"level":"DEBUG","ts":"2025-12-05T23:31:19.188Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=3"}
{"level":"INFO","ts":"2025-12-05T23:31:19.188Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dgzckd, queueLength=0"}
{"level":"INFO","ts":"2025-12-05T23:31:19.188Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dgv5kp, queueLength=0"}
{"level":"INFO","ts":"2025-12-05T23:31:19.188Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dczzts, queueLength=0"}
{"level":"DEBUG","ts":"2025-12-05T23:31:19.188Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=3"}
{"level":"DEBUG","ts":"2025-12-05T23:31:19.192Z","msg":"Pod-to-variant matching successful: totalPods=3, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:3]"}
{"level":"DEBUG","ts":"2025-12-05T23:31:19.192Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=3"}
{"level":"DEBUG","ts":"2025-12-05T23:31:19.192Z","msg":"Collected capacity metrics for observability in model-only mode: modelID=unsloth/Meta-Llama-3.1-8B, replicaCount=3"}
{"level":"INFO","ts":"2025-12-05T23:31:19.193Z","msg":"Found SLO for model: model=unsloth/Meta-Llama-3.1-8B, class=Premium, slo-tpot=10, slo-ttft=1000"}
{"level":"DEBUG","ts":"2025-12-05T23:31:19.203Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=3, accelerator=H100, ttft=22.66ms, itl=11.38ms, cost=300.00, maxBatch=256, arrivalRate=1938.48, avgInputTokens=219.18, avgOutputTokens=475.12"}
{"level":"DEBUG","ts":"2025-12-05T23:31:19.203Z","msg":"Experimental model tuner is enabled globally (EXPERIMENTAL_MODEL_TUNER_ENABLED=true) tuning model performance parameters for active VAs"}
{"level":"DEBUG","ts":"2025-12-05T23:31:19.203Z","msg":"Using state vals from VA status to tune variant ms-inference-scheduling-llm-d-modelservice-decode: alpha= 7.671219, beta= 0.039009, gamma= 17.585199, delta= 0.000321"}
{"level":"DEBUG","ts":"2025-12-05T23:31:19.203Z","msg":"[Tuner Config] variant=ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler | Initial state: alpha=7.671219, beta=0.039009, gamma=17.585199, delta=0.000321 | SLO targets: TTFT=1000.00ms, ITL=10.00ms | Has covariance=true"}
{"level":"WARN","ts":"2025-12-05T23:31:19.203Z","msg":"Tuner validation failed (NIS=8.19), validation error: normalized innovation squared (NIS=8.19) exceeds threshold (7.38), rejecting update as outlier - returning previous state: alpha=7.671219, beta=0.039009, gamma=17.585199, delta=0.000321"}
{"level":"WARN","ts":"2025-12-05T23:31:19.203Z","msg":"Tuner NIS validation failed for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler, server ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler (NIS=8.19 exceeds threshold 7.38) - Keeping previous state: alpha=7.671219, beta=0.039009, gamma=17.585199, delta=0.000321"}
{"level":"INFO","ts":"2025-12-05T23:31:19.203Z","msg":"Keeping previous tuned parameters for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler due to NIS validation failure (NIS=8.190544)"}
{"level":"DEBUG","ts":"2025-12-05T23:31:19.203Z","msg":"Model tuner results: model=unsloth/Meta-Llama-3.1-8B, accelerator=H100, alpha=7.671219, beta=0.039009, gamma=17.585199, delta=0.000321, NIS=8.19"}
{"level":"DEBUG","ts":"2025-12-05T23:31:19.203Z","msg":"Tuned parameters unchanged for variant ms-inference-scheduling-llm-d-modelservice-decode, skipping status update"}
{"level":"DEBUG","ts":"2025-12-05T23:31:19.211Z","msg":"Optimization solution - system: Solution: \ns=ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler; c=Premium; m=unsloth/Meta-Llama-3.1-8B; rate=1938.48; inTk=219; outTk=475; sol=1, sat=false, alloc={acc=H100; numRep=3; maxBatch=512; cost=300, val=0, itl=9.638373, ttft=21.130253, rho=0.096539475, maxRPM=739.6226}; slo-itl=10, slo-ttft=1000, slo-tps=0 \nAllocationByType: \nname=NVIDIA-H100-80GB-HBM3, count=3, limit=0, cost=300 \ntotalCost=300 \n"}
{"level":"DEBUG","ts":"2025-12-05T23:31:19.211Z","msg":"Setting accelerator name: Name=H100, allocationData={H100 3 512 300 9.638373 21.130253 {1938.48 219 475}}"}
{"level":"INFO","ts":"2025-12-05T23:31:19.212Z","msg":"Model-based optimization completed for model: unsloth/Meta-Llama-3.1-8B - model-based targets: map[ms-inference-scheduling-llm-d-modelservice-decode:3]"}
{"level":"WARN","ts":"2025-12-05T23:31:19.212Z","msg":"saturation analysis unavailable, using model-based targets only: modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-05T23:31:19.212Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-12-05T23:31:19.212Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=3→target=3"}
{"level":"DEBUG","ts":"2025-12-05T23:31:19.212Z","msg":"Found VA in map: variant=ms-inference-scheduling-llm-d-modelservice-decode, hasCurrentAlloc=true, accelerator=H100"}
{"level":"INFO","ts":"2025-12-05T23:31:19.212Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 3, desired-replicas: 3, accelerator: H100"}
{"level":"INFO","ts":"2025-12-05T23:31:19.212Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=3, accelerator=H100, SaturationOnly=false"}
{"level":"INFO","ts":"2025-12-05T23:31:19.217Z","msg":"Applied Saturation decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=3, target=3, reason=model-based only (Saturation unavailable)"}
{"level":"INFO","ts":"2025-12-05T23:31:19.217Z","msg":"Reconciliation completed successfully: mode=model-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-12-05T23:32:19.188Z","msg":"saturation scaling config cache updated: entries=1, has_default=true"}
{"level":"INFO","ts":"2025-12-05T23:32:19.188Z","msg":"saturation scaling configuration loaded successfully"}
{"level":"INFO","ts":"2025-12-05T23:32:19.188Z","msg":"Operating in MODEL-ONLY mode: model-based optimization only"}
{"level":"INFO","ts":"2025-12-05T23:32:19.188Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-12-05T23:32:19.188Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-12-05T23:32:19.195Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dgzckd, usage=0.071 (7.1%)"}
{"level":"INFO","ts":"2025-12-05T23:32:19.195Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dgv5kp, usage=0.069 (6.9%)"}
{"level":"INFO","ts":"2025-12-05T23:32:19.195Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dczzts, usage=0.077 (7.7%)"}
{"level":"DEBUG","ts":"2025-12-05T23:32:19.195Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=3"}
{"level":"INFO","ts":"2025-12-05T23:32:19.195Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dgzckd, queueLength=0"}
{"level":"INFO","ts":"2025-12-05T23:32:19.195Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dgv5kp, queueLength=0"}
{"level":"INFO","ts":"2025-12-05T23:32:19.195Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dczzts, queueLength=0"}
{"level":"DEBUG","ts":"2025-12-05T23:32:19.195Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=3"}
{"level":"DEBUG","ts":"2025-12-05T23:32:19.198Z","msg":"Pod-to-variant matching successful: totalPods=3, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:3]"}
{"level":"DEBUG","ts":"2025-12-05T23:32:19.198Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=3"}
{"level":"DEBUG","ts":"2025-12-05T23:32:19.198Z","msg":"Collected capacity metrics for observability in model-only mode: modelID=unsloth/Meta-Llama-3.1-8B, replicaCount=3"}
{"level":"INFO","ts":"2025-12-05T23:32:19.199Z","msg":"Found SLO for model: model=unsloth/Meta-Llama-3.1-8B, class=Premium, slo-tpot=10, slo-ttft=1000"}
{"level":"DEBUG","ts":"2025-12-05T23:32:19.210Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=3, accelerator=H100, ttft=19.46ms, itl=9.46ms, cost=300.00, maxBatch=256, arrivalRate=1914.36, avgInputTokens=234.82, avgOutputTokens=452.43"}
{"level":"DEBUG","ts":"2025-12-05T23:32:19.210Z","msg":"Experimental model tuner is enabled globally (EXPERIMENTAL_MODEL_TUNER_ENABLED=true) tuning model performance parameters for active VAs"}
{"level":"DEBUG","ts":"2025-12-05T23:32:19.210Z","msg":"Using state vals from VA status to tune variant ms-inference-scheduling-llm-d-modelservice-decode: alpha= 7.671219, beta= 0.039009, gamma= 17.585199, delta= 0.000321"}
{"level":"DEBUG","ts":"2025-12-05T23:32:19.210Z","msg":"[Tuner Config] variant=ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler | Initial state: alpha=7.671219, beta=0.039009, gamma=17.585199, delta=0.000321 | SLO targets: TTFT=1000.00ms, ITL=10.00ms | Has covariance=true"}
{"level":"INFO","ts":"2025-12-05T23:32:19.210Z","msg":"Tuner validation succeeded for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler, server ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler - New NIS: 0.007603"}
{"level":"DEBUG","ts":"2025-12-05T23:32:19.210Z","msg":"Model tuner results: model=unsloth/Meta-Llama-3.1-8B, accelerator=H100, alpha=7.649397, beta=0.038968, gamma=17.570084, delta=0.000321, NIS=0.01"}
{"level":"DEBUG","ts":"2025-12-05T23:32:19.210Z","msg":"Updated tuner status for variant ms-inference-scheduling-llm-d-modelservice-decode: alpha=7.649397, beta=0.038968, gamma=17.570084, delta=0.000321, NIS=0.007603"}
{"level":"INFO","ts":"2025-12-05T23:32:19.210Z","msg":"Tuned performance parameters result: variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler - alpha: 7.649397, beta: 0.038968, gamma: 17.570084, delta: 0.000321"}
{"level":"DEBUG","ts":"2025-12-05T23:32:19.220Z","msg":"Optimization solution - system: Solution: \ns=ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler; c=Premium; m=unsloth/Meta-Llama-3.1-8B; rate=1914.36; inTk=234; outTk=452; sol=1, sat=false, alloc={acc=H100; numRep=3; maxBatch=512; cost=300, val=0, itl=9.466466, ttft=21.072598, rho=0.08912178, maxRPM=785.3603}; slo-itl=10, slo-ttft=1000, slo-tps=0 \nAllocationByType: \nname=NVIDIA-H100-80GB-HBM3, count=3, limit=0, cost=300 \ntotalCost=300 \n"}
{"level":"DEBUG","ts":"2025-12-05T23:32:19.220Z","msg":"Setting accelerator name: Name=H100, allocationData={H100 3 512 300 9.466466 21.072598 {1914.36 234 452}}"}
{"level":"INFO","ts":"2025-12-05T23:32:19.220Z","msg":"Model-based optimization completed for model: unsloth/Meta-Llama-3.1-8B - model-based targets: map[ms-inference-scheduling-llm-d-modelservice-decode:3]"}
{"level":"WARN","ts":"2025-12-05T23:32:19.220Z","msg":"saturation analysis unavailable, using model-based targets only: modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-05T23:32:19.220Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-12-05T23:32:19.220Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=3→target=3"}
{"level":"DEBUG","ts":"2025-12-05T23:32:19.220Z","msg":"Found VA in map: variant=ms-inference-scheduling-llm-d-modelservice-decode, hasCurrentAlloc=true, accelerator=H100"}
{"level":"INFO","ts":"2025-12-05T23:32:19.220Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 3, desired-replicas: 3, accelerator: H100"}
{"level":"INFO","ts":"2025-12-05T23:32:19.220Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=3, accelerator=H100, SaturationOnly=false"}
{"level":"INFO","ts":"2025-12-05T23:32:19.226Z","msg":"Applied Saturation decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=3, target=3, reason=model-based only (Saturation unavailable)"}
{"level":"INFO","ts":"2025-12-05T23:32:19.226Z","msg":"Reconciliation completed successfully: mode=model-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-12-05T23:32:19.226Z","msg":"saturation scaling config cache updated: entries=1, has_default=true"}
{"level":"INFO","ts":"2025-12-05T23:32:19.226Z","msg":"saturation scaling configuration loaded successfully"}
{"level":"INFO","ts":"2025-12-05T23:32:19.226Z","msg":"Operating in MODEL-ONLY mode: model-based optimization only"}
{"level":"INFO","ts":"2025-12-05T23:32:19.226Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-12-05T23:32:19.226Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-12-05T23:32:19.228Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dgzckd, queueLength=0"}
{"level":"INFO","ts":"2025-12-05T23:32:19.228Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dgv5kp, queueLength=0"}
{"level":"INFO","ts":"2025-12-05T23:32:19.228Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dczzts, queueLength=0"}
{"level":"DEBUG","ts":"2025-12-05T23:32:19.228Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=3"}
{"level":"INFO","ts":"2025-12-05T23:32:19.228Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dgzckd, usage=0.071 (7.1%)"}
{"level":"INFO","ts":"2025-12-05T23:32:19.228Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dgv5kp, usage=0.069 (6.9%)"}
{"level":"INFO","ts":"2025-12-05T23:32:19.228Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dczzts, usage=0.077 (7.7%)"}
{"level":"DEBUG","ts":"2025-12-05T23:32:19.228Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=3"}
{"level":"DEBUG","ts":"2025-12-05T23:32:19.231Z","msg":"Pod-to-variant matching successful: totalPods=3, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:3]"}
{"level":"DEBUG","ts":"2025-12-05T23:32:19.231Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=3"}
{"level":"DEBUG","ts":"2025-12-05T23:32:19.231Z","msg":"Collected capacity metrics for observability in model-only mode: modelID=unsloth/Meta-Llama-3.1-8B, replicaCount=3"}
{"level":"INFO","ts":"2025-12-05T23:32:19.231Z","msg":"Found SLO for model: model=unsloth/Meta-Llama-3.1-8B, class=Premium, slo-tpot=10, slo-ttft=1000"}
{"level":"DEBUG","ts":"2025-12-05T23:32:19.240Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=3, accelerator=H100, ttft=19.46ms, itl=9.46ms, cost=300.00, maxBatch=256, arrivalRate=1914.36, avgInputTokens=234.82, avgOutputTokens=452.43"}
{"level":"DEBUG","ts":"2025-12-05T23:32:19.240Z","msg":"Experimental model tuner is enabled globally (EXPERIMENTAL_MODEL_TUNER_ENABLED=true) tuning model performance parameters for active VAs"}
{"level":"DEBUG","ts":"2025-12-05T23:32:19.240Z","msg":"Using state vals from VA status to tune variant ms-inference-scheduling-llm-d-modelservice-decode: alpha= 7.649397, beta= 0.038968, gamma= 17.570084, delta= 0.000321"}
{"level":"DEBUG","ts":"2025-12-05T23:32:19.240Z","msg":"[Tuner Config] variant=ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler | Initial state: alpha=7.649397, beta=0.038968, gamma=17.570084, delta=0.000321 | SLO targets: TTFT=1000.00ms, ITL=10.00ms | Has covariance=true"}
{"level":"INFO","ts":"2025-12-05T23:32:19.241Z","msg":"Tuner validation succeeded for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler, server ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler - New NIS: 0.004066"}
{"level":"DEBUG","ts":"2025-12-05T23:32:19.241Z","msg":"Model tuner results: model=unsloth/Meta-Llama-3.1-8B, accelerator=H100, alpha=7.645300, beta=0.038963, gamma=17.553419, delta=0.000321, NIS=0.00"}
{"level":"DEBUG","ts":"2025-12-05T23:32:19.241Z","msg":"Updated tuner status for variant ms-inference-scheduling-llm-d-modelservice-decode: alpha=7.645300, beta=0.038963, gamma=17.553419, delta=0.000321, NIS=0.004066"}
{"level":"INFO","ts":"2025-12-05T23:32:19.241Z","msg":"Tuned performance parameters result: variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler - alpha: 7.645300, beta: 0.038963, gamma: 17.553419, delta: 0.000321"}
{"level":"DEBUG","ts":"2025-12-05T23:32:19.250Z","msg":"Optimization solution - system: Solution: \ns=ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler; c=Premium; m=unsloth/Meta-Llama-3.1-8B; rate=1914.36; inTk=234; outTk=452; sol=1, sat=false, alloc={acc=H100; numRep=3; maxBatch=512; cost=300, val=0, itl=9.461158, ttft=21.054008, rho=0.08907166, maxRPM=786.8448}; slo-itl=10, slo-ttft=1000, slo-tps=0 \nAllocationByType: \nname=NVIDIA-H100-80GB-HBM3, count=3, limit=0, cost=300 \ntotalCost=300 \n"}
{"level":"DEBUG","ts":"2025-12-05T23:32:19.250Z","msg":"Setting accelerator name: Name=H100, allocationData={H100 3 512 300 9.461158 21.054008 {1914.36 234 452}}"}
{"level":"INFO","ts":"2025-12-05T23:32:19.250Z","msg":"Model-based optimization completed for model: unsloth/Meta-Llama-3.1-8B - model-based targets: map[ms-inference-scheduling-llm-d-modelservice-decode:3]"}
{"level":"WARN","ts":"2025-12-05T23:32:19.250Z","msg":"saturation analysis unavailable, using model-based targets only: modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-05T23:32:19.250Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-12-05T23:32:19.250Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=3→target=3"}
{"level":"DEBUG","ts":"2025-12-05T23:32:19.250Z","msg":"Found VA in map: variant=ms-inference-scheduling-llm-d-modelservice-decode, hasCurrentAlloc=true, accelerator=H100"}
{"level":"INFO","ts":"2025-12-05T23:32:19.250Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 3, desired-replicas: 3, accelerator: H100"}
{"level":"INFO","ts":"2025-12-05T23:32:19.250Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=3, accelerator=H100, SaturationOnly=false"}
{"level":"INFO","ts":"2025-12-05T23:32:19.256Z","msg":"Applied Saturation decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=3, target=3, reason=model-based only (Saturation unavailable)"}
{"level":"INFO","ts":"2025-12-05T23:32:19.256Z","msg":"Reconciliation completed successfully: mode=model-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-12-05T23:33:19.227Z","msg":"saturation scaling config cache updated: entries=1, has_default=true"}
{"level":"INFO","ts":"2025-12-05T23:33:19.227Z","msg":"saturation scaling configuration loaded successfully"}
{"level":"INFO","ts":"2025-12-05T23:33:19.227Z","msg":"Operating in MODEL-ONLY mode: model-based optimization only"}
{"level":"INFO","ts":"2025-12-05T23:33:19.227Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-12-05T23:33:19.227Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-12-05T23:33:19.235Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dgzckd, queueLength=0"}
{"level":"INFO","ts":"2025-12-05T23:33:19.235Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dgv5kp, queueLength=0"}
{"level":"INFO","ts":"2025-12-05T23:33:19.235Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dczzts, queueLength=0"}
{"level":"DEBUG","ts":"2025-12-05T23:33:19.235Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=3"}
{"level":"INFO","ts":"2025-12-05T23:33:19.245Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dgzckd, usage=0.066 (6.6%)"}
{"level":"INFO","ts":"2025-12-05T23:33:19.245Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dgv5kp, usage=0.067 (6.7%)"}
{"level":"INFO","ts":"2025-12-05T23:33:19.245Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dczzts, usage=0.079 (7.9%)"}
{"level":"DEBUG","ts":"2025-12-05T23:33:19.245Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=3"}
{"level":"DEBUG","ts":"2025-12-05T23:33:19.249Z","msg":"Pod-to-variant matching successful: totalPods=3, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:3]"}
{"level":"DEBUG","ts":"2025-12-05T23:33:19.249Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=3"}
{"level":"DEBUG","ts":"2025-12-05T23:33:19.249Z","msg":"Collected capacity metrics for observability in model-only mode: modelID=unsloth/Meta-Llama-3.1-8B, replicaCount=3"}
{"level":"INFO","ts":"2025-12-05T23:33:19.249Z","msg":"Found SLO for model: model=unsloth/Meta-Llama-3.1-8B, class=Premium, slo-tpot=10, slo-ttft=1000"}
{"level":"DEBUG","ts":"2025-12-05T23:33:19.259Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=3, accelerator=H100, ttft=19.46ms, itl=9.25ms, cost=300.00, maxBatch=256, arrivalRate=1968.39, avgInputTokens=246.02, avgOutputTokens=425.84"}
{"level":"DEBUG","ts":"2025-12-05T23:33:19.259Z","msg":"Experimental model tuner is enabled globally (EXPERIMENTAL_MODEL_TUNER_ENABLED=true) tuning model performance parameters for active VAs"}
{"level":"DEBUG","ts":"2025-12-05T23:33:19.259Z","msg":"Using state vals from VA status to tune variant ms-inference-scheduling-llm-d-modelservice-decode: alpha= 7.645300, beta= 0.038963, gamma= 17.553419, delta= 0.000321"}
{"level":"DEBUG","ts":"2025-12-05T23:33:19.259Z","msg":"[Tuner Config] variant=ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler | Initial state: alpha=7.645300, beta=0.038963, gamma=17.553419, delta=0.000321 | SLO targets: TTFT=1000.00ms, ITL=10.00ms | Has covariance=true"}
{"level":"INFO","ts":"2025-12-05T23:33:19.260Z","msg":"Tuner validation succeeded for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler, server ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler - New NIS: 0.059859"}
{"level":"DEBUG","ts":"2025-12-05T23:33:19.260Z","msg":"Model tuner results: model=unsloth/Meta-Llama-3.1-8B, accelerator=H100, alpha=7.556008, beta=0.038882, gamma=17.535168, delta=0.000321, NIS=0.06"}
{"level":"DEBUG","ts":"2025-12-05T23:33:19.260Z","msg":"Updated tuner status for variant ms-inference-scheduling-llm-d-modelservice-decode: alpha=7.556008, beta=0.038882, gamma=17.535168, delta=0.000321, NIS=0.059859"}
{"level":"INFO","ts":"2025-12-05T23:33:19.260Z","msg":"Tuned performance parameters result: variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler - alpha: 7.556008, beta: 0.038882, gamma: 17.535168, delta: 0.000321"}
{"level":"DEBUG","ts":"2025-12-05T23:33:19.269Z","msg":"Optimization solution - system: Solution: \ns=ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler; c=Premium; m=unsloth/Meta-Llama-3.1-8B; rate=1968.39; inTk=246; outTk=425; sol=1, sat=false, alloc={acc=H100; numRep=3; maxBatch=512; cost=300, val=0, itl=9.276136, ttft=21.028557, rho=0.08445347, maxRPM=870.7188}; slo-itl=10, slo-ttft=1000, slo-tps=0 \nAllocationByType: \nname=NVIDIA-H100-80GB-HBM3, count=3, limit=0, cost=300 \ntotalCost=300 \n"}
{"level":"DEBUG","ts":"2025-12-05T23:33:19.269Z","msg":"Setting accelerator name: Name=H100, allocationData={H100 3 512 300 9.276136 21.028557 {1968.39 246 425}}"}
{"level":"INFO","ts":"2025-12-05T23:33:19.269Z","msg":"Model-based optimization completed for model: unsloth/Meta-Llama-3.1-8B - model-based targets: map[ms-inference-scheduling-llm-d-modelservice-decode:3]"}
{"level":"WARN","ts":"2025-12-05T23:33:19.269Z","msg":"saturation analysis unavailable, using model-based targets only: modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-05T23:33:19.269Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-12-05T23:33:19.269Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=3→target=3"}
{"level":"DEBUG","ts":"2025-12-05T23:33:19.269Z","msg":"Found VA in map: variant=ms-inference-scheduling-llm-d-modelservice-decode, hasCurrentAlloc=true, accelerator=H100"}
{"level":"INFO","ts":"2025-12-05T23:33:19.269Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 3, desired-replicas: 3, accelerator: H100"}
{"level":"INFO","ts":"2025-12-05T23:33:19.269Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=3, accelerator=H100, SaturationOnly=false"}
{"level":"INFO","ts":"2025-12-05T23:33:19.274Z","msg":"Applied Saturation decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=3, target=3, reason=model-based only (Saturation unavailable)"}
{"level":"INFO","ts":"2025-12-05T23:33:19.275Z","msg":"Reconciliation completed successfully: mode=model-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-12-05T23:33:19.275Z","msg":"saturation scaling config cache updated: entries=1, has_default=true"}
{"level":"INFO","ts":"2025-12-05T23:33:19.275Z","msg":"saturation scaling configuration loaded successfully"}
{"level":"INFO","ts":"2025-12-05T23:33:19.275Z","msg":"Operating in MODEL-ONLY mode: model-based optimization only"}
{"level":"INFO","ts":"2025-12-05T23:33:19.275Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-12-05T23:33:19.275Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-12-05T23:33:19.276Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dgzckd, queueLength=0"}
{"level":"INFO","ts":"2025-12-05T23:33:19.276Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dgv5kp, queueLength=0"}
{"level":"INFO","ts":"2025-12-05T23:33:19.276Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dczzts, queueLength=0"}
{"level":"DEBUG","ts":"2025-12-05T23:33:19.276Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=3"}
{"level":"INFO","ts":"2025-12-05T23:33:19.277Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dgzckd, usage=0.066 (6.6%)"}
{"level":"INFO","ts":"2025-12-05T23:33:19.277Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dgv5kp, usage=0.067 (6.7%)"}
{"level":"INFO","ts":"2025-12-05T23:33:19.277Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dczzts, usage=0.079 (7.9%)"}
{"level":"DEBUG","ts":"2025-12-05T23:33:19.277Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=3"}
{"level":"DEBUG","ts":"2025-12-05T23:33:19.279Z","msg":"Pod-to-variant matching successful: totalPods=3, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:3]"}
{"level":"DEBUG","ts":"2025-12-05T23:33:19.279Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=3"}
{"level":"DEBUG","ts":"2025-12-05T23:33:19.279Z","msg":"Collected capacity metrics for observability in model-only mode: modelID=unsloth/Meta-Llama-3.1-8B, replicaCount=3"}
{"level":"INFO","ts":"2025-12-05T23:33:19.279Z","msg":"Found SLO for model: model=unsloth/Meta-Llama-3.1-8B, class=Premium, slo-tpot=10, slo-ttft=1000"}
{"level":"DEBUG","ts":"2025-12-05T23:33:19.288Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=3, accelerator=H100, ttft=19.46ms, itl=9.25ms, cost=300.00, maxBatch=256, arrivalRate=1968.39, avgInputTokens=246.02, avgOutputTokens=425.84"}
{"level":"DEBUG","ts":"2025-12-05T23:33:19.288Z","msg":"Experimental model tuner is enabled globally (EXPERIMENTAL_MODEL_TUNER_ENABLED=true) tuning model performance parameters for active VAs"}
{"level":"DEBUG","ts":"2025-12-05T23:33:19.288Z","msg":"Using state vals from VA status to tune variant ms-inference-scheduling-llm-d-modelservice-decode: alpha= 7.556008, beta= 0.038882, gamma= 17.535168, delta= 0.000321"}
{"level":"DEBUG","ts":"2025-12-05T23:33:19.288Z","msg":"[Tuner Config] variant=ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler | Initial state: alpha=7.556008, beta=0.038882, gamma=17.535168, delta=0.000321 | SLO targets: TTFT=1000.00ms, ITL=10.00ms | Has covariance=true"}
{"level":"INFO","ts":"2025-12-05T23:33:19.288Z","msg":"Tuner validation succeeded for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler, server ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler - New NIS: 0.005696"}
{"level":"DEBUG","ts":"2025-12-05T23:33:19.288Z","msg":"Model tuner results: model=unsloth/Meta-Llama-3.1-8B, accelerator=H100, alpha=7.539411, beta=0.038864, gamma=17.515759, delta=0.000321, NIS=0.01"}
{"level":"DEBUG","ts":"2025-12-05T23:33:19.288Z","msg":"Updated tuner status for variant ms-inference-scheduling-llm-d-modelservice-decode: alpha=7.539411, beta=0.038864, gamma=17.515759, delta=0.000321, NIS=0.005696"}
{"level":"INFO","ts":"2025-12-05T23:33:19.288Z","msg":"Tuned performance parameters result: variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler - alpha: 7.539411, beta: 0.038864, gamma: 17.515759, delta: 0.000321"}
{"level":"DEBUG","ts":"2025-12-05T23:33:19.298Z","msg":"Optimization solution - system: Solution: \ns=ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler; c=Premium; m=unsloth/Meta-Llama-3.1-8B; rate=1968.39; inTk=246; outTk=425; sol=1, sat=false, alloc={acc=H100; numRep=3; maxBatch=512; cost=300, val=0, itl=9.254929, ttft=21.001366, rho=0.084260836, maxRPM=877.12573}; slo-itl=10, slo-ttft=1000, slo-tps=0 \nAllocationByType: \nname=NVIDIA-H100-80GB-HBM3, count=3, limit=0, cost=300 \ntotalCost=300 \n"}
{"level":"DEBUG","ts":"2025-12-05T23:33:19.298Z","msg":"Setting accelerator name: Name=H100, allocationData={H100 3 512 300 9.254929 21.001366 {1968.39 246 425}}"}
{"level":"INFO","ts":"2025-12-05T23:33:19.298Z","msg":"Model-based optimization completed for model: unsloth/Meta-Llama-3.1-8B - model-based targets: map[ms-inference-scheduling-llm-d-modelservice-decode:3]"}
{"level":"WARN","ts":"2025-12-05T23:33:19.298Z","msg":"saturation analysis unavailable, using model-based targets only: modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-05T23:33:19.298Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-12-05T23:33:19.298Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=3→target=3"}
{"level":"DEBUG","ts":"2025-12-05T23:33:19.298Z","msg":"Found VA in map: variant=ms-inference-scheduling-llm-d-modelservice-decode, hasCurrentAlloc=true, accelerator=H100"}
{"level":"INFO","ts":"2025-12-05T23:33:19.298Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 3, desired-replicas: 3, accelerator: H100"}
{"level":"INFO","ts":"2025-12-05T23:33:19.298Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=3, accelerator=H100, SaturationOnly=false"}
{"level":"INFO","ts":"2025-12-05T23:33:19.305Z","msg":"Applied Saturation decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=3, target=3, reason=model-based only (Saturation unavailable)"}
{"level":"INFO","ts":"2025-12-05T23:33:19.305Z","msg":"Reconciliation completed successfully: mode=model-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-12-05T23:34:19.275Z","msg":"saturation scaling config cache updated: entries=1, has_default=true"}
{"level":"INFO","ts":"2025-12-05T23:34:19.275Z","msg":"saturation scaling configuration loaded successfully"}
{"level":"INFO","ts":"2025-12-05T23:34:19.275Z","msg":"Operating in MODEL-ONLY mode: model-based optimization only"}
{"level":"INFO","ts":"2025-12-05T23:34:19.275Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-12-05T23:34:19.275Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-12-05T23:34:19.318Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dgzckd, usage=0.035 (3.5%)"}
{"level":"INFO","ts":"2025-12-05T23:34:19.318Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dgv5kp, usage=0.063 (6.3%)"}
{"level":"INFO","ts":"2025-12-05T23:34:19.318Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dczzts, usage=0.036 (3.6%)"}
{"level":"DEBUG","ts":"2025-12-05T23:34:19.318Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=3"}
{"level":"INFO","ts":"2025-12-05T23:34:19.318Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dgzckd, queueLength=0"}
{"level":"INFO","ts":"2025-12-05T23:34:19.318Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dgv5kp, queueLength=0"}
{"level":"INFO","ts":"2025-12-05T23:34:19.318Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dczzts, queueLength=0"}
{"level":"DEBUG","ts":"2025-12-05T23:34:19.318Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=3"}
{"level":"DEBUG","ts":"2025-12-05T23:34:19.322Z","msg":"Pod-to-variant matching successful: totalPods=3, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:3]"}
{"level":"DEBUG","ts":"2025-12-05T23:34:19.322Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=3"}
{"level":"DEBUG","ts":"2025-12-05T23:34:19.322Z","msg":"Collected capacity metrics for observability in model-only mode: modelID=unsloth/Meta-Llama-3.1-8B, replicaCount=3"}
{"level":"INFO","ts":"2025-12-05T23:34:19.322Z","msg":"Found SLO for model: model=unsloth/Meta-Llama-3.1-8B, class=Premium, slo-tpot=10, slo-ttft=1000"}
{"level":"DEBUG","ts":"2025-12-05T23:34:19.334Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=3, accelerator=H100, ttft=17.19ms, itl=8.14ms, cost=300.00, maxBatch=256, arrivalRate=1189.60, avgInputTokens=232.53, avgOutputTokens=429.09"}
{"level":"DEBUG","ts":"2025-12-05T23:34:19.334Z","msg":"Experimental model tuner is enabled globally (EXPERIMENTAL_MODEL_TUNER_ENABLED=true) tuning model performance parameters for active VAs"}
{"level":"DEBUG","ts":"2025-12-05T23:34:19.335Z","msg":"Using state vals from VA status to tune variant ms-inference-scheduling-llm-d-modelservice-decode: alpha= 7.539411, beta= 0.038864, gamma= 17.515759, delta= 0.000321"}
{"level":"DEBUG","ts":"2025-12-05T23:34:19.335Z","msg":"[Tuner Config] variant=ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler | Initial state: alpha=7.539411, beta=0.038864, gamma=17.515759, delta=0.000321 | SLO targets: TTFT=1000.00ms, ITL=10.00ms | Has covariance=true"}
{"level":"INFO","ts":"2025-12-05T23:34:19.335Z","msg":"Tuner validation succeeded for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler, server ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler - New NIS: 0.472822"}
{"level":"DEBUG","ts":"2025-12-05T23:34:19.335Z","msg":"Model tuner results: model=unsloth/Meta-Llama-3.1-8B, accelerator=H100, alpha=7.249742, beta=0.039760, gamma=17.488733, delta=0.000321, NIS=0.47"}
{"level":"DEBUG","ts":"2025-12-05T23:34:19.335Z","msg":"Updated tuner status for variant ms-inference-scheduling-llm-d-modelservice-decode: alpha=7.249742, beta=0.039760, gamma=17.488733, delta=0.000321, NIS=0.472822"}
{"level":"INFO","ts":"2025-12-05T23:34:19.335Z","msg":"Tuned performance parameters result: variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler - alpha: 7.249742, beta: 0.039760, gamma: 17.488733, delta: 0.000321"}
{"level":"DEBUG","ts":"2025-12-05T23:34:19.343Z","msg":"Optimization solution - system: Solution: \ns=ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler; c=Premium; m=unsloth/Meta-Llama-3.1-8B; rate=1189.6; inTk=232; outTk=429; sol=1, sat=false, alloc={acc=H100; numRep=2; maxBatch=512; cost=200, val=-100, itl=8.778429, ttft=20.351969, rho=0.073140256, maxRPM=950.6424}; slo-itl=10, slo-ttft=1000, slo-tps=0 \nAllocationByType: \nname=NVIDIA-H100-80GB-HBM3, count=2, limit=0, cost=200 \ntotalCost=200 \n"}
{"level":"DEBUG","ts":"2025-12-05T23:34:19.343Z","msg":"Setting accelerator name: Name=H100, allocationData={H100 2 512 200 8.778429 20.351969 {1189.6 232 429}}"}
{"level":"INFO","ts":"2025-12-05T23:34:19.343Z","msg":"Model-based optimization completed for model: unsloth/Meta-Llama-3.1-8B - model-based targets: map[ms-inference-scheduling-llm-d-modelservice-decode:2]"}
{"level":"WARN","ts":"2025-12-05T23:34:19.343Z","msg":"saturation analysis unavailable, using model-based targets only: modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-05T23:34:19.343Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-12-05T23:34:19.343Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=scale-down, current=3→target=2"}
{"level":"DEBUG","ts":"2025-12-05T23:34:19.343Z","msg":"Found VA in map: variant=ms-inference-scheduling-llm-d-modelservice-decode, hasCurrentAlloc=true, accelerator=H100"}
{"level":"INFO","ts":"2025-12-05T23:34:19.343Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 3, desired-replicas: 2, accelerator: H100"}
{"level":"INFO","ts":"2025-12-05T23:34:19.343Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=2, accelerator=H100, SaturationOnly=false"}
{"level":"INFO","ts":"2025-12-05T23:34:19.349Z","msg":"Applied Saturation decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=scale-down, current=3, target=2, reason=model-based only (Saturation unavailable)"}
{"level":"INFO","ts":"2025-12-05T23:34:19.349Z","msg":"Reconciliation completed successfully: mode=model-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-12-05T23:34:19.349Z","msg":"saturation scaling config cache updated: entries=1, has_default=true"}
{"level":"INFO","ts":"2025-12-05T23:34:19.349Z","msg":"saturation scaling configuration loaded successfully"}
{"level":"INFO","ts":"2025-12-05T23:34:19.349Z","msg":"Operating in MODEL-ONLY mode: model-based optimization only"}
{"level":"INFO","ts":"2025-12-05T23:34:19.349Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-12-05T23:34:19.349Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-12-05T23:34:19.351Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dgzckd, usage=0.035 (3.5%)"}
{"level":"INFO","ts":"2025-12-05T23:34:19.351Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dgv5kp, usage=0.063 (6.3%)"}
{"level":"INFO","ts":"2025-12-05T23:34:19.351Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dczzts, usage=0.036 (3.6%)"}
{"level":"DEBUG","ts":"2025-12-05T23:34:19.351Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=3"}
{"level":"INFO","ts":"2025-12-05T23:34:19.351Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dgzckd, queueLength=0"}
{"level":"INFO","ts":"2025-12-05T23:34:19.351Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dgv5kp, queueLength=0"}
{"level":"INFO","ts":"2025-12-05T23:34:19.351Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dczzts, queueLength=0"}
{"level":"DEBUG","ts":"2025-12-05T23:34:19.351Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=3"}
{"level":"DEBUG","ts":"2025-12-05T23:34:19.353Z","msg":"Pod-to-variant matching successful: totalPods=3, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:3]"}
{"level":"DEBUG","ts":"2025-12-05T23:34:19.353Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=3"}
{"level":"DEBUG","ts":"2025-12-05T23:34:19.353Z","msg":"Collected capacity metrics for observability in model-only mode: modelID=unsloth/Meta-Llama-3.1-8B, replicaCount=3"}
{"level":"INFO","ts":"2025-12-05T23:34:19.353Z","msg":"Found SLO for model: model=unsloth/Meta-Llama-3.1-8B, class=Premium, slo-tpot=10, slo-ttft=1000"}
{"level":"DEBUG","ts":"2025-12-05T23:34:19.362Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=3, accelerator=H100, ttft=17.19ms, itl=8.14ms, cost=300.00, maxBatch=256, arrivalRate=1189.60, avgInputTokens=232.53, avgOutputTokens=429.09"}
{"level":"DEBUG","ts":"2025-12-05T23:34:19.362Z","msg":"Experimental model tuner is enabled globally (EXPERIMENTAL_MODEL_TUNER_ENABLED=true) tuning model performance parameters for active VAs"}
{"level":"DEBUG","ts":"2025-12-05T23:34:19.362Z","msg":"Using state vals from VA status to tune variant ms-inference-scheduling-llm-d-modelservice-decode: alpha= 7.249742, beta= 0.039760, gamma= 17.488733, delta= 0.000321"}
{"level":"DEBUG","ts":"2025-12-05T23:34:19.362Z","msg":"[Tuner Config] variant=ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler | Initial state: alpha=7.249742, beta=0.039760, gamma=17.488733, delta=0.000321 | SLO targets: TTFT=1000.00ms, ITL=10.00ms | Has covariance=true"}
{"level":"INFO","ts":"2025-12-05T23:34:19.362Z","msg":"Tuner validation succeeded for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler, server ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler - New NIS: 0.028397"}
{"level":"DEBUG","ts":"2025-12-05T23:34:19.362Z","msg":"Model tuner results: model=unsloth/Meta-Llama-3.1-8B, accelerator=H100, alpha=7.194972, beta=0.039785, gamma=17.458561, delta=0.000321, NIS=0.03"}
{"level":"DEBUG","ts":"2025-12-05T23:34:19.362Z","msg":"Updated tuner status for variant ms-inference-scheduling-llm-d-modelservice-decode: alpha=7.194972, beta=0.039785, gamma=17.458561, delta=0.000321, NIS=0.028397"}
{"level":"INFO","ts":"2025-12-05T23:34:19.362Z","msg":"Tuned performance parameters result: variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler - alpha: 7.194972, beta: 0.039785, gamma: 17.458561, delta: 0.000321"}
{"level":"DEBUG","ts":"2025-12-05T23:34:19.366Z","msg":"Optimization solution - system: Solution: \ns=ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler; c=Premium; m=unsloth/Meta-Llama-3.1-8B; rate=1189.6; inTk=232; outTk=429; sol=1, sat=false, alloc={acc=H100; numRep=2; maxBatch=512; cost=200, val=-100, itl=8.713646, ttft=20.301298, rho=0.07260241, maxRPM=969.2306}; slo-itl=10, slo-ttft=1000, slo-tps=0 \nAllocationByType: \nname=NVIDIA-H100-80GB-HBM3, count=2, limit=0, cost=200 \ntotalCost=200 \n"}
{"level":"DEBUG","ts":"2025-12-05T23:34:19.366Z","msg":"Setting accelerator name: Name=H100, allocationData={H100 2 512 200 8.713646 20.301298 {1189.6 232 429}}"}
{"level":"INFO","ts":"2025-12-05T23:34:19.366Z","msg":"Model-based optimization completed for model: unsloth/Meta-Llama-3.1-8B - model-based targets: map[ms-inference-scheduling-llm-d-modelservice-decode:2]"}
{"level":"WARN","ts":"2025-12-05T23:34:19.366Z","msg":"saturation analysis unavailable, using model-based targets only: modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-05T23:34:19.366Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-12-05T23:34:19.366Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=scale-down, current=3→target=2"}
{"level":"DEBUG","ts":"2025-12-05T23:34:19.366Z","msg":"Found VA in map: variant=ms-inference-scheduling-llm-d-modelservice-decode, hasCurrentAlloc=true, accelerator=H100"}
{"level":"INFO","ts":"2025-12-05T23:34:19.366Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 3, desired-replicas: 2, accelerator: H100"}
{"level":"INFO","ts":"2025-12-05T23:34:19.366Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=2, accelerator=H100, SaturationOnly=false"}
{"level":"INFO","ts":"2025-12-05T23:34:19.371Z","msg":"Applied Saturation decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=scale-down, current=3, target=2, reason=model-based only (Saturation unavailable)"}
{"level":"INFO","ts":"2025-12-05T23:34:19.371Z","msg":"Reconciliation completed successfully: mode=model-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-12-05T23:35:19.350Z","msg":"saturation scaling config cache updated: entries=1, has_default=true"}
{"level":"INFO","ts":"2025-12-05T23:35:19.350Z","msg":"saturation scaling configuration loaded successfully"}
{"level":"INFO","ts":"2025-12-05T23:35:19.350Z","msg":"Operating in MODEL-ONLY mode: model-based optimization only"}
{"level":"INFO","ts":"2025-12-05T23:35:19.350Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-12-05T23:35:19.350Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-12-05T23:35:19.355Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dgzckd, usage=0.055 (5.5%)"}
{"level":"INFO","ts":"2025-12-05T23:35:19.355Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dgv5kp, usage=0.026 (2.6%)"}
{"level":"INFO","ts":"2025-12-05T23:35:19.355Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dczzts, usage=0.056 (5.6%)"}
{"level":"DEBUG","ts":"2025-12-05T23:35:19.355Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=3"}
{"level":"INFO","ts":"2025-12-05T23:35:19.355Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dgzckd, queueLength=0"}
{"level":"INFO","ts":"2025-12-05T23:35:19.355Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dgv5kp, queueLength=0"}
{"level":"INFO","ts":"2025-12-05T23:35:19.355Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dczzts, queueLength=0"}
{"level":"DEBUG","ts":"2025-12-05T23:35:19.355Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=3"}
{"level":"DEBUG","ts":"2025-12-05T23:35:19.358Z","msg":"Pod-to-variant matching successful: totalPods=3, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:3]"}
{"level":"DEBUG","ts":"2025-12-05T23:35:19.358Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=3"}
{"level":"DEBUG","ts":"2025-12-05T23:35:19.358Z","msg":"Collected capacity metrics for observability in model-only mode: modelID=unsloth/Meta-Llama-3.1-8B, replicaCount=3"}
{"level":"INFO","ts":"2025-12-05T23:35:19.358Z","msg":"Found SLO for model: model=unsloth/Meta-Llama-3.1-8B, class=Premium, slo-tpot=10, slo-ttft=1000"}
{"level":"DEBUG","ts":"2025-12-05T23:35:19.368Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=2, accelerator=H100, ttft=18.39ms, itl=8.84ms, cost=200.00, maxBatch=256, arrivalRate=1172.54, avgInputTokens=241.38, avgOutputTokens=426.85"}
{"level":"DEBUG","ts":"2025-12-05T23:35:19.368Z","msg":"Experimental model tuner is enabled globally (EXPERIMENTAL_MODEL_TUNER_ENABLED=true) tuning model performance parameters for active VAs"}
{"level":"DEBUG","ts":"2025-12-05T23:35:19.368Z","msg":"Using state vals from VA status to tune variant ms-inference-scheduling-llm-d-modelservice-decode: alpha= 7.194972, beta= 0.039785, gamma= 17.458561, delta= 0.000321"}
{"level":"DEBUG","ts":"2025-12-05T23:35:19.368Z","msg":"[Tuner Config] variant=ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler | Initial state: alpha=7.194972, beta=0.039785, gamma=17.458561, delta=0.000321 | SLO targets: TTFT=1000.00ms, ITL=10.00ms | Has covariance=true"}
{"level":"INFO","ts":"2025-12-05T23:35:19.369Z","msg":"Tuner validation succeeded for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler, server ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler - New NIS: 0.088281"}
{"level":"DEBUG","ts":"2025-12-05T23:35:19.369Z","msg":"Model tuner results: model=unsloth/Meta-Llama-3.1-8B, accelerator=H100, alpha=7.287721, beta=0.040235, gamma=17.428787, delta=0.000321, NIS=0.09"}
{"level":"DEBUG","ts":"2025-12-05T23:35:19.369Z","msg":"Updated tuner status for variant ms-inference-scheduling-llm-d-modelservice-decode: alpha=7.287721, beta=0.040235, gamma=17.428787, delta=0.000321, NIS=0.088281"}
{"level":"INFO","ts":"2025-12-05T23:35:19.369Z","msg":"Tuned performance parameters result: variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler - alpha: 7.287721, beta: 0.040235, gamma: 17.428787, delta: 0.000321"}
{"level":"DEBUG","ts":"2025-12-05T23:35:19.372Z","msg":"Optimization solution - system: Solution: \ns=ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler; c=Premium; m=unsloth/Meta-Llama-3.1-8B; rate=1172.54; inTk=241; outTk=426; sol=1, sat=false, alloc={acc=H100; numRep=2; maxBatch=512; cost=200, val=0, itl=8.807573, ttft=20.351, rho=0.071825124, maxRPM=932.59894}; slo-itl=10, slo-ttft=1000, slo-tps=0 \nAllocationByType: \nname=NVIDIA-H100-80GB-HBM3, count=2, limit=0, cost=200 \ntotalCost=200 \n"}
{"level":"DEBUG","ts":"2025-12-05T23:35:19.372Z","msg":"Setting accelerator name: Name=H100, allocationData={H100 2 512 200 8.807573 20.351 {1172.54 241 426}}"}
{"level":"INFO","ts":"2025-12-05T23:35:19.372Z","msg":"Model-based optimization completed for model: unsloth/Meta-Llama-3.1-8B - model-based targets: map[ms-inference-scheduling-llm-d-modelservice-decode:2]"}
{"level":"WARN","ts":"2025-12-05T23:35:19.372Z","msg":"saturation analysis unavailable, using model-based targets only: modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-05T23:35:19.372Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-12-05T23:35:19.373Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=2→target=2"}
{"level":"DEBUG","ts":"2025-12-05T23:35:19.373Z","msg":"Found VA in map: variant=ms-inference-scheduling-llm-d-modelservice-decode, hasCurrentAlloc=true, accelerator=H100"}
{"level":"INFO","ts":"2025-12-05T23:35:19.373Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 2, desired-replicas: 2, accelerator: H100"}
{"level":"INFO","ts":"2025-12-05T23:35:19.373Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=2, accelerator=H100, SaturationOnly=false"}
{"level":"INFO","ts":"2025-12-05T23:35:19.379Z","msg":"Applied Saturation decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=2, target=2, reason=model-based only (Saturation unavailable)"}
{"level":"INFO","ts":"2025-12-05T23:35:19.379Z","msg":"Reconciliation completed successfully: mode=model-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-12-05T23:35:19.380Z","msg":"saturation scaling config cache updated: entries=1, has_default=true"}
{"level":"INFO","ts":"2025-12-05T23:35:19.380Z","msg":"saturation scaling configuration loaded successfully"}
{"level":"INFO","ts":"2025-12-05T23:35:19.380Z","msg":"Operating in MODEL-ONLY mode: model-based optimization only"}
{"level":"INFO","ts":"2025-12-05T23:35:19.380Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-12-05T23:35:19.380Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-12-05T23:35:19.382Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dgzckd, queueLength=0"}
{"level":"INFO","ts":"2025-12-05T23:35:19.382Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dgv5kp, queueLength=0"}
{"level":"INFO","ts":"2025-12-05T23:35:19.382Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dczzts, queueLength=0"}
{"level":"DEBUG","ts":"2025-12-05T23:35:19.382Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=3"}
{"level":"INFO","ts":"2025-12-05T23:35:19.382Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dgzckd, usage=0.055 (5.5%)"}
{"level":"INFO","ts":"2025-12-05T23:35:19.382Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dgv5kp, usage=0.026 (2.6%)"}
{"level":"INFO","ts":"2025-12-05T23:35:19.382Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dczzts, usage=0.056 (5.6%)"}
{"level":"DEBUG","ts":"2025-12-05T23:35:19.382Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=3"}
{"level":"DEBUG","ts":"2025-12-05T23:35:19.384Z","msg":"Pod-to-variant matching successful: totalPods=3, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:3]"}
{"level":"DEBUG","ts":"2025-12-05T23:35:19.384Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=3"}
{"level":"DEBUG","ts":"2025-12-05T23:35:19.384Z","msg":"Collected capacity metrics for observability in model-only mode: modelID=unsloth/Meta-Llama-3.1-8B, replicaCount=3"}
{"level":"INFO","ts":"2025-12-05T23:35:19.384Z","msg":"Found SLO for model: model=unsloth/Meta-Llama-3.1-8B, class=Premium, slo-tpot=10, slo-ttft=1000"}
{"level":"DEBUG","ts":"2025-12-05T23:35:19.394Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=2, accelerator=H100, ttft=18.39ms, itl=8.84ms, cost=200.00, maxBatch=256, arrivalRate=1172.54, avgInputTokens=241.38, avgOutputTokens=426.85"}
{"level":"DEBUG","ts":"2025-12-05T23:35:19.394Z","msg":"Experimental model tuner is enabled globally (EXPERIMENTAL_MODEL_TUNER_ENABLED=true) tuning model performance parameters for active VAs"}
{"level":"DEBUG","ts":"2025-12-05T23:35:19.394Z","msg":"Using state vals from VA status to tune variant ms-inference-scheduling-llm-d-modelservice-decode: alpha= 7.287721, beta= 0.040235, gamma= 17.428787, delta= 0.000321"}
{"level":"DEBUG","ts":"2025-12-05T23:35:19.394Z","msg":"[Tuner Config] variant=ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler | Initial state: alpha=7.287721, beta=0.040235, gamma=17.428787, delta=0.000321 | SLO targets: TTFT=1000.00ms, ITL=10.00ms | Has covariance=true"}
{"level":"INFO","ts":"2025-12-05T23:35:19.394Z","msg":"Tuner validation succeeded for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler, server ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler - New NIS: 0.009184"}
{"level":"DEBUG","ts":"2025-12-05T23:35:19.394Z","msg":"Model tuner results: model=unsloth/Meta-Llama-3.1-8B, accelerator=H100, alpha=7.307698, beta=0.040273, gamma=17.396891, delta=0.000321, NIS=0.01"}
{"level":"DEBUG","ts":"2025-12-05T23:35:19.394Z","msg":"Updated tuner status for variant ms-inference-scheduling-llm-d-modelservice-decode: alpha=7.307698, beta=0.040273, gamma=17.396891, delta=0.000321, NIS=0.009184"}
{"level":"INFO","ts":"2025-12-05T23:35:19.394Z","msg":"Tuned performance parameters result: variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler - alpha: 7.307698, beta: 0.040273, gamma: 17.396891, delta: 0.000321"}
{"level":"DEBUG","ts":"2025-12-05T23:35:19.404Z","msg":"Optimization solution - system: Solution: \ns=ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler; c=Premium; m=unsloth/Meta-Llama-3.1-8B; rate=1172.54; inTk=241; outTk=426; sol=1, sat=false, alloc={acc=H100; numRep=2; maxBatch=512; cost=200, val=0, itl=8.833288, ttft=20.327341, rho=0.07203324, maxRPM=924.74817}; slo-itl=10, slo-ttft=1000, slo-tps=0 \nAllocationByType: \nname=NVIDIA-H100-80GB-HBM3, count=2, limit=0, cost=200 \ntotalCost=200 \n"}
{"level":"DEBUG","ts":"2025-12-05T23:35:19.404Z","msg":"Setting accelerator name: Name=H100, allocationData={H100 2 512 200 8.833288 20.327341 {1172.54 241 426}}"}
{"level":"INFO","ts":"2025-12-05T23:35:19.404Z","msg":"Model-based optimization completed for model: unsloth/Meta-Llama-3.1-8B - model-based targets: map[ms-inference-scheduling-llm-d-modelservice-decode:2]"}
{"level":"WARN","ts":"2025-12-05T23:35:19.404Z","msg":"saturation analysis unavailable, using model-based targets only: modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-05T23:35:19.404Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-12-05T23:35:19.404Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=2→target=2"}
{"level":"DEBUG","ts":"2025-12-05T23:35:19.404Z","msg":"Found VA in map: variant=ms-inference-scheduling-llm-d-modelservice-decode, hasCurrentAlloc=true, accelerator=H100"}
{"level":"INFO","ts":"2025-12-05T23:35:19.404Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 2, desired-replicas: 2, accelerator: H100"}
{"level":"INFO","ts":"2025-12-05T23:35:19.404Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=2, accelerator=H100, SaturationOnly=false"}
{"level":"INFO","ts":"2025-12-05T23:35:19.410Z","msg":"Applied Saturation decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=2, target=2, reason=model-based only (Saturation unavailable)"}
{"level":"INFO","ts":"2025-12-05T23:35:19.410Z","msg":"Reconciliation completed successfully: mode=model-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-12-05T23:36:19.380Z","msg":"saturation scaling config cache updated: entries=1, has_default=true"}
{"level":"INFO","ts":"2025-12-05T23:36:19.380Z","msg":"saturation scaling configuration loaded successfully"}
{"level":"INFO","ts":"2025-12-05T23:36:19.380Z","msg":"Operating in MODEL-ONLY mode: model-based optimization only"}
{"level":"INFO","ts":"2025-12-05T23:36:19.380Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-12-05T23:36:19.380Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-12-05T23:36:19.391Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dgzckd, queueLength=0"}
{"level":"INFO","ts":"2025-12-05T23:36:19.391Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dczzts, queueLength=0"}
{"level":"DEBUG","ts":"2025-12-05T23:36:19.391Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=2"}
{"level":"INFO","ts":"2025-12-05T23:36:19.391Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dgzckd, usage=0.072 (7.2%)"}
{"level":"INFO","ts":"2025-12-05T23:36:19.391Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dczzts, usage=0.058 (5.8%)"}
{"level":"DEBUG","ts":"2025-12-05T23:36:19.391Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=2"}
{"level":"DEBUG","ts":"2025-12-05T23:36:19.394Z","msg":"Pod-to-variant matching successful: totalPods=2, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:2]"}
{"level":"DEBUG","ts":"2025-12-05T23:36:19.394Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=2"}
{"level":"DEBUG","ts":"2025-12-05T23:36:19.394Z","msg":"Collected capacity metrics for observability in model-only mode: modelID=unsloth/Meta-Llama-3.1-8B, replicaCount=2"}
{"level":"INFO","ts":"2025-12-05T23:36:19.394Z","msg":"Found SLO for model: model=unsloth/Meta-Llama-3.1-8B, class=Premium, slo-tpot=10, slo-ttft=1000"}
{"level":"DEBUG","ts":"2025-12-05T23:36:19.404Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=2, accelerator=H100, ttft=18.71ms, itl=9.06ms, cost=200.00, maxBatch=256, arrivalRate=1188.42, avgInputTokens=228.60, avgOutputTokens=464.15"}
{"level":"DEBUG","ts":"2025-12-05T23:36:19.404Z","msg":"Experimental model tuner is enabled globally (EXPERIMENTAL_MODEL_TUNER_ENABLED=true) tuning model performance parameters for active VAs"}
{"level":"DEBUG","ts":"2025-12-05T23:36:19.404Z","msg":"Using state vals from VA status to tune variant ms-inference-scheduling-llm-d-modelservice-decode: alpha= 7.307698, beta= 0.040273, gamma= 17.396891, delta= 0.000321"}
{"level":"DEBUG","ts":"2025-12-05T23:36:19.404Z","msg":"[Tuner Config] variant=ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler | Initial state: alpha=7.307698, beta=0.040273, gamma=17.396891, delta=0.000321 | SLO targets: TTFT=1000.00ms, ITL=10.00ms | Has covariance=true"}
{"level":"INFO","ts":"2025-12-05T23:36:19.404Z","msg":"Tuner validation succeeded for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler, server ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler - New NIS: 0.009222"}
{"level":"DEBUG","ts":"2025-12-05T23:36:19.404Z","msg":"Model tuner results: model=unsloth/Meta-Llama-3.1-8B, accelerator=H100, alpha=7.329494, beta=0.040343, gamma=17.365942, delta=0.000321, NIS=0.01"}
{"level":"DEBUG","ts":"2025-12-05T23:36:19.404Z","msg":"Updated tuner status for variant ms-inference-scheduling-llm-d-modelservice-decode: alpha=7.329494, beta=0.040343, gamma=17.365942, delta=0.000321, NIS=0.009222"}
{"level":"INFO","ts":"2025-12-05T23:36:19.404Z","msg":"Tuned performance parameters result: variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler - alpha: 7.329494, beta: 0.040343, gamma: 17.365942, delta: 0.000321"}
{"level":"DEBUG","ts":"2025-12-05T23:36:19.413Z","msg":"Optimization solution - system: Solution: \ns=ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler; c=Premium; m=unsloth/Meta-Llama-3.1-8B; rate=1188.42; inTk=228; outTk=464; sol=1, sat=false, alloc={acc=H100; numRep=2; maxBatch=512; cost=200, val=0, itl=9.052627, ttft=20.491892, rho=0.08146902, maxRPM=840.82697}; slo-itl=10, slo-ttft=1000, slo-tps=0 \nAllocationByType: \nname=NVIDIA-H100-80GB-HBM3, count=2, limit=0, cost=200 \ntotalCost=200 \n"}
{"level":"DEBUG","ts":"2025-12-05T23:36:19.413Z","msg":"Setting accelerator name: Name=H100, allocationData={H100 2 512 200 9.052627 20.491892 {1188.42 228 464}}"}
{"level":"INFO","ts":"2025-12-05T23:36:19.413Z","msg":"Model-based optimization completed for model: unsloth/Meta-Llama-3.1-8B - model-based targets: map[ms-inference-scheduling-llm-d-modelservice-decode:2]"}
{"level":"WARN","ts":"2025-12-05T23:36:19.413Z","msg":"saturation analysis unavailable, using model-based targets only: modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-05T23:36:19.413Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-12-05T23:36:19.413Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=2→target=2"}
{"level":"DEBUG","ts":"2025-12-05T23:36:19.413Z","msg":"Found VA in map: variant=ms-inference-scheduling-llm-d-modelservice-decode, hasCurrentAlloc=true, accelerator=H100"}
{"level":"INFO","ts":"2025-12-05T23:36:19.413Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 2, desired-replicas: 2, accelerator: H100"}
{"level":"INFO","ts":"2025-12-05T23:36:19.413Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=2, accelerator=H100, SaturationOnly=false"}
{"level":"INFO","ts":"2025-12-05T23:36:19.419Z","msg":"Applied Saturation decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=2, target=2, reason=model-based only (Saturation unavailable)"}
{"level":"INFO","ts":"2025-12-05T23:36:19.419Z","msg":"Reconciliation completed successfully: mode=model-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-12-05T23:36:19.419Z","msg":"saturation scaling config cache updated: entries=1, has_default=true"}
{"level":"INFO","ts":"2025-12-05T23:36:19.419Z","msg":"saturation scaling configuration loaded successfully"}
{"level":"INFO","ts":"2025-12-05T23:36:19.419Z","msg":"Operating in MODEL-ONLY mode: model-based optimization only"}
{"level":"INFO","ts":"2025-12-05T23:36:19.419Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-12-05T23:36:19.419Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-12-05T23:36:19.421Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dgzckd, usage=0.072 (7.2%)"}
{"level":"INFO","ts":"2025-12-05T23:36:19.421Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dczzts, usage=0.058 (5.8%)"}
{"level":"DEBUG","ts":"2025-12-05T23:36:19.421Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=2"}
{"level":"INFO","ts":"2025-12-05T23:36:19.421Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dgzckd, queueLength=0"}
{"level":"INFO","ts":"2025-12-05T23:36:19.421Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dczzts, queueLength=0"}
{"level":"DEBUG","ts":"2025-12-05T23:36:19.421Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=2"}
{"level":"DEBUG","ts":"2025-12-05T23:36:19.426Z","msg":"Pod-to-variant matching successful: totalPods=2, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:2]"}
{"level":"DEBUG","ts":"2025-12-05T23:36:19.426Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=2"}
{"level":"DEBUG","ts":"2025-12-05T23:36:19.426Z","msg":"Collected capacity metrics for observability in model-only mode: modelID=unsloth/Meta-Llama-3.1-8B, replicaCount=2"}
{"level":"INFO","ts":"2025-12-05T23:36:19.426Z","msg":"Found SLO for model: model=unsloth/Meta-Llama-3.1-8B, class=Premium, slo-tpot=10, slo-ttft=1000"}
{"level":"DEBUG","ts":"2025-12-05T23:36:19.435Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=2, accelerator=H100, ttft=18.71ms, itl=9.06ms, cost=200.00, maxBatch=256, arrivalRate=1188.42, avgInputTokens=228.60, avgOutputTokens=464.15"}
{"level":"DEBUG","ts":"2025-12-05T23:36:19.435Z","msg":"Experimental model tuner is enabled globally (EXPERIMENTAL_MODEL_TUNER_ENABLED=true) tuning model performance parameters for active VAs"}
{"level":"DEBUG","ts":"2025-12-05T23:36:19.435Z","msg":"Using state vals from VA status to tune variant ms-inference-scheduling-llm-d-modelservice-decode: alpha= 7.329494, beta= 0.040343, gamma= 17.365942, delta= 0.000321"}
{"level":"DEBUG","ts":"2025-12-05T23:36:19.435Z","msg":"[Tuner Config] variant=ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler | Initial state: alpha=7.329494, beta=0.040343, gamma=17.365942, delta=0.000321 | SLO targets: TTFT=1000.00ms, ITL=10.00ms | Has covariance=true"}
{"level":"INFO","ts":"2025-12-05T23:36:19.436Z","msg":"Tuner validation succeeded for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler, server ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler - New NIS: 0.004966"}
{"level":"DEBUG","ts":"2025-12-05T23:36:19.436Z","msg":"Model tuner results: model=unsloth/Meta-Llama-3.1-8B, accelerator=H100, alpha=7.333964, beta=0.040351, gamma=17.333982, delta=0.000321, NIS=0.00"}
{"level":"DEBUG","ts":"2025-12-05T23:36:19.436Z","msg":"Updated tuner status for variant ms-inference-scheduling-llm-d-modelservice-decode: alpha=7.333964, beta=0.040351, gamma=17.333982, delta=0.000321, NIS=0.004966"}
{"level":"INFO","ts":"2025-12-05T23:36:19.436Z","msg":"Tuned performance parameters result: variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler - alpha: 7.333964, beta: 0.040351, gamma: 17.333982, delta: 0.000321"}
{"level":"DEBUG","ts":"2025-12-05T23:36:19.521Z","msg":"Optimization solution - system: Solution: \ns=ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler; c=Premium; m=unsloth/Meta-Llama-3.1-8B; rate=1188.42; inTk=228; outTk=464; sol=1, sat=false, alloc={acc=H100; numRep=2; maxBatch=512; cost=200, val=0, itl=9.058506, ttft=20.461885, rho=0.081521094, maxRPM=839.24023}; slo-itl=10, slo-ttft=1000, slo-tps=0 \nAllocationByType: \nname=NVIDIA-H100-80GB-HBM3, count=2, limit=0, cost=200 \ntotalCost=200 \n"}
{"level":"DEBUG","ts":"2025-12-05T23:36:19.521Z","msg":"Setting accelerator name: Name=H100, allocationData={H100 2 512 200 9.058506 20.461885 {1188.42 228 464}}"}
{"level":"INFO","ts":"2025-12-05T23:36:19.521Z","msg":"Model-based optimization completed for model: unsloth/Meta-Llama-3.1-8B - model-based targets: map[ms-inference-scheduling-llm-d-modelservice-decode:2]"}
{"level":"WARN","ts":"2025-12-05T23:36:19.521Z","msg":"saturation analysis unavailable, using model-based targets only: modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-05T23:36:19.521Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-12-05T23:36:19.521Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=2→target=2"}
{"level":"DEBUG","ts":"2025-12-05T23:36:19.521Z","msg":"Found VA in map: variant=ms-inference-scheduling-llm-d-modelservice-decode, hasCurrentAlloc=true, accelerator=H100"}
{"level":"INFO","ts":"2025-12-05T23:36:19.521Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 2, desired-replicas: 2, accelerator: H100"}
{"level":"INFO","ts":"2025-12-05T23:36:19.521Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=2, accelerator=H100, SaturationOnly=false"}
{"level":"INFO","ts":"2025-12-05T23:36:19.618Z","msg":"Applied Saturation decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=2, target=2, reason=model-based only (Saturation unavailable)"}
{"level":"INFO","ts":"2025-12-05T23:36:19.618Z","msg":"Reconciliation completed successfully: mode=model-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-12-05T23:37:19.420Z","msg":"saturation scaling config cache updated: entries=1, has_default=true"}
{"level":"INFO","ts":"2025-12-05T23:37:19.420Z","msg":"saturation scaling configuration loaded successfully"}
{"level":"INFO","ts":"2025-12-05T23:37:19.420Z","msg":"Operating in MODEL-ONLY mode: model-based optimization only"}
{"level":"INFO","ts":"2025-12-05T23:37:19.420Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-12-05T23:37:19.420Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-12-05T23:37:19.424Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dgzckd, queueLength=0"}
{"level":"INFO","ts":"2025-12-05T23:37:19.424Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dczzts, queueLength=0"}
{"level":"DEBUG","ts":"2025-12-05T23:37:19.424Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=2"}
{"level":"INFO","ts":"2025-12-05T23:37:19.424Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dgzckd, usage=0.027 (2.7%)"}
{"level":"INFO","ts":"2025-12-05T23:37:19.424Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dczzts, usage=0.031 (3.1%)"}
{"level":"DEBUG","ts":"2025-12-05T23:37:19.424Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=2"}
{"level":"DEBUG","ts":"2025-12-05T23:37:19.427Z","msg":"Pod-to-variant matching successful: totalPods=2, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:2]"}
{"level":"DEBUG","ts":"2025-12-05T23:37:19.427Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=2"}
{"level":"DEBUG","ts":"2025-12-05T23:37:19.427Z","msg":"Collected capacity metrics for observability in model-only mode: modelID=unsloth/Meta-Llama-3.1-8B, replicaCount=2"}
{"level":"INFO","ts":"2025-12-05T23:37:19.427Z","msg":"Found SLO for model: model=unsloth/Meta-Llama-3.1-8B, class=Premium, slo-tpot=10, slo-ttft=1000"}
{"level":"DEBUG","ts":"2025-12-05T23:37:19.437Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=2, accelerator=H100, ttft=16.17ms, itl=7.64ms, cost=200.00, maxBatch=256, arrivalRate=582.14, avgInputTokens=230.48, avgOutputTokens=459.75"}
{"level":"DEBUG","ts":"2025-12-05T23:37:19.437Z","msg":"Experimental model tuner is enabled globally (EXPERIMENTAL_MODEL_TUNER_ENABLED=true) tuning model performance parameters for active VAs"}
{"level":"DEBUG","ts":"2025-12-05T23:37:19.437Z","msg":"Using state vals from VA status to tune variant ms-inference-scheduling-llm-d-modelservice-decode: alpha= 7.333964, beta= 0.040351, gamma= 17.333982, delta= 0.000321"}
{"level":"DEBUG","ts":"2025-12-05T23:37:19.437Z","msg":"[Tuner Config] variant=ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler | Initial state: alpha=7.333964, beta=0.040351, gamma=17.333982, delta=0.000321 | SLO targets: TTFT=1000.00ms, ITL=10.00ms | Has covariance=true"}
{"level":"INFO","ts":"2025-12-05T23:37:19.437Z","msg":"Tuner validation succeeded for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler, server ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler - New NIS: 0.709922"}
{"level":"DEBUG","ts":"2025-12-05T23:37:19.437Z","msg":"Model tuner results: model=unsloth/Meta-Llama-3.1-8B, accelerator=H100, alpha=6.960149, beta=0.042455, gamma=17.290457, delta=0.000321, NIS=0.71"}
{"level":"DEBUG","ts":"2025-12-05T23:37:19.437Z","msg":"Updated tuner status for variant ms-inference-scheduling-llm-d-modelservice-decode: alpha=6.960149, beta=0.042455, gamma=17.290457, delta=0.000321, NIS=0.709922"}
{"level":"INFO","ts":"2025-12-05T23:37:19.437Z","msg":"Tuned performance parameters result: variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler - alpha: 6.960149, beta: 0.042455, gamma: 17.290457, delta: 0.000321"}
{"level":"DEBUG","ts":"2025-12-05T23:37:19.447Z","msg":"Optimization solution - system: Solution: \ns=ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler; c=Premium; m=unsloth/Meta-Llama-3.1-8B; rate=582.14; inTk=230; outTk=459; sol=1, sat=false, alloc={acc=H100; numRep=1; maxBatch=512; cost=100, val=-100, itl=8.641131, ttft=20.213673, rho=0.075379774, maxRPM=920.3775}; slo-itl=10, slo-ttft=1000, slo-tps=0 \nAllocationByType: \nname=NVIDIA-H100-80GB-HBM3, count=1, limit=0, cost=100 \ntotalCost=100 \n"}
{"level":"DEBUG","ts":"2025-12-05T23:37:19.447Z","msg":"Setting accelerator name: Name=H100, allocationData={H100 1 512 100 8.641131 20.213673 {582.14 230 459}}"}
{"level":"INFO","ts":"2025-12-05T23:37:19.447Z","msg":"Model-based optimization completed for model: unsloth/Meta-Llama-3.1-8B - model-based targets: map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"WARN","ts":"2025-12-05T23:37:19.447Z","msg":"saturation analysis unavailable, using model-based targets only: modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-05T23:37:19.447Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-12-05T23:37:19.447Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=scale-down, current=2→target=1"}
{"level":"DEBUG","ts":"2025-12-05T23:37:19.447Z","msg":"Found VA in map: variant=ms-inference-scheduling-llm-d-modelservice-decode, hasCurrentAlloc=true, accelerator=H100"}
{"level":"INFO","ts":"2025-12-05T23:37:19.447Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 2, desired-replicas: 1, accelerator: H100"}
{"level":"INFO","ts":"2025-12-05T23:37:19.447Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=1, accelerator=H100, SaturationOnly=false"}
{"level":"INFO","ts":"2025-12-05T23:37:19.453Z","msg":"Applied Saturation decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=scale-down, current=2, target=1, reason=model-based only (Saturation unavailable)"}
{"level":"INFO","ts":"2025-12-05T23:37:19.453Z","msg":"Reconciliation completed successfully: mode=model-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-12-05T23:37:19.618Z","msg":"saturation scaling config cache updated: entries=1, has_default=true"}
{"level":"INFO","ts":"2025-12-05T23:37:19.618Z","msg":"saturation scaling configuration loaded successfully"}
{"level":"INFO","ts":"2025-12-05T23:37:19.618Z","msg":"Operating in MODEL-ONLY mode: model-based optimization only"}
{"level":"INFO","ts":"2025-12-05T23:37:19.618Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-12-05T23:37:19.618Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-12-05T23:37:19.620Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dgzckd, usage=0.027 (2.7%)"}
{"level":"INFO","ts":"2025-12-05T23:37:19.620Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dczzts, usage=0.031 (3.1%)"}
{"level":"DEBUG","ts":"2025-12-05T23:37:19.620Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=2"}
{"level":"INFO","ts":"2025-12-05T23:37:19.620Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dgzckd, queueLength=0"}
{"level":"INFO","ts":"2025-12-05T23:37:19.620Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dczzts, queueLength=0"}
{"level":"DEBUG","ts":"2025-12-05T23:37:19.620Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=2"}
{"level":"DEBUG","ts":"2025-12-05T23:37:19.622Z","msg":"Pod-to-variant matching successful: totalPods=2, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:2]"}
{"level":"DEBUG","ts":"2025-12-05T23:37:19.622Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=2"}
{"level":"DEBUG","ts":"2025-12-05T23:37:19.622Z","msg":"Collected capacity metrics for observability in model-only mode: modelID=unsloth/Meta-Llama-3.1-8B, replicaCount=2"}
{"level":"INFO","ts":"2025-12-05T23:37:19.623Z","msg":"Found SLO for model: model=unsloth/Meta-Llama-3.1-8B, class=Premium, slo-tpot=10, slo-ttft=1000"}
{"level":"DEBUG","ts":"2025-12-05T23:37:19.631Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=2, accelerator=H100, ttft=16.17ms, itl=7.64ms, cost=200.00, maxBatch=256, arrivalRate=582.14, avgInputTokens=230.48, avgOutputTokens=459.75"}
{"level":"DEBUG","ts":"2025-12-05T23:37:19.631Z","msg":"Experimental model tuner is enabled globally (EXPERIMENTAL_MODEL_TUNER_ENABLED=true) tuning model performance parameters for active VAs"}
{"level":"DEBUG","ts":"2025-12-05T23:37:19.631Z","msg":"Using state vals from VA status to tune variant ms-inference-scheduling-llm-d-modelservice-decode: alpha= 6.960149, beta= 0.042455, gamma= 17.290457, delta= 0.000321"}
{"level":"DEBUG","ts":"2025-12-05T23:37:19.631Z","msg":"[Tuner Config] variant=ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler | Initial state: alpha=6.960149, beta=0.042455, gamma=17.290457, delta=0.000321 | SLO targets: TTFT=1000.00ms, ITL=10.00ms | Has covariance=true"}
{"level":"INFO","ts":"2025-12-05T23:37:19.632Z","msg":"Tuner validation succeeded for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler, server ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler - New NIS: 0.043735"}
{"level":"DEBUG","ts":"2025-12-05T23:37:19.632Z","msg":"Model tuner results: model=unsloth/Meta-Llama-3.1-8B, accelerator=H100, alpha=6.892584, beta=0.042546, gamma=17.242720, delta=0.000321, NIS=0.04"}
{"level":"DEBUG","ts":"2025-12-05T23:37:19.632Z","msg":"Updated tuner status for variant ms-inference-scheduling-llm-d-modelservice-decode: alpha=6.892584, beta=0.042546, gamma=17.242720, delta=0.000321, NIS=0.043735"}
{"level":"INFO","ts":"2025-12-05T23:37:19.632Z","msg":"Tuned performance parameters result: variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler - alpha: 6.892584, beta: 0.042546, gamma: 17.242720, delta: 0.000321"}
{"level":"DEBUG","ts":"2025-12-05T23:37:19.641Z","msg":"Optimization solution - system: Solution: \ns=ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler; c=Premium; m=unsloth/Meta-Llama-3.1-8B; rate=582.14; inTk=230; outTk=459; sol=1, sat=false, alloc={acc=H100; numRep=1; maxBatch=512; cost=100, val=-100, itl=8.562216, ttft=20.139997, rho=0.07469347, maxRPM=939.07153}; slo-itl=10, slo-ttft=1000, slo-tps=0 \nAllocationByType: \nname=NVIDIA-H100-80GB-HBM3, count=1, limit=0, cost=100 \ntotalCost=100 \n"}
{"level":"DEBUG","ts":"2025-12-05T23:37:19.641Z","msg":"Setting accelerator name: Name=H100, allocationData={H100 1 512 100 8.562216 20.139997 {582.14 230 459}}"}
{"level":"INFO","ts":"2025-12-05T23:37:19.641Z","msg":"Model-based optimization completed for model: unsloth/Meta-Llama-3.1-8B - model-based targets: map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"WARN","ts":"2025-12-05T23:37:19.641Z","msg":"saturation analysis unavailable, using model-based targets only: modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-05T23:37:19.641Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-12-05T23:37:19.641Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=scale-down, current=2→target=1"}
{"level":"DEBUG","ts":"2025-12-05T23:37:19.641Z","msg":"Found VA in map: variant=ms-inference-scheduling-llm-d-modelservice-decode, hasCurrentAlloc=true, accelerator=H100"}
{"level":"INFO","ts":"2025-12-05T23:37:19.641Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 2, desired-replicas: 1, accelerator: H100"}
{"level":"INFO","ts":"2025-12-05T23:37:19.641Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=1, accelerator=H100, SaturationOnly=false"}
{"level":"INFO","ts":"2025-12-05T23:37:19.647Z","msg":"Applied Saturation decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=scale-down, current=2, target=1, reason=model-based only (Saturation unavailable)"}
{"level":"INFO","ts":"2025-12-05T23:37:19.647Z","msg":"Reconciliation completed successfully: mode=model-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-12-05T23:38:19.454Z","msg":"saturation scaling config cache updated: entries=1, has_default=true"}
{"level":"INFO","ts":"2025-12-05T23:38:19.454Z","msg":"saturation scaling configuration loaded successfully"}
{"level":"INFO","ts":"2025-12-05T23:38:19.454Z","msg":"Operating in MODEL-ONLY mode: model-based optimization only"}
{"level":"INFO","ts":"2025-12-05T23:38:19.455Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-12-05T23:38:19.455Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-12-05T23:38:19.464Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dczzts, queueLength=0"}
{"level":"DEBUG","ts":"2025-12-05T23:38:19.464Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"INFO","ts":"2025-12-05T23:38:19.464Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dczzts, usage=0.055 (5.5%)"}
{"level":"DEBUG","ts":"2025-12-05T23:38:19.464Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"DEBUG","ts":"2025-12-05T23:38:19.467Z","msg":"Pod-to-variant matching successful: totalPods=1, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"DEBUG","ts":"2025-12-05T23:38:19.467Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=1"}
{"level":"DEBUG","ts":"2025-12-05T23:38:19.467Z","msg":"Collected capacity metrics for observability in model-only mode: modelID=unsloth/Meta-Llama-3.1-8B, replicaCount=1"}
{"level":"INFO","ts":"2025-12-05T23:38:19.467Z","msg":"Found SLO for model: model=unsloth/Meta-Llama-3.1-8B, class=Premium, slo-tpot=10, slo-ttft=1000"}
{"level":"DEBUG","ts":"2025-12-05T23:38:19.475Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=1, accelerator=H100, ttft=18.94ms, itl=9.06ms, cost=100.00, maxBatch=256, arrivalRate=573.75, avgInputTokens=239.06, avgOutputTokens=439.99"}
{"level":"DEBUG","ts":"2025-12-05T23:38:19.475Z","msg":"Experimental model tuner is enabled globally (EXPERIMENTAL_MODEL_TUNER_ENABLED=true) tuning model performance parameters for active VAs"}
{"level":"DEBUG","ts":"2025-12-05T23:38:19.475Z","msg":"Using state vals from VA status to tune variant ms-inference-scheduling-llm-d-modelservice-decode: alpha= 6.892584, beta= 0.042546, gamma= 17.242720, delta= 0.000321"}
{"level":"DEBUG","ts":"2025-12-05T23:38:19.475Z","msg":"[Tuner Config] variant=ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler | Initial state: alpha=6.892584, beta=0.042546, gamma=17.242720, delta=0.000321 | SLO targets: TTFT=1000.00ms, ITL=10.00ms | Has covariance=true"}
{"level":"INFO","ts":"2025-12-05T23:38:19.476Z","msg":"Tuner validation succeeded for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler, server ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler - New NIS: 1.110439"}
{"level":"DEBUG","ts":"2025-12-05T23:38:19.476Z","msg":"Model tuner results: model=unsloth/Meta-Llama-3.1-8B, accelerator=H100, alpha=7.197744, beta=0.045281, gamma=17.221949, delta=0.000321, NIS=1.11"}
{"level":"DEBUG","ts":"2025-12-05T23:38:19.476Z","msg":"Updated tuner status for variant ms-inference-scheduling-llm-d-modelservice-decode: alpha=7.197744, beta=0.045281, gamma=17.221949, delta=0.000321, NIS=1.110439"}
{"level":"INFO","ts":"2025-12-05T23:38:19.476Z","msg":"Tuned performance parameters result: variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler - alpha: 7.197744, beta: 0.045281, gamma: 17.221949, delta: 0.000321"}
{"level":"DEBUG","ts":"2025-12-05T23:38:19.485Z","msg":"Optimization solution - system: Solution: \ns=ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler; c=Premium; m=unsloth/Meta-Llama-3.1-8B; rate=573.75; inTk=239; outTk=439; sol=1, sat=false, alloc={acc=H100; numRep=1; maxBatch=512; cost=100, val=0, itl=8.948957, ttft=20.189009, rho=0.07358329, maxRPM=829.8987}; slo-itl=10, slo-ttft=1000, slo-tps=0 \nAllocationByType: \nname=NVIDIA-H100-80GB-HBM3, count=1, limit=0, cost=100 \ntotalCost=100 \n"}
{"level":"DEBUG","ts":"2025-12-05T23:38:19.485Z","msg":"Setting accelerator name: Name=H100, allocationData={H100 1 512 100 8.948957 20.189009 {573.75 239 439}}"}
{"level":"INFO","ts":"2025-12-05T23:38:19.485Z","msg":"Model-based optimization completed for model: unsloth/Meta-Llama-3.1-8B - model-based targets: map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"WARN","ts":"2025-12-05T23:38:19.485Z","msg":"saturation analysis unavailable, using model-based targets only: modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-05T23:38:19.485Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-12-05T23:38:19.485Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1→target=1"}
{"level":"DEBUG","ts":"2025-12-05T23:38:19.485Z","msg":"Found VA in map: variant=ms-inference-scheduling-llm-d-modelservice-decode, hasCurrentAlloc=true, accelerator=H100"}
{"level":"INFO","ts":"2025-12-05T23:38:19.485Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 1, desired-replicas: 1, accelerator: H100"}
{"level":"INFO","ts":"2025-12-05T23:38:19.485Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=1, accelerator=H100, SaturationOnly=false"}
{"level":"INFO","ts":"2025-12-05T23:38:19.491Z","msg":"Applied Saturation decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1, target=1, reason=model-based only (Saturation unavailable)"}
{"level":"INFO","ts":"2025-12-05T23:38:19.491Z","msg":"Reconciliation completed successfully: mode=model-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-12-05T23:38:19.648Z","msg":"saturation scaling config cache updated: entries=1, has_default=true"}
{"level":"INFO","ts":"2025-12-05T23:38:19.648Z","msg":"saturation scaling configuration loaded successfully"}
{"level":"INFO","ts":"2025-12-05T23:38:19.648Z","msg":"Operating in MODEL-ONLY mode: model-based optimization only"}
{"level":"INFO","ts":"2025-12-05T23:38:19.648Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-12-05T23:38:19.648Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-12-05T23:38:19.651Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dczzts, queueLength=0"}
{"level":"DEBUG","ts":"2025-12-05T23:38:19.651Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"INFO","ts":"2025-12-05T23:38:19.651Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dczzts, usage=0.055 (5.5%)"}
{"level":"DEBUG","ts":"2025-12-05T23:38:19.651Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"DEBUG","ts":"2025-12-05T23:38:19.653Z","msg":"Pod-to-variant matching successful: totalPods=1, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"DEBUG","ts":"2025-12-05T23:38:19.653Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=1"}
{"level":"DEBUG","ts":"2025-12-05T23:38:19.653Z","msg":"Collected capacity metrics for observability in model-only mode: modelID=unsloth/Meta-Llama-3.1-8B, replicaCount=1"}
{"level":"INFO","ts":"2025-12-05T23:38:19.653Z","msg":"Found SLO for model: model=unsloth/Meta-Llama-3.1-8B, class=Premium, slo-tpot=10, slo-ttft=1000"}
{"level":"DEBUG","ts":"2025-12-05T23:38:19.661Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=1, accelerator=H100, ttft=18.94ms, itl=9.06ms, cost=100.00, maxBatch=256, arrivalRate=573.75, avgInputTokens=239.06, avgOutputTokens=439.99"}
{"level":"DEBUG","ts":"2025-12-05T23:38:19.661Z","msg":"Experimental model tuner is enabled globally (EXPERIMENTAL_MODEL_TUNER_ENABLED=true) tuning model performance parameters for active VAs"}
{"level":"DEBUG","ts":"2025-12-05T23:38:19.661Z","msg":"Using state vals from VA status to tune variant ms-inference-scheduling-llm-d-modelservice-decode: alpha= 7.197744, beta= 0.045281, gamma= 17.221949, delta= 0.000321"}
{"level":"DEBUG","ts":"2025-12-05T23:38:19.661Z","msg":"[Tuner Config] variant=ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler | Initial state: alpha=7.197744, beta=0.045281, gamma=17.221949, delta=0.000321 | SLO targets: TTFT=1000.00ms, ITL=10.00ms | Has covariance=true"}
{"level":"INFO","ts":"2025-12-05T23:38:19.662Z","msg":"Tuner validation succeeded for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler, server ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler - New NIS: 0.039751"}
{"level":"DEBUG","ts":"2025-12-05T23:38:19.662Z","msg":"Model tuner results: model=unsloth/Meta-Llama-3.1-8B, accelerator=H100, alpha=7.260808, beta=0.045518, gamma=17.195566, delta=0.000321, NIS=0.04"}
{"level":"DEBUG","ts":"2025-12-05T23:38:19.662Z","msg":"Updated tuner status for variant ms-inference-scheduling-llm-d-modelservice-decode: alpha=7.260808, beta=0.045518, gamma=17.195566, delta=0.000321, NIS=0.039751"}
{"level":"INFO","ts":"2025-12-05T23:38:19.662Z","msg":"Tuned performance parameters result: variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler - alpha: 7.260808, beta: 0.045518, gamma: 17.195566, delta: 0.000321"}
{"level":"DEBUG","ts":"2025-12-05T23:38:19.671Z","msg":"Optimization solution - system: Solution: \ns=ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler; c=Premium; m=unsloth/Meta-Llama-3.1-8B; rate=573.75; inTk=239; outTk=439; sol=1, sat=false, alloc={acc=H100; numRep=1; maxBatch=512; cost=100, val=0, itl=9.0382185, ttft=20.191292, rho=0.07431353, maxRPM=806.64526}; slo-itl=10, slo-ttft=1000, slo-tps=0 \nAllocationByType: \nname=NVIDIA-H100-80GB-HBM3, count=1, limit=0, cost=100 \ntotalCost=100 \n"}
{"level":"DEBUG","ts":"2025-12-05T23:38:19.671Z","msg":"Setting accelerator name: Name=H100, allocationData={H100 1 512 100 9.0382185 20.191292 {573.75 239 439}}"}
{"level":"INFO","ts":"2025-12-05T23:38:19.671Z","msg":"Model-based optimization completed for model: unsloth/Meta-Llama-3.1-8B - model-based targets: map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"WARN","ts":"2025-12-05T23:38:19.671Z","msg":"saturation analysis unavailable, using model-based targets only: modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-05T23:38:19.671Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-12-05T23:38:19.671Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1→target=1"}
{"level":"DEBUG","ts":"2025-12-05T23:38:19.671Z","msg":"Found VA in map: variant=ms-inference-scheduling-llm-d-modelservice-decode, hasCurrentAlloc=true, accelerator=H100"}
{"level":"INFO","ts":"2025-12-05T23:38:19.671Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 1, desired-replicas: 1, accelerator: H100"}
{"level":"INFO","ts":"2025-12-05T23:38:19.671Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=1, accelerator=H100, SaturationOnly=false"}
{"level":"INFO","ts":"2025-12-05T23:38:19.677Z","msg":"Applied Saturation decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1, target=1, reason=model-based only (Saturation unavailable)"}
{"level":"INFO","ts":"2025-12-05T23:38:19.677Z","msg":"Reconciliation completed successfully: mode=model-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-12-05T23:39:19.492Z","msg":"saturation scaling config cache updated: entries=1, has_default=true"}
{"level":"INFO","ts":"2025-12-05T23:39:19.492Z","msg":"saturation scaling configuration loaded successfully"}
{"level":"INFO","ts":"2025-12-05T23:39:19.492Z","msg":"Operating in MODEL-ONLY mode: model-based optimization only"}
{"level":"INFO","ts":"2025-12-05T23:39:19.492Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-12-05T23:39:19.492Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-12-05T23:39:19.497Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dczzts, usage=0.084 (8.4%)"}
{"level":"DEBUG","ts":"2025-12-05T23:39:19.497Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"INFO","ts":"2025-12-05T23:39:19.497Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dczzts, queueLength=0"}
{"level":"DEBUG","ts":"2025-12-05T23:39:19.497Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"DEBUG","ts":"2025-12-05T23:39:19.499Z","msg":"Pod-to-variant matching successful: totalPods=1, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"DEBUG","ts":"2025-12-05T23:39:19.499Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=1"}
{"level":"DEBUG","ts":"2025-12-05T23:39:19.499Z","msg":"Collected capacity metrics for observability in model-only mode: modelID=unsloth/Meta-Llama-3.1-8B, replicaCount=1"}
{"level":"INFO","ts":"2025-12-05T23:39:19.499Z","msg":"Found SLO for model: model=unsloth/Meta-Llama-3.1-8B, class=Premium, slo-tpot=10, slo-ttft=1000"}
{"level":"DEBUG","ts":"2025-12-05T23:39:19.508Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=1, accelerator=H100, ttft=19.45ms, itl=9.45ms, cost=100.00, maxBatch=256, arrivalRate=598.69, avgInputTokens=223.85, avgOutputTokens=472.63"}
{"level":"DEBUG","ts":"2025-12-05T23:39:19.508Z","msg":"Experimental model tuner is enabled globally (EXPERIMENTAL_MODEL_TUNER_ENABLED=true) tuning model performance parameters for active VAs"}
{"level":"DEBUG","ts":"2025-12-05T23:39:19.508Z","msg":"Using state vals from VA status to tune variant ms-inference-scheduling-llm-d-modelservice-decode: alpha= 7.260808, beta= 0.045518, gamma= 17.195566, delta= 0.000321"}
{"level":"DEBUG","ts":"2025-12-05T23:39:19.508Z","msg":"[Tuner Config] variant=ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler | Initial state: alpha=7.260808, beta=0.045518, gamma=17.195566, delta=0.000321 | SLO targets: TTFT=1000.00ms, ITL=10.00ms | Has covariance=true"}
{"level":"INFO","ts":"2025-12-05T23:39:19.509Z","msg":"Tuner validation succeeded for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler, server ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler - New NIS: 0.058993"}
{"level":"DEBUG","ts":"2025-12-05T23:39:19.509Z","msg":"Model tuner results: model=unsloth/Meta-Llama-3.1-8B, accelerator=H100, alpha=7.335711, beta=0.045906, gamma=17.174093, delta=0.000321, NIS=0.06"}
{"level":"DEBUG","ts":"2025-12-05T23:39:19.509Z","msg":"Updated tuner status for variant ms-inference-scheduling-llm-d-modelservice-decode: alpha=7.335711, beta=0.045906, gamma=17.174093, delta=0.000321, NIS=0.058993"}
{"level":"INFO","ts":"2025-12-05T23:39:19.509Z","msg":"Tuned performance parameters result: variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler - alpha: 7.335711, beta: 0.045906, gamma: 17.174093, delta: 0.000321"}
{"level":"DEBUG","ts":"2025-12-05T23:39:19.518Z","msg":"Optimization solution - system: Solution: \ns=ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler; c=Premium; m=unsloth/Meta-Llama-3.1-8B; rate=598.69; inTk=223; outTk=472; sol=1, sat=false, alloc={acc=H100; numRep=1; maxBatch=512; cost=100, val=0, itl=9.424187, ttft=20.43072, rho=0.086904034, maxRPM=723.32513}; slo-itl=10, slo-ttft=1000, slo-tps=0 \nAllocationByType: \nname=NVIDIA-H100-80GB-HBM3, count=1, limit=0, cost=100 \ntotalCost=100 \n"}
{"level":"DEBUG","ts":"2025-12-05T23:39:19.518Z","msg":"Setting accelerator name: Name=H100, allocationData={H100 1 512 100 9.424187 20.43072 {598.69 223 472}}"}
{"level":"INFO","ts":"2025-12-05T23:39:19.518Z","msg":"Model-based optimization completed for model: unsloth/Meta-Llama-3.1-8B - model-based targets: map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"WARN","ts":"2025-12-05T23:39:19.518Z","msg":"saturation analysis unavailable, using model-based targets only: modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-05T23:39:19.518Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-12-05T23:39:19.518Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1→target=1"}
{"level":"DEBUG","ts":"2025-12-05T23:39:19.518Z","msg":"Found VA in map: variant=ms-inference-scheduling-llm-d-modelservice-decode, hasCurrentAlloc=true, accelerator=H100"}
{"level":"INFO","ts":"2025-12-05T23:39:19.518Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 1, desired-replicas: 1, accelerator: H100"}
{"level":"INFO","ts":"2025-12-05T23:39:19.518Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=1, accelerator=H100, SaturationOnly=false"}
{"level":"INFO","ts":"2025-12-05T23:39:19.524Z","msg":"Applied Saturation decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1, target=1, reason=model-based only (Saturation unavailable)"}
{"level":"INFO","ts":"2025-12-05T23:39:19.524Z","msg":"Reconciliation completed successfully: mode=model-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-12-05T23:39:19.677Z","msg":"saturation scaling config cache updated: entries=1, has_default=true"}
{"level":"INFO","ts":"2025-12-05T23:39:19.677Z","msg":"saturation scaling configuration loaded successfully"}
{"level":"INFO","ts":"2025-12-05T23:39:19.677Z","msg":"Operating in MODEL-ONLY mode: model-based optimization only"}
{"level":"INFO","ts":"2025-12-05T23:39:19.677Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-12-05T23:39:19.677Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-12-05T23:39:19.681Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dczzts, queueLength=0"}
{"level":"DEBUG","ts":"2025-12-05T23:39:19.681Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"INFO","ts":"2025-12-05T23:39:19.681Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dczzts, usage=0.084 (8.4%)"}
{"level":"DEBUG","ts":"2025-12-05T23:39:19.681Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"DEBUG","ts":"2025-12-05T23:39:19.683Z","msg":"Pod-to-variant matching successful: totalPods=1, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"DEBUG","ts":"2025-12-05T23:39:19.683Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=1"}
{"level":"DEBUG","ts":"2025-12-05T23:39:19.683Z","msg":"Collected capacity metrics for observability in model-only mode: modelID=unsloth/Meta-Llama-3.1-8B, replicaCount=1"}
{"level":"INFO","ts":"2025-12-05T23:39:19.683Z","msg":"Found SLO for model: model=unsloth/Meta-Llama-3.1-8B, class=Premium, slo-tpot=10, slo-ttft=1000"}
{"level":"DEBUG","ts":"2025-12-05T23:39:19.691Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=1, accelerator=H100, ttft=19.45ms, itl=9.45ms, cost=100.00, maxBatch=256, arrivalRate=598.69, avgInputTokens=223.85, avgOutputTokens=472.63"}
{"level":"DEBUG","ts":"2025-12-05T23:39:19.691Z","msg":"Experimental model tuner is enabled globally (EXPERIMENTAL_MODEL_TUNER_ENABLED=true) tuning model performance parameters for active VAs"}
{"level":"DEBUG","ts":"2025-12-05T23:39:19.691Z","msg":"Using state vals from VA status to tune variant ms-inference-scheduling-llm-d-modelservice-decode: alpha= 7.335711, beta= 0.045906, gamma= 17.174093, delta= 0.000321"}
{"level":"DEBUG","ts":"2025-12-05T23:39:19.691Z","msg":"[Tuner Config] variant=ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler | Initial state: alpha=7.335711, beta=0.045906, gamma=17.174093, delta=0.000321 | SLO targets: TTFT=1000.00ms, ITL=10.00ms | Has covariance=true"}
{"level":"INFO","ts":"2025-12-05T23:39:19.692Z","msg":"Tuner validation succeeded for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler, server ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler - New NIS: 0.003331"}
{"level":"DEBUG","ts":"2025-12-05T23:39:19.692Z","msg":"Model tuner results: model=unsloth/Meta-Llama-3.1-8B, accelerator=H100, alpha=7.350417, beta=0.045946, gamma=17.152187, delta=0.000321, NIS=0.00"}
{"level":"DEBUG","ts":"2025-12-05T23:39:19.692Z","msg":"Updated tuner status for variant ms-inference-scheduling-llm-d-modelservice-decode: alpha=7.350417, beta=0.045946, gamma=17.152187, delta=0.000321, NIS=0.003331"}
{"level":"INFO","ts":"2025-12-05T23:39:19.692Z","msg":"Tuned performance parameters result: variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler - alpha: 7.350417, beta: 0.045946, gamma: 17.152187, delta: 0.000321"}
{"level":"DEBUG","ts":"2025-12-05T23:39:19.701Z","msg":"Optimization solution - system: Solution: \ns=ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler; c=Premium; m=unsloth/Meta-Llama-3.1-8B; rate=598.69; inTk=223; outTk=472; sol=1, sat=false, alloc={acc=H100; numRep=1; maxBatch=512; cost=100, val=0, itl=9.445274, ttft=20.415892, rho=0.08709732, maxRPM=718.6268}; slo-itl=10, slo-ttft=1000, slo-tps=0 \nAllocationByType: \nname=NVIDIA-H100-80GB-HBM3, count=1, limit=0, cost=100 \ntotalCost=100 \n"}
{"level":"DEBUG","ts":"2025-12-05T23:39:19.701Z","msg":"Setting accelerator name: Name=H100, allocationData={H100 1 512 100 9.445274 20.415892 {598.69 223 472}}"}
{"level":"INFO","ts":"2025-12-05T23:39:19.701Z","msg":"Model-based optimization completed for model: unsloth/Meta-Llama-3.1-8B - model-based targets: map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"WARN","ts":"2025-12-05T23:39:19.701Z","msg":"saturation analysis unavailable, using model-based targets only: modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-05T23:39:19.701Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-12-05T23:39:19.701Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1→target=1"}
{"level":"DEBUG","ts":"2025-12-05T23:39:19.701Z","msg":"Found VA in map: variant=ms-inference-scheduling-llm-d-modelservice-decode, hasCurrentAlloc=true, accelerator=H100"}
{"level":"INFO","ts":"2025-12-05T23:39:19.701Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 1, desired-replicas: 1, accelerator: H100"}
{"level":"INFO","ts":"2025-12-05T23:39:19.701Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=1, accelerator=H100, SaturationOnly=false"}
{"level":"INFO","ts":"2025-12-05T23:39:19.706Z","msg":"Applied Saturation decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1, target=1, reason=model-based only (Saturation unavailable)"}
{"level":"INFO","ts":"2025-12-05T23:39:19.706Z","msg":"Reconciliation completed successfully: mode=model-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-12-05T23:40:19.525Z","msg":"saturation scaling config cache updated: entries=1, has_default=true"}
{"level":"INFO","ts":"2025-12-05T23:40:19.525Z","msg":"saturation scaling configuration loaded successfully"}
{"level":"INFO","ts":"2025-12-05T23:40:19.525Z","msg":"Operating in MODEL-ONLY mode: model-based optimization only"}
{"level":"INFO","ts":"2025-12-05T23:40:19.525Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-12-05T23:40:19.525Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-12-05T23:40:19.534Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dczzts, queueLength=0"}
{"level":"DEBUG","ts":"2025-12-05T23:40:19.534Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"INFO","ts":"2025-12-05T23:40:19.534Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dczzts, usage=0.000 (0.0%)"}
{"level":"DEBUG","ts":"2025-12-05T23:40:19.534Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"DEBUG","ts":"2025-12-05T23:40:19.536Z","msg":"Pod-to-variant matching successful: totalPods=1, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"DEBUG","ts":"2025-12-05T23:40:19.536Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=1"}
{"level":"DEBUG","ts":"2025-12-05T23:40:19.536Z","msg":"Collected capacity metrics for observability in model-only mode: modelID=unsloth/Meta-Llama-3.1-8B, replicaCount=1"}
{"level":"INFO","ts":"2025-12-05T23:40:19.536Z","msg":"Found SLO for model: model=unsloth/Meta-Llama-3.1-8B, class=Premium, slo-tpot=10, slo-ttft=1000"}
{"level":"DEBUG","ts":"2025-12-05T23:40:19.545Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=1, accelerator=H100, ttft=0.00ms, itl=0.00ms, cost=100.00, maxBatch=256, arrivalRate=0.00, avgInputTokens=0.00, avgOutputTokens=0.00"}
{"level":"DEBUG","ts":"2025-12-05T23:40:19.545Z","msg":"Experimental model tuner is enabled globally (EXPERIMENTAL_MODEL_TUNER_ENABLED=true) tuning model performance parameters for active VAs"}
{"level":"DEBUG","ts":"2025-12-05T23:40:19.545Z","msg":"Using state vals from VA status to tune variant ms-inference-scheduling-llm-d-modelservice-decode: alpha= 7.350417, beta= 0.045946, gamma= 17.152187, delta= 0.000321"}
{"level":"DEBUG","ts":"2025-12-05T23:40:19.545Z","msg":"[Tuner Config] variant=ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler | Initial state: alpha=7.350417, beta=0.045946, gamma=17.152187, delta=0.000321 | SLO targets: TTFT=1000.00ms, ITL=10.00ms | Has covariance=true"}
{"level":"WARN","ts":"2025-12-05T23:40:19.545Z","msg":"Tuner failed completely for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler: failed to get/create tuner: failed to create tuner: invalid environment: &{0 0 0 256 0 0 1}. Using fallback parameters."}
{"level":"INFO","ts":"2025-12-05T23:40:19.545Z","msg":"Using tuned parameters from status for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler"}
{"level":"DEBUG","ts":"2025-12-05T23:40:19.545Z","msg":"Updated SystemData for model=unsloth/Meta-Llama-3.1-8B, accelerator=H100: alpha=7.350417, beta=0.045946, gamma=17.152187, delta=0.000321"}
{"level":"DEBUG","ts":"2025-12-05T23:40:19.545Z","msg":"Optimization solution - system: Solution: \ns=ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler; c=Premium; m=unsloth/Meta-Llama-3.1-8B; rate=0; inTk=0; outTk=0; sol=1, sat=false, alloc={acc=H100; numRep=1; maxBatch=512; cost=100, val=0, itl=7.3963633, ttft=17.152508, rho=0, maxRPM=639636.56}; slo-itl=10, slo-ttft=1000, slo-tps=0 \nAllocationByType: \nname=NVIDIA-H100-80GB-HBM3, count=1, limit=0, cost=100 \ntotalCost=100 \n"}
{"level":"DEBUG","ts":"2025-12-05T23:40:19.545Z","msg":"Setting accelerator name: Name=H100, allocationData={H100 1 512 100 7.3963633 17.152508 {0 0 0}}"}
{"level":"INFO","ts":"2025-12-05T23:40:19.545Z","msg":"Model-based optimization completed for model: unsloth/Meta-Llama-3.1-8B - model-based targets: map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"WARN","ts":"2025-12-05T23:40:19.545Z","msg":"saturation analysis unavailable, using model-based targets only: modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-05T23:40:19.545Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-12-05T23:40:19.545Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1→target=1"}
{"level":"DEBUG","ts":"2025-12-05T23:40:19.545Z","msg":"Found VA in map: variant=ms-inference-scheduling-llm-d-modelservice-decode, hasCurrentAlloc=true, accelerator=H100"}
{"level":"INFO","ts":"2025-12-05T23:40:19.545Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 1, desired-replicas: 1, accelerator: H100"}
{"level":"INFO","ts":"2025-12-05T23:40:19.545Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=1, accelerator=H100, SaturationOnly=false"}
{"level":"INFO","ts":"2025-12-05T23:40:19.552Z","msg":"Applied Saturation decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1, target=1, reason=model-based only (Saturation unavailable)"}
{"level":"INFO","ts":"2025-12-05T23:40:19.552Z","msg":"Reconciliation completed successfully: mode=model-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-12-05T23:40:19.707Z","msg":"saturation scaling config cache updated: entries=1, has_default=true"}
{"level":"INFO","ts":"2025-12-05T23:40:19.707Z","msg":"saturation scaling configuration loaded successfully"}
{"level":"INFO","ts":"2025-12-05T23:40:19.707Z","msg":"Operating in MODEL-ONLY mode: model-based optimization only"}
{"level":"INFO","ts":"2025-12-05T23:40:19.707Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-12-05T23:40:19.707Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-12-05T23:40:19.709Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dczzts, queueLength=0"}
{"level":"DEBUG","ts":"2025-12-05T23:40:19.709Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"INFO","ts":"2025-12-05T23:40:19.709Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dczzts, usage=0.000 (0.0%)"}
{"level":"DEBUG","ts":"2025-12-05T23:40:19.709Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"DEBUG","ts":"2025-12-05T23:40:19.711Z","msg":"Pod-to-variant matching successful: totalPods=1, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"DEBUG","ts":"2025-12-05T23:40:19.711Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=1"}
{"level":"DEBUG","ts":"2025-12-05T23:40:19.711Z","msg":"Collected capacity metrics for observability in model-only mode: modelID=unsloth/Meta-Llama-3.1-8B, replicaCount=1"}
{"level":"INFO","ts":"2025-12-05T23:40:19.711Z","msg":"Found SLO for model: model=unsloth/Meta-Llama-3.1-8B, class=Premium, slo-tpot=10, slo-ttft=1000"}
{"level":"DEBUG","ts":"2025-12-05T23:40:19.720Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=1, accelerator=H100, ttft=0.00ms, itl=0.00ms, cost=100.00, maxBatch=256, arrivalRate=0.00, avgInputTokens=0.00, avgOutputTokens=0.00"}
{"level":"DEBUG","ts":"2025-12-05T23:40:19.720Z","msg":"Experimental model tuner is enabled globally (EXPERIMENTAL_MODEL_TUNER_ENABLED=true) tuning model performance parameters for active VAs"}
{"level":"DEBUG","ts":"2025-12-05T23:40:19.720Z","msg":"Using state vals from VA status to tune variant ms-inference-scheduling-llm-d-modelservice-decode: alpha= 7.350417, beta= 0.045946, gamma= 17.152187, delta= 0.000321"}
{"level":"DEBUG","ts":"2025-12-05T23:40:19.720Z","msg":"[Tuner Config] variant=ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler | Initial state: alpha=7.350417, beta=0.045946, gamma=17.152187, delta=0.000321 | SLO targets: TTFT=1000.00ms, ITL=10.00ms | Has covariance=true"}
{"level":"WARN","ts":"2025-12-05T23:40:19.720Z","msg":"Tuner failed completely for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler: failed to get/create tuner: failed to create tuner: invalid environment: &{0 0 0 256 0 0 1}. Using fallback parameters."}
{"level":"INFO","ts":"2025-12-05T23:40:19.720Z","msg":"Using tuned parameters from status for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler"}
{"level":"DEBUG","ts":"2025-12-05T23:40:19.720Z","msg":"Updated SystemData for model=unsloth/Meta-Llama-3.1-8B, accelerator=H100: alpha=7.350417, beta=0.045946, gamma=17.152187, delta=0.000321"}
{"level":"DEBUG","ts":"2025-12-05T23:40:19.720Z","msg":"Optimization solution - system: Solution: \ns=ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler; c=Premium; m=unsloth/Meta-Llama-3.1-8B; rate=0; inTk=0; outTk=0; sol=1, sat=false, alloc={acc=H100; numRep=1; maxBatch=512; cost=100, val=0, itl=7.3963633, ttft=17.152508, rho=0, maxRPM=639636.56}; slo-itl=10, slo-ttft=1000, slo-tps=0 \nAllocationByType: \nname=NVIDIA-H100-80GB-HBM3, count=1, limit=0, cost=100 \ntotalCost=100 \n"}
{"level":"DEBUG","ts":"2025-12-05T23:40:19.720Z","msg":"Setting accelerator name: Name=H100, allocationData={H100 1 512 100 7.3963633 17.152508 {0 0 0}}"}
{"level":"INFO","ts":"2025-12-05T23:40:19.720Z","msg":"Model-based optimization completed for model: unsloth/Meta-Llama-3.1-8B - model-based targets: map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"WARN","ts":"2025-12-05T23:40:19.720Z","msg":"saturation analysis unavailable, using model-based targets only: modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-05T23:40:19.720Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-12-05T23:40:19.720Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1→target=1"}
{"level":"DEBUG","ts":"2025-12-05T23:40:19.720Z","msg":"Found VA in map: variant=ms-inference-scheduling-llm-d-modelservice-decode, hasCurrentAlloc=true, accelerator=H100"}
{"level":"INFO","ts":"2025-12-05T23:40:19.720Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 1, desired-replicas: 1, accelerator: H100"}
{"level":"INFO","ts":"2025-12-05T23:40:19.720Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=1, accelerator=H100, SaturationOnly=false"}
{"level":"INFO","ts":"2025-12-05T23:40:19.725Z","msg":"Applied Saturation decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1, target=1, reason=model-based only (Saturation unavailable)"}
{"level":"INFO","ts":"2025-12-05T23:40:19.725Z","msg":"Reconciliation completed successfully: mode=model-only, modelsProcessed=1, decisionsApplied=1"}
