{"level":"INFO","ts":"2025-12-10T12:06:10.782Z","msg":"Reconciling VariantAutoscaling: name=ms-inference-scheduling-llm-d-modelservice-decode, namespace=llm-d-inference-scheduler, modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-10T12:06:10.782Z","msg":"saturation scaling config cache updated: entries=1, has_default=true"}
{"level":"INFO","ts":"2025-12-10T12:06:10.782Z","msg":"saturation scaling configuration loaded successfully"}
{"level":"INFO","ts":"2025-12-10T12:06:10.782Z","msg":"Operating in MODEL-ONLY mode: model-based optimization only"}
{"level":"INFO","ts":"2025-12-10T12:06:10.782Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-12-10T12:06:10.782Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-12-10T12:06:10.790Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dgftqv, queueLength=0"}
{"level":"DEBUG","ts":"2025-12-10T12:06:10.790Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"INFO","ts":"2025-12-10T12:06:10.790Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dgftqv, usage=0.000 (0.0%)"}
{"level":"DEBUG","ts":"2025-12-10T12:06:10.790Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"DEBUG","ts":"2025-12-10T12:06:10.792Z","msg":"Pod-to-variant matching successful: totalPods=1, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"DEBUG","ts":"2025-12-10T12:06:10.792Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=1"}
{"level":"DEBUG","ts":"2025-12-10T12:06:10.792Z","msg":"Collected capacity metrics for observability in model-only mode: modelID=unsloth/Meta-Llama-3.1-8B, replicaCount=1"}
{"level":"DEBUG","ts":"2025-12-10T12:06:10.804Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=1, accelerator=H100, ttft=0.00ms, itl=0.00ms, cost=100.00, maxBatch=256, arrivalRate=0.00, avgInputTokens=0.00, avgOutputTokens=0.00"}
{"level":"DEBUG","ts":"2025-12-10T12:06:10.804Z","msg":"Experimental model tuner is enabled globally: tuning model performance parameters for active VAs"}
{"level":"DEBUG","ts":"2025-12-10T12:06:10.804Z","msg":"Using state vals from VA status to tune variant ms-inference-scheduling-llm-d-modelservice-decode: alpha= 7.302203, beta= 0.031806, gamma= 15.462225, delta= 0.000246"}
{"level":"DEBUG","ts":"2025-12-10T12:06:10.804Z","msg":"Skipping tuning for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler due to insufficient load. Using fallback parameters."}
{"level":"INFO","ts":"2025-12-10T12:06:10.804Z","msg":"Using tuned parameters from status for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler"}
{"level":"DEBUG","ts":"2025-12-10T12:06:10.804Z","msg":"Updated SystemData for model=unsloth/Meta-Llama-3.1-8B, accelerator=H100: alpha=7.302203, beta=0.031806, gamma=15.462225, delta=0.000246"}
{"level":"DEBUG","ts":"2025-12-10T12:06:10.804Z","msg":"Optimization solution - system: Solution: \ns=ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler; c=Premium; m=unsloth/Meta-Llama-3.1-8B; rate=0; inTk=0; outTk=0; sol=1, sat=false, alloc={acc=H100; numRep=1; maxBatch=512; cost=100, val=0, itl=7.334009, ttft=15.462471, rho=0, maxRPM=786696.94}; slo-itl=10, slo-ttft=1000, slo-tps=0 \nAllocationByType: \nname=NVIDIA-H100-80GB-HBM3, count=1, limit=0, cost=100 \ntotalCost=100 \n"}
{"level":"DEBUG","ts":"2025-12-10T12:06:10.804Z","msg":"Setting accelerator name: Name=H100, allocationData={H100 1 512 100 7.334009 15.462471 {0 0 0}}"}
{"level":"INFO","ts":"2025-12-10T12:06:10.804Z","msg":"Model-based optimization completed for model: unsloth/Meta-Llama-3.1-8B - model-based targets: map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"WARN","ts":"2025-12-10T12:06:10.804Z","msg":"saturation analysis unavailable, using model-based targets only: modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-10T12:06:10.804Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-12-10T12:06:10.804Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1→target=1"}
{"level":"INFO","ts":"2025-12-10T12:06:10.804Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 1, desired-replicas: 1, accelerator: H100"}
{"level":"INFO","ts":"2025-12-10T12:06:10.804Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=1, accelerator=H100, SaturationOnly=false"}
{"level":"INFO","ts":"2025-12-10T12:06:10.810Z","msg":"Applied Saturation decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1, target=1, reason=model-based only (Saturation unavailable)"}
{"level":"INFO","ts":"2025-12-10T12:06:10.810Z","msg":"Reconciliation completed successfully: mode=model-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-12-10T12:07:10.811Z","msg":"Reconciling VariantAutoscaling: name=ms-inference-scheduling-llm-d-modelservice-decode, namespace=llm-d-inference-scheduler, modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-10T12:07:10.811Z","msg":"saturation scaling config cache updated: entries=1, has_default=true"}
{"level":"INFO","ts":"2025-12-10T12:07:10.811Z","msg":"saturation scaling configuration loaded successfully"}
{"level":"INFO","ts":"2025-12-10T12:07:10.811Z","msg":"Operating in MODEL-ONLY mode: model-based optimization only"}
{"level":"INFO","ts":"2025-12-10T12:07:10.811Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-12-10T12:07:10.811Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-12-10T12:07:10.816Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dgftqv, queueLength=0"}
{"level":"DEBUG","ts":"2025-12-10T12:07:10.816Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"INFO","ts":"2025-12-10T12:07:10.816Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dgftqv, usage=0.000 (0.0%)"}
{"level":"DEBUG","ts":"2025-12-10T12:07:10.816Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"DEBUG","ts":"2025-12-10T12:07:10.818Z","msg":"Pod-to-variant matching successful: totalPods=1, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"DEBUG","ts":"2025-12-10T12:07:10.818Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=1"}
{"level":"DEBUG","ts":"2025-12-10T12:07:10.818Z","msg":"Collected capacity metrics for observability in model-only mode: modelID=unsloth/Meta-Llama-3.1-8B, replicaCount=1"}
{"level":"DEBUG","ts":"2025-12-10T12:07:10.829Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=1, accelerator=H100, ttft=0.00ms, itl=0.00ms, cost=100.00, maxBatch=256, arrivalRate=0.00, avgInputTokens=0.00, avgOutputTokens=0.00"}
{"level":"DEBUG","ts":"2025-12-10T12:07:10.829Z","msg":"Experimental model tuner is enabled globally: tuning model performance parameters for active VAs"}
{"level":"DEBUG","ts":"2025-12-10T12:07:10.829Z","msg":"Using state vals from VA status to tune variant ms-inference-scheduling-llm-d-modelservice-decode: alpha= 7.302203, beta= 0.031806, gamma= 15.462225, delta= 0.000246"}
{"level":"DEBUG","ts":"2025-12-10T12:07:10.829Z","msg":"Skipping tuning for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler due to insufficient load. Using fallback parameters."}
{"level":"INFO","ts":"2025-12-10T12:07:10.829Z","msg":"Using tuned parameters from status for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler"}
{"level":"DEBUG","ts":"2025-12-10T12:07:10.829Z","msg":"Updated SystemData for model=unsloth/Meta-Llama-3.1-8B, accelerator=H100: alpha=7.302203, beta=0.031806, gamma=15.462225, delta=0.000246"}
{"level":"DEBUG","ts":"2025-12-10T12:07:10.829Z","msg":"Optimization solution - system: Solution: \ns=ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler; c=Premium; m=unsloth/Meta-Llama-3.1-8B; rate=0; inTk=0; outTk=0; sol=1, sat=false, alloc={acc=H100; numRep=1; maxBatch=512; cost=100, val=0, itl=7.334009, ttft=15.462471, rho=0, maxRPM=786696.94}; slo-itl=10, slo-ttft=1000, slo-tps=0 \nAllocationByType: \nname=NVIDIA-H100-80GB-HBM3, count=1, limit=0, cost=100 \ntotalCost=100 \n"}
{"level":"DEBUG","ts":"2025-12-10T12:07:10.829Z","msg":"Setting accelerator name: Name=H100, allocationData={H100 1 512 100 7.334009 15.462471 {0 0 0}}"}
{"level":"INFO","ts":"2025-12-10T12:07:10.829Z","msg":"Model-based optimization completed for model: unsloth/Meta-Llama-3.1-8B - model-based targets: map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"WARN","ts":"2025-12-10T12:07:10.829Z","msg":"saturation analysis unavailable, using model-based targets only: modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-10T12:07:10.829Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-12-10T12:07:10.829Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1→target=1"}
{"level":"INFO","ts":"2025-12-10T12:07:10.829Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 1, desired-replicas: 1, accelerator: H100"}
{"level":"INFO","ts":"2025-12-10T12:07:10.829Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=1, accelerator=H100, SaturationOnly=false"}
{"level":"INFO","ts":"2025-12-10T12:07:10.836Z","msg":"Applied Saturation decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1, target=1, reason=model-based only (Saturation unavailable)"}
{"level":"INFO","ts":"2025-12-10T12:07:10.836Z","msg":"Reconciliation completed successfully: mode=model-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-12-10T12:08:10.836Z","msg":"Reconciling VariantAutoscaling: name=ms-inference-scheduling-llm-d-modelservice-decode, namespace=llm-d-inference-scheduler, modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-10T12:08:10.836Z","msg":"saturation scaling config cache updated: entries=1, has_default=true"}
{"level":"INFO","ts":"2025-12-10T12:08:10.836Z","msg":"saturation scaling configuration loaded successfully"}
{"level":"INFO","ts":"2025-12-10T12:08:10.836Z","msg":"Operating in MODEL-ONLY mode: model-based optimization only"}
{"level":"INFO","ts":"2025-12-10T12:08:10.836Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-12-10T12:08:10.836Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-12-10T12:08:10.846Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dgftqv, usage=0.000 (0.0%)"}
{"level":"DEBUG","ts":"2025-12-10T12:08:10.846Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"INFO","ts":"2025-12-10T12:08:10.846Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dgftqv, queueLength=0"}
{"level":"DEBUG","ts":"2025-12-10T12:08:10.846Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"DEBUG","ts":"2025-12-10T12:08:10.849Z","msg":"Pod-to-variant matching successful: totalPods=1, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"DEBUG","ts":"2025-12-10T12:08:10.849Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=1"}
{"level":"DEBUG","ts":"2025-12-10T12:08:10.849Z","msg":"Collected capacity metrics for observability in model-only mode: modelID=unsloth/Meta-Llama-3.1-8B, replicaCount=1"}
{"level":"DEBUG","ts":"2025-12-10T12:08:10.863Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=1, accelerator=H100, ttft=0.00ms, itl=0.00ms, cost=100.00, maxBatch=256, arrivalRate=0.00, avgInputTokens=0.00, avgOutputTokens=0.00"}
{"level":"DEBUG","ts":"2025-12-10T12:08:10.863Z","msg":"Experimental model tuner is enabled globally: tuning model performance parameters for active VAs"}
{"level":"DEBUG","ts":"2025-12-10T12:08:10.863Z","msg":"Using state vals from VA status to tune variant ms-inference-scheduling-llm-d-modelservice-decode: alpha= 7.302203, beta= 0.031806, gamma= 15.462225, delta= 0.000246"}
{"level":"DEBUG","ts":"2025-12-10T12:08:10.863Z","msg":"Skipping tuning for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler due to insufficient load. Using fallback parameters."}
{"level":"INFO","ts":"2025-12-10T12:08:10.863Z","msg":"Using tuned parameters from status for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler"}
{"level":"DEBUG","ts":"2025-12-10T12:08:10.863Z","msg":"Updated SystemData for model=unsloth/Meta-Llama-3.1-8B, accelerator=H100: alpha=7.302203, beta=0.031806, gamma=15.462225, delta=0.000246"}
{"level":"DEBUG","ts":"2025-12-10T12:08:10.863Z","msg":"Optimization solution - system: Solution: \ns=ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler; c=Premium; m=unsloth/Meta-Llama-3.1-8B; rate=0; inTk=0; outTk=0; sol=1, sat=false, alloc={acc=H100; numRep=1; maxBatch=512; cost=100, val=0, itl=7.334009, ttft=15.462471, rho=0, maxRPM=786696.94}; slo-itl=10, slo-ttft=1000, slo-tps=0 \nAllocationByType: \nname=NVIDIA-H100-80GB-HBM3, count=1, limit=0, cost=100 \ntotalCost=100 \n"}
{"level":"DEBUG","ts":"2025-12-10T12:08:10.863Z","msg":"Setting accelerator name: Name=H100, allocationData={H100 1 512 100 7.334009 15.462471 {0 0 0}}"}
{"level":"INFO","ts":"2025-12-10T12:08:10.863Z","msg":"Model-based optimization completed for model: unsloth/Meta-Llama-3.1-8B - model-based targets: map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"WARN","ts":"2025-12-10T12:08:10.863Z","msg":"saturation analysis unavailable, using model-based targets only: modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-10T12:08:10.863Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-12-10T12:08:10.863Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1→target=1"}
{"level":"INFO","ts":"2025-12-10T12:08:10.863Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 1, desired-replicas: 1, accelerator: H100"}
{"level":"INFO","ts":"2025-12-10T12:08:10.863Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=1, accelerator=H100, SaturationOnly=false"}
{"level":"INFO","ts":"2025-12-10T12:08:10.869Z","msg":"Applied Saturation decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1, target=1, reason=model-based only (Saturation unavailable)"}
{"level":"INFO","ts":"2025-12-10T12:08:10.869Z","msg":"Reconciliation completed successfully: mode=model-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-12-10T12:09:10.870Z","msg":"Reconciling VariantAutoscaling: name=ms-inference-scheduling-llm-d-modelservice-decode, namespace=llm-d-inference-scheduler, modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-10T12:09:10.870Z","msg":"saturation scaling config cache updated: entries=1, has_default=true"}
{"level":"INFO","ts":"2025-12-10T12:09:10.870Z","msg":"saturation scaling configuration loaded successfully"}
{"level":"INFO","ts":"2025-12-10T12:09:10.870Z","msg":"Operating in MODEL-ONLY mode: model-based optimization only"}
{"level":"INFO","ts":"2025-12-10T12:09:10.870Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-12-10T12:09:10.870Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-12-10T12:09:10.877Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dgftqv, queueLength=0"}
{"level":"DEBUG","ts":"2025-12-10T12:09:10.877Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"INFO","ts":"2025-12-10T12:09:10.877Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dgftqv, usage=0.000 (0.0%)"}
{"level":"DEBUG","ts":"2025-12-10T12:09:10.877Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"DEBUG","ts":"2025-12-10T12:09:10.879Z","msg":"Pod-to-variant matching successful: totalPods=1, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"DEBUG","ts":"2025-12-10T12:09:10.879Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=1"}
{"level":"DEBUG","ts":"2025-12-10T12:09:10.879Z","msg":"Collected capacity metrics for observability in model-only mode: modelID=unsloth/Meta-Llama-3.1-8B, replicaCount=1"}
{"level":"DEBUG","ts":"2025-12-10T12:09:10.890Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=1, accelerator=H100, ttft=0.00ms, itl=0.00ms, cost=100.00, maxBatch=256, arrivalRate=0.00, avgInputTokens=0.00, avgOutputTokens=0.00"}
{"level":"DEBUG","ts":"2025-12-10T12:09:10.890Z","msg":"Experimental model tuner is enabled globally: tuning model performance parameters for active VAs"}
{"level":"DEBUG","ts":"2025-12-10T12:09:10.890Z","msg":"Using state vals from VA status to tune variant ms-inference-scheduling-llm-d-modelservice-decode: alpha= 7.302203, beta= 0.031806, gamma= 15.462225, delta= 0.000246"}
{"level":"DEBUG","ts":"2025-12-10T12:09:10.890Z","msg":"Skipping tuning for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler due to insufficient load. Using fallback parameters."}
{"level":"INFO","ts":"2025-12-10T12:09:10.890Z","msg":"Using tuned parameters from status for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler"}
{"level":"DEBUG","ts":"2025-12-10T12:09:10.890Z","msg":"Updated SystemData for model=unsloth/Meta-Llama-3.1-8B, accelerator=H100: alpha=7.302203, beta=0.031806, gamma=15.462225, delta=0.000246"}
{"level":"DEBUG","ts":"2025-12-10T12:09:10.890Z","msg":"Optimization solution - system: Solution: \ns=ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler; c=Premium; m=unsloth/Meta-Llama-3.1-8B; rate=0; inTk=0; outTk=0; sol=1, sat=false, alloc={acc=H100; numRep=1; maxBatch=512; cost=100, val=0, itl=7.334009, ttft=15.462471, rho=0, maxRPM=786696.94}; slo-itl=10, slo-ttft=1000, slo-tps=0 \nAllocationByType: \nname=NVIDIA-H100-80GB-HBM3, count=1, limit=0, cost=100 \ntotalCost=100 \n"}
{"level":"DEBUG","ts":"2025-12-10T12:09:10.890Z","msg":"Setting accelerator name: Name=H100, allocationData={H100 1 512 100 7.334009 15.462471 {0 0 0}}"}
{"level":"INFO","ts":"2025-12-10T12:09:10.890Z","msg":"Model-based optimization completed for model: unsloth/Meta-Llama-3.1-8B - model-based targets: map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"WARN","ts":"2025-12-10T12:09:10.890Z","msg":"saturation analysis unavailable, using model-based targets only: modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-10T12:09:10.890Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-12-10T12:09:10.890Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1→target=1"}
{"level":"INFO","ts":"2025-12-10T12:09:10.891Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 1, desired-replicas: 1, accelerator: H100"}
{"level":"INFO","ts":"2025-12-10T12:09:10.891Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=1, accelerator=H100, SaturationOnly=false"}
{"level":"INFO","ts":"2025-12-10T12:09:10.897Z","msg":"Applied Saturation decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1, target=1, reason=model-based only (Saturation unavailable)"}
{"level":"INFO","ts":"2025-12-10T12:09:10.897Z","msg":"Reconciliation completed successfully: mode=model-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-12-10T12:10:10.898Z","msg":"Reconciling VariantAutoscaling: name=ms-inference-scheduling-llm-d-modelservice-decode, namespace=llm-d-inference-scheduler, modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-10T12:10:10.898Z","msg":"saturation scaling config cache updated: entries=1, has_default=true"}
{"level":"INFO","ts":"2025-12-10T12:10:10.898Z","msg":"saturation scaling configuration loaded successfully"}
{"level":"INFO","ts":"2025-12-10T12:10:10.898Z","msg":"Operating in MODEL-ONLY mode: model-based optimization only"}
{"level":"INFO","ts":"2025-12-10T12:10:10.898Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-12-10T12:10:10.898Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-12-10T12:10:10.907Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dgftqv, queueLength=0"}
{"level":"DEBUG","ts":"2025-12-10T12:10:10.907Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"INFO","ts":"2025-12-10T12:10:10.919Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dgftqv, usage=0.000 (0.0%)"}
{"level":"DEBUG","ts":"2025-12-10T12:10:10.919Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"DEBUG","ts":"2025-12-10T12:10:10.970Z","msg":"Pod-to-variant matching successful: totalPods=1, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"DEBUG","ts":"2025-12-10T12:10:10.970Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=1"}
{"level":"DEBUG","ts":"2025-12-10T12:10:10.970Z","msg":"Collected capacity metrics for observability in model-only mode: modelID=unsloth/Meta-Llama-3.1-8B, replicaCount=1"}
{"level":"DEBUG","ts":"2025-12-10T12:10:11.074Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=1, accelerator=H100, ttft=0.00ms, itl=0.00ms, cost=100.00, maxBatch=256, arrivalRate=0.00, avgInputTokens=0.00, avgOutputTokens=0.00"}
{"level":"DEBUG","ts":"2025-12-10T12:10:11.074Z","msg":"Experimental model tuner is enabled globally: tuning model performance parameters for active VAs"}
{"level":"DEBUG","ts":"2025-12-10T12:10:11.074Z","msg":"Using state vals from VA status to tune variant ms-inference-scheduling-llm-d-modelservice-decode: alpha= 7.302203, beta= 0.031806, gamma= 15.462225, delta= 0.000246"}
{"level":"DEBUG","ts":"2025-12-10T12:10:11.074Z","msg":"Skipping tuning for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler due to insufficient load. Using fallback parameters."}
{"level":"INFO","ts":"2025-12-10T12:10:11.074Z","msg":"Using tuned parameters from status for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler"}
{"level":"DEBUG","ts":"2025-12-10T12:10:11.074Z","msg":"Updated SystemData for model=unsloth/Meta-Llama-3.1-8B, accelerator=H100: alpha=7.302203, beta=0.031806, gamma=15.462225, delta=0.000246"}
{"level":"DEBUG","ts":"2025-12-10T12:10:11.074Z","msg":"Optimization solution - system: Solution: \ns=ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler; c=Premium; m=unsloth/Meta-Llama-3.1-8B; rate=0; inTk=0; outTk=0; sol=1, sat=false, alloc={acc=H100; numRep=1; maxBatch=512; cost=100, val=0, itl=7.334009, ttft=15.462471, rho=0, maxRPM=786696.94}; slo-itl=10, slo-ttft=1000, slo-tps=0 \nAllocationByType: \nname=NVIDIA-H100-80GB-HBM3, count=1, limit=0, cost=100 \ntotalCost=100 \n"}
{"level":"DEBUG","ts":"2025-12-10T12:10:11.074Z","msg":"Setting accelerator name: Name=H100, allocationData={H100 1 512 100 7.334009 15.462471 {0 0 0}}"}
{"level":"INFO","ts":"2025-12-10T12:10:11.074Z","msg":"Model-based optimization completed for model: unsloth/Meta-Llama-3.1-8B - model-based targets: map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"WARN","ts":"2025-12-10T12:10:11.074Z","msg":"saturation analysis unavailable, using model-based targets only: modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-10T12:10:11.074Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-12-10T12:10:11.074Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1→target=1"}
{"level":"INFO","ts":"2025-12-10T12:10:11.074Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 1, desired-replicas: 1, accelerator: H100"}
{"level":"INFO","ts":"2025-12-10T12:10:11.074Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=1, accelerator=H100, SaturationOnly=false"}
{"level":"INFO","ts":"2025-12-10T12:10:11.081Z","msg":"Applied Saturation decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1, target=1, reason=model-based only (Saturation unavailable)"}
{"level":"INFO","ts":"2025-12-10T12:10:11.081Z","msg":"Reconciliation completed successfully: mode=model-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-12-10T12:11:11.081Z","msg":"Reconciling VariantAutoscaling: name=ms-inference-scheduling-llm-d-modelservice-decode, namespace=llm-d-inference-scheduler, modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-10T12:11:11.081Z","msg":"saturation scaling config cache updated: entries=1, has_default=true"}
{"level":"INFO","ts":"2025-12-10T12:11:11.081Z","msg":"saturation scaling configuration loaded successfully"}
{"level":"INFO","ts":"2025-12-10T12:11:11.081Z","msg":"Operating in MODEL-ONLY mode: model-based optimization only"}
{"level":"INFO","ts":"2025-12-10T12:11:11.082Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-12-10T12:11:11.082Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-12-10T12:11:11.085Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dgftqv, usage=0.003 (0.3%)"}
{"level":"DEBUG","ts":"2025-12-10T12:11:11.085Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"INFO","ts":"2025-12-10T12:11:11.085Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dgftqv, queueLength=0"}
{"level":"DEBUG","ts":"2025-12-10T12:11:11.085Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"DEBUG","ts":"2025-12-10T12:11:11.087Z","msg":"Pod-to-variant matching successful: totalPods=1, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"DEBUG","ts":"2025-12-10T12:11:11.087Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=1"}
{"level":"DEBUG","ts":"2025-12-10T12:11:11.087Z","msg":"Collected capacity metrics for observability in model-only mode: modelID=unsloth/Meta-Llama-3.1-8B, replicaCount=1"}
{"level":"DEBUG","ts":"2025-12-10T12:11:11.100Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=1, accelerator=H100, ttft=23.76ms, itl=6.59ms, cost=100.00, maxBatch=256, arrivalRate=0.00, avgInputTokens=0.00, avgOutputTokens=0.00"}
{"level":"DEBUG","ts":"2025-12-10T12:11:11.100Z","msg":"Experimental model tuner is enabled globally: tuning model performance parameters for active VAs"}
{"level":"DEBUG","ts":"2025-12-10T12:11:11.100Z","msg":"Using state vals from VA status to tune variant ms-inference-scheduling-llm-d-modelservice-decode: alpha= 7.302203, beta= 0.031806, gamma= 15.462225, delta= 0.000246"}
{"level":"DEBUG","ts":"2025-12-10T12:11:11.100Z","msg":"Skipping tuning for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler due to insufficient load. Using fallback parameters."}
{"level":"INFO","ts":"2025-12-10T12:11:11.100Z","msg":"Using tuned parameters from status for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler"}
{"level":"DEBUG","ts":"2025-12-10T12:11:11.100Z","msg":"Updated SystemData for model=unsloth/Meta-Llama-3.1-8B, accelerator=H100: alpha=7.302203, beta=0.031806, gamma=15.462225, delta=0.000246"}
{"level":"DEBUG","ts":"2025-12-10T12:11:11.100Z","msg":"Optimization solution - system: Solution: \ns=ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler; c=Premium; m=unsloth/Meta-Llama-3.1-8B; rate=0; inTk=0; outTk=0; sol=1, sat=false, alloc={acc=H100; numRep=1; maxBatch=512; cost=100, val=0, itl=7.334009, ttft=15.462471, rho=0, maxRPM=786696.94}; slo-itl=10, slo-ttft=1000, slo-tps=0 \nAllocationByType: \nname=NVIDIA-H100-80GB-HBM3, count=1, limit=0, cost=100 \ntotalCost=100 \n"}
{"level":"DEBUG","ts":"2025-12-10T12:11:11.100Z","msg":"Setting accelerator name: Name=H100, allocationData={H100 1 512 100 7.334009 15.462471 {0 0 0}}"}
{"level":"INFO","ts":"2025-12-10T12:11:11.100Z","msg":"Model-based optimization completed for model: unsloth/Meta-Llama-3.1-8B - model-based targets: map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"WARN","ts":"2025-12-10T12:11:11.100Z","msg":"saturation analysis unavailable, using model-based targets only: modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-10T12:11:11.100Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-12-10T12:11:11.100Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1→target=1"}
{"level":"INFO","ts":"2025-12-10T12:11:11.100Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 1, desired-replicas: 1, accelerator: H100"}
{"level":"INFO","ts":"2025-12-10T12:11:11.100Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=1, accelerator=H100, SaturationOnly=false"}
{"level":"INFO","ts":"2025-12-10T12:11:11.108Z","msg":"Applied Saturation decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1, target=1, reason=model-based only (Saturation unavailable)"}
{"level":"INFO","ts":"2025-12-10T12:11:11.108Z","msg":"Reconciliation completed successfully: mode=model-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-12-10T12:12:11.109Z","msg":"Reconciling VariantAutoscaling: name=ms-inference-scheduling-llm-d-modelservice-decode, namespace=llm-d-inference-scheduler, modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-10T12:12:11.109Z","msg":"saturation scaling config cache updated: entries=1, has_default=true"}
{"level":"INFO","ts":"2025-12-10T12:12:11.109Z","msg":"saturation scaling configuration loaded successfully"}
{"level":"INFO","ts":"2025-12-10T12:12:11.109Z","msg":"Operating in MODEL-ONLY mode: model-based optimization only"}
{"level":"INFO","ts":"2025-12-10T12:12:11.109Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-12-10T12:12:11.109Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-12-10T12:12:11.118Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dgftqv, usage=0.038 (3.8%)"}
{"level":"INFO","ts":"2025-12-10T12:12:11.118Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dgftqv, queueLength=0"}
{"level":"DEBUG","ts":"2025-12-10T12:12:11.118Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"DEBUG","ts":"2025-12-10T12:12:11.118Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"DEBUG","ts":"2025-12-10T12:12:11.121Z","msg":"Pod-to-variant matching successful: totalPods=1, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"DEBUG","ts":"2025-12-10T12:12:11.121Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=1"}
{"level":"DEBUG","ts":"2025-12-10T12:12:11.121Z","msg":"Collected capacity metrics for observability in model-only mode: modelID=unsloth/Meta-Llama-3.1-8B, replicaCount=1"}
{"level":"DEBUG","ts":"2025-12-10T12:12:11.133Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=1, accelerator=H100, ttft=16.40ms, itl=7.92ms, cost=100.00, maxBatch=256, arrivalRate=283.35, avgInputTokens=233.45, avgOutputTokens=506.45"}
{"level":"DEBUG","ts":"2025-12-10T12:12:11.133Z","msg":"Experimental model tuner is enabled globally: tuning model performance parameters for active VAs"}
{"level":"DEBUG","ts":"2025-12-10T12:12:11.133Z","msg":"Using state vals from VA status to tune variant ms-inference-scheduling-llm-d-modelservice-decode: alpha= 7.302203, beta= 0.031806, gamma= 15.462225, delta= 0.000246"}
{"level":"DEBUG","ts":"2025-12-10T12:12:11.133Z","msg":"[Tuner Config] variant=ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler | Initial state: alpha=7.302203, beta=0.031806, gamma=15.462225, delta=0.000246 | Expected obs (for R): TTFT=16.40ms, ITL=7.92ms | Has covariance=true"}
{"level":"INFO","ts":"2025-12-10T12:12:11.133Z","msg":"Tuner validation succeeded for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler, server ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler - New NIS: 0.048013"}
{"level":"DEBUG","ts":"2025-12-10T12:12:11.133Z","msg":"Model tuner results: model=unsloth/Meta-Llama-3.1-8B, accelerator=H100, alpha=7.286507, beta=0.031840, gamma=15.294211, delta=0.000246, NIS=0.05"}
{"level":"DEBUG","ts":"2025-12-10T12:12:11.133Z","msg":"Updated tuner status for variant ms-inference-scheduling-llm-d-modelservice-decode: alpha=7.286507, beta=0.031840, gamma=15.294211, delta=0.000246, NIS=0.048013"}
{"level":"INFO","ts":"2025-12-10T12:12:11.133Z","msg":"Tuned performance parameters result: variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler - alpha: 7.286507, beta: 0.031840, gamma: 15.294211, delta: 0.000246"}
{"level":"DEBUG","ts":"2025-12-10T12:12:11.142Z","msg":"Optimization solution - system: Solution: \ns=ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler; c=Premium; m=unsloth/Meta-Llama-3.1-8B; rate=283.35; inTk=233; outTk=506; sol=1, sat=false, alloc={acc=H100; numRep=1; maxBatch=512; cost=100, val=0, itl=7.9224033, ttft=16.439243, rho=0.037053667, maxRPM=996.6742}; slo-itl=10, slo-ttft=1000, slo-tps=0 \nAllocationByType: \nname=NVIDIA-H100-80GB-HBM3, count=1, limit=0, cost=100 \ntotalCost=100 \n"}
{"level":"DEBUG","ts":"2025-12-10T12:12:11.142Z","msg":"Setting accelerator name: Name=H100, allocationData={H100 1 512 100 7.9224033 16.439243 {283.35 233 506}}"}
{"level":"INFO","ts":"2025-12-10T12:12:11.142Z","msg":"Model-based optimization completed for model: unsloth/Meta-Llama-3.1-8B - model-based targets: map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"WARN","ts":"2025-12-10T12:12:11.142Z","msg":"saturation analysis unavailable, using model-based targets only: modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-10T12:12:11.142Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-12-10T12:12:11.142Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1→target=1"}
{"level":"INFO","ts":"2025-12-10T12:12:11.142Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 1, desired-replicas: 1, accelerator: H100"}
{"level":"INFO","ts":"2025-12-10T12:12:11.142Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=1, accelerator=H100, SaturationOnly=false"}
{"level":"INFO","ts":"2025-12-10T12:12:11.149Z","msg":"Applied Saturation decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1, target=1, reason=model-based only (Saturation unavailable)"}
{"level":"INFO","ts":"2025-12-10T12:12:11.149Z","msg":"Reconciliation completed successfully: mode=model-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-12-10T12:13:11.150Z","msg":"Reconciling VariantAutoscaling: name=ms-inference-scheduling-llm-d-modelservice-decode, namespace=llm-d-inference-scheduler, modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-10T12:13:11.150Z","msg":"saturation scaling config cache updated: entries=1, has_default=true"}
{"level":"INFO","ts":"2025-12-10T12:13:11.150Z","msg":"saturation scaling configuration loaded successfully"}
{"level":"INFO","ts":"2025-12-10T12:13:11.150Z","msg":"Operating in MODEL-ONLY mode: model-based optimization only"}
{"level":"INFO","ts":"2025-12-10T12:13:11.150Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-12-10T12:13:11.150Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-12-10T12:13:11.153Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dgftqv, queueLength=0"}
{"level":"DEBUG","ts":"2025-12-10T12:13:11.153Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"INFO","ts":"2025-12-10T12:13:11.153Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dgftqv, usage=0.032 (3.2%)"}
{"level":"DEBUG","ts":"2025-12-10T12:13:11.153Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"DEBUG","ts":"2025-12-10T12:13:11.155Z","msg":"Pod-to-variant matching successful: totalPods=1, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"DEBUG","ts":"2025-12-10T12:13:11.155Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=1"}
{"level":"DEBUG","ts":"2025-12-10T12:13:11.155Z","msg":"Collected capacity metrics for observability in model-only mode: modelID=unsloth/Meta-Llama-3.1-8B, replicaCount=1"}
{"level":"DEBUG","ts":"2025-12-10T12:13:11.168Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=1, accelerator=H100, ttft=15.97ms, itl=7.69ms, cost=100.00, maxBatch=256, arrivalRate=291.41, avgInputTokens=235.79, avgOutputTokens=459.50"}
{"level":"DEBUG","ts":"2025-12-10T12:13:11.168Z","msg":"Experimental model tuner is enabled globally: tuning model performance parameters for active VAs"}
{"level":"DEBUG","ts":"2025-12-10T12:13:11.168Z","msg":"Using state vals from VA status to tune variant ms-inference-scheduling-llm-d-modelservice-decode: alpha= 7.286507, beta= 0.031840, gamma= 15.294211, delta= 0.000246"}
{"level":"DEBUG","ts":"2025-12-10T12:13:11.168Z","msg":"[Tuner Config] variant=ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler | Initial state: alpha=7.286507, beta=0.031840, gamma=15.294211, delta=0.000246 | Expected obs (for R): TTFT=15.97ms, ITL=7.69ms | Has covariance=true"}
{"level":"INFO","ts":"2025-12-10T12:13:11.168Z","msg":"Tuner validation succeeded for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler, server ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler - New NIS: 0.321572"}
{"level":"DEBUG","ts":"2025-12-10T12:13:11.168Z","msg":"Model tuner results: model=unsloth/Meta-Llama-3.1-8B, accelerator=H100, alpha=7.138163, beta=0.031907, gamma=14.985481, delta=0.000246, NIS=0.32"}
{"level":"DEBUG","ts":"2025-12-10T12:13:11.168Z","msg":"Updated tuner status for variant ms-inference-scheduling-llm-d-modelservice-decode: alpha=7.138163, beta=0.031907, gamma=14.985481, delta=0.000246, NIS=0.321572"}
{"level":"INFO","ts":"2025-12-10T12:13:11.168Z","msg":"Tuned performance parameters result: variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler - alpha: 7.138163, beta: 0.031907, gamma: 14.985481, delta: 0.000246"}
{"level":"DEBUG","ts":"2025-12-10T12:13:11.178Z","msg":"Optimization solution - system: Solution: \ns=ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler; c=Premium; m=unsloth/Meta-Llama-3.1-8B; rate=291.41; inTk=235; outTk=459; sol=1, sat=false, alloc={acc=H100; numRep=1; maxBatch=512; cost=100, val=0, itl=7.720523, ttft=16.041233, rho=0.03369467, maxRPM=1156.8119}; slo-itl=10, slo-ttft=1000, slo-tps=0 \nAllocationByType: \nname=NVIDIA-H100-80GB-HBM3, count=1, limit=0, cost=100 \ntotalCost=100 \n"}
{"level":"DEBUG","ts":"2025-12-10T12:13:11.178Z","msg":"Setting accelerator name: Name=H100, allocationData={H100 1 512 100 7.720523 16.041233 {291.41 235 459}}"}
{"level":"INFO","ts":"2025-12-10T12:13:11.178Z","msg":"Model-based optimization completed for model: unsloth/Meta-Llama-3.1-8B - model-based targets: map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"WARN","ts":"2025-12-10T12:13:11.178Z","msg":"saturation analysis unavailable, using model-based targets only: modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-10T12:13:11.178Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-12-10T12:13:11.178Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1→target=1"}
{"level":"INFO","ts":"2025-12-10T12:13:11.178Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 1, desired-replicas: 1, accelerator: H100"}
{"level":"INFO","ts":"2025-12-10T12:13:11.178Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=1, accelerator=H100, SaturationOnly=false"}
{"level":"INFO","ts":"2025-12-10T12:13:11.183Z","msg":"Applied Saturation decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1, target=1, reason=model-based only (Saturation unavailable)"}
{"level":"INFO","ts":"2025-12-10T12:13:11.183Z","msg":"Reconciliation completed successfully: mode=model-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-12-10T12:14:11.184Z","msg":"Reconciling VariantAutoscaling: name=ms-inference-scheduling-llm-d-modelservice-decode, namespace=llm-d-inference-scheduler, modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-10T12:14:11.184Z","msg":"saturation scaling config cache updated: entries=1, has_default=true"}
{"level":"INFO","ts":"2025-12-10T12:14:11.184Z","msg":"saturation scaling configuration loaded successfully"}
{"level":"INFO","ts":"2025-12-10T12:14:11.184Z","msg":"Operating in MODEL-ONLY mode: model-based optimization only"}
{"level":"INFO","ts":"2025-12-10T12:14:11.184Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-12-10T12:14:11.184Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-12-10T12:14:11.194Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dgftqv, queueLength=0"}
{"level":"DEBUG","ts":"2025-12-10T12:14:11.194Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"INFO","ts":"2025-12-10T12:14:11.194Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dgftqv, usage=0.026 (2.6%)"}
{"level":"DEBUG","ts":"2025-12-10T12:14:11.194Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"DEBUG","ts":"2025-12-10T12:14:11.196Z","msg":"Pod-to-variant matching successful: totalPods=1, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"DEBUG","ts":"2025-12-10T12:14:11.196Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=1"}
{"level":"DEBUG","ts":"2025-12-10T12:14:11.196Z","msg":"Collected capacity metrics for observability in model-only mode: modelID=unsloth/Meta-Llama-3.1-8B, replicaCount=1"}
{"level":"DEBUG","ts":"2025-12-10T12:14:11.207Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=1, accelerator=H100, ttft=16.23ms, itl=7.68ms, cost=100.00, maxBatch=256, arrivalRate=290.06, avgInputTokens=227.46, avgOutputTokens=447.89"}
{"level":"DEBUG","ts":"2025-12-10T12:14:11.207Z","msg":"Experimental model tuner is enabled globally: tuning model performance parameters for active VAs"}
{"level":"DEBUG","ts":"2025-12-10T12:14:11.207Z","msg":"Using state vals from VA status to tune variant ms-inference-scheduling-llm-d-modelservice-decode: alpha= 7.138163, beta= 0.031907, gamma= 14.985481, delta= 0.000246"}
{"level":"DEBUG","ts":"2025-12-10T12:14:11.207Z","msg":"[Tuner Config] variant=ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler | Initial state: alpha=7.138163, beta=0.031907, gamma=14.985481, delta=0.000246 | Expected obs (for R): TTFT=16.23ms, ITL=7.68ms | Has covariance=true"}
{"level":"INFO","ts":"2025-12-10T12:14:11.208Z","msg":"Tuner validation succeeded for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler, server ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler - New NIS: 0.079619"}
{"level":"DEBUG","ts":"2025-12-10T12:14:11.208Z","msg":"Model tuner results: model=unsloth/Meta-Llama-3.1-8B, accelerator=H100, alpha=7.121801, beta=0.031908, gamma=15.193787, delta=0.000246, NIS=0.08"}
{"level":"DEBUG","ts":"2025-12-10T12:14:11.208Z","msg":"Updated tuner status for variant ms-inference-scheduling-llm-d-modelservice-decode: alpha=7.121801, beta=0.031908, gamma=15.193787, delta=0.000246, NIS=0.079619"}
{"level":"INFO","ts":"2025-12-10T12:14:11.208Z","msg":"Tuned performance parameters result: variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler - alpha: 7.121801, beta: 0.031908, gamma: 15.193787, delta: 0.000246"}
{"level":"DEBUG","ts":"2025-12-10T12:14:11.219Z","msg":"Optimization solution - system: Solution: \ns=ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler; c=Premium; m=unsloth/Meta-Llama-3.1-8B; rate=290.06; inTk=227; outTk=447; sol=1, sat=false, alloc={acc=H100; numRep=1; maxBatch=512; cost=100, val=0, itl=7.684903, ttft=16.1788, rho=0.0325151, maxRPM=1194.623}; slo-itl=10, slo-ttft=1000, slo-tps=0 \nAllocationByType: \nname=NVIDIA-H100-80GB-HBM3, count=1, limit=0, cost=100 \ntotalCost=100 \n"}
{"level":"DEBUG","ts":"2025-12-10T12:14:11.219Z","msg":"Setting accelerator name: Name=H100, allocationData={H100 1 512 100 7.684903 16.1788 {290.06 227 447}}"}
{"level":"INFO","ts":"2025-12-10T12:14:11.219Z","msg":"Model-based optimization completed for model: unsloth/Meta-Llama-3.1-8B - model-based targets: map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"WARN","ts":"2025-12-10T12:14:11.219Z","msg":"saturation analysis unavailable, using model-based targets only: modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-10T12:14:11.219Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-12-10T12:14:11.219Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1→target=1"}
{"level":"INFO","ts":"2025-12-10T12:14:11.220Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 1, desired-replicas: 1, accelerator: H100"}
{"level":"INFO","ts":"2025-12-10T12:14:11.220Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=1, accelerator=H100, SaturationOnly=false"}
{"level":"INFO","ts":"2025-12-10T12:14:11.226Z","msg":"Applied Saturation decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1, target=1, reason=model-based only (Saturation unavailable)"}
{"level":"INFO","ts":"2025-12-10T12:14:11.226Z","msg":"Reconciliation completed successfully: mode=model-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-12-10T12:15:11.226Z","msg":"Reconciling VariantAutoscaling: name=ms-inference-scheduling-llm-d-modelservice-decode, namespace=llm-d-inference-scheduler, modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-10T12:15:11.226Z","msg":"saturation scaling config cache updated: entries=1, has_default=true"}
{"level":"INFO","ts":"2025-12-10T12:15:11.226Z","msg":"saturation scaling configuration loaded successfully"}
{"level":"INFO","ts":"2025-12-10T12:15:11.226Z","msg":"Operating in MODEL-ONLY mode: model-based optimization only"}
{"level":"INFO","ts":"2025-12-10T12:15:11.226Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-12-10T12:15:11.226Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-12-10T12:15:11.233Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dgftqv, queueLength=0"}
{"level":"DEBUG","ts":"2025-12-10T12:15:11.233Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"INFO","ts":"2025-12-10T12:15:11.233Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dgftqv, usage=0.123 (12.3%)"}
{"level":"DEBUG","ts":"2025-12-10T12:15:11.233Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"DEBUG","ts":"2025-12-10T12:15:11.236Z","msg":"Pod-to-variant matching successful: totalPods=1, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"DEBUG","ts":"2025-12-10T12:15:11.236Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=1"}
{"level":"DEBUG","ts":"2025-12-10T12:15:11.236Z","msg":"Collected capacity metrics for observability in model-only mode: modelID=unsloth/Meta-Llama-3.1-8B, replicaCount=1"}
{"level":"DEBUG","ts":"2025-12-10T12:15:11.250Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=1, accelerator=H100, ttft=24.26ms, itl=12.15ms, cost=100.00, maxBatch=256, arrivalRate=960.16, avgInputTokens=248.51, avgOutputTokens=441.44"}
{"level":"DEBUG","ts":"2025-12-10T12:15:11.250Z","msg":"Experimental model tuner is enabled globally: tuning model performance parameters for active VAs"}
{"level":"DEBUG","ts":"2025-12-10T12:15:11.250Z","msg":"Using state vals from VA status to tune variant ms-inference-scheduling-llm-d-modelservice-decode: alpha= 7.121801, beta= 0.031908, gamma= 15.193787, delta= 0.000246"}
{"level":"DEBUG","ts":"2025-12-10T12:15:11.250Z","msg":"[Tuner Config] variant=ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler | Initial state: alpha=7.121801, beta=0.031908, gamma=15.193787, delta=0.000246 | Expected obs (for R): TTFT=24.26ms, ITL=12.15ms | Has covariance=true"}
{"level":"WARN","ts":"2025-12-10T12:15:11.250Z","msg":"Tuner validation failed (NIS=28.82), validation error: normalized innovation squared (NIS=28.82) exceeds threshold (7.38), rejecting update as outlier - returning previous state: alpha=7.121801, beta=0.031908, gamma=15.193787, delta=0.000246"}
{"level":"WARN","ts":"2025-12-10T12:15:11.250Z","msg":"Tuner NIS validation failed for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler, server ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler (NIS=28.82 exceeds threshold 7.38) - Keeping previous state: alpha=7.121801, beta=0.031908, gamma=15.193787, delta=0.000246"}
{"level":"INFO","ts":"2025-12-10T12:15:11.250Z","msg":"Keeping previous tuned parameters for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler due to NIS validation failure (NIS=28.819099)"}
{"level":"DEBUG","ts":"2025-12-10T12:15:11.250Z","msg":"Model tuner results: model=unsloth/Meta-Llama-3.1-8B, accelerator=H100, alpha=7.121801, beta=0.031908, gamma=15.193787, delta=0.000246, NIS=28.82"}
{"level":"DEBUG","ts":"2025-12-10T12:15:11.250Z","msg":"Updated tuner status for variant ms-inference-scheduling-llm-d-modelservice-decode: alpha=7.121801, beta=0.031908, gamma=15.193787, delta=0.000246, NIS=28.819099"}
{"level":"DEBUG","ts":"2025-12-10T12:15:11.260Z","msg":"Optimization solution - system: Solution: \ns=ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler; c=Premium; m=unsloth/Meta-Llama-3.1-8B; rate=960.16; inTk=248; outTk=441; sol=1, sat=false, alloc={acc=H100; numRep=1; maxBatch=512; cost=100, val=0, itl=9.239334, ttft=19.242504, rho=0.12766343, maxRPM=1210.7126}; slo-itl=10, slo-ttft=1000, slo-tps=0 \nAllocationByType: \nname=NVIDIA-H100-80GB-HBM3, count=1, limit=0, cost=100 \ntotalCost=100 \n"}
{"level":"DEBUG","ts":"2025-12-10T12:15:11.260Z","msg":"Setting accelerator name: Name=H100, allocationData={H100 1 512 100 9.239334 19.242504 {960.16 248 441}}"}
{"level":"INFO","ts":"2025-12-10T12:15:11.260Z","msg":"Model-based optimization completed for model: unsloth/Meta-Llama-3.1-8B - model-based targets: map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"WARN","ts":"2025-12-10T12:15:11.260Z","msg":"saturation analysis unavailable, using model-based targets only: modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-10T12:15:11.260Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-12-10T12:15:11.260Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1→target=1"}
{"level":"INFO","ts":"2025-12-10T12:15:11.260Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 1, desired-replicas: 1, accelerator: H100"}
{"level":"INFO","ts":"2025-12-10T12:15:11.260Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=1, accelerator=H100, SaturationOnly=false"}
{"level":"INFO","ts":"2025-12-10T12:15:11.265Z","msg":"Applied Saturation decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1, target=1, reason=model-based only (Saturation unavailable)"}
{"level":"INFO","ts":"2025-12-10T12:15:11.265Z","msg":"Reconciliation completed successfully: mode=model-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-12-10T12:16:11.266Z","msg":"Reconciling VariantAutoscaling: name=ms-inference-scheduling-llm-d-modelservice-decode, namespace=llm-d-inference-scheduler, modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-10T12:16:11.266Z","msg":"saturation scaling config cache updated: entries=1, has_default=true"}
{"level":"INFO","ts":"2025-12-10T12:16:11.266Z","msg":"saturation scaling configuration loaded successfully"}
{"level":"INFO","ts":"2025-12-10T12:16:11.266Z","msg":"Operating in MODEL-ONLY mode: model-based optimization only"}
{"level":"INFO","ts":"2025-12-10T12:16:11.266Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-12-10T12:16:11.266Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-12-10T12:16:11.274Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dgftqv, usage=0.139 (13.9%)"}
{"level":"DEBUG","ts":"2025-12-10T12:16:11.274Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"INFO","ts":"2025-12-10T12:16:11.274Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dgftqv, queueLength=0"}
{"level":"DEBUG","ts":"2025-12-10T12:16:11.274Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"DEBUG","ts":"2025-12-10T12:16:11.276Z","msg":"Pod-to-variant matching successful: totalPods=1, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"DEBUG","ts":"2025-12-10T12:16:11.276Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=1"}
{"level":"DEBUG","ts":"2025-12-10T12:16:11.276Z","msg":"Collected capacity metrics for observability in model-only mode: modelID=unsloth/Meta-Llama-3.1-8B, replicaCount=1"}
{"level":"DEBUG","ts":"2025-12-10T12:16:11.291Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=1, accelerator=H100, ttft=23.66ms, itl=12.21ms, cost=100.00, maxBatch=256, arrivalRate=1027.31, avgInputTokens=234.92, avgOutputTokens=458.81"}
{"level":"DEBUG","ts":"2025-12-10T12:16:11.291Z","msg":"Experimental model tuner is enabled globally: tuning model performance parameters for active VAs"}
{"level":"DEBUG","ts":"2025-12-10T12:16:11.291Z","msg":"Using state vals from VA status to tune variant ms-inference-scheduling-llm-d-modelservice-decode: alpha= 7.121801, beta= 0.031908, gamma= 15.193787, delta= 0.000246"}
{"level":"DEBUG","ts":"2025-12-10T12:16:11.291Z","msg":"[Tuner Config] variant=ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler | Initial state: alpha=7.121801, beta=0.031908, gamma=15.193787, delta=0.000246 | Expected obs (for R): TTFT=23.66ms, ITL=12.21ms | Has covariance=true"}
{"level":"WARN","ts":"2025-12-10T12:16:11.292Z","msg":"Tuner validation failed (NIS=18.36), validation error: normalized innovation squared (NIS=18.36) exceeds threshold (7.38), rejecting update as outlier - returning previous state: alpha=7.121801, beta=0.031908, gamma=15.193787, delta=0.000246"}
{"level":"WARN","ts":"2025-12-10T12:16:11.292Z","msg":"Tuner NIS validation failed for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler, server ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler (NIS=18.36 exceeds threshold 7.38) - Keeping previous state: alpha=7.121801, beta=0.031908, gamma=15.193787, delta=0.000246"}
{"level":"INFO","ts":"2025-12-10T12:16:11.292Z","msg":"Keeping previous tuned parameters for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler due to NIS validation failure (NIS=18.363485)"}
{"level":"DEBUG","ts":"2025-12-10T12:16:11.292Z","msg":"Model tuner results: model=unsloth/Meta-Llama-3.1-8B, accelerator=H100, alpha=7.121801, beta=0.031908, gamma=15.193787, delta=0.000246, NIS=18.36"}
{"level":"DEBUG","ts":"2025-12-10T12:16:11.292Z","msg":"Updated tuner status for variant ms-inference-scheduling-llm-d-modelservice-decode: alpha=7.121801, beta=0.031908, gamma=15.193787, delta=0.000246, NIS=18.363485"}
{"level":"DEBUG","ts":"2025-12-10T12:16:11.301Z","msg":"Optimization solution - system: Solution: \ns=ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler; c=Premium; m=unsloth/Meta-Llama-3.1-8B; rate=1027.31; inTk=234; outTk=458; sol=1, sat=false, alloc={acc=H100; numRep=1; maxBatch=512; cost=100, val=0, itl=9.54833, ttft=19.571396, rho=0.14657758, maxRPM=1165.9569}; slo-itl=10, slo-ttft=1000, slo-tps=0 \nAllocationByType: \nname=NVIDIA-H100-80GB-HBM3, count=1, limit=0, cost=100 \ntotalCost=100 \n"}
{"level":"DEBUG","ts":"2025-12-10T12:16:11.301Z","msg":"Setting accelerator name: Name=H100, allocationData={H100 1 512 100 9.54833 19.571396 {1027.31 234 458}}"}
{"level":"INFO","ts":"2025-12-10T12:16:11.301Z","msg":"Model-based optimization completed for model: unsloth/Meta-Llama-3.1-8B - model-based targets: map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"WARN","ts":"2025-12-10T12:16:11.301Z","msg":"saturation analysis unavailable, using model-based targets only: modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-10T12:16:11.301Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-12-10T12:16:11.301Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1→target=1"}
{"level":"INFO","ts":"2025-12-10T12:16:11.301Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 1, desired-replicas: 1, accelerator: H100"}
{"level":"INFO","ts":"2025-12-10T12:16:11.301Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=1, accelerator=H100, SaturationOnly=false"}
{"level":"INFO","ts":"2025-12-10T12:16:11.307Z","msg":"Applied Saturation decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1, target=1, reason=model-based only (Saturation unavailable)"}
{"level":"INFO","ts":"2025-12-10T12:16:11.307Z","msg":"Reconciliation completed successfully: mode=model-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-12-10T12:17:11.308Z","msg":"Reconciling VariantAutoscaling: name=ms-inference-scheduling-llm-d-modelservice-decode, namespace=llm-d-inference-scheduler, modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-10T12:17:11.308Z","msg":"saturation scaling config cache updated: entries=1, has_default=true"}
{"level":"INFO","ts":"2025-12-10T12:17:11.308Z","msg":"saturation scaling configuration loaded successfully"}
{"level":"INFO","ts":"2025-12-10T12:17:11.308Z","msg":"Operating in MODEL-ONLY mode: model-based optimization only"}
{"level":"INFO","ts":"2025-12-10T12:17:11.308Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-12-10T12:17:11.308Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-12-10T12:17:11.316Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dgftqv, queueLength=0"}
{"level":"INFO","ts":"2025-12-10T12:17:11.316Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dgftqv, usage=0.134 (13.4%)"}
{"level":"DEBUG","ts":"2025-12-10T12:17:11.316Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"DEBUG","ts":"2025-12-10T12:17:11.316Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"DEBUG","ts":"2025-12-10T12:17:11.318Z","msg":"Pod-to-variant matching successful: totalPods=1, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"DEBUG","ts":"2025-12-10T12:17:11.318Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=1"}
{"level":"DEBUG","ts":"2025-12-10T12:17:11.318Z","msg":"Collected capacity metrics for observability in model-only mode: modelID=unsloth/Meta-Llama-3.1-8B, replicaCount=1"}
{"level":"DEBUG","ts":"2025-12-10T12:17:11.330Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=1, accelerator=H100, ttft=22.72ms, itl=12.18ms, cost=100.00, maxBatch=256, arrivalRate=1042.08, avgInputTokens=213.15, avgOutputTokens=483.86"}
{"level":"DEBUG","ts":"2025-12-10T12:17:11.330Z","msg":"Experimental model tuner is enabled globally: tuning model performance parameters for active VAs"}
{"level":"DEBUG","ts":"2025-12-10T12:17:11.330Z","msg":"Using state vals from VA status to tune variant ms-inference-scheduling-llm-d-modelservice-decode: alpha= 7.121801, beta= 0.031908, gamma= 15.193787, delta= 0.000246"}
{"level":"DEBUG","ts":"2025-12-10T12:17:11.330Z","msg":"[Tuner Config] variant=ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler | Initial state: alpha=7.121801, beta=0.031908, gamma=15.193787, delta=0.000246 | Expected obs (for R): TTFT=22.72ms, ITL=12.18ms | Has covariance=true"}
{"level":"WARN","ts":"2025-12-10T12:17:11.332Z","msg":"Tuner validation failed (NIS=11.58), validation error: normalized innovation squared (NIS=11.58) exceeds threshold (7.38), rejecting update as outlier - returning previous state: alpha=7.121801, beta=0.031908, gamma=15.193787, delta=0.000246"}
{"level":"WARN","ts":"2025-12-10T12:17:11.332Z","msg":"Tuner NIS validation failed for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler, server ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler (NIS=11.58 exceeds threshold 7.38) - Keeping previous state: alpha=7.121801, beta=0.031908, gamma=15.193787, delta=0.000246"}
{"level":"INFO","ts":"2025-12-10T12:17:11.332Z","msg":"Keeping previous tuned parameters for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler due to NIS validation failure (NIS=11.579203)"}
{"level":"DEBUG","ts":"2025-12-10T12:17:11.332Z","msg":"Model tuner results: model=unsloth/Meta-Llama-3.1-8B, accelerator=H100, alpha=7.121801, beta=0.031908, gamma=15.193787, delta=0.000246, NIS=11.58"}
{"level":"DEBUG","ts":"2025-12-10T12:17:11.332Z","msg":"Updated tuner status for variant ms-inference-scheduling-llm-d-modelservice-decode: alpha=7.121801, beta=0.031908, gamma=15.193787, delta=0.000246, NIS=11.579203"}
{"level":"DEBUG","ts":"2025-12-10T12:17:11.341Z","msg":"Optimization solution - system: Solution: \ns=ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler; c=Premium; m=unsloth/Meta-Llama-3.1-8B; rate=1042.08; inTk=213; outTk=483; sol=1, sat=false, alloc={acc=H100; numRep=1; maxBatch=512; cost=100, val=0, itl=9.775794, ttft=19.552065, rho=0.16050084, maxRPM=1105.8386}; slo-itl=10, slo-ttft=1000, slo-tps=0 \nAllocationByType: \nname=NVIDIA-H100-80GB-HBM3, count=1, limit=0, cost=100 \ntotalCost=100 \n"}
{"level":"DEBUG","ts":"2025-12-10T12:17:11.341Z","msg":"Setting accelerator name: Name=H100, allocationData={H100 1 512 100 9.775794 19.552065 {1042.08 213 483}}"}
{"level":"INFO","ts":"2025-12-10T12:17:11.341Z","msg":"Model-based optimization completed for model: unsloth/Meta-Llama-3.1-8B - model-based targets: map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"WARN","ts":"2025-12-10T12:17:11.341Z","msg":"saturation analysis unavailable, using model-based targets only: modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-10T12:17:11.341Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-12-10T12:17:11.341Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1→target=1"}
{"level":"INFO","ts":"2025-12-10T12:17:11.341Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 1, desired-replicas: 1, accelerator: H100"}
{"level":"INFO","ts":"2025-12-10T12:17:11.341Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=1, accelerator=H100, SaturationOnly=false"}
{"level":"INFO","ts":"2025-12-10T12:17:11.347Z","msg":"Applied Saturation decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1, target=1, reason=model-based only (Saturation unavailable)"}
{"level":"INFO","ts":"2025-12-10T12:17:11.347Z","msg":"Reconciliation completed successfully: mode=model-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-12-10T12:18:11.347Z","msg":"Reconciling VariantAutoscaling: name=ms-inference-scheduling-llm-d-modelservice-decode, namespace=llm-d-inference-scheduler, modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-10T12:18:11.347Z","msg":"saturation scaling config cache updated: entries=1, has_default=true"}
{"level":"INFO","ts":"2025-12-10T12:18:11.347Z","msg":"saturation scaling configuration loaded successfully"}
{"level":"INFO","ts":"2025-12-10T12:18:11.347Z","msg":"Operating in MODEL-ONLY mode: model-based optimization only"}
{"level":"INFO","ts":"2025-12-10T12:18:11.347Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-12-10T12:18:11.347Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-12-10T12:18:11.357Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dgftqv, usage=0.433 (43.3%)"}
{"level":"DEBUG","ts":"2025-12-10T12:18:11.357Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"INFO","ts":"2025-12-10T12:18:11.357Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dgftqv, queueLength=0"}
{"level":"DEBUG","ts":"2025-12-10T12:18:11.357Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"DEBUG","ts":"2025-12-10T12:18:11.359Z","msg":"Pod-to-variant matching successful: totalPods=1, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"DEBUG","ts":"2025-12-10T12:18:11.359Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=1"}
{"level":"DEBUG","ts":"2025-12-10T12:18:11.359Z","msg":"Collected capacity metrics for observability in model-only mode: modelID=unsloth/Meta-Llama-3.1-8B, replicaCount=1"}
{"level":"DEBUG","ts":"2025-12-10T12:18:11.371Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=1, accelerator=H100, ttft=38.14ms, itl=21.37ms, cost=100.00, maxBatch=256, arrivalRate=1310.65, avgInputTokens=262.84, avgOutputTokens=364.01"}
{"level":"DEBUG","ts":"2025-12-10T12:18:11.371Z","msg":"Experimental model tuner is enabled globally: tuning model performance parameters for active VAs"}
{"level":"DEBUG","ts":"2025-12-10T12:18:11.371Z","msg":"Using state vals from VA status to tune variant ms-inference-scheduling-llm-d-modelservice-decode: alpha= 7.121801, beta= 0.031908, gamma= 15.193787, delta= 0.000246"}
{"level":"DEBUG","ts":"2025-12-10T12:18:11.371Z","msg":"[Tuner Config] variant=ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler | Initial state: alpha=7.121801, beta=0.031908, gamma=15.193787, delta=0.000246 | Expected obs (for R): TTFT=38.14ms, ITL=21.37ms | Has covariance=true"}
{"level":"WARN","ts":"2025-12-10T12:18:11.372Z","msg":"Tuner validation failed (NIS=258.54), validation error: normalized innovation squared (NIS=258.54) exceeds threshold (7.38), rejecting update as outlier - returning previous state: alpha=7.121801, beta=0.031908, gamma=15.193787, delta=0.000246"}
{"level":"WARN","ts":"2025-12-10T12:18:11.372Z","msg":"Tuner NIS validation failed for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler, server ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler (NIS=258.54 exceeds threshold 7.38) - Keeping previous state: alpha=7.121801, beta=0.031908, gamma=15.193787, delta=0.000246"}
{"level":"INFO","ts":"2025-12-10T12:18:11.372Z","msg":"Keeping previous tuned parameters for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler due to NIS validation failure (NIS=258.535842)"}
{"level":"DEBUG","ts":"2025-12-10T12:18:11.372Z","msg":"Model tuner results: model=unsloth/Meta-Llama-3.1-8B, accelerator=H100, alpha=7.121801, beta=0.031908, gamma=15.193787, delta=0.000246, NIS=258.54"}
{"level":"DEBUG","ts":"2025-12-10T12:18:11.372Z","msg":"Updated tuner status for variant ms-inference-scheduling-llm-d-modelservice-decode: alpha=7.121801, beta=0.031908, gamma=15.193787, delta=0.000246, NIS=258.535842"}
{"level":"DEBUG","ts":"2025-12-10T12:18:11.382Z","msg":"Optimization solution - system: Solution: \ns=ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler; c=Premium; m=unsloth/Meta-Llama-3.1-8B; rate=1310.65; inTk=262; outTk=364; sol=1, sat=false, alloc={acc=H100; numRep=1; maxBatch=512; cost=100, val=0, itl=9.595582, ttft=20.190657, rho=0.14946987, maxRPM=1465.9452}; slo-itl=10, slo-ttft=1000, slo-tps=0 \nAllocationByType: \nname=NVIDIA-H100-80GB-HBM3, count=1, limit=0, cost=100 \ntotalCost=100 \n"}
{"level":"DEBUG","ts":"2025-12-10T12:18:11.382Z","msg":"Setting accelerator name: Name=H100, allocationData={H100 1 512 100 9.595582 20.190657 {1310.65 262 364}}"}
{"level":"INFO","ts":"2025-12-10T12:18:11.382Z","msg":"Model-based optimization completed for model: unsloth/Meta-Llama-3.1-8B - model-based targets: map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"WARN","ts":"2025-12-10T12:18:11.382Z","msg":"saturation analysis unavailable, using model-based targets only: modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-10T12:18:11.382Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-12-10T12:18:11.382Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1→target=1"}
{"level":"INFO","ts":"2025-12-10T12:18:11.382Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 1, desired-replicas: 1, accelerator: H100"}
{"level":"INFO","ts":"2025-12-10T12:18:11.382Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=1, accelerator=H100, SaturationOnly=false"}
{"level":"INFO","ts":"2025-12-10T12:18:11.388Z","msg":"Applied Saturation decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1, target=1, reason=model-based only (Saturation unavailable)"}
{"level":"INFO","ts":"2025-12-10T12:18:11.388Z","msg":"Reconciliation completed successfully: mode=model-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-12-10T12:19:11.388Z","msg":"Reconciling VariantAutoscaling: name=ms-inference-scheduling-llm-d-modelservice-decode, namespace=llm-d-inference-scheduler, modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-10T12:19:11.388Z","msg":"saturation scaling config cache updated: entries=1, has_default=true"}
{"level":"INFO","ts":"2025-12-10T12:19:11.388Z","msg":"saturation scaling configuration loaded successfully"}
{"level":"INFO","ts":"2025-12-10T12:19:11.388Z","msg":"Operating in MODEL-ONLY mode: model-based optimization only"}
{"level":"INFO","ts":"2025-12-10T12:19:11.388Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-12-10T12:19:11.388Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-12-10T12:19:11.392Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dgftqv, queueLength=0"}
{"level":"DEBUG","ts":"2025-12-10T12:19:11.392Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"INFO","ts":"2025-12-10T12:19:11.392Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dgftqv, usage=0.536 (53.6%)"}
{"level":"DEBUG","ts":"2025-12-10T12:19:11.392Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"DEBUG","ts":"2025-12-10T12:19:11.394Z","msg":"Pod-to-variant matching successful: totalPods=1, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"DEBUG","ts":"2025-12-10T12:19:11.394Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=1"}
{"level":"DEBUG","ts":"2025-12-10T12:19:11.394Z","msg":"Collected capacity metrics for observability in model-only mode: modelID=unsloth/Meta-Llama-3.1-8B, replicaCount=1"}
{"level":"DEBUG","ts":"2025-12-10T12:19:11.407Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=1, accelerator=H100, ttft=54.61ms, itl=32.74ms, cost=100.00, maxBatch=256, arrivalRate=1572.52, avgInputTokens=236.19, avgOutputTokens=429.90"}
{"level":"DEBUG","ts":"2025-12-10T12:19:11.407Z","msg":"Experimental model tuner is enabled globally: tuning model performance parameters for active VAs"}
{"level":"DEBUG","ts":"2025-12-10T12:19:11.407Z","msg":"Using state vals from VA status to tune variant ms-inference-scheduling-llm-d-modelservice-decode: alpha= 7.121801, beta= 0.031908, gamma= 15.193787, delta= 0.000246"}
{"level":"DEBUG","ts":"2025-12-10T12:19:11.407Z","msg":"[Tuner Config] variant=ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler | Initial state: alpha=7.121801, beta=0.031908, gamma=15.193787, delta=0.000246 | Expected obs (for R): TTFT=54.61ms, ITL=32.74ms | Has covariance=true"}
{"level":"WARN","ts":"2025-12-10T12:19:11.408Z","msg":"Tuner validation failed (NIS=332.23), validation error: normalized innovation squared (NIS=332.23) exceeds threshold (7.38), rejecting update as outlier - returning previous state: alpha=7.121801, beta=0.031908, gamma=15.193787, delta=0.000246"}
{"level":"WARN","ts":"2025-12-10T12:19:11.408Z","msg":"Tuner NIS validation failed for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler, server ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler (NIS=332.23 exceeds threshold 7.38) - Keeping previous state: alpha=7.121801, beta=0.031908, gamma=15.193787, delta=0.000246"}
{"level":"INFO","ts":"2025-12-10T12:19:11.409Z","msg":"Keeping previous tuned parameters for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler due to NIS validation failure (NIS=332.231958)"}
{"level":"DEBUG","ts":"2025-12-10T12:19:11.409Z","msg":"Model tuner results: model=unsloth/Meta-Llama-3.1-8B, accelerator=H100, alpha=7.121801, beta=0.031908, gamma=15.193787, delta=0.000246, NIS=332.23"}
{"level":"DEBUG","ts":"2025-12-10T12:19:11.409Z","msg":"Updated tuner status for variant ms-inference-scheduling-llm-d-modelservice-decode: alpha=7.121801, beta=0.031908, gamma=15.193787, delta=0.000246, NIS=332.231958"}
{"level":"DEBUG","ts":"2025-12-10T12:19:11.418Z","msg":"Optimization solution - system: Solution: \ns=ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler; c=Premium; m=unsloth/Meta-Llama-3.1-8B; rate=1572.52; inTk=236; outTk=429; sol=1, sat=false, alloc={acc=H100; numRep=2; maxBatch=512; cost=200, val=100, itl=8.722216, ttft=18.105711, rho=0.09601008, maxRPM=1244.5718}; slo-itl=10, slo-ttft=1000, slo-tps=0 \nAllocationByType: \nname=NVIDIA-H100-80GB-HBM3, count=2, limit=0, cost=200 \ntotalCost=200 \n"}
{"level":"DEBUG","ts":"2025-12-10T12:19:11.419Z","msg":"Setting accelerator name: Name=H100, allocationData={H100 2 512 200 8.722216 18.105711 {1572.52 236 429}}"}
{"level":"INFO","ts":"2025-12-10T12:19:11.419Z","msg":"Model-based optimization completed for model: unsloth/Meta-Llama-3.1-8B - model-based targets: map[ms-inference-scheduling-llm-d-modelservice-decode:2]"}
{"level":"WARN","ts":"2025-12-10T12:19:11.419Z","msg":"saturation analysis unavailable, using model-based targets only: modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-10T12:19:11.419Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-12-10T12:19:11.419Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=scale-up, current=1→target=2"}
{"level":"INFO","ts":"2025-12-10T12:19:11.419Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 1, desired-replicas: 2, accelerator: H100"}
{"level":"INFO","ts":"2025-12-10T12:19:11.419Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=2, accelerator=H100, SaturationOnly=false"}
{"level":"INFO","ts":"2025-12-10T12:19:11.425Z","msg":"Applied Saturation decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=scale-up, current=1, target=2, reason=model-based only (Saturation unavailable)"}
{"level":"INFO","ts":"2025-12-10T12:19:11.425Z","msg":"Reconciliation completed successfully: mode=model-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-12-10T12:20:11.426Z","msg":"Reconciling VariantAutoscaling: name=ms-inference-scheduling-llm-d-modelservice-decode, namespace=llm-d-inference-scheduler, modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-10T12:20:11.426Z","msg":"saturation scaling config cache updated: entries=1, has_default=true"}
{"level":"INFO","ts":"2025-12-10T12:20:11.426Z","msg":"saturation scaling configuration loaded successfully"}
{"level":"INFO","ts":"2025-12-10T12:20:11.426Z","msg":"Operating in MODEL-ONLY mode: model-based optimization only"}
{"level":"INFO","ts":"2025-12-10T12:20:11.426Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-12-10T12:20:11.426Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-12-10T12:20:11.438Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dgftqv, queueLength=0"}
{"level":"DEBUG","ts":"2025-12-10T12:20:11.438Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"INFO","ts":"2025-12-10T12:20:11.438Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dgftqv, usage=0.620 (62.0%)"}
{"level":"DEBUG","ts":"2025-12-10T12:20:11.438Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"DEBUG","ts":"2025-12-10T12:20:11.442Z","msg":"Pod-to-variant matching successful: totalPods=1, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"DEBUG","ts":"2025-12-10T12:20:11.442Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=1"}
{"level":"DEBUG","ts":"2025-12-10T12:20:11.442Z","msg":"Collected capacity metrics for observability in model-only mode: modelID=unsloth/Meta-Llama-3.1-8B, replicaCount=1"}
{"level":"DEBUG","ts":"2025-12-10T12:20:11.465Z","msg":"Replica count mismatch for llm-d-inference-scheduler/unsloth/Meta-Llama-3.1-8B: spec=2, reporting_metrics=1"}
{"level":"DEBUG","ts":"2025-12-10T12:20:11.465Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=1, accelerator=H100, ttft=63.67ms, itl=38.03ms, cost=100.00, maxBatch=256, arrivalRate=1584.60, avgInputTokens=246.19, avgOutputTokens=423.28"}
{"level":"DEBUG","ts":"2025-12-10T12:20:11.465Z","msg":"Experimental model tuner is enabled globally: tuning model performance parameters for active VAs"}
{"level":"DEBUG","ts":"2025-12-10T12:20:11.465Z","msg":"Using state vals from VA status to tune variant ms-inference-scheduling-llm-d-modelservice-decode: alpha= 7.121801, beta= 0.031908, gamma= 15.193787, delta= 0.000246"}
{"level":"DEBUG","ts":"2025-12-10T12:20:11.465Z","msg":"[Tuner Config] variant=ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler | Initial state: alpha=7.121801, beta=0.031908, gamma=15.193787, delta=0.000246 | Expected obs (for R): TTFT=63.67ms, ITL=38.03ms | Has covariance=true"}
{"level":"WARN","ts":"2025-12-10T12:20:11.466Z","msg":"Tuner validation failed (NIS=472.02), validation error: normalized innovation squared (NIS=472.02) exceeds threshold (7.38), rejecting update as outlier - returning previous state: alpha=7.121801, beta=0.031908, gamma=15.193787, delta=0.000246"}
{"level":"WARN","ts":"2025-12-10T12:20:11.466Z","msg":"Tuner NIS validation failed for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler, server ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler (NIS=472.02 exceeds threshold 7.38) - Keeping previous state: alpha=7.121801, beta=0.031908, gamma=15.193787, delta=0.000246"}
{"level":"INFO","ts":"2025-12-10T12:20:11.466Z","msg":"Keeping previous tuned parameters for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler due to NIS validation failure (NIS=472.017410)"}
{"level":"DEBUG","ts":"2025-12-10T12:20:11.466Z","msg":"Model tuner results: model=unsloth/Meta-Llama-3.1-8B, accelerator=H100, alpha=7.121801, beta=0.031908, gamma=15.193787, delta=0.000246, NIS=472.02"}
{"level":"DEBUG","ts":"2025-12-10T12:20:11.466Z","msg":"Updated tuner status for variant ms-inference-scheduling-llm-d-modelservice-decode: alpha=7.121801, beta=0.031908, gamma=15.193787, delta=0.000246, NIS=472.017410"}
{"level":"DEBUG","ts":"2025-12-10T12:20:11.475Z","msg":"Optimization solution - system: Solution: \ns=ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler; c=Premium; m=unsloth/Meta-Llama-3.1-8B; rate=1584.6; inTk=246; outTk=423; sol=1, sat=false, alloc={acc=H100; numRep=2; maxBatch=512; cost=200, val=100, itl=8.710103, ttft=18.206125, rho=0.09526865, maxRPM=1262.11}; slo-itl=10, slo-ttft=1000, slo-tps=0 \nAllocationByType: \nname=NVIDIA-H100-80GB-HBM3, count=2, limit=0, cost=200 \ntotalCost=200 \n"}
{"level":"DEBUG","ts":"2025-12-10T12:20:11.475Z","msg":"Setting accelerator name: Name=H100, allocationData={H100 2 512 200 8.710103 18.206125 {1584.6 246 423}}"}
{"level":"INFO","ts":"2025-12-10T12:20:11.475Z","msg":"Model-based optimization completed for model: unsloth/Meta-Llama-3.1-8B - model-based targets: map[ms-inference-scheduling-llm-d-modelservice-decode:2]"}
{"level":"WARN","ts":"2025-12-10T12:20:11.475Z","msg":"saturation analysis unavailable, using model-based targets only: modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-10T12:20:11.475Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-12-10T12:20:11.475Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=scale-up, current=1→target=2"}
{"level":"INFO","ts":"2025-12-10T12:20:11.475Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 1, desired-replicas: 2, accelerator: H100"}
{"level":"INFO","ts":"2025-12-10T12:20:11.475Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=2, accelerator=H100, SaturationOnly=false"}
{"level":"INFO","ts":"2025-12-10T12:20:11.481Z","msg":"Applied Saturation decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=scale-up, current=1, target=2, reason=model-based only (Saturation unavailable)"}
{"level":"INFO","ts":"2025-12-10T12:20:11.481Z","msg":"Reconciliation completed successfully: mode=model-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-12-10T12:21:11.482Z","msg":"Reconciling VariantAutoscaling: name=ms-inference-scheduling-llm-d-modelservice-decode, namespace=llm-d-inference-scheduler, modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-10T12:21:11.482Z","msg":"saturation scaling config cache updated: entries=1, has_default=true"}
{"level":"INFO","ts":"2025-12-10T12:21:11.482Z","msg":"saturation scaling configuration loaded successfully"}
{"level":"INFO","ts":"2025-12-10T12:21:11.482Z","msg":"Operating in MODEL-ONLY mode: model-based optimization only"}
{"level":"INFO","ts":"2025-12-10T12:21:11.482Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-12-10T12:21:11.482Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-12-10T12:21:11.489Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dgftqv, usage=0.824 (82.4%)"}
{"level":"DEBUG","ts":"2025-12-10T12:21:11.489Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"INFO","ts":"2025-12-10T12:21:11.489Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dgftqv, queueLength=0"}
{"level":"DEBUG","ts":"2025-12-10T12:21:11.489Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"DEBUG","ts":"2025-12-10T12:21:11.491Z","msg":"Pod-to-variant matching successful: totalPods=1, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"DEBUG","ts":"2025-12-10T12:21:11.491Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=1"}
{"level":"DEBUG","ts":"2025-12-10T12:21:11.491Z","msg":"Collected capacity metrics for observability in model-only mode: modelID=unsloth/Meta-Llama-3.1-8B, replicaCount=1"}
{"level":"DEBUG","ts":"2025-12-10T12:21:11.504Z","msg":"Replica count mismatch for llm-d-inference-scheduler/unsloth/Meta-Llama-3.1-8B: spec=2, reporting_metrics=1"}
{"level":"DEBUG","ts":"2025-12-10T12:21:11.504Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=1, accelerator=H100, ttft=77.57ms, itl=47.02ms, cost=100.00, maxBatch=256, arrivalRate=1563.12, avgInputTokens=238.55, avgOutputTokens=405.74"}
{"level":"DEBUG","ts":"2025-12-10T12:21:11.504Z","msg":"Experimental model tuner is enabled globally: tuning model performance parameters for active VAs"}
{"level":"DEBUG","ts":"2025-12-10T12:21:11.504Z","msg":"Using state vals from VA status to tune variant ms-inference-scheduling-llm-d-modelservice-decode: alpha= 7.121801, beta= 0.031908, gamma= 15.193787, delta= 0.000246"}
{"level":"DEBUG","ts":"2025-12-10T12:21:11.504Z","msg":"[Tuner Config] variant=ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler | Initial state: alpha=7.121801, beta=0.031908, gamma=15.193787, delta=0.000246 | Expected obs (for R): TTFT=77.57ms, ITL=47.02ms | Has covariance=true"}
{"level":"WARN","ts":"2025-12-10T12:21:11.505Z","msg":"Tuner validation failed (NIS=828.50), validation error: normalized innovation squared (NIS=828.50) exceeds threshold (7.38), rejecting update as outlier - returning previous state: alpha=7.121801, beta=0.031908, gamma=15.193787, delta=0.000246"}
{"level":"WARN","ts":"2025-12-10T12:21:11.505Z","msg":"Tuner NIS validation failed for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler, server ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler (NIS=828.50 exceeds threshold 7.38) - Keeping previous state: alpha=7.121801, beta=0.031908, gamma=15.193787, delta=0.000246"}
{"level":"INFO","ts":"2025-12-10T12:21:11.505Z","msg":"Keeping previous tuned parameters for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler due to NIS validation failure (NIS=828.498844)"}
{"level":"DEBUG","ts":"2025-12-10T12:21:11.505Z","msg":"Model tuner results: model=unsloth/Meta-Llama-3.1-8B, accelerator=H100, alpha=7.121801, beta=0.031908, gamma=15.193787, delta=0.000246, NIS=828.50"}
{"level":"DEBUG","ts":"2025-12-10T12:21:11.505Z","msg":"Updated tuner status for variant ms-inference-scheduling-llm-d-modelservice-decode: alpha=7.121801, beta=0.031908, gamma=15.193787, delta=0.000246, NIS=828.498844"}
{"level":"DEBUG","ts":"2025-12-10T12:21:11.515Z","msg":"Optimization solution - system: Solution: \ns=ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler; c=Premium; m=unsloth/Meta-Llama-3.1-8B; rate=1563.12; inTk=238; outTk=405; sol=1, sat=false, alloc={acc=H100; numRep=2; maxBatch=512; cost=200, val=100, itl=8.60629, ttft=17.917675, rho=0.08891412, maxRPM=1318.1172}; slo-itl=10, slo-ttft=1000, slo-tps=0 \nAllocationByType: \nname=NVIDIA-H100-80GB-HBM3, count=2, limit=0, cost=200 \ntotalCost=200 \n"}
{"level":"DEBUG","ts":"2025-12-10T12:21:11.515Z","msg":"Setting accelerator name: Name=H100, allocationData={H100 2 512 200 8.60629 17.917675 {1563.12 238 405}}"}
{"level":"INFO","ts":"2025-12-10T12:21:11.515Z","msg":"Model-based optimization completed for model: unsloth/Meta-Llama-3.1-8B - model-based targets: map[ms-inference-scheduling-llm-d-modelservice-decode:2]"}
{"level":"WARN","ts":"2025-12-10T12:21:11.515Z","msg":"saturation analysis unavailable, using model-based targets only: modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-10T12:21:11.515Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-12-10T12:21:11.515Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=scale-up, current=1→target=2"}
{"level":"INFO","ts":"2025-12-10T12:21:11.515Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 1, desired-replicas: 2, accelerator: H100"}
{"level":"INFO","ts":"2025-12-10T12:21:11.515Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=2, accelerator=H100, SaturationOnly=false"}
{"level":"INFO","ts":"2025-12-10T12:21:11.522Z","msg":"Applied Saturation decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=scale-up, current=1, target=2, reason=model-based only (Saturation unavailable)"}
{"level":"INFO","ts":"2025-12-10T12:21:11.522Z","msg":"Reconciliation completed successfully: mode=model-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-12-10T12:22:11.522Z","msg":"Reconciling VariantAutoscaling: name=ms-inference-scheduling-llm-d-modelservice-decode, namespace=llm-d-inference-scheduler, modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-10T12:22:11.522Z","msg":"saturation scaling config cache updated: entries=1, has_default=true"}
{"level":"INFO","ts":"2025-12-10T12:22:11.522Z","msg":"saturation scaling configuration loaded successfully"}
{"level":"INFO","ts":"2025-12-10T12:22:11.522Z","msg":"Operating in MODEL-ONLY mode: model-based optimization only"}
{"level":"INFO","ts":"2025-12-10T12:22:11.522Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-12-10T12:22:11.522Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-12-10T12:22:11.533Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dgftqv, usage=0.998 (99.8%)"}
{"level":"INFO","ts":"2025-12-10T12:22:11.533Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984d5vkpq, usage=0.234 (23.4%)"}
{"level":"DEBUG","ts":"2025-12-10T12:22:11.533Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=2"}
{"level":"INFO","ts":"2025-12-10T12:22:11.533Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dgftqv, queueLength=145"}
{"level":"INFO","ts":"2025-12-10T12:22:11.533Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984d5vkpq, queueLength=0"}
{"level":"DEBUG","ts":"2025-12-10T12:22:11.533Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=2"}
{"level":"DEBUG","ts":"2025-12-10T12:22:11.535Z","msg":"Pod-to-variant matching successful: totalPods=2, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:2]"}
{"level":"DEBUG","ts":"2025-12-10T12:22:11.535Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=2"}
{"level":"DEBUG","ts":"2025-12-10T12:22:11.535Z","msg":"Collected capacity metrics for observability in model-only mode: modelID=unsloth/Meta-Llama-3.1-8B, replicaCount=2"}
{"level":"DEBUG","ts":"2025-12-10T12:22:11.549Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=2, accelerator=H100, ttft=3197.81ms, itl=55.29ms, cost=200.00, maxBatch=256, arrivalRate=1457.57, avgInputTokens=219.82, avgOutputTokens=499.77"}
{"level":"DEBUG","ts":"2025-12-10T12:22:11.549Z","msg":"Experimental model tuner is enabled globally: tuning model performance parameters for active VAs"}
{"level":"DEBUG","ts":"2025-12-10T12:22:11.549Z","msg":"Using state vals from VA status to tune variant ms-inference-scheduling-llm-d-modelservice-decode: alpha= 7.121801, beta= 0.031908, gamma= 15.193787, delta= 0.000246"}
{"level":"DEBUG","ts":"2025-12-10T12:22:11.549Z","msg":"[Tuner Config] variant=ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler | Initial state: alpha=7.121801, beta=0.031908, gamma=15.193787, delta=0.000246 | Expected obs (for R): TTFT=3197.81ms, ITL=55.29ms | Has covariance=true"}
{"level":"WARN","ts":"2025-12-10T12:22:11.550Z","msg":"Tuner validation failed (NIS=2433.83), validation error: normalized innovation squared (NIS=2433.83) exceeds threshold (7.38), rejecting update as outlier - returning previous state: alpha=7.121801, beta=0.031908, gamma=15.193787, delta=0.000246"}
{"level":"WARN","ts":"2025-12-10T12:22:11.550Z","msg":"Tuner NIS validation failed for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler, server ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler (NIS=2433.83 exceeds threshold 7.38) - Keeping previous state: alpha=7.121801, beta=0.031908, gamma=15.193787, delta=0.000246"}
{"level":"INFO","ts":"2025-12-10T12:22:11.550Z","msg":"Keeping previous tuned parameters for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler due to NIS validation failure (NIS=2433.825255)"}
{"level":"DEBUG","ts":"2025-12-10T12:22:11.550Z","msg":"Model tuner results: model=unsloth/Meta-Llama-3.1-8B, accelerator=H100, alpha=7.121801, beta=0.031908, gamma=15.193787, delta=0.000246, NIS=2433.83"}
{"level":"DEBUG","ts":"2025-12-10T12:22:11.550Z","msg":"Updated tuner status for variant ms-inference-scheduling-llm-d-modelservice-decode: alpha=7.121801, beta=0.031908, gamma=15.193787, delta=0.000246, NIS=2433.825255"}
{"level":"DEBUG","ts":"2025-12-10T12:22:11.559Z","msg":"Optimization solution - system: Solution: \ns=ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler; c=Premium; m=unsloth/Meta-Llama-3.1-8B; rate=1457.57; inTk=219; outTk=499; sol=1, sat=false, alloc={acc=H100; numRep=2; maxBatch=512; cost=200, val=0, itl=8.873385, ttft=18.151192, rho=0.105263345, maxRPM=1070.4229}; slo-itl=10, slo-ttft=1000, slo-tps=0 \nAllocationByType: \nname=NVIDIA-H100-80GB-HBM3, count=2, limit=0, cost=200 \ntotalCost=200 \n"}
{"level":"DEBUG","ts":"2025-12-10T12:22:11.559Z","msg":"Setting accelerator name: Name=H100, allocationData={H100 2 512 200 8.873385 18.151192 {1457.57 219 499}}"}
{"level":"INFO","ts":"2025-12-10T12:22:11.559Z","msg":"Model-based optimization completed for model: unsloth/Meta-Llama-3.1-8B - model-based targets: map[ms-inference-scheduling-llm-d-modelservice-decode:2]"}
{"level":"WARN","ts":"2025-12-10T12:22:11.559Z","msg":"saturation analysis unavailable, using model-based targets only: modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-10T12:22:11.559Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-12-10T12:22:11.559Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=2→target=2"}
{"level":"INFO","ts":"2025-12-10T12:22:11.559Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 2, desired-replicas: 2, accelerator: H100"}
{"level":"INFO","ts":"2025-12-10T12:22:11.559Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=2, accelerator=H100, SaturationOnly=false"}
{"level":"INFO","ts":"2025-12-10T12:22:11.567Z","msg":"Applied Saturation decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=2, target=2, reason=model-based only (Saturation unavailable)"}
{"level":"INFO","ts":"2025-12-10T12:22:11.567Z","msg":"Reconciliation completed successfully: mode=model-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-12-10T12:23:11.568Z","msg":"Reconciling VariantAutoscaling: name=ms-inference-scheduling-llm-d-modelservice-decode, namespace=llm-d-inference-scheduler, modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-10T12:23:11.568Z","msg":"saturation scaling config cache updated: entries=1, has_default=true"}
{"level":"INFO","ts":"2025-12-10T12:23:11.568Z","msg":"saturation scaling configuration loaded successfully"}
{"level":"INFO","ts":"2025-12-10T12:23:11.568Z","msg":"Operating in MODEL-ONLY mode: model-based optimization only"}
{"level":"INFO","ts":"2025-12-10T12:23:11.568Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-12-10T12:23:11.568Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-12-10T12:23:11.576Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dgftqv, usage=0.531 (53.1%)"}
{"level":"INFO","ts":"2025-12-10T12:23:11.576Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984d5vkpq, usage=0.134 (13.4%)"}
{"level":"DEBUG","ts":"2025-12-10T12:23:11.576Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=2"}
{"level":"INFO","ts":"2025-12-10T12:23:11.576Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dgftqv, queueLength=0"}
{"level":"INFO","ts":"2025-12-10T12:23:11.576Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984d5vkpq, queueLength=0"}
{"level":"DEBUG","ts":"2025-12-10T12:23:11.576Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=2"}
{"level":"DEBUG","ts":"2025-12-10T12:23:11.581Z","msg":"Pod-to-variant matching successful: totalPods=2, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:2]"}
{"level":"DEBUG","ts":"2025-12-10T12:23:11.581Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=2"}
{"level":"DEBUG","ts":"2025-12-10T12:23:11.581Z","msg":"Collected capacity metrics for observability in model-only mode: modelID=unsloth/Meta-Llama-3.1-8B, replicaCount=2"}
{"level":"DEBUG","ts":"2025-12-10T12:23:11.599Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=2, accelerator=H100, ttft=29.53ms, itl=17.34ms, cost=200.00, maxBatch=256, arrivalRate=2291.05, avgInputTokens=215.95, avgOutputTokens=527.37"}
{"level":"DEBUG","ts":"2025-12-10T12:23:11.599Z","msg":"Experimental model tuner is enabled globally: tuning model performance parameters for active VAs"}
{"level":"DEBUG","ts":"2025-12-10T12:23:11.599Z","msg":"Using state vals from VA status to tune variant ms-inference-scheduling-llm-d-modelservice-decode: alpha= 7.121801, beta= 0.031908, gamma= 15.193787, delta= 0.000246"}
{"level":"DEBUG","ts":"2025-12-10T12:23:11.599Z","msg":"[Tuner Config] variant=ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler | Initial state: alpha=7.121801, beta=0.031908, gamma=15.193787, delta=0.000246 | Expected obs (for R): TTFT=29.53ms, ITL=17.34ms | Has covariance=true"}
{"level":"WARN","ts":"2025-12-10T12:23:11.600Z","msg":"Tuner validation failed (NIS=53.12), validation error: normalized innovation squared (NIS=53.12) exceeds threshold (7.38), rejecting update as outlier - returning previous state: alpha=7.121801, beta=0.031908, gamma=15.193787, delta=0.000246"}
{"level":"WARN","ts":"2025-12-10T12:23:11.600Z","msg":"Tuner NIS validation failed for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler, server ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler (NIS=53.12 exceeds threshold 7.38) - Keeping previous state: alpha=7.121801, beta=0.031908, gamma=15.193787, delta=0.000246"}
{"level":"INFO","ts":"2025-12-10T12:23:11.600Z","msg":"Keeping previous tuned parameters for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler due to NIS validation failure (NIS=53.124195)"}
{"level":"DEBUG","ts":"2025-12-10T12:23:11.600Z","msg":"Model tuner results: model=unsloth/Meta-Llama-3.1-8B, accelerator=H100, alpha=7.121801, beta=0.031908, gamma=15.193787, delta=0.000246, NIS=53.12"}
{"level":"DEBUG","ts":"2025-12-10T12:23:11.600Z","msg":"Updated tuner status for variant ms-inference-scheduling-llm-d-modelservice-decode: alpha=7.121801, beta=0.031908, gamma=15.193787, delta=0.000246, NIS=53.124195"}
{"level":"DEBUG","ts":"2025-12-10T12:23:11.610Z","msg":"Optimization solution - system: Solution: \ns=ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler; c=Premium; m=unsloth/Meta-Llama-3.1-8B; rate=2291.05; inTk=215; outTk=527; sol=1, sat=false, alloc={acc=H100; numRep=3; maxBatch=512; cost=300, val=100, itl=9.10659, ttft=18.48373, rho=0.11953807, maxRPM=1013.67566}; slo-itl=10, slo-ttft=1000, slo-tps=0 \nAllocationByType: \nname=NVIDIA-H100-80GB-HBM3, count=3, limit=0, cost=300 \ntotalCost=300 \n"}
{"level":"DEBUG","ts":"2025-12-10T12:23:11.610Z","msg":"Setting accelerator name: Name=H100, allocationData={H100 3 512 300 9.10659 18.48373 {2291.05 215 527}}"}
{"level":"INFO","ts":"2025-12-10T12:23:11.610Z","msg":"Model-based optimization completed for model: unsloth/Meta-Llama-3.1-8B - model-based targets: map[ms-inference-scheduling-llm-d-modelservice-decode:3]"}
{"level":"WARN","ts":"2025-12-10T12:23:11.610Z","msg":"saturation analysis unavailable, using model-based targets only: modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-10T12:23:11.610Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-12-10T12:23:11.610Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=scale-up, current=2→target=3"}
{"level":"INFO","ts":"2025-12-10T12:23:11.610Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 2, desired-replicas: 3, accelerator: H100"}
{"level":"INFO","ts":"2025-12-10T12:23:11.610Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=3, accelerator=H100, SaturationOnly=false"}
{"level":"INFO","ts":"2025-12-10T12:23:11.624Z","msg":"Applied Saturation decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=scale-up, current=2, target=3, reason=model-based only (Saturation unavailable)"}
{"level":"INFO","ts":"2025-12-10T12:23:11.624Z","msg":"Reconciliation completed successfully: mode=model-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-12-10T12:24:11.626Z","msg":"Reconciling VariantAutoscaling: name=ms-inference-scheduling-llm-d-modelservice-decode, namespace=llm-d-inference-scheduler, modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-10T12:24:11.627Z","msg":"saturation scaling config cache updated: entries=1, has_default=true"}
{"level":"INFO","ts":"2025-12-10T12:24:11.627Z","msg":"saturation scaling configuration loaded successfully"}
{"level":"INFO","ts":"2025-12-10T12:24:11.627Z","msg":"Operating in MODEL-ONLY mode: model-based optimization only"}
{"level":"INFO","ts":"2025-12-10T12:24:11.627Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-12-10T12:24:11.627Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-12-10T12:24:11.662Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dgftqv, queueLength=0"}
{"level":"INFO","ts":"2025-12-10T12:24:11.662Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984d5vkpq, queueLength=0"}
{"level":"DEBUG","ts":"2025-12-10T12:24:11.662Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=2"}
{"level":"INFO","ts":"2025-12-10T12:24:11.662Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dgftqv, usage=0.131 (13.1%)"}
{"level":"INFO","ts":"2025-12-10T12:24:11.662Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984d5vkpq, usage=0.123 (12.3%)"}
{"level":"DEBUG","ts":"2025-12-10T12:24:11.662Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=2"}
{"level":"DEBUG","ts":"2025-12-10T12:24:11.665Z","msg":"Pod-to-variant matching successful: totalPods=2, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:2]"}
{"level":"DEBUG","ts":"2025-12-10T12:24:11.665Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=2"}
{"level":"DEBUG","ts":"2025-12-10T12:24:11.665Z","msg":"Collected capacity metrics for observability in model-only mode: modelID=unsloth/Meta-Llama-3.1-8B, replicaCount=2"}
{"level":"DEBUG","ts":"2025-12-10T12:24:11.679Z","msg":"Replica count mismatch for llm-d-inference-scheduler/unsloth/Meta-Llama-3.1-8B: spec=3, reporting_metrics=2"}
{"level":"DEBUG","ts":"2025-12-10T12:24:11.679Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=2, accelerator=H100, ttft=22.97ms, itl=11.53ms, cost=200.00, maxBatch=256, arrivalRate=1896.13, avgInputTokens=239.56, avgOutputTokens=446.10"}
{"level":"DEBUG","ts":"2025-12-10T12:24:11.679Z","msg":"Experimental model tuner is enabled globally: tuning model performance parameters for active VAs"}
{"level":"DEBUG","ts":"2025-12-10T12:24:11.679Z","msg":"Using state vals from VA status to tune variant ms-inference-scheduling-llm-d-modelservice-decode: alpha= 7.121801, beta= 0.031908, gamma= 15.193787, delta= 0.000246"}
{"level":"DEBUG","ts":"2025-12-10T12:24:11.679Z","msg":"[Tuner Config] variant=ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler | Initial state: alpha=7.121801, beta=0.031908, gamma=15.193787, delta=0.000246 | Expected obs (for R): TTFT=22.97ms, ITL=11.53ms | Has covariance=true"}
{"level":"WARN","ts":"2025-12-10T12:24:11.680Z","msg":"Tuner validation failed (NIS=18.12), validation error: normalized innovation squared (NIS=18.12) exceeds threshold (7.38), rejecting update as outlier - returning previous state: alpha=7.121801, beta=0.031908, gamma=15.193787, delta=0.000246"}
{"level":"WARN","ts":"2025-12-10T12:24:11.680Z","msg":"Tuner NIS validation failed for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler, server ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler (NIS=18.12 exceeds threshold 7.38) - Keeping previous state: alpha=7.121801, beta=0.031908, gamma=15.193787, delta=0.000246"}
{"level":"INFO","ts":"2025-12-10T12:24:11.680Z","msg":"Keeping previous tuned parameters for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler due to NIS validation failure (NIS=18.124766)"}
{"level":"DEBUG","ts":"2025-12-10T12:24:11.680Z","msg":"Model tuner results: model=unsloth/Meta-Llama-3.1-8B, accelerator=H100, alpha=7.121801, beta=0.031908, gamma=15.193787, delta=0.000246, NIS=18.12"}
{"level":"DEBUG","ts":"2025-12-10T12:24:11.680Z","msg":"Updated tuner status for variant ms-inference-scheduling-llm-d-modelservice-decode: alpha=7.121801, beta=0.031908, gamma=15.193787, delta=0.000246, NIS=18.124766"}
{"level":"DEBUG","ts":"2025-12-10T12:24:11.689Z","msg":"Optimization solution - system: Solution: \ns=ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler; c=Premium; m=unsloth/Meta-Llama-3.1-8B; rate=1896.13; inTk=239; outTk=446; sol=1, sat=false, alloc={acc=H100; numRep=2; maxBatch=512; cost=200, val=0, itl=9.23539, ttft=19.088306, rho=0.12742206, maxRPM=1197.2206}; slo-itl=10, slo-ttft=1000, slo-tps=0 \nAllocationByType: \nname=NVIDIA-H100-80GB-HBM3, count=2, limit=0, cost=200 \ntotalCost=200 \n"}
{"level":"DEBUG","ts":"2025-12-10T12:24:11.689Z","msg":"Setting accelerator name: Name=H100, allocationData={H100 2 512 200 9.23539 19.088306 {1896.13 239 446}}"}
{"level":"INFO","ts":"2025-12-10T12:24:11.689Z","msg":"Model-based optimization completed for model: unsloth/Meta-Llama-3.1-8B - model-based targets: map[ms-inference-scheduling-llm-d-modelservice-decode:2]"}
{"level":"WARN","ts":"2025-12-10T12:24:11.689Z","msg":"saturation analysis unavailable, using model-based targets only: modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-10T12:24:11.689Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-12-10T12:24:11.689Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=2→target=2"}
{"level":"INFO","ts":"2025-12-10T12:24:11.689Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 2, desired-replicas: 2, accelerator: H100"}
{"level":"INFO","ts":"2025-12-10T12:24:11.689Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=2, accelerator=H100, SaturationOnly=false"}
{"level":"INFO","ts":"2025-12-10T12:24:11.697Z","msg":"Applied Saturation decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=2, target=2, reason=model-based only (Saturation unavailable)"}
{"level":"INFO","ts":"2025-12-10T12:24:11.697Z","msg":"Reconciliation completed successfully: mode=model-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-12-10T12:25:11.697Z","msg":"Reconciling VariantAutoscaling: name=ms-inference-scheduling-llm-d-modelservice-decode, namespace=llm-d-inference-scheduler, modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-10T12:25:11.697Z","msg":"saturation scaling config cache updated: entries=1, has_default=true"}
{"level":"INFO","ts":"2025-12-10T12:25:11.697Z","msg":"saturation scaling configuration loaded successfully"}
{"level":"INFO","ts":"2025-12-10T12:25:11.697Z","msg":"Operating in MODEL-ONLY mode: model-based optimization only"}
{"level":"INFO","ts":"2025-12-10T12:25:11.698Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-12-10T12:25:11.698Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-12-10T12:25:11.709Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dgftqv, queueLength=0"}
{"level":"INFO","ts":"2025-12-10T12:25:11.709Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984d5vkpq, queueLength=0"}
{"level":"DEBUG","ts":"2025-12-10T12:25:11.709Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=2"}
{"level":"INFO","ts":"2025-12-10T12:25:11.709Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dgftqv, usage=0.083 (8.3%)"}
{"level":"INFO","ts":"2025-12-10T12:25:11.709Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984d5vkpq, usage=0.071 (7.1%)"}
{"level":"DEBUG","ts":"2025-12-10T12:25:11.709Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=2"}
{"level":"DEBUG","ts":"2025-12-10T12:25:11.712Z","msg":"Pod-to-variant matching successful: totalPods=2, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:2]"}
{"level":"DEBUG","ts":"2025-12-10T12:25:11.712Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=2"}
{"level":"DEBUG","ts":"2025-12-10T12:25:11.712Z","msg":"Collected capacity metrics for observability in model-only mode: modelID=unsloth/Meta-Llama-3.1-8B, replicaCount=2"}
{"level":"DEBUG","ts":"2025-12-10T12:25:11.725Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=2, accelerator=H100, ttft=18.97ms, itl=9.17ms, cost=200.00, maxBatch=256, arrivalRate=1295.30, avgInputTokens=227.22, avgOutputTokens=448.65"}
{"level":"DEBUG","ts":"2025-12-10T12:25:11.725Z","msg":"Experimental model tuner is enabled globally: tuning model performance parameters for active VAs"}
{"level":"DEBUG","ts":"2025-12-10T12:25:11.725Z","msg":"Using state vals from VA status to tune variant ms-inference-scheduling-llm-d-modelservice-decode: alpha= 7.121801, beta= 0.031908, gamma= 15.193787, delta= 0.000246"}
{"level":"DEBUG","ts":"2025-12-10T12:25:11.725Z","msg":"[Tuner Config] variant=ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler | Initial state: alpha=7.121801, beta=0.031908, gamma=15.193787, delta=0.000246 | Expected obs (for R): TTFT=18.97ms, ITL=9.17ms | Has covariance=true"}
{"level":"INFO","ts":"2025-12-10T12:25:11.725Z","msg":"Tuner validation succeeded for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler, server ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler - New NIS: 3.282429"}
{"level":"DEBUG","ts":"2025-12-10T12:25:11.725Z","msg":"Model tuner results: model=unsloth/Meta-Llama-3.1-8B, accelerator=H100, alpha=7.474334, beta=0.035634, gamma=16.139812, delta=0.000247, NIS=3.28"}
{"level":"DEBUG","ts":"2025-12-10T12:25:11.725Z","msg":"Updated tuner status for variant ms-inference-scheduling-llm-d-modelservice-decode: alpha=7.474334, beta=0.035634, gamma=16.139812, delta=0.000247, NIS=3.282429"}
{"level":"INFO","ts":"2025-12-10T12:25:11.725Z","msg":"Tuned performance parameters result: variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler - alpha: 7.474334, beta: 0.035634, gamma: 16.139812, delta: 0.000247"}
{"level":"DEBUG","ts":"2025-12-10T12:25:11.735Z","msg":"Optimization solution - system: Solution: \ns=ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler; c=Premium; m=unsloth/Meta-Llama-3.1-8B; rate=1295.3; inTk=227; outTk=448; sol=1, sat=false, alloc={acc=H100; numRep=2; maxBatch=512; cost=200, val=0, itl=9.077975, ttft=18.658943, rho=0.08594253, maxRPM=933.74774}; slo-itl=10, slo-ttft=1000, slo-tps=0 \nAllocationByType: \nname=NVIDIA-H100-80GB-HBM3, count=2, limit=0, cost=200 \ntotalCost=200 \n"}
{"level":"DEBUG","ts":"2025-12-10T12:25:11.735Z","msg":"Setting accelerator name: Name=H100, allocationData={H100 2 512 200 9.077975 18.658943 {1295.3 227 448}}"}
{"level":"INFO","ts":"2025-12-10T12:25:11.735Z","msg":"Model-based optimization completed for model: unsloth/Meta-Llama-3.1-8B - model-based targets: map[ms-inference-scheduling-llm-d-modelservice-decode:2]"}
{"level":"WARN","ts":"2025-12-10T12:25:11.735Z","msg":"saturation analysis unavailable, using model-based targets only: modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-10T12:25:11.735Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-12-10T12:25:11.735Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=2→target=2"}
{"level":"INFO","ts":"2025-12-10T12:25:11.735Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 2, desired-replicas: 2, accelerator: H100"}
{"level":"INFO","ts":"2025-12-10T12:25:11.735Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=2, accelerator=H100, SaturationOnly=false"}
{"level":"INFO","ts":"2025-12-10T12:25:11.740Z","msg":"Applied Saturation decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=2, target=2, reason=model-based only (Saturation unavailable)"}
{"level":"INFO","ts":"2025-12-10T12:25:11.741Z","msg":"Reconciliation completed successfully: mode=model-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-12-10T12:26:11.741Z","msg":"Reconciling VariantAutoscaling: name=ms-inference-scheduling-llm-d-modelservice-decode, namespace=llm-d-inference-scheduler, modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-10T12:26:11.742Z","msg":"saturation scaling config cache updated: entries=1, has_default=true"}
{"level":"INFO","ts":"2025-12-10T12:26:11.742Z","msg":"saturation scaling configuration loaded successfully"}
{"level":"INFO","ts":"2025-12-10T12:26:11.742Z","msg":"Operating in MODEL-ONLY mode: model-based optimization only"}
{"level":"INFO","ts":"2025-12-10T12:26:11.742Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-12-10T12:26:11.742Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-12-10T12:26:11.750Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dgftqv, usage=0.055 (5.5%)"}
{"level":"INFO","ts":"2025-12-10T12:26:11.750Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984d5vkpq, usage=0.070 (7.0%)"}
{"level":"DEBUG","ts":"2025-12-10T12:26:11.750Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=2"}
{"level":"INFO","ts":"2025-12-10T12:26:11.751Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dgftqv, queueLength=0"}
{"level":"INFO","ts":"2025-12-10T12:26:11.751Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984d5vkpq, queueLength=0"}
{"level":"DEBUG","ts":"2025-12-10T12:26:11.751Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=2"}
{"level":"DEBUG","ts":"2025-12-10T12:26:11.757Z","msg":"Pod-to-variant matching successful: totalPods=2, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:2]"}
{"level":"DEBUG","ts":"2025-12-10T12:26:11.757Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=2"}
{"level":"DEBUG","ts":"2025-12-10T12:26:11.757Z","msg":"Collected capacity metrics for observability in model-only mode: modelID=unsloth/Meta-Llama-3.1-8B, replicaCount=2"}
{"level":"DEBUG","ts":"2025-12-10T12:26:11.777Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=2, accelerator=H100, ttft=18.90ms, itl=9.09ms, cost=200.00, maxBatch=256, arrivalRate=1157.18, avgInputTokens=229.37, avgOutputTokens=471.23"}
{"level":"DEBUG","ts":"2025-12-10T12:26:11.777Z","msg":"Experimental model tuner is enabled globally: tuning model performance parameters for active VAs"}
{"level":"DEBUG","ts":"2025-12-10T12:26:11.777Z","msg":"Using state vals from VA status to tune variant ms-inference-scheduling-llm-d-modelservice-decode: alpha= 7.474334, beta= 0.035634, gamma= 16.139812, delta= 0.000247"}
{"level":"DEBUG","ts":"2025-12-10T12:26:11.777Z","msg":"[Tuner Config] variant=ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler | Initial state: alpha=7.474334, beta=0.035634, gamma=16.139812, delta=0.000247 | Expected obs (for R): TTFT=18.90ms, ITL=9.09ms | Has covariance=true"}
{"level":"INFO","ts":"2025-12-10T12:26:11.778Z","msg":"Tuner validation succeeded for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler, server ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler - New NIS: 0.181553"}
{"level":"DEBUG","ts":"2025-12-10T12:26:11.778Z","msg":"Model tuner results: model=unsloth/Meta-Llama-3.1-8B, accelerator=H100, alpha=7.557260, beta=0.035798, gamma=16.425249, delta=0.000247, NIS=0.18"}
{"level":"DEBUG","ts":"2025-12-10T12:26:11.778Z","msg":"Updated tuner status for variant ms-inference-scheduling-llm-d-modelservice-decode: alpha=7.557260, beta=0.035798, gamma=16.425249, delta=0.000247, NIS=0.181553"}
{"level":"INFO","ts":"2025-12-10T12:26:11.778Z","msg":"Tuned performance parameters result: variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler - alpha: 7.557260, beta: 0.035798, gamma: 16.425249, delta: 0.000247"}
{"level":"DEBUG","ts":"2025-12-10T12:26:11.788Z","msg":"Optimization solution - system: Solution: \ns=ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler; c=Premium; m=unsloth/Meta-Llama-3.1-8B; rate=1157.18; inTk=229; outTk=471; sol=1, sat=false, alloc={acc=H100; numRep=2; maxBatch=512; cost=200, val=0, itl=9.071337, ttft=18.819157, rho=0.08065507, maxRPM=854.66064}; slo-itl=10, slo-ttft=1000, slo-tps=0 \nAllocationByType: \nname=NVIDIA-H100-80GB-HBM3, count=2, limit=0, cost=200 \ntotalCost=200 \n"}
{"level":"DEBUG","ts":"2025-12-10T12:26:11.788Z","msg":"Setting accelerator name: Name=H100, allocationData={H100 2 512 200 9.071337 18.819157 {1157.18 229 471}}"}
{"level":"INFO","ts":"2025-12-10T12:26:11.788Z","msg":"Model-based optimization completed for model: unsloth/Meta-Llama-3.1-8B - model-based targets: map[ms-inference-scheduling-llm-d-modelservice-decode:2]"}
{"level":"WARN","ts":"2025-12-10T12:26:11.788Z","msg":"saturation analysis unavailable, using model-based targets only: modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-10T12:26:11.788Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-12-10T12:26:11.788Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=2→target=2"}
{"level":"INFO","ts":"2025-12-10T12:26:11.788Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 2, desired-replicas: 2, accelerator: H100"}
{"level":"INFO","ts":"2025-12-10T12:26:11.788Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=2, accelerator=H100, SaturationOnly=false"}
{"level":"INFO","ts":"2025-12-10T12:26:11.795Z","msg":"Applied Saturation decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=2, target=2, reason=model-based only (Saturation unavailable)"}
{"level":"INFO","ts":"2025-12-10T12:26:11.795Z","msg":"Reconciliation completed successfully: mode=model-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-12-10T12:27:11.795Z","msg":"Reconciling VariantAutoscaling: name=ms-inference-scheduling-llm-d-modelservice-decode, namespace=llm-d-inference-scheduler, modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-10T12:27:11.795Z","msg":"saturation scaling config cache updated: entries=1, has_default=true"}
{"level":"INFO","ts":"2025-12-10T12:27:11.795Z","msg":"saturation scaling configuration loaded successfully"}
{"level":"INFO","ts":"2025-12-10T12:27:11.795Z","msg":"Operating in MODEL-ONLY mode: model-based optimization only"}
{"level":"INFO","ts":"2025-12-10T12:27:11.795Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-12-10T12:27:11.795Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-12-10T12:27:11.801Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dgftqv, usage=0.057 (5.7%)"}
{"level":"INFO","ts":"2025-12-10T12:27:11.801Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984d5vkpq, usage=0.088 (8.8%)"}
{"level":"INFO","ts":"2025-12-10T12:27:11.801Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dgftqv, queueLength=0"}
{"level":"INFO","ts":"2025-12-10T12:27:11.801Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984d5vkpq, queueLength=0"}
{"level":"DEBUG","ts":"2025-12-10T12:27:11.801Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=2"}
{"level":"DEBUG","ts":"2025-12-10T12:27:11.801Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=2"}
{"level":"DEBUG","ts":"2025-12-10T12:27:11.804Z","msg":"Pod-to-variant matching successful: totalPods=2, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:2]"}
{"level":"DEBUG","ts":"2025-12-10T12:27:11.804Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=2"}
{"level":"DEBUG","ts":"2025-12-10T12:27:11.804Z","msg":"Collected capacity metrics for observability in model-only mode: modelID=unsloth/Meta-Llama-3.1-8B, replicaCount=2"}
{"level":"DEBUG","ts":"2025-12-10T12:27:11.817Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=2, accelerator=H100, ttft=18.87ms, itl=9.38ms, cost=200.00, maxBatch=256, arrivalRate=1207.30, avgInputTokens=221.17, avgOutputTokens=504.15"}
{"level":"DEBUG","ts":"2025-12-10T12:27:11.817Z","msg":"Experimental model tuner is enabled globally: tuning model performance parameters for active VAs"}
{"level":"DEBUG","ts":"2025-12-10T12:27:11.817Z","msg":"Using state vals from VA status to tune variant ms-inference-scheduling-llm-d-modelservice-decode: alpha= 7.557260, beta= 0.035798, gamma= 16.425249, delta= 0.000247"}
{"level":"DEBUG","ts":"2025-12-10T12:27:11.817Z","msg":"[Tuner Config] variant=ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler | Initial state: alpha=7.557260, beta=0.035798, gamma=16.425249, delta=0.000247 | Expected obs (for R): TTFT=18.87ms, ITL=9.38ms | Has covariance=true"}
{"level":"INFO","ts":"2025-12-10T12:27:11.818Z","msg":"Tuner validation succeeded for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler, server ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler - New NIS: 0.067037"}
{"level":"DEBUG","ts":"2025-12-10T12:27:11.818Z","msg":"Model tuner results: model=unsloth/Meta-Llama-3.1-8B, accelerator=H100, alpha=7.613257, beta=0.035990, gamma=16.263306, delta=0.000247, NIS=0.07"}
{"level":"DEBUG","ts":"2025-12-10T12:27:11.818Z","msg":"Updated tuner status for variant ms-inference-scheduling-llm-d-modelservice-decode: alpha=7.613257, beta=0.035990, gamma=16.263306, delta=0.000247, NIS=0.067037"}
{"level":"INFO","ts":"2025-12-10T12:27:11.818Z","msg":"Tuned performance parameters result: variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler - alpha: 7.613257, beta: 0.035990, gamma: 16.263306, delta: 0.000247"}
{"level":"DEBUG","ts":"2025-12-10T12:27:11.827Z","msg":"Optimization solution - system: Solution: \ns=ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler; c=Premium; m=unsloth/Meta-Llama-3.1-8B; rate=1207.3; inTk=221; outTk=504; sol=1, sat=false, alloc={acc=H100; numRep=2; maxBatch=512; cost=200, val=0, itl=9.360999, ttft=18.912949, rho=0.092895605, maxRPM=776.069}; slo-itl=10, slo-ttft=1000, slo-tps=0 \nAllocationByType: \nname=NVIDIA-H100-80GB-HBM3, count=2, limit=0, cost=200 \ntotalCost=200 \n"}
{"level":"DEBUG","ts":"2025-12-10T12:27:11.827Z","msg":"Setting accelerator name: Name=H100, allocationData={H100 2 512 200 9.360999 18.912949 {1207.3 221 504}}"}
{"level":"INFO","ts":"2025-12-10T12:27:11.827Z","msg":"Model-based optimization completed for model: unsloth/Meta-Llama-3.1-8B - model-based targets: map[ms-inference-scheduling-llm-d-modelservice-decode:2]"}
{"level":"WARN","ts":"2025-12-10T12:27:11.827Z","msg":"saturation analysis unavailable, using model-based targets only: modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-10T12:27:11.827Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-12-10T12:27:11.827Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=2→target=2"}
{"level":"INFO","ts":"2025-12-10T12:27:11.827Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 2, desired-replicas: 2, accelerator: H100"}
{"level":"INFO","ts":"2025-12-10T12:27:11.827Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=2, accelerator=H100, SaturationOnly=false"}
{"level":"INFO","ts":"2025-12-10T12:27:11.834Z","msg":"Applied Saturation decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=2, target=2, reason=model-based only (Saturation unavailable)"}
{"level":"INFO","ts":"2025-12-10T12:27:11.834Z","msg":"Reconciliation completed successfully: mode=model-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-12-10T12:28:11.834Z","msg":"Reconciling VariantAutoscaling: name=ms-inference-scheduling-llm-d-modelservice-decode, namespace=llm-d-inference-scheduler, modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-10T12:28:11.834Z","msg":"saturation scaling config cache updated: entries=1, has_default=true"}
{"level":"INFO","ts":"2025-12-10T12:28:11.834Z","msg":"saturation scaling configuration loaded successfully"}
{"level":"INFO","ts":"2025-12-10T12:28:11.834Z","msg":"Operating in MODEL-ONLY mode: model-based optimization only"}
{"level":"INFO","ts":"2025-12-10T12:28:11.834Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-12-10T12:28:11.834Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-12-10T12:28:11.841Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dgftqv, queueLength=0"}
{"level":"INFO","ts":"2025-12-10T12:28:11.841Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984d5vkpq, queueLength=0"}
{"level":"DEBUG","ts":"2025-12-10T12:28:11.841Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=2"}
{"level":"INFO","ts":"2025-12-10T12:28:11.842Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dgftqv, usage=0.069 (6.9%)"}
{"level":"INFO","ts":"2025-12-10T12:28:11.842Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984d5vkpq, usage=0.070 (7.0%)"}
{"level":"DEBUG","ts":"2025-12-10T12:28:11.842Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=2"}
{"level":"DEBUG","ts":"2025-12-10T12:28:11.846Z","msg":"Pod-to-variant matching successful: totalPods=2, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:2]"}
{"level":"DEBUG","ts":"2025-12-10T12:28:11.846Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=2"}
{"level":"DEBUG","ts":"2025-12-10T12:28:11.846Z","msg":"Collected capacity metrics for observability in model-only mode: modelID=unsloth/Meta-Llama-3.1-8B, replicaCount=2"}
{"level":"DEBUG","ts":"2025-12-10T12:28:11.861Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=2, accelerator=H100, ttft=16.57ms, itl=8.08ms, cost=200.00, maxBatch=256, arrivalRate=711.81, avgInputTokens=211.57, avgOutputTokens=534.67"}
{"level":"DEBUG","ts":"2025-12-10T12:28:11.861Z","msg":"Experimental model tuner is enabled globally: tuning model performance parameters for active VAs"}
{"level":"DEBUG","ts":"2025-12-10T12:28:11.861Z","msg":"Using state vals from VA status to tune variant ms-inference-scheduling-llm-d-modelservice-decode: alpha= 7.613257, beta= 0.035990, gamma= 16.263306, delta= 0.000247"}
{"level":"DEBUG","ts":"2025-12-10T12:28:11.861Z","msg":"[Tuner Config] variant=ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler | Initial state: alpha=7.613257, beta=0.035990, gamma=16.263306, delta=0.000247 | Expected obs (for R): TTFT=16.57ms, ITL=8.08ms | Has covariance=true"}
{"level":"INFO","ts":"2025-12-10T12:28:11.861Z","msg":"Tuner validation succeeded for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler, server ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler - New NIS: 2.257676"}
{"level":"DEBUG","ts":"2025-12-10T12:28:11.861Z","msg":"Model tuner results: model=unsloth/Meta-Llama-3.1-8B, accelerator=H100, alpha=7.136139, beta=0.037829, gamma=15.360634, delta=0.000247, NIS=2.26"}
{"level":"DEBUG","ts":"2025-12-10T12:28:11.861Z","msg":"Updated tuner status for variant ms-inference-scheduling-llm-d-modelservice-decode: alpha=7.136139, beta=0.037829, gamma=15.360634, delta=0.000247, NIS=2.257676"}
{"level":"INFO","ts":"2025-12-10T12:28:11.861Z","msg":"Tuned performance parameters result: variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler - alpha: 7.136139, beta: 0.037829, gamma: 15.360634, delta: 0.000247"}
{"level":"DEBUG","ts":"2025-12-10T12:28:11.865Z","msg":"Optimization solution - system: Solution: \ns=ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler; c=Premium; m=unsloth/Meta-Llama-3.1-8B; rate=711.81; inTk=211; outTk=534; sol=1, sat=false, alloc={acc=H100; numRep=1; maxBatch=512; cost=100, val=-100, itl=9.440437, ttft=18.536482, rho=0.11701975, maxRPM=837.93286}; slo-itl=10, slo-ttft=1000, slo-tps=0 \nAllocationByType: \nname=NVIDIA-H100-80GB-HBM3, count=1, limit=0, cost=100 \ntotalCost=100 \n"}
{"level":"DEBUG","ts":"2025-12-10T12:28:11.865Z","msg":"Setting accelerator name: Name=H100, allocationData={H100 1 512 100 9.440437 18.536482 {711.81 211 534}}"}
{"level":"INFO","ts":"2025-12-10T12:28:11.865Z","msg":"Model-based optimization completed for model: unsloth/Meta-Llama-3.1-8B - model-based targets: map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"WARN","ts":"2025-12-10T12:28:11.865Z","msg":"saturation analysis unavailable, using model-based targets only: modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-10T12:28:11.865Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-12-10T12:28:11.865Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=scale-down, current=2→target=1"}
{"level":"INFO","ts":"2025-12-10T12:28:11.865Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 2, desired-replicas: 1, accelerator: H100"}
{"level":"INFO","ts":"2025-12-10T12:28:11.865Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=1, accelerator=H100, SaturationOnly=false"}
{"level":"INFO","ts":"2025-12-10T12:28:11.872Z","msg":"Applied Saturation decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=scale-down, current=2, target=1, reason=model-based only (Saturation unavailable)"}
{"level":"INFO","ts":"2025-12-10T12:28:11.872Z","msg":"Reconciliation completed successfully: mode=model-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-12-10T12:29:11.873Z","msg":"Reconciling VariantAutoscaling: name=ms-inference-scheduling-llm-d-modelservice-decode, namespace=llm-d-inference-scheduler, modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-10T12:29:11.873Z","msg":"saturation scaling config cache updated: entries=1, has_default=true"}
{"level":"INFO","ts":"2025-12-10T12:29:11.873Z","msg":"saturation scaling configuration loaded successfully"}
{"level":"INFO","ts":"2025-12-10T12:29:11.873Z","msg":"Operating in MODEL-ONLY mode: model-based optimization only"}
{"level":"INFO","ts":"2025-12-10T12:29:11.873Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-12-10T12:29:11.873Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-12-10T12:29:11.882Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dgftqv, queueLength=0"}
{"level":"INFO","ts":"2025-12-10T12:29:11.882Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984d5vkpq, queueLength=0"}
{"level":"DEBUG","ts":"2025-12-10T12:29:11.882Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=2"}
{"level":"INFO","ts":"2025-12-10T12:29:11.882Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dgftqv, usage=0.065 (6.5%)"}
{"level":"INFO","ts":"2025-12-10T12:29:11.882Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984d5vkpq, usage=0.022 (2.2%)"}
{"level":"DEBUG","ts":"2025-12-10T12:29:11.882Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=2"}
{"level":"DEBUG","ts":"2025-12-10T12:29:11.885Z","msg":"Pod-to-variant matching successful: totalPods=2, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:2]"}
{"level":"DEBUG","ts":"2025-12-10T12:29:11.885Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=2"}
{"level":"DEBUG","ts":"2025-12-10T12:29:11.885Z","msg":"Collected capacity metrics for observability in model-only mode: modelID=unsloth/Meta-Llama-3.1-8B, replicaCount=2"}
{"level":"DEBUG","ts":"2025-12-10T12:29:11.898Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=1, accelerator=H100, ttft=18.37ms, itl=8.62ms, cost=100.00, maxBatch=256, arrivalRate=488.81, avgInputTokens=257.66, avgOutputTokens=383.21"}
{"level":"DEBUG","ts":"2025-12-10T12:29:11.898Z","msg":"Experimental model tuner is enabled globally: tuning model performance parameters for active VAs"}
{"level":"DEBUG","ts":"2025-12-10T12:29:11.898Z","msg":"Using state vals from VA status to tune variant ms-inference-scheduling-llm-d-modelservice-decode: alpha= 7.136139, beta= 0.037829, gamma= 15.360634, delta= 0.000247"}
{"level":"DEBUG","ts":"2025-12-10T12:29:11.898Z","msg":"[Tuner Config] variant=ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler | Initial state: alpha=7.136139, beta=0.037829, gamma=15.360634, delta=0.000247 | Expected obs (for R): TTFT=18.37ms, ITL=8.62ms | Has covariance=true"}
{"level":"INFO","ts":"2025-12-10T12:29:11.899Z","msg":"Tuner validation succeeded for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler, server ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler - New NIS: 2.584940"}
{"level":"DEBUG","ts":"2025-12-10T12:29:11.899Z","msg":"Model tuner results: model=unsloth/Meta-Llama-3.1-8B, accelerator=H100, alpha=7.495615, beta=0.037541, gamma=16.321415, delta=0.000247, NIS=2.58"}
{"level":"DEBUG","ts":"2025-12-10T12:29:11.899Z","msg":"Updated tuner status for variant ms-inference-scheduling-llm-d-modelservice-decode: alpha=7.495615, beta=0.037541, gamma=16.321415, delta=0.000247, NIS=2.584940"}
{"level":"INFO","ts":"2025-12-10T12:29:11.899Z","msg":"Tuned performance parameters result: variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler - alpha: 7.495615, beta: 0.037541, gamma: 16.321415, delta: 0.000247"}
{"level":"DEBUG","ts":"2025-12-10T12:29:11.908Z","msg":"Optimization solution - system: Solution: \ns=ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler; c=Premium; m=unsloth/Meta-Llama-3.1-8B; rate=488.81; inTk=257; outTk=383; sol=1, sat=false, alloc={acc=H100; numRep=1; maxBatch=512; cost=100, val=0, itl=8.535957, ttft=18.080893, rho=0.052171826, maxRPM=1026.5709}; slo-itl=10, slo-ttft=1000, slo-tps=0 \nAllocationByType: \nname=NVIDIA-H100-80GB-HBM3, count=1, limit=0, cost=100 \ntotalCost=100 \n"}
{"level":"DEBUG","ts":"2025-12-10T12:29:11.908Z","msg":"Setting accelerator name: Name=H100, allocationData={H100 1 512 100 8.535957 18.080893 {488.81 257 383}}"}
{"level":"INFO","ts":"2025-12-10T12:29:11.908Z","msg":"Model-based optimization completed for model: unsloth/Meta-Llama-3.1-8B - model-based targets: map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"WARN","ts":"2025-12-10T12:29:11.908Z","msg":"saturation analysis unavailable, using model-based targets only: modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-10T12:29:11.908Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-12-10T12:29:11.908Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1→target=1"}
{"level":"INFO","ts":"2025-12-10T12:29:11.908Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 1, desired-replicas: 1, accelerator: H100"}
{"level":"INFO","ts":"2025-12-10T12:29:11.908Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=1, accelerator=H100, SaturationOnly=false"}
{"level":"INFO","ts":"2025-12-10T12:29:11.913Z","msg":"Applied Saturation decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1, target=1, reason=model-based only (Saturation unavailable)"}
{"level":"INFO","ts":"2025-12-10T12:29:11.913Z","msg":"Reconciliation completed successfully: mode=model-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-12-10T12:30:11.914Z","msg":"Reconciling VariantAutoscaling: name=ms-inference-scheduling-llm-d-modelservice-decode, namespace=llm-d-inference-scheduler, modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-10T12:30:11.914Z","msg":"saturation scaling config cache updated: entries=1, has_default=true"}
{"level":"INFO","ts":"2025-12-10T12:30:11.914Z","msg":"saturation scaling configuration loaded successfully"}
{"level":"INFO","ts":"2025-12-10T12:30:11.914Z","msg":"Operating in MODEL-ONLY mode: model-based optimization only"}
{"level":"INFO","ts":"2025-12-10T12:30:11.914Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-12-10T12:30:11.914Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-12-10T12:30:11.928Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dgftqv, queueLength=0"}
{"level":"DEBUG","ts":"2025-12-10T12:30:11.928Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"INFO","ts":"2025-12-10T12:30:11.928Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dgftqv, usage=0.053 (5.3%)"}
{"level":"DEBUG","ts":"2025-12-10T12:30:11.928Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"DEBUG","ts":"2025-12-10T12:30:11.930Z","msg":"Pod-to-variant matching successful: totalPods=1, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"DEBUG","ts":"2025-12-10T12:30:11.930Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=1"}
{"level":"DEBUG","ts":"2025-12-10T12:30:11.930Z","msg":"Collected capacity metrics for observability in model-only mode: modelID=unsloth/Meta-Llama-3.1-8B, replicaCount=1"}
{"level":"DEBUG","ts":"2025-12-10T12:30:11.942Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=1, accelerator=H100, ttft=19.12ms, itl=9.26ms, cost=100.00, maxBatch=256, arrivalRate=625.78, avgInputTokens=240.90, avgOutputTokens=439.51"}
{"level":"DEBUG","ts":"2025-12-10T12:30:11.942Z","msg":"Experimental model tuner is enabled globally: tuning model performance parameters for active VAs"}
{"level":"DEBUG","ts":"2025-12-10T12:30:11.942Z","msg":"Using state vals from VA status to tune variant ms-inference-scheduling-llm-d-modelservice-decode: alpha= 7.495615, beta= 0.037541, gamma= 16.321415, delta= 0.000247"}
{"level":"DEBUG","ts":"2025-12-10T12:30:11.942Z","msg":"[Tuner Config] variant=ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler | Initial state: alpha=7.495615, beta=0.037541, gamma=16.321415, delta=0.000247 | Expected obs (for R): TTFT=19.12ms, ITL=9.26ms | Has covariance=true"}
{"level":"INFO","ts":"2025-12-10T12:30:11.943Z","msg":"Tuner validation succeeded for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler, server ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler - New NIS: 0.124624"}
{"level":"DEBUG","ts":"2025-12-10T12:30:11.943Z","msg":"Model tuner results: model=unsloth/Meta-Llama-3.1-8B, accelerator=H100, alpha=7.581754, beta=0.038130, gamma=16.495916, delta=0.000247, NIS=0.12"}
{"level":"DEBUG","ts":"2025-12-10T12:30:11.943Z","msg":"Updated tuner status for variant ms-inference-scheduling-llm-d-modelservice-decode: alpha=7.581754, beta=0.038130, gamma=16.495916, delta=0.000247, NIS=0.124624"}
{"level":"INFO","ts":"2025-12-10T12:30:11.943Z","msg":"Tuned performance parameters result: variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler - alpha: 7.581754, beta: 0.038130, gamma: 16.495916, delta: 0.000247"}
{"level":"DEBUG","ts":"2025-12-10T12:30:11.952Z","msg":"Optimization solution - system: Solution: \ns=ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler; c=Premium; m=unsloth/Meta-Llama-3.1-8B; rate=625.78; inTk=240; outTk=439; sol=1, sat=false, alloc={acc=H100; numRep=1; maxBatch=512; cost=100, val=0, itl=9.236306, ttft=19.069458, rho=0.082797125, maxRPM=851.14026}; slo-itl=10, slo-ttft=1000, slo-tps=0 \nAllocationByType: \nname=NVIDIA-H100-80GB-HBM3, count=1, limit=0, cost=100 \ntotalCost=100 \n"}
{"level":"DEBUG","ts":"2025-12-10T12:30:11.952Z","msg":"Setting accelerator name: Name=H100, allocationData={H100 1 512 100 9.236306 19.069458 {625.78 240 439}}"}
{"level":"INFO","ts":"2025-12-10T12:30:11.952Z","msg":"Model-based optimization completed for model: unsloth/Meta-Llama-3.1-8B - model-based targets: map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"WARN","ts":"2025-12-10T12:30:11.952Z","msg":"saturation analysis unavailable, using model-based targets only: modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-10T12:30:11.952Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-12-10T12:30:11.952Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1→target=1"}
{"level":"INFO","ts":"2025-12-10T12:30:11.952Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 1, desired-replicas: 1, accelerator: H100"}
{"level":"INFO","ts":"2025-12-10T12:30:11.952Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=1, accelerator=H100, SaturationOnly=false"}
{"level":"INFO","ts":"2025-12-10T12:30:11.958Z","msg":"Applied Saturation decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1, target=1, reason=model-based only (Saturation unavailable)"}
{"level":"INFO","ts":"2025-12-10T12:30:11.958Z","msg":"Reconciliation completed successfully: mode=model-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-12-10T12:31:11.958Z","msg":"Reconciling VariantAutoscaling: name=ms-inference-scheduling-llm-d-modelservice-decode, namespace=llm-d-inference-scheduler, modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-10T12:31:11.958Z","msg":"saturation scaling config cache updated: entries=1, has_default=true"}
{"level":"INFO","ts":"2025-12-10T12:31:11.958Z","msg":"saturation scaling configuration loaded successfully"}
{"level":"INFO","ts":"2025-12-10T12:31:11.958Z","msg":"Operating in MODEL-ONLY mode: model-based optimization only"}
{"level":"INFO","ts":"2025-12-10T12:31:11.958Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-12-10T12:31:11.958Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-12-10T12:31:11.966Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dgftqv, queueLength=0"}
{"level":"DEBUG","ts":"2025-12-10T12:31:11.966Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"INFO","ts":"2025-12-10T12:31:11.977Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dgftqv, usage=0.039 (3.9%)"}
{"level":"DEBUG","ts":"2025-12-10T12:31:11.977Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"DEBUG","ts":"2025-12-10T12:31:11.979Z","msg":"Pod-to-variant matching successful: totalPods=1, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"DEBUG","ts":"2025-12-10T12:31:11.979Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=1"}
{"level":"DEBUG","ts":"2025-12-10T12:31:11.979Z","msg":"Collected capacity metrics for observability in model-only mode: modelID=unsloth/Meta-Llama-3.1-8B, replicaCount=1"}
{"level":"DEBUG","ts":"2025-12-10T12:31:11.991Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=1, accelerator=H100, ttft=18.30ms, itl=8.58ms, cost=100.00, maxBatch=256, arrivalRate=437.78, avgInputTokens=227.51, avgOutputTokens=468.72"}
{"level":"DEBUG","ts":"2025-12-10T12:31:11.991Z","msg":"Experimental model tuner is enabled globally: tuning model performance parameters for active VAs"}
{"level":"DEBUG","ts":"2025-12-10T12:31:11.991Z","msg":"Using state vals from VA status to tune variant ms-inference-scheduling-llm-d-modelservice-decode: alpha= 7.581754, beta= 0.038130, gamma= 16.495916, delta= 0.000247"}
{"level":"DEBUG","ts":"2025-12-10T12:31:11.991Z","msg":"[Tuner Config] variant=ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler | Initial state: alpha=7.581754, beta=0.038130, gamma=16.495916, delta=0.000247 | Expected obs (for R): TTFT=18.30ms, ITL=8.58ms | Has covariance=true"}
{"level":"INFO","ts":"2025-12-10T12:31:11.992Z","msg":"Tuner validation succeeded for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler, server ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler - New NIS: 0.124105"}
{"level":"DEBUG","ts":"2025-12-10T12:31:11.992Z","msg":"Model tuner results: model=unsloth/Meta-Llama-3.1-8B, accelerator=H100, alpha=7.441106, beta=0.038384, gamma=16.573370, delta=0.000247, NIS=0.12"}
{"level":"DEBUG","ts":"2025-12-10T12:31:11.992Z","msg":"Updated tuner status for variant ms-inference-scheduling-llm-d-modelservice-decode: alpha=7.441106, beta=0.038384, gamma=16.573370, delta=0.000247, NIS=0.124105"}
{"level":"INFO","ts":"2025-12-10T12:31:11.992Z","msg":"Tuned performance parameters result: variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler - alpha: 7.441106, beta: 0.038384, gamma: 16.573370, delta: 0.000247"}
{"level":"DEBUG","ts":"2025-12-10T12:31:12.001Z","msg":"Optimization solution - system: Solution: \ns=ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler; c=Premium; m=unsloth/Meta-Llama-3.1-8B; rate=437.78; inTk=227; outTk=468; sol=1, sat=false, alloc={acc=H100; numRep=1; maxBatch=512; cost=100, val=0, itl=8.610816, ttft=18.282045, rho=0.057565976, maxRPM=840.0115}; slo-itl=10, slo-ttft=1000, slo-tps=0 \nAllocationByType: \nname=NVIDIA-H100-80GB-HBM3, count=1, limit=0, cost=100 \ntotalCost=100 \n"}
{"level":"DEBUG","ts":"2025-12-10T12:31:12.001Z","msg":"Setting accelerator name: Name=H100, allocationData={H100 1 512 100 8.610816 18.282045 {437.78 227 468}}"}
{"level":"INFO","ts":"2025-12-10T12:31:12.001Z","msg":"Model-based optimization completed for model: unsloth/Meta-Llama-3.1-8B - model-based targets: map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"WARN","ts":"2025-12-10T12:31:12.001Z","msg":"saturation analysis unavailable, using model-based targets only: modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-10T12:31:12.001Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-12-10T12:31:12.001Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1→target=1"}
{"level":"INFO","ts":"2025-12-10T12:31:12.001Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 1, desired-replicas: 1, accelerator: H100"}
{"level":"INFO","ts":"2025-12-10T12:31:12.001Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=1, accelerator=H100, SaturationOnly=false"}
{"level":"INFO","ts":"2025-12-10T12:31:12.008Z","msg":"Applied Saturation decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1, target=1, reason=model-based only (Saturation unavailable)"}
{"level":"INFO","ts":"2025-12-10T12:31:12.008Z","msg":"Reconciliation completed successfully: mode=model-only, modelsProcessed=1, decisionsApplied=1"}
{"level":"INFO","ts":"2025-12-10T12:32:12.009Z","msg":"Reconciling VariantAutoscaling: name=ms-inference-scheduling-llm-d-modelservice-decode, namespace=llm-d-inference-scheduler, modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-10T12:32:12.009Z","msg":"saturation scaling config cache updated: entries=1, has_default=true"}
{"level":"INFO","ts":"2025-12-10T12:32:12.009Z","msg":"saturation scaling configuration loaded successfully"}
{"level":"INFO","ts":"2025-12-10T12:32:12.009Z","msg":"Operating in MODEL-ONLY mode: model-based optimization only"}
{"level":"INFO","ts":"2025-12-10T12:32:12.009Z","msg":"Grouped VAs by model: modelCount=1, totalVAs=1"}
{"level":"INFO","ts":"2025-12-10T12:32:12.009Z","msg":"Processing model: modelID=unsloth/Meta-Llama-3.1-8B, variantCount=1"}
{"level":"INFO","ts":"2025-12-10T12:32:12.024Z","msg":"Queue metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dgftqv, queueLength=0"}
{"level":"DEBUG","ts":"2025-12-10T12:32:12.024Z","msg":"Queue metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"INFO","ts":"2025-12-10T12:32:12.026Z","msg":"KV cache metric: pod=ms-inference-scheduling-llm-d-modelservice-decode-7b86984dgftqv, usage=0.000 (0.0%)"}
{"level":"DEBUG","ts":"2025-12-10T12:32:12.026Z","msg":"KV cache metrics collected (max over 1m): modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, podCount=1"}
{"level":"DEBUG","ts":"2025-12-10T12:32:12.028Z","msg":"Pod-to-variant matching successful: totalPods=1, variantCounts=map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"DEBUG","ts":"2025-12-10T12:32:12.028Z","msg":"Collected replica metrics: modelID=unsloth/Meta-Llama-3.1-8B, namespace=llm-d-inference-scheduler, replicaCount=1"}
{"level":"DEBUG","ts":"2025-12-10T12:32:12.028Z","msg":"Collected capacity metrics for observability in model-only mode: modelID=unsloth/Meta-Llama-3.1-8B, replicaCount=1"}
{"level":"DEBUG","ts":"2025-12-10T12:32:12.041Z","msg":"Metrics collected for VA: variant=ms-inference-scheduling-llm-d-modelservice-decode, replicas=1, accelerator=H100, ttft=0.00ms, itl=0.00ms, cost=100.00, maxBatch=256, arrivalRate=0.00, avgInputTokens=0.00, avgOutputTokens=0.00"}
{"level":"DEBUG","ts":"2025-12-10T12:32:12.041Z","msg":"Experimental model tuner is enabled globally: tuning model performance parameters for active VAs"}
{"level":"DEBUG","ts":"2025-12-10T12:32:12.041Z","msg":"Using state vals from VA status to tune variant ms-inference-scheduling-llm-d-modelservice-decode: alpha= 7.441106, beta= 0.038384, gamma= 16.573370, delta= 0.000247"}
{"level":"DEBUG","ts":"2025-12-10T12:32:12.041Z","msg":"Skipping tuning for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler due to insufficient load. Using fallback parameters."}
{"level":"INFO","ts":"2025-12-10T12:32:12.041Z","msg":"Using tuned parameters from status for variant ms-inference-scheduling-llm-d-modelservice-decode/llm-d-inference-scheduler"}
{"level":"DEBUG","ts":"2025-12-10T12:32:12.041Z","msg":"Updated SystemData for model=unsloth/Meta-Llama-3.1-8B, accelerator=H100: alpha=7.441106, beta=0.038384, gamma=16.573370, delta=0.000247"}
{"level":"DEBUG","ts":"2025-12-10T12:32:12.041Z","msg":"Optimization solution - system: Solution: \ns=ms-inference-scheduling-llm-d-modelservice-decode:llm-d-inference-scheduler; c=Premium; m=unsloth/Meta-Llama-3.1-8B; rate=0; inTk=0; outTk=0; sol=1, sat=false, alloc={acc=H100; numRep=1; maxBatch=512; cost=100, val=0, itl=7.47949, ttft=16.573616, rho=0, maxRPM=703500.7}; slo-itl=10, slo-ttft=1000, slo-tps=0 \nAllocationByType: \nname=NVIDIA-H100-80GB-HBM3, count=1, limit=0, cost=100 \ntotalCost=100 \n"}
{"level":"DEBUG","ts":"2025-12-10T12:32:12.041Z","msg":"Setting accelerator name: Name=H100, allocationData={H100 1 512 100 7.47949 16.573616 {0 0 0}}"}
{"level":"INFO","ts":"2025-12-10T12:32:12.041Z","msg":"Model-based optimization completed for model: unsloth/Meta-Llama-3.1-8B - model-based targets: map[ms-inference-scheduling-llm-d-modelservice-decode:1]"}
{"level":"WARN","ts":"2025-12-10T12:32:12.041Z","msg":"saturation analysis unavailable, using model-based targets only: modelID=unsloth/Meta-Llama-3.1-8B"}
{"level":"INFO","ts":"2025-12-10T12:32:12.041Z","msg":"Applying scaling decisions: totalDecisions=1"}
{"level":"INFO","ts":"2025-12-10T12:32:12.041Z","msg":"Processing decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1→target=1"}
{"level":"INFO","ts":"2025-12-10T12:32:12.041Z","msg":"EmitReplicaMetrics completed - variant: ms-inference-scheduling-llm-d-modelservice-decode, current-replicas: 1, desired-replicas: 1, accelerator: H100"}
{"level":"INFO","ts":"2025-12-10T12:32:12.041Z","msg":"Successfully emitted metrics for external autoscalers: variant=ms-inference-scheduling-llm-d-modelservice-decode, targetReplicas=1, accelerator=H100, SaturationOnly=false"}
{"level":"INFO","ts":"2025-12-10T12:32:12.047Z","msg":"Applied Saturation decision: variant=ms-inference-scheduling-llm-d-modelservice-decode, action=no-change, current=1, target=1, reason=model-based only (Saturation unavailable)"}
{"level":"INFO","ts":"2025-12-10T12:32:12.047Z","msg":"Reconciliation completed successfully: mode=model-only, modelsProcessed=1, decisionsApplied=1"}
