{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f0b9c67d",
   "metadata": {},
   "source": [
    "# Workload-Variant-Autoscaler (WVA) Performance Analysis\n",
    "\n",
    "This notebook analyzes WVA controller logs to visualize autoscaling behavior, warmup gaps, and SLO compliance.\n",
    "\n",
    "**Workflow:**\n",
    "1. Extract metrics from WVA controller logs using the bash script\n",
    "2. Load and process the CSV data\n",
    "3. Detect scaling phases and warmup gaps\n",
    "4. Visualize performance metrics and scaling behavior\n",
    "5. Perform statistical analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5285cb01",
   "metadata": {},
   "source": [
    "## 1. Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5d981cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib.patches import Rectangle\n",
    "from matplotlib.ticker import MultipleLocator\n",
    "import subprocess\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "# Plotting style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "%matplotlib inline\n",
    "\n",
    "# X-axis granularity configuration (in minutes)\n",
    "X_MAJOR_TICK_INTERVAL = 1.0  # Major tick every 1 minute\n",
    "X_MINOR_TICK_INTERVAL = 0.5  # Minor tick every 0.5 minutes (30 seconds)\n",
    "\n",
    "# Configuration\n",
    "LOG_DIR = f'./txt_logs'\n",
    "LOG_NAME = 'experiment_rls'\n",
    "# LOG_NAME = 'rls_blend_new'\n",
    "LOG_FILE = f'{LOG_DIR}/{LOG_NAME}.txt'  # Change this to your log file\n",
    "EXTRACT_SCRIPT = './extract_metrics.sh'\n",
    "\n",
    "# Generate unique experiment directory based on timestamp\n",
    "EXPERIMENT_NAME = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "EXPERIMENT_DIR = f'experiments/{EXPERIMENT_NAME}_{LOG_NAME}'\n",
    "OUTPUT_CSV = f'{EXPERIMENT_DIR}/extracted_metrics.csv'\n",
    "\n",
    "# Create experiment directory structure\n",
    "os.makedirs(f'{EXPERIMENT_DIR}/plots', exist_ok=True)\n",
    "os.makedirs(f'{EXPERIMENT_DIR}/data', exist_ok=True)\n",
    "os.makedirs(f'{EXPERIMENT_DIR}/analysis', exist_ok=True)\n",
    "\n",
    "print(f\"‚úÖ Configuration loaded\")\n",
    "print(f\"   Experiment: {EXPERIMENT_NAME}\")\n",
    "print(f\"   Experiment directory: {EXPERIMENT_DIR}\")\n",
    "print(f\"   Log file: {LOG_FILE}\")\n",
    "print(f\"   Extract script: {EXTRACT_SCRIPT}\")\n",
    "print(f\"   Output CSV: {OUTPUT_CSV}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc0333a0",
   "metadata": {},
   "source": [
    "**üìä X-Axis Granularity Settings:**\n",
    "- **Major ticks** (bold lines with labels): Every **0.5 minutes** (30 seconds)\n",
    "- **Minor ticks** (light grid lines): Every **0.1 minutes** (6 seconds)\n",
    "\n",
    "üí° *To adjust granularity, change `X_MAJOR_TICK_INTERVAL` and `X_MINOR_TICK_INTERVAL` above:*\n",
    "- For **coarser** view: Use 1.0 (major) and 0.5 (minor) for 1-minute intervals\n",
    "- For **finer** view: Use 0.25 (major) and 0.05 (minor) for 15-second intervals\n",
    "- For **very fine** view: Use 0.1 (major) and 0.02 (minor) for 6-second intervals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90e4300f",
   "metadata": {},
   "source": [
    "## 2. Extract Metrics from Logs\n",
    "\n",
    "Run the bash script to parse the WVA controller logs and extract optimization metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2912cb61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the script executable\n",
    "!chmod +x {EXTRACT_SCRIPT}\n",
    "\n",
    "# Run the extraction script\n",
    "print(f\"üìä Extracting metrics from {LOG_FILE}...\")\n",
    "result = subprocess.run(\n",
    "    [EXTRACT_SCRIPT, LOG_FILE],\n",
    "    capture_output=True,\n",
    "    text=True\n",
    ")\n",
    "\n",
    "# Save to CSV\n",
    "with open(OUTPUT_CSV, 'w') as f:\n",
    "    f.write(result.stdout)\n",
    "\n",
    "print(f\"‚úÖ Metrics extracted to {OUTPUT_CSV}\")\n",
    "print(f\"   Lines extracted: {len(result.stdout.splitlines())}\")\n",
    "\n",
    "# Preview the first few lines\n",
    "print(\"\\nüìã Preview of extracted data:\")\n",
    "!head -5 {OUTPUT_CSV}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de3a76ea",
   "metadata": {},
   "source": [
    "## 3. Load and Process Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ff9dc14",
   "metadata": {},
   "source": [
    "### 3.1 Diagnostics (Run if you get errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a8f818",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üîç Diagnosing extraction issue...\\n\")\n",
    "\n",
    "# 1. Check log file\n",
    "print(f\"1. Log file check:\")\n",
    "log_path = LOG_FILE\n",
    "BASH_SCRIPT = EXTRACT_SCRIPT\n",
    "if os.path.exists(log_path):\n",
    "    size = os.path.getsize(log_path)\n",
    "    print(f\"   ‚úì File exists: {log_path}\")\n",
    "    print(f\"   ‚úì Size: {size:,} bytes\")\n",
    "    \n",
    "    # Sample first few lines\n",
    "    print(f\"\\n   First 5 lines of log file:\")\n",
    "    with open(log_path, 'r') as f:\n",
    "        for i, line in enumerate(f):\n",
    "            if i >= 5:\n",
    "                break\n",
    "            print(f\"   {i+1}: {line[:100]}...\")\n",
    "else:\n",
    "    print(f\"   ‚úó File not found: {log_path}\")\n",
    "\n",
    "# 2. Check CSV output\n",
    "print(f\"\\n2. CSV output check:\")\n",
    "if os.path.exists(OUTPUT_CSV):\n",
    "    size = os.path.getsize(OUTPUT_CSV)\n",
    "    print(f\"   ‚úì File exists: {OUTPUT_CSV}\")\n",
    "    print(f\"   ‚úì Size: {size:,} bytes\")\n",
    "    \n",
    "    if size > 0:\n",
    "        # Read CSV and show structure\n",
    "        df_test = pd.read_csv(OUTPUT_CSV)\n",
    "        print(f\"   ‚úì Rows: {len(df_test)}\")\n",
    "        print(f\"   ‚úì Columns: {list(df_test.columns)}\")\n",
    "        \n",
    "        if len(df_test) > 0:\n",
    "            print(f\"\\n   First row:\")\n",
    "            print(df_test.head(1).to_string())\n",
    "        else:\n",
    "            print(f\"   ‚úó CSV has no data rows (only headers)\")\n",
    "    else:\n",
    "        print(f\"   ‚úó CSV is empty (0 bytes)\")\n",
    "else:\n",
    "    print(f\"   ‚úó File not found: {OUTPUT_CSV}\")\n",
    "\n",
    "# 3. Check for JSON patterns in log file\n",
    "print(f\"\\n3. Log format check:\")\n",
    "if os.path.exists(log_path):\n",
    "    with open(log_path, 'r') as f:\n",
    "        content = f.read()\n",
    "        \n",
    "    # Look for expected patterns\n",
    "    has_optimization = 'optimizationMetrics' in content\n",
    "    has_itl = 'itlAverage' in content or '\"itl\"' in content\n",
    "    has_ttft = 'ttftAverage' in content or '\"ttft\"' in content\n",
    "    \n",
    "    print(f\"   optimizationMetrics found: {'‚úì' if has_optimization else '‚úó'}\")\n",
    "    print(f\"   ITL metrics found: {'‚úì' if has_itl else '‚úó'}\")\n",
    "    print(f\"   TTFT metrics found: {'‚úì' if has_ttft else '‚úó'}\")\n",
    "    \n",
    "    if not (has_optimization or has_itl):\n",
    "        print(f\"\\n   ‚ö†Ô∏è  Log file doesn't appear to contain expected metrics.\")\n",
    "        print(f\"   Expected format: JSON with 'optimizationMetrics' field\")\n",
    "        print(f\"\\n   Sample expected format:\")\n",
    "        print(f'   {{\"optimizationMetrics\":{{\"itlAverage\":12.5,\"ttftAverage\":850,\"rate\":45.2,...}}}}')\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"If you see issues above, check:\")\n",
    "print(\"  1. Is LOG_FILE pointing to the correct WVA controller log file?\")\n",
    "print(\"  2. Does the log contain optimizationMetrics JSON entries?\")\n",
    "print(\"  3. Try running the extraction manually:\")\n",
    "print(f\"     {BASH_SCRIPT} {LOG_FILE}\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "776dcb40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV data\n",
    "df = pd.read_csv(OUTPUT_CSV)\n",
    "\n",
    "# Check if data was extracted\n",
    "if len(df) == 0:\n",
    "    print(\"‚ùå ERROR: No data was extracted from the log file!\")\n",
    "    print(\"\\nPossible issues:\")\n",
    "    print(\"1. Log file format doesn't match the extraction script\")\n",
    "    print(\"2. Log file doesn't contain optimization data\")\n",
    "    print(\"3. Log file path is incorrect\")\n",
    "    print(f\"\\nLog file: {LOG_FILE}\")\n",
    "    print(f\"CSV file: {OUTPUT_CSV}\")\n",
    "    print(\"\\nPlease check:\")\n",
    "    print(\"- Does the log file exist and have content?\")\n",
    "    print(\"- Does it contain lines with 'System data prepared for optimization'?\")\n",
    "    print(\"- Is the extraction script working correctly?\")\n",
    "    print(\"\\nRun this to check the log file:\")\n",
    "    print(f\"  !grep -i 'optimization' {LOG_FILE} | head -5\")\n",
    "    raise ValueError(\"No data extracted - please check log file and extraction script\")\n",
    "\n",
    "# Convert timestamp to datetime\n",
    "df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "\n",
    "# Calculate time in minutes from start\n",
    "df['time_minutes'] = (df['timestamp'] - df['timestamp'].min()).dt.total_seconds() / 60\n",
    "\n",
    "# Display basic statistics\n",
    "print(\"üìä Data Summary:\")\n",
    "print(f\"   Duration: {df['time_minutes'].max():.1f} minutes\")\n",
    "print(f\"   Data points: {len(df)}\")\n",
    "print(f\"   Replica range: {df['numRep'].min():.0f} ‚Üí {df['numRep'].max():.0f}\")\n",
    "print(f\"   Load range: {df['rate'].min():.1f} ‚Üí {df['rate'].max():.1f} rpm\")\n",
    "print(f\"   SLO ITL: {df['slo_itl'].iloc[0]:.0f} ms\")\n",
    "print(f\"   SLO TTFT: {df['slo_ttft'].iloc[0]:.0f} ms\")\n",
    "\n",
    "# Display first few rows\n",
    "print(\"\\nüìã Data preview:\")\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdbea744",
   "metadata": {},
   "source": [
    "## 4. Detect Scaling Events and Warmup Gaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77c9b0af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_scaling_events(df, slo_threshold=None):\n",
    "    \"\"\"\n",
    "    Detect scaling events and warmup gaps where actual TPOT exceeds SLO.\n",
    "    \"\"\"\n",
    "    if slo_threshold is None:\n",
    "        slo_threshold = df['slo_itl'].iloc[0]\n",
    "    \n",
    "    scaling_events = []\n",
    "    warmup_gaps = []\n",
    "    \n",
    "    # Detect replica changes\n",
    "    for i in range(1, len(df)):\n",
    "        if df['numRep'].iloc[i] != df['numRep'].iloc[i-1]:\n",
    "            scaling_events.append({\n",
    "                'time': df['time_minutes'].iloc[i],\n",
    "                'from_replicas': df['numRep'].iloc[i-1],\n",
    "                'to_replicas': df['numRep'].iloc[i],\n",
    "                'load': df['rate'].iloc[i]\n",
    "            })\n",
    "    \n",
    "    # Detect warmup gaps (TPOT > SLO)\n",
    "    in_gap = False\n",
    "    gap_start = None\n",
    "    \n",
    "    for i in range(len(df)):\n",
    "        tpot = df['itlAverage'].iloc[i]\n",
    "        \n",
    "        if tpot > slo_threshold and not in_gap:\n",
    "            # Start of warmup gap\n",
    "            in_gap = True\n",
    "            gap_start = i\n",
    "        elif tpot <= slo_threshold and in_gap:\n",
    "            # End of warmup gap\n",
    "            in_gap = False\n",
    "            warmup_gaps.append({\n",
    "                'start_time': df['time_minutes'].iloc[gap_start],\n",
    "                'end_time': df['time_minutes'].iloc[i-1],\n",
    "                'duration': df['time_minutes'].iloc[i-1] - df['time_minutes'].iloc[gap_start],\n",
    "                'peak_violation': df['itlAverage'].iloc[gap_start:i].max(),\n",
    "                'avg_tpot': df['itlAverage'].iloc[gap_start:i].mean(),\n",
    "                'avg_load': df['rate'].iloc[gap_start:i].mean()\n",
    "            })\n",
    "    \n",
    "    return scaling_events, warmup_gaps\n",
    "\n",
    "# Detect events\n",
    "scaling_events, warmup_gaps = detect_scaling_events(df)\n",
    "\n",
    "print(\"üîÑ Scaling Events Detected:\")\n",
    "for i, event in enumerate(scaling_events, 1):\n",
    "    print(f\"   {i}. t={event['time']:.1f}min: {event['from_replicas']:.0f} ‚Üí {event['to_replicas']:.0f} replicas (load: {event['load']:.1f} rpm)\")\n",
    "\n",
    "print(\"\\n‚ö†Ô∏è  Warmup Gaps Detected (TPOT > SLO):\")\n",
    "for i, gap in enumerate(warmup_gaps, 1):\n",
    "    print(f\"   {i}. t={gap['start_time']:.1f}-{gap['end_time']:.1f}min ({gap['duration']:.1f}min)\")\n",
    "    print(f\"      Peak: {gap['peak_violation']:.2f}ms, Avg: {gap['avg_tpot']:.2f}ms, Load: {gap['avg_load']:.0f} rpm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdb2ff18",
   "metadata": {},
   "source": [
    "## 5. Visualization: ITL Performance with Warmup Gaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51783ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(20, 8))\n",
    "\n",
    "# Highlight warmup gaps\n",
    "for i, gap in enumerate(warmup_gaps):\n",
    "    color = ['red', 'orange', 'yellow'][i % 3]\n",
    "    rect = Rectangle(\n",
    "        (gap['start_time'], 0),\n",
    "        gap['end_time'] - gap['start_time'],\n",
    "        df['itlAverage'].max() * 1.2,\n",
    "        alpha=0.15,\n",
    "        color=color,\n",
    "        label=f\"Warmup Gap {i+1}\"\n",
    "    )\n",
    "    ax.add_patch(rect)\n",
    "\n",
    "# Plot ITL metrics\n",
    "ax.plot(df['time_minutes'], df['itlAverage'], 'o-', \n",
    "        linewidth=3, markersize=6, color='#dc2626', \n",
    "        label='Actual ITL', zorder=3)\n",
    "ax.plot(df['time_minutes'], df['itl'], 's--', \n",
    "        linewidth=2, markersize=4, color='#2563eb', \n",
    "        label='Predicted ITL', zorder=3)\n",
    "ax.axhline(y=df['slo_itl'].iloc[0], color='#ef4444', \n",
    "           linestyle=':', linewidth=2, label='SLO Target', zorder=2)\n",
    "\n",
    "# Mark scaling events\n",
    "for event in scaling_events:\n",
    "    ax.axvline(x=event['time'], color='#9ca3af', \n",
    "               linestyle='--', alpha=0.7, linewidth=1, zorder=1)\n",
    "    ax.text(event['time'], df['itlAverage'].max() * 1.1, \n",
    "            f\"{event['from_replicas']:.0f}‚Üí{event['to_replicas']:.0f}\",\n",
    "            ha='center', va='bottom', fontsize=9, color='#6b7280',\n",
    "            bbox=dict(boxstyle='round,pad=0.3', facecolor='white', alpha=0.8))\n",
    "\n",
    "ax.set_xlabel('Time (minutes from start)', fontsize=12)\n",
    "ax.set_ylabel('Inter-Token Latency (ms)', fontsize=12)\n",
    "ax.set_title('WVA Performance: ITL vs Time with Warmup Gap Analysis', \n",
    "             fontweight='bold', fontsize=16)\n",
    "ax.legend(loc='upper left')\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.set_ylim(0, df['itlAverage'].max() * 1.2)\n",
    "\n",
    "# Set x-axis granularity\n",
    "ax.xaxis.set_major_locator(MultipleLocator(X_MAJOR_TICK_INTERVAL))\n",
    "ax.xaxis.set_minor_locator(MultipleLocator(X_MINOR_TICK_INTERVAL))\n",
    "ax.grid(True, which='major', alpha=0.3, linewidth=1)\n",
    "ax.grid(True, which='minor', alpha=0.15, linewidth=0.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plot_path = f'{EXPERIMENT_DIR}/plots/itl_analysis.png'\n",
    "plt.savefig(plot_path, dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"‚úÖ Plot saved as '{plot_path}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c66471f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(20, 8))\n",
    "\n",
    "# Highlight warmup gaps\n",
    "for i, gap in enumerate(warmup_gaps):\n",
    "    color = ['red', 'orange', 'yellow'][i % 3]\n",
    "    rect = Rectangle(\n",
    "        (gap['start_time'], 0),\n",
    "        gap['end_time'] - gap['start_time'],\n",
    "        df['ttftAverage'].max() * 1.2,\n",
    "        alpha=0.15,\n",
    "        color=color,\n",
    "        label=f\"Warmup Gap {i+1}\"\n",
    "    )\n",
    "    ax.add_patch(rect)\n",
    "\n",
    "# Plot TTFT metrics\n",
    "ax.plot(df['time_minutes'], df['ttftAverage'], 'o-', \n",
    "        linewidth=3, markersize=6, color='#059669', \n",
    "        label='Actual TTFT', zorder=3)\n",
    "ax.plot(df['time_minutes'], df['ttft'], 's--', \n",
    "        linewidth=2, markersize=4, color='#0891b2', \n",
    "        label='Predicted TTFT', zorder=3)\n",
    "ax.axhline(y=df['slo_ttft'].iloc[0], color='#10b981', \n",
    "           linestyle=':', linewidth=2, label='SLO Target', zorder=2)\n",
    "\n",
    "# Mark scaling events\n",
    "for event in scaling_events:\n",
    "    ax.axvline(x=event['time'], color='#9ca3af', \n",
    "               linestyle='--', alpha=0.7, linewidth=1, zorder=1)\n",
    "    ax.text(event['time'], df['ttftAverage'].max() * 1.1, \n",
    "            f\"{event['from_replicas']:.0f}‚Üí{event['to_replicas']:.0f}\",\n",
    "            ha='center', va='bottom', fontsize=9, color='#6b7280',\n",
    "            bbox=dict(boxstyle='round,pad=0.3', facecolor='white', alpha=0.8))\n",
    "\n",
    "ax.set_xlabel('Time (minutes from start)', fontsize=12)\n",
    "ax.set_ylabel('Time to First Token (ms)', fontsize=12)\n",
    "ax.set_title('WVA Performance: TTFT vs Time with Warmup Gap Analysis', \n",
    "             fontweight='bold', fontsize=16)\n",
    "ax.legend(loc='upper left')\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.set_ylim(0, df['ttftAverage'].max() * 1.2)\n",
    "\n",
    "# Set x-axis granularity\n",
    "ax.xaxis.set_major_locator(MultipleLocator(X_MAJOR_TICK_INTERVAL))\n",
    "ax.xaxis.set_minor_locator(MultipleLocator(X_MINOR_TICK_INTERVAL))\n",
    "ax.grid(True, which='major', alpha=0.3, linewidth=1)\n",
    "ax.grid(True, which='minor', alpha=0.15, linewidth=0.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plot_path = f'{EXPERIMENT_DIR}/plots/ttft_analysis.png'\n",
    "plt.savefig(plot_path, dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"‚úÖ Plot saved as '{plot_path}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4ba8b0d",
   "metadata": {},
   "source": [
    "## 6. Visualization: TTFT Performance with Warmup Gaps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3693a8bf",
   "metadata": {},
   "source": [
    "## 7. Visualization: Load Pattern Evolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0320800",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(20, 6))\n",
    "\n",
    "ax.plot(df['time_minutes'], df['rate'], 'o-', \n",
    "        linewidth=3, markersize=4, color='#7c3aed', \n",
    "        label='Arrival Rate (rpm)')\n",
    "\n",
    "# Mark scaling events\n",
    "for event in scaling_events:\n",
    "    ax.axvline(x=event['time'], color='#f59e0b', \n",
    "               linestyle='--', alpha=0.7, linewidth=1)\n",
    "    ax.text(event['time'], df['rate'].max() * 0.9,\n",
    "            f\"Scale\\n{event['from_replicas']:.0f}‚Üí{event['to_replicas']:.0f}\",\n",
    "            ha='center', va='top', fontsize=9, color='#f59e0b')\n",
    "\n",
    "ax.set_xlabel('Time (minutes)', fontsize=12)\n",
    "ax.set_ylabel('Requests per minute', fontsize=12)\n",
    "ax.set_title(f\"Load Pattern Evolution (Peak: {df['rate'].max():.0f} rpm)\",\n",
    "             fontweight='bold', fontsize=14)\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Set x-axis granularity\n",
    "ax.xaxis.set_major_locator(MultipleLocator(X_MAJOR_TICK_INTERVAL))\n",
    "ax.xaxis.set_minor_locator(MultipleLocator(X_MINOR_TICK_INTERVAL))\n",
    "ax.grid(True, which='major', alpha=0.3, linewidth=1)\n",
    "ax.grid(True, which='minor', alpha=0.15, linewidth=0.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plot_path = f'{EXPERIMENT_DIR}/plots/load_pattern.png'\n",
    "plt.savefig(plot_path, dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"‚úÖ Plot saved as '{plot_path}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d59bd326",
   "metadata": {},
   "source": [
    "## 8. Visualization: ITL vs Replica Scaling Timeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bf4052e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax1 = plt.subplots(figsize=(20, 8))\n",
    "ax2 = ax1.twinx()\n",
    "\n",
    "# ITL on left axis\n",
    "line1 = ax1.plot(df['time_minutes'], df['itlAverage'], 'o-', \n",
    "                 linewidth=3, color='#dc2626', label='Actual ITL', zorder=3)\n",
    "line2 = ax1.plot(df['time_minutes'], df['itl'], 's--', \n",
    "                 linewidth=2, color='#2563eb', label='Predicted ITL', zorder=3)\n",
    "ax1.axhline(y=df['slo_itl'].iloc[0], color='#ef4444', \n",
    "            linestyle=':', linewidth=2, label='SLO')\n",
    "\n",
    "# Replicas on right axis\n",
    "line3 = ax2.step(df['time_minutes'], df['numRep'], where='post',\n",
    "                 linewidth=4, color='#7c3aed', alpha=0.7, label='Replicas')\n",
    "\n",
    "# Highlight warmup gaps\n",
    "for gap in warmup_gaps:\n",
    "    ax1.axvspan(gap['start_time'], gap['end_time'], \n",
    "                alpha=0.1, color='red', zorder=1)\n",
    "\n",
    "ax1.set_xlabel('Time (minutes)', fontsize=12)\n",
    "ax1.set_ylabel('Inter-Token Latency (ms)', color='black', fontsize=12)\n",
    "ax2.set_ylabel('Number of Replicas', color='#7c3aed', fontsize=12)\n",
    "ax2.tick_params(axis='y', labelcolor='#7c3aed')\n",
    "\n",
    "# Combine legends\n",
    "lines1, labels1 = ax1.get_legend_handles_labels()\n",
    "lines2, labels2 = ax2.get_legend_handles_labels()\n",
    "ax1.legend(lines1 + lines2, labels1 + labels2, loc='upper left')\n",
    "\n",
    "ax1.set_title('ITL vs Replica Scaling Timeline', fontweight='bold', fontsize=16)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Set x-axis granularity\n",
    "ax1.xaxis.set_major_locator(MultipleLocator(X_MAJOR_TICK_INTERVAL))\n",
    "ax1.xaxis.set_minor_locator(MultipleLocator(X_MINOR_TICK_INTERVAL))\n",
    "ax1.grid(True, which='major', alpha=0.3, linewidth=1)\n",
    "ax1.grid(True, which='minor', alpha=0.15, linewidth=0.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plot_path = f'{EXPERIMENT_DIR}/plots/itl_replicas_timeline.png'\n",
    "plt.savefig(plot_path, dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"‚úÖ Plot saved as '{plot_path}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "722af0a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax1 = plt.subplots(figsize=(20, 8))\n",
    "ax2 = ax1.twinx()\n",
    "\n",
    "# TTFT on left axis\n",
    "line1 = ax1.plot(df['time_minutes'], df['ttftAverage'], 'o-', \n",
    "                 linewidth=3, color='#059669', label='Actual TTFT', zorder=3)\n",
    "line2 = ax1.plot(df['time_minutes'], df['ttft'], 's--', \n",
    "                 linewidth=2, color='#0891b2', label='Predicted TTFT', zorder=3)\n",
    "ax1.axhline(y=df['slo_ttft'].iloc[0], color='#10b981', \n",
    "            linestyle=':', linewidth=2, label='SLO')\n",
    "\n",
    "# Replicas on right axis\n",
    "line3 = ax2.step(df['time_minutes'], df['numRep'], where='post',\n",
    "                 linewidth=4, color='#7c3aed', alpha=0.7, label='Replicas')\n",
    "\n",
    "# Highlight warmup gaps\n",
    "for gap in warmup_gaps:\n",
    "    ax1.axvspan(gap['start_time'], gap['end_time'], \n",
    "                alpha=0.1, color='red', zorder=1)\n",
    "\n",
    "ax1.set_xlabel('Time (minutes)', fontsize=12)\n",
    "ax1.set_ylabel('Time to First Token (ms)', color='black', fontsize=12)\n",
    "ax2.set_ylabel('Number of Replicas', color='#7c3aed', fontsize=12)\n",
    "ax2.tick_params(axis='y', labelcolor='#7c3aed')\n",
    "\n",
    "# Combine legends\n",
    "lines1, labels1 = ax1.get_legend_handles_labels()\n",
    "lines2, labels2 = ax2.get_legend_handles_labels()\n",
    "ax1.legend(lines1 + lines2, labels1 + labels2, loc='upper left')\n",
    "\n",
    "ax1.set_title('TTFT vs Replica Scaling Timeline', fontweight='bold', fontsize=16)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Set x-axis granularity\n",
    "ax1.xaxis.set_major_locator(MultipleLocator(X_MAJOR_TICK_INTERVAL))\n",
    "ax1.xaxis.set_minor_locator(MultipleLocator(X_MINOR_TICK_INTERVAL))\n",
    "ax1.grid(True, which='major', alpha=0.3, linewidth=1)\n",
    "ax1.grid(True, which='minor', alpha=0.15, linewidth=0.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plot_path = f'{EXPERIMENT_DIR}/plots/ttft_replicas_timeline.png'\n",
    "plt.savefig(plot_path, dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"‚úÖ Plot saved as '{plot_path}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62eeb678",
   "metadata": {},
   "source": [
    "## 9. Visualization: TTFT vs Replica Scaling Timeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88368143",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(20, 12), sharex=True)\n",
    "\n",
    "# ITL subplot\n",
    "for i, gap in enumerate(warmup_gaps):\n",
    "    color = ['red', 'orange', 'yellow'][i % 3]\n",
    "    rect = Rectangle(\n",
    "        (gap['start_time'], 0),\n",
    "        gap['end_time'] - gap['start_time'],\n",
    "        df['itlAverage'].max() * 1.2,\n",
    "        alpha=0.15,\n",
    "        color=color,\n",
    "        label=f\"Warmup Gap {i+1}\" if i == 0 else \"\"\n",
    "    )\n",
    "    ax1.add_patch(rect)\n",
    "\n",
    "ax1.plot(df['time_minutes'], df['itlAverage'], 'o-', \n",
    "         linewidth=3, markersize=6, color='#dc2626', label='Actual ITL', zorder=3)\n",
    "ax1.plot(df['time_minutes'], df['itl'], 's--', \n",
    "         linewidth=2, markersize=4, color='#2563eb', label='Predicted ITL', zorder=3)\n",
    "ax1.axhline(y=df['slo_itl'].iloc[0], color='#ef4444', \n",
    "            linestyle=':', linewidth=2, label='SLO Target', zorder=2)\n",
    "\n",
    "for event in scaling_events:\n",
    "    ax1.axvline(x=event['time'], color='#9ca3af', linestyle='--', alpha=0.7, linewidth=1, zorder=1)\n",
    "    ax1.text(event['time'], df['itlAverage'].max() * 1.1, \n",
    "             f\"{event['from_replicas']:.0f}‚Üí{event['to_replicas']:.0f}\",\n",
    "             ha='center', va='bottom', fontsize=9, color='#6b7280',\n",
    "             bbox=dict(boxstyle='round,pad=0.3', facecolor='white', alpha=0.8))\n",
    "\n",
    "ax1.set_ylabel('Inter-Token Latency (ms)', fontsize=12)\n",
    "ax1.set_title('Combined ITL and TTFT Performance Analysis', fontweight='bold', fontsize=16)\n",
    "ax1.legend(loc='upper left')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.set_ylim(0, df['itlAverage'].max() * 1.2)\n",
    "\n",
    "# TTFT subplot\n",
    "for i, gap in enumerate(warmup_gaps):\n",
    "    color = ['red', 'orange', 'yellow'][i % 3]\n",
    "    rect = Rectangle(\n",
    "        (gap['start_time'], 0),\n",
    "        gap['end_time'] - gap['start_time'],\n",
    "        df['ttftAverage'].max() * 1.2,\n",
    "        alpha=0.15,\n",
    "        color=color\n",
    "    )\n",
    "    ax2.add_patch(rect)\n",
    "\n",
    "ax2.plot(df['time_minutes'], df['ttftAverage'], 'o-', \n",
    "         linewidth=3, markersize=6, color='#059669', label='Actual TTFT', zorder=3)\n",
    "ax2.plot(df['time_minutes'], df['ttft'], 's--', \n",
    "         linewidth=2, markersize=4, color='#0891b2', label='Predicted TTFT', zorder=3)\n",
    "ax2.axhline(y=df['slo_ttft'].iloc[0], color='#10b981', \n",
    "            linestyle=':', linewidth=2, label='SLO Target', zorder=2)\n",
    "\n",
    "for event in scaling_events:\n",
    "    ax2.axvline(x=event['time'], color='#9ca3af', linestyle='--', alpha=0.7, linewidth=1, zorder=1)\n",
    "\n",
    "ax2.set_xlabel('Time (minutes from start)', fontsize=12)\n",
    "ax2.set_ylabel('Time to First Token (ms)', fontsize=12)\n",
    "ax2.legend(loc='upper left')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.set_ylim(0, df['ttftAverage'].max() * 1.2)\n",
    "\n",
    "# Set x-axis granularity (applies to both subplots since sharex=True)\n",
    "ax2.xaxis.set_major_locator(MultipleLocator(X_MAJOR_TICK_INTERVAL))\n",
    "ax2.xaxis.set_minor_locator(MultipleLocator(X_MINOR_TICK_INTERVAL))\n",
    "ax1.grid(True, which='major', alpha=0.3, linewidth=1)\n",
    "ax1.grid(True, which='minor', alpha=0.15, linewidth=0.5)\n",
    "ax2.grid(True, which='major', alpha=0.3, linewidth=1)\n",
    "ax2.grid(True, which='minor', alpha=0.15, linewidth=0.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plot_path = f'{EXPERIMENT_DIR}/plots/combined_itl_ttft.png'\n",
    "plt.savefig(plot_path, dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"‚úÖ Plot saved as '{plot_path}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ccbcbaf",
   "metadata": {},
   "source": [
    "## 10. Visualization: Combined ITL & TTFT Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4abd8e57",
   "metadata": {},
   "source": [
    "## 11. Statistical Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53a2244c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"WVA PERFORMANCE ANALYSIS SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nüìä Experiment Overview:\")\n",
    "print(f\"   Experiment ID: {EXPERIMENT_NAME}\")\n",
    "print(f\"   Duration: {df['time_minutes'].max():.1f} minutes\")\n",
    "print(f\"   Data points: {len(df)}\")\n",
    "print(f\"   Scaling pattern: {df['numRep'].min():.0f} ‚Üí {df['numRep'].max():.0f} replicas\")\n",
    "print(f\"   Peak load: {df['rate'].max():.0f} rpm\")\n",
    "print(f\"   SLO ITL: {df['slo_itl'].iloc[0]:.0f} ms\")\n",
    "print(f\"   SLO TTFT: {df['slo_ttft'].iloc[0]:.0f} ms\")\n",
    "\n",
    "print(f\"\\n‚ö†Ô∏è  Warmup Gap Analysis:\")\n",
    "for i, gap in enumerate(warmup_gaps, 1):\n",
    "    print(f\"\\n   Gap {i} (t={gap['start_time']:.1f}-{gap['end_time']:.1f}min):\")\n",
    "    print(f\"      Duration: {gap['duration']:.1f} minutes\")\n",
    "    print(f\"      Peak ITL violation: {gap['peak_violation']:.2f} ms\")\n",
    "    print(f\"      Average ITL: {gap['avg_tpot']:.2f} ms\")\n",
    "    print(f\"      Average load: {gap['avg_load']:.0f} rpm\")\n",
    "    print(f\"      SLO exceedance: {gap['avg_tpot'] - df['slo_itl'].iloc[0]:.2f} ms\")\n",
    "\n",
    "print(f\"\\nüìà ITL Performance Metrics:\")\n",
    "print(f\"   Mean ITL: {df['itlAverage'].mean():.2f} ms\")\n",
    "print(f\"   Median ITL: {df['itlAverage'].median():.2f} ms\")\n",
    "print(f\"   Peak ITL: {df['itlAverage'].max():.2f} ms\")\n",
    "print(f\"   Min ITL: {df['itlAverage'].min():.2f} ms\")\n",
    "print(f\"   Std Dev: {df['itlAverage'].std():.2f} ms\")\n",
    "\n",
    "print(f\"\\nüìà TTFT Performance Metrics:\")\n",
    "print(f\"   Mean TTFT: {df['ttftAverage'].mean():.2f} ms\")\n",
    "print(f\"   Median TTFT: {df['ttftAverage'].median():.2f} ms\")\n",
    "print(f\"   Peak TTFT: {df['ttftAverage'].max():.2f} ms\")\n",
    "print(f\"   Min TTFT: {df['ttftAverage'].min():.2f} ms\")\n",
    "print(f\"   Std Dev: {df['ttftAverage'].std():.2f} ms\")\n",
    "\n",
    "# SLO compliance\n",
    "itl_violations = df[df['itlAverage'] > df['slo_itl']]\n",
    "ttft_violations = df[df['ttftAverage'] > df['slo_ttft']]\n",
    "itl_compliance_rate = (1 - len(itl_violations) / len(df)) * 100\n",
    "ttft_compliance_rate = (1 - len(ttft_violations) / len(df)) * 100\n",
    "\n",
    "print(f\"\\n‚úÖ SLO Compliance:\")\n",
    "print(f\"   ITL Compliance: {itl_compliance_rate:.1f}%\")\n",
    "print(f\"   ITL Violations: {len(itl_violations)} / {len(df)} data points\")\n",
    "print(f\"   TTFT Compliance: {ttft_compliance_rate:.1f}%\")\n",
    "print(f\"   TTFT Violations: {len(ttft_violations)} / {len(df)} data points\")\n",
    "\n",
    "print(f\"\\nüîÑ Scaling Events:\")\n",
    "for i, event in enumerate(scaling_events, 1):\n",
    "    print(f\"   {i}. t={event['time']:.1f}min: {event['from_replicas']:.0f} ‚Üí {event['to_replicas']:.0f} replicas (load: {event['load']:.1f} rpm)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "\n",
    "# Save summary to file\n",
    "summary_path = f'{EXPERIMENT_DIR}/analysis/summary.txt'\n",
    "with open(summary_path, 'w') as f:\n",
    "    f.write(\"=\"*60 + \"\\n\")\n",
    "    f.write(\"WVA PERFORMANCE ANALYSIS SUMMARY\\n\")\n",
    "    f.write(\"=\"*60 + \"\\n\")\n",
    "    f.write(f\"\\nExperiment ID: {EXPERIMENT_NAME}\\n\")\n",
    "    f.write(f\"Duration: {df['time_minutes'].max():.1f} minutes\\n\")\n",
    "    f.write(f\"ITL Compliance: {itl_compliance_rate:.1f}%\\n\")\n",
    "    f.write(f\"TTFT Compliance: {ttft_compliance_rate:.1f}%\\n\")\n",
    "    f.write(f\"Peak Load: {df['rate'].max():.0f} rpm\\n\")\n",
    "    f.write(f\"Scaling Pattern: {df['numRep'].min():.0f} ‚Üí {df['numRep'].max():.0f} replicas\\n\")\n",
    "\n",
    "print(f\"\\n‚úÖ Summary saved to '{summary_path}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4889516",
   "metadata": {},
   "source": [
    "## 12. Export Results\n",
    "\n",
    "Export the processed data and analysis results for further use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90c1ff26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export processed data with phases\n",
    "processed_csv = f'{EXPERIMENT_DIR}/data/processed_metrics.csv'\n",
    "df.to_csv(processed_csv, index=False)\n",
    "print(f\"‚úÖ Processed data exported to '{processed_csv}'\")\n",
    "\n",
    "# Export warmup gaps analysis\n",
    "if warmup_gaps:\n",
    "    gaps_df = pd.DataFrame(warmup_gaps)\n",
    "    gaps_csv = f'{EXPERIMENT_DIR}/analysis/warmup_gaps.csv'\n",
    "    gaps_df.to_csv(gaps_csv, index=False)\n",
    "    print(f\"‚úÖ Warmup gaps analysis exported to '{gaps_csv}'\")\n",
    "\n",
    "# Export scaling events\n",
    "if scaling_events:\n",
    "    events_df = pd.DataFrame(scaling_events)\n",
    "    events_csv = f'{EXPERIMENT_DIR}/analysis/scaling_events.csv'\n",
    "    events_df.to_csv(events_csv, index=False)\n",
    "    print(f\"‚úÖ Scaling events exported to '{events_csv}'\")\n",
    "\n",
    "# Copy original log file to experiment directory\n",
    "import shutil\n",
    "log_copy = f'{EXPERIMENT_DIR}/data/original_log.txt'\n",
    "shutil.copy(LOG_FILE, log_copy)\n",
    "print(f\"‚úÖ Original log file copied to '{log_copy}'\")\n",
    "\n",
    "# Create experiment manifest\n",
    "manifest_path = f'{EXPERIMENT_DIR}/manifest.txt'\n",
    "with open(manifest_path, 'w') as f:\n",
    "    f.write(f\"Experiment ID: {EXPERIMENT_NAME}\\n\")\n",
    "    f.write(f\"Created: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "    f.write(f\"Log File: {LOG_FILE}\\n\")\n",
    "    f.write(f\"Duration: {df['time_minutes'].max():.1f} minutes\\n\")\n",
    "    f.write(f\"Data Points: {len(df)}\\n\")\n",
    "    f.write(f\"\\nDirectory Structure:\\n\")\n",
    "    f.write(f\"  - data/: Raw and processed data files\\n\")\n",
    "    f.write(f\"  - plots/: Generated visualization plots\\n\")\n",
    "    f.write(f\"  - analysis/: Statistical analysis results\\n\")\n",
    "    \n",
    "print(f\"‚úÖ Manifest created at '{manifest_path}'\")\n",
    "print(f\"\\nüìÅ Experiment directory: {EXPERIMENT_DIR}\")\n",
    "print(f\"   All outputs have been organized in subdirectories\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf8c04dd",
   "metadata": {},
   "source": [
    "## 13. Custom Analysis (Optional)\n",
    "\n",
    "Use this cell for custom queries and exploration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2be90cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Find periods where predicted ITL underestimated actual ITL\n",
    "itl_underestimation = df[df['itlAverage'] > df['itl']]\n",
    "print(\"ITL Controller Underestimation Analysis:\")\n",
    "print(f\"Occurred in {len(itl_underestimation)} / {len(df)} samples ({len(itl_underestimation)/len(df)*100:.1f}%)\")\n",
    "print(f\"Average underestimation: {(itl_underestimation['itlAverage'] - itl_underestimation['itl']).mean():.2f} ms\")\n",
    "\n",
    "# Show samples with largest ITL underestimation\n",
    "print(\"\\nTop 5 largest ITL underestimations:\")\n",
    "itl_underestimation['gap'] = itl_underestimation['itlAverage'] - itl_underestimation['itl']\n",
    "display(itl_underestimation.nlargest(5, 'gap')[['time_minutes', 'itlAverage', 'itl', 'gap', 'numRep', 'rate']])\n",
    "\n",
    "# Example: Find periods where predicted TTFT underestimated actual TTFT\n",
    "ttft_underestimation = df[df['ttftAverage'] > df['ttft']]\n",
    "print(\"\\nTTFT Controller Underestimation Analysis:\")\n",
    "print(f\"Occurred in {len(ttft_underestimation)} / {len(df)} samples ({len(ttft_underestimation)/len(df)*100:.1f}%)\")\n",
    "print(f\"Average underestimation: {(ttft_underestimation['ttftAverage'] - ttft_underestimation['ttft']).mean():.2f} ms\")\n",
    "\n",
    "# Show samples with largest TTFT underestimation\n",
    "print(\"\\nTop 5 largest TTFT underestimations:\")\n",
    "ttft_underestimation['gap'] = ttft_underestimation['ttftAverage'] - ttft_underestimation['ttft']\n",
    "display(ttft_underestimation.nlargest(5, 'gap')[['time_minutes', 'ttftAverage', 'ttft', 'gap', 'numRep', 'rate']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d59c1ec",
   "metadata": {},
   "source": [
    "## 14. Experiment Summary\n",
    "\n",
    "Review the complete experiment directory structure and outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94f31612",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(f\"EXPERIMENT {EXPERIMENT_NAME} - COMPLETE\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\\nüìÅ Experiment Directory: {EXPERIMENT_DIR}\")\n",
    "print(\"\\nDirectory Structure:\")\n",
    "\n",
    "# Walk through the experiment directory\n",
    "for root, dirs, files in os.walk(EXPERIMENT_DIR):\n",
    "    level = root.replace(EXPERIMENT_DIR, '').count(os.sep)\n",
    "    indent = ' ' * 2 * level\n",
    "    print(f'{indent}{os.path.basename(root)}/')\n",
    "    subindent = ' ' * 2 * (level + 1)\n",
    "    for file in files:\n",
    "        file_path = os.path.join(root, file)\n",
    "        file_size = os.path.getsize(file_path)\n",
    "        print(f'{subindent}üìÑ {file} ({file_size:,} bytes)')\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"Generated Outputs:\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\nüìä Plots (6 visualizations):\")\n",
    "print(\"   1. itl_analysis.png - ITL performance with warmup gaps\")\n",
    "print(\"   2. ttft_analysis.png - TTFT performance with warmup gaps\")\n",
    "print(\"   3. load_pattern.png - Arrival rate evolution\")\n",
    "print(\"   4. itl_replicas_timeline.png - ITL vs replica scaling\")\n",
    "print(\"   5. ttft_replicas_timeline.png - TTFT vs replica scaling\")\n",
    "print(\"   6. combined_itl_ttft.png - Combined ITL & TTFT analysis\")\n",
    "\n",
    "print(\"\\nüìà Data Files:\")\n",
    "print(\"   ‚Ä¢ extracted_metrics.csv - Raw extracted metrics\")\n",
    "print(\"   ‚Ä¢ processed_metrics.csv - Processed data with time calculations\")\n",
    "print(\"   ‚Ä¢ original_log.txt - Copy of original log file\")\n",
    "\n",
    "print(\"\\nüìã Analysis Files:\")\n",
    "print(\"   ‚Ä¢ warmup_gaps.csv - Detected warmup gap statistics\")\n",
    "print(\"   ‚Ä¢ scaling_events.csv - Scaling event log\")\n",
    "print(\"   ‚Ä¢ summary.txt - Text summary of key findings\")\n",
    "\n",
    "print(\"\\nüìù Experiment Metadata:\")\n",
    "print(\"   ‚Ä¢ manifest.txt - Experiment configuration and metadata\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(f\"‚úÖ All outputs saved to: {EXPERIMENT_DIR}\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3d1de2d",
   "metadata": {},
   "source": [
    "## 15. Calibration Analysis - Understanding the ITL Gap\n",
    "\n",
    "This section analyzes the gap between predicted and observed ITL to identify calibration opportunities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c8f2889",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"CALIBRATION ANALYSIS: ITL Prediction Gap\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Calculate prediction error\n",
    "df['itl_error'] = df['itlAverage'] - df['itl']\n",
    "df['itl_error_pct'] = (df['itl_error'] / df['itl']) * 100\n",
    "\n",
    "# Calculate effective batch size (approximation)\n",
    "# effectiveBatch ‚âà rate per replica / service rate\n",
    "# For simplicity, we'll use a proxy: rate/numRep\n",
    "df['rate_per_replica'] = df['rate'] / df['numRep']\n",
    "\n",
    "print(f\"\\nüìä Prediction Error Statistics:\")\n",
    "print(f\"   Mean Error: {df['itl_error'].mean():.2f} ms\")\n",
    "print(f\"   Median Error: {df['itl_error'].median():.2f} ms\")\n",
    "print(f\"   Std Dev Error: {df['itl_error'].std():.2f} ms\")\n",
    "print(f\"   Mean % Error: {df['itl_error_pct'].mean():.1f}%\")\n",
    "print(f\"   Max Underestimate: {df['itl_error'].max():.2f} ms\")\n",
    "print(f\"   Max Overestimate: {df['itl_error'].min():.2f} ms\")\n",
    "\n",
    "# Identify systematic bias\n",
    "overestimates = df[df['itl_error'] < 0]\n",
    "underestimates = df[df['itl_error'] > 0]\n",
    "\n",
    "print(f\"\\n‚öñÔ∏è  Bias Analysis:\")\n",
    "print(f\"   Overestimates: {len(overestimates)} / {len(df)} ({len(overestimates)/len(df)*100:.1f}%)\")\n",
    "print(f\"   Underestimates: {len(underestimates)} / {len(df)} ({len(underestimates)/len(df)*100:.1f}%)\")\n",
    "\n",
    "if len(underestimates) > len(overestimates):\n",
    "    print(f\"   ‚ö†Ô∏è  System tends to UNDERESTIMATE ITL (optimistic)\")\n",
    "else:\n",
    "    print(f\"   ‚ÑπÔ∏è  System tends to OVERESTIMATE ITL (conservative)\")\n",
    "\n",
    "# Correlation with load\n",
    "print(f\"\\nüîó Correlation Analysis:\")\n",
    "print(f\"   Error vs Rate: {df[['itl_error', 'rate']].corr().iloc[0,1]:.3f}\")\n",
    "print(f\"   Error vs Replicas: {df[['itl_error', 'numRep']].corr().iloc[0,1]:.3f}\")\n",
    "print(f\"   Error vs Rate/Replica: {df[['itl_error', 'rate_per_replica']].corr().iloc[0,1]:.3f}\")\n",
    "\n",
    "# Identify worst predictions\n",
    "print(f\"\\n‚ùå Top 5 Worst Predictions (Largest Underestimates):\")\n",
    "worst = df.nlargest(5, 'itl_error')[['time_minutes', 'itlAverage', 'itl', 'itl_error', \n",
    "                                       'rate', 'numRep', 'rate_per_replica']]\n",
    "display(worst)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üí° Calibration Opportunities:\")\n",
    "print(\"=\"*70)\n",
    "print(\"1. Linear Regression: Fit alpha/beta to observed ITL vs load\")\n",
    "print(\"2. Query Prometheus: Get last 1h of ITL observations\")\n",
    "print(\"3. Filter Outliers: Remove anomalous points (> 2.5 std dev)\")\n",
    "print(\"4. Update Parameters: Use calibrated alpha/beta for predictions\")\n",
    "print(f\"5. Expected Improvement: Reduce mean error from {df['itl_error'].mean():.2f}ms\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c174547",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize prediction error over time\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(20, 10), sharex=True)\n",
    "\n",
    "# Top: Predicted vs Actual ITL\n",
    "ax1.plot(df['time_minutes'], df['itlAverage'], 'o-', \n",
    "         linewidth=3, color='#dc2626', label='Actual ITL (observed)', zorder=3)\n",
    "ax1.plot(df['time_minutes'], df['itl'], 's--', \n",
    "         linewidth=2, color='#2563eb', label='Predicted ITL (model)', zorder=3)\n",
    "ax1.fill_between(df['time_minutes'], df['itl'], df['itlAverage'], \n",
    "                  where=(df['itlAverage'] > df['itl']), \n",
    "                  alpha=0.3, color='red', label='Underestimate')\n",
    "ax1.fill_between(df['time_minutes'], df['itl'], df['itlAverage'], \n",
    "                  where=(df['itlAverage'] <= df['itl']), \n",
    "                  alpha=0.3, color='green', label='Overestimate')\n",
    "ax1.set_ylabel('ITL (ms)', fontsize=12)\n",
    "ax1.set_title('ITL Prediction Gap Analysis', fontweight='bold', fontsize=16)\n",
    "ax1.legend(loc='upper left')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Bottom: Prediction Error\n",
    "ax2.plot(df['time_minutes'], df['itl_error'], 'o-', \n",
    "         linewidth=3, color='#7c3aed', label='Prediction Error')\n",
    "ax2.axhline(y=0, color='black', linestyle='-', linewidth=1, alpha=0.5)\n",
    "ax2.axhline(y=df['itl_error'].mean(), color='orange', linestyle='--', \n",
    "            linewidth=2, label=f'Mean Error: {df[\"itl_error\"].mean():.2f}ms')\n",
    "ax2.fill_between(df['time_minutes'], 0, df['itl_error'], \n",
    "                  where=(df['itl_error'] > 0), alpha=0.3, color='red')\n",
    "ax2.fill_between(df['time_minutes'], 0, df['itl_error'], \n",
    "                  where=(df['itl_error'] <= 0), alpha=0.3, color='green')\n",
    "ax2.set_xlabel('Time (minutes)', fontsize=12)\n",
    "ax2.set_ylabel('Error (Actual - Predicted) ms', fontsize=12)\n",
    "ax2.legend(loc='upper left')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Set x-axis granularity\n",
    "ax2.xaxis.set_major_locator(MultipleLocator(X_MAJOR_TICK_INTERVAL))\n",
    "ax2.xaxis.set_minor_locator(MultipleLocator(X_MINOR_TICK_INTERVAL))\n",
    "ax1.grid(True, which='major', alpha=0.3, linewidth=1)\n",
    "ax1.grid(True, which='minor', alpha=0.15, linewidth=0.5)\n",
    "ax2.grid(True, which='major', alpha=0.3, linewidth=1)\n",
    "ax2.grid(True, which='minor', alpha=0.15, linewidth=0.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plot_path = f'{EXPERIMENT_DIR}/plots/calibration_gap_analysis.png'\n",
    "plt.savefig(plot_path, dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"‚úÖ Plot saved as '{plot_path}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aaf169f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter plot: Error vs Load characteristics\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# Error vs Arrival Rate\n",
    "axes[0, 0].scatter(df['rate'], df['itl_error'], alpha=0.6, s=100, c='purple')\n",
    "axes[0, 0].axhline(y=0, color='black', linestyle='--', alpha=0.5)\n",
    "axes[0, 0].set_xlabel('Arrival Rate (rpm)', fontsize=11)\n",
    "axes[0, 0].set_ylabel('Prediction Error (ms)', fontsize=11)\n",
    "axes[0, 0].set_title('Error vs Arrival Rate', fontweight='bold', fontsize=13)\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Error vs Number of Replicas\n",
    "axes[0, 1].scatter(df['numRep'], df['itl_error'], alpha=0.6, s=100, c='teal')\n",
    "axes[0, 1].axhline(y=0, color='black', linestyle='--', alpha=0.5)\n",
    "axes[0, 1].set_xlabel('Number of Replicas', fontsize=11)\n",
    "axes[0, 1].set_ylabel('Prediction Error (ms)', fontsize=11)\n",
    "axes[0, 1].set_title('Error vs Replicas', fontweight='bold', fontsize=13)\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Error vs Rate per Replica (proxy for batch size)\n",
    "axes[1, 0].scatter(df['rate_per_replica'], df['itl_error'], alpha=0.6, s=100, c='orange')\n",
    "axes[1, 0].axhline(y=0, color='black', linestyle='--', alpha=0.5)\n",
    "axes[1, 0].set_xlabel('Rate per Replica (rpm)', fontsize=11)\n",
    "axes[1, 0].set_ylabel('Prediction Error (ms)', fontsize=11)\n",
    "axes[1, 0].set_title('Error vs Load per Replica', fontweight='bold', fontsize=13)\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Error distribution histogram\n",
    "axes[1, 1].hist(df['itl_error'], bins=20, color='steelblue', alpha=0.7, edgecolor='black')\n",
    "axes[1, 1].axvline(x=0, color='black', linestyle='--', linewidth=2)\n",
    "axes[1, 1].axvline(x=df['itl_error'].mean(), color='red', linestyle='--', \n",
    "                   linewidth=2, label=f'Mean: {df[\"itl_error\"].mean():.2f}ms')\n",
    "axes[1, 1].set_xlabel('Prediction Error (ms)', fontsize=11)\n",
    "axes[1, 1].set_ylabel('Frequency', fontsize=11)\n",
    "axes[1, 1].set_title('Error Distribution', fontweight='bold', fontsize=13)\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plot_path = f'{EXPERIMENT_DIR}/plots/calibration_correlation_analysis.png'\n",
    "plt.savefig(plot_path, dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"‚úÖ Plot saved as '{plot_path}'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
